\documentclass{article}%
\usepackage{amsmath}
\usepackage{amssymb}%
\setcounter{MaxMatrixCols}{30}%
\usepackage{amsfonts}%
\usepackage{graphicx}
%TCIDATA{OutputFilter=latex2.dll}
%TCIDATA{Version=5.50.0.2953}
%TCIDATA{CSTFile=LaTeX article (bright).cst}
%TCIDATA{Created=Thursday, March 08, 2007 06:44:18}
%TCIDATA{LastRevised=Friday, April 05, 2024 17:38:03}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=BibTeX}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Standard LaTeX\Blank - Standard LaTeX Article">}
%TCIDATA{Language=American English}
%TCIDATA{ComputeDefs=
%$\varepsilon$
%$F_{V(\mathbf{A})}(x_{1},...,x_{n})=\{t_{1},...,t_{k}\}$
%}
%BeginMSIPreambleData
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
%EndMSIPreambleData
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\begin{document}

\title{Computabilidad y logica}
\author{

}
\maketitle
\tableofcontents

\section{Notacion y conceptos basicos}

Usaremos $\mathbf{R}$ para denotar el conjunto de los numeros reales,
$\mathbf{Z}$ para denotar el conjunto de los numeros enteros, $\mathbf{N}$
para denotar el conjunto de los numeros naturales y $\omega$ para denotar al
conjunto $\mathbf{N}\cup\{0\}$.

Dado un conjunto $A$, usaremos $\mathcal{P}(A)$ para denotar el conjunto
formado por todos los subconjuntos de $A$, es decir:%
\[
\mathcal{P}(A)=\{S:S\subseteq A\}
\]
Si $A$ es un conjunto finito, entonces $\left\vert A\right\vert $ denotara la
cantidad de elementos de $A$.

Para $x,y\in\omega$, definamos%
\[
x\dot{-}y=\left\{
\begin{array}
[c]{lll}%
x-y &  & \text{si }x\geq y\\
0 &  & \text{caso contrario}%
\end{array}
\right.
\]
Dados $x,y\in\omega$ diremos que $x$ \textit{divide a }$y$ cuando haya un
$z\in\omega$ tal que $y=z.x$. Notar que $0$ divide a $0$, $3$ divide a $0$ y
$0$ no divide a $23$. Escribiremos $x\mid y$ para expresar que $x$ divide a
$y$. Dados $x,y\in\omega$, diremos que $x$ e $y$ son \textit{coprimos} cuando
$1$ sea el unico elemento de $\omega$ que divide a ambos. Notese que $x$ e $y$
no son son coprimos sii existe un numero primo $p\in\omega$ que los divide a ambos

Si bien no hay una definicion natural en matematica de cuanto vale $0^{0}$
($0$ elevado a la $0$), por convencion para nosotros $0^{0}=1$

\subsection{Conjuntos}

Supondremos que el lector sabe las nociones basicas sobre conjuntos, aunque
resaltaremos algunas de las mas importantes para que el lector las repase.

La propiedad de \textit{extensionalidad} nos dice que, dados conjuntos $A,B$,
se tiene que $A=B$ si y solo si para cada objeto $x$ se da que%
\[
x\in A\text{ si y solo si }x\in B
\]
Esta propiedad es importante metodologicamente ya que a la hora de probar que
dos conjuntos $A,B$ son iguales, extensionalidad nos asegura que basta con ver
que se dan las dos inclusiones $A\subseteq B$ y $B\subseteq A$.

Otro tema importante es manejar correctamente la notacion cuando definimos un
conjunto usando llaves y mediante propiedades que caracterizan la pertenencia
al mismo. Algunos ejemplos

\begin{enumerate}
\item[-] $\{x\in\mathbf{N}:x=1$ o $x\geq5\}$

\item[-] $\{x:x\in\mathbf{R}$ y $x^{2}\geq100\}$

\item[-] $\{x:x=100\}$

\item[-] $\{x^{2}+1:x\in\omega\}$

\item[-] $\{x+y+z:x,y,z\in\{1,2\}\}$
\end{enumerate}

Dejamos al lector la tarea de entender en forma precisa que conjunto se esta
denotando en cada caso.

\subsection{Producto carteciano}

Dados conjuntos $A_{1},...,A_{n}$, con $n\geq2$, usaremos $A_{1}%
\times...\times A_{n}$ para denotar el \textit{producto Cartesiano} de
$A_{1},...,A_{n}$, es decir el conjunto formado por todas las $n$-uplas
$(a_{1},...,a_{n})$ tales que $a_{1}\in A_{1},...,a_{n}\in A_{n}$. Si
$A_{1}=...=A_{n}=A$, con $n\geq2$, entonces escribiremos $A^{n}$ en lugar de
$A_{1}\times...\times A_{n}$. Para $n=1$, definimos $A^{n}=A$, es decir
$A^{1}=A$. Usaremos $\Diamond$ para denotar la unica $0$-upla. Definimos
entonces $A^{0}=\{\Diamond\}$. Si $A$ es un conjunto denotaremos con
$A^{\mathbf{N}}$ al conjunto formado por todas las infinituplas $(a_{1}%
,a_{2},...)$ tales que $a_{i}\in A$ para cada $i\in\mathbf{N}$. Por ejemplo%
\[
(1,2,3,4,...)\in\omega^{\mathbf{N}}%
\]
donde $(1,2,3,4,...)$ es una forma intuitiva de denotar la infinitupla cuyo
$i$-esimo elemento es el numero natural $i$.

Si $(A_{1},A_{2},...)$ es una infinitupla de conjuntos, entonces usaremos
$\bigcup\nolimits_{i=1}^{\infty}A_{i}$ o $\bigcup\nolimits_{i\geq1}A_{i}$ para
denotar al conjunto%
\[
\{a:a\in A_{i}\mathrm{,\ para\ algun\ }i\in\mathbf{N}\}
\]


\subsection{Alfabetos}

Un \textit{alfabeto} es un conjunto finito de simbolos. Notese que $\emptyset$
es un alfabeto. Si $\Sigma$ es un alfabeto, entonces $\Sigma^{\ast}$ denotara
el conjunto de todas las palabras formadas con simbolos de $\Sigma$. Las
palabras de longitud $1$ son exactamente los elementos de $\Sigma$, en
particular esto nos dice que $\Sigma\subseteq\Sigma^{\ast}$. La unica palabra
de longitud $0$ es denotada con $\varepsilon$. Ya que en $\varepsilon$ no
ocurren simbolos, tenemos que $\varepsilon\in\Sigma^{\ast}$, para cualquier
alfabeto, mas aun notese que $\emptyset^{\ast}=\{\varepsilon\}$. Usaremos
$\left\vert \alpha\right\vert $ para denotar la longitud de la palabra
$\alpha$. Si $\alpha\in\Sigma^{\ast}$ y $\sigma\in\Sigma$, usaremos
$\left\vert \alpha\right\vert _{\sigma}$ para denotar la cantidad de
ocurrencias del simbolo $\sigma$ en $\alpha$. Usaremos $\Sigma^{+}$ para
denotar al conjunto $\Sigma^{\ast}-\{\varepsilon\}$. Notese que funciones,
$n$-uplas y palabras son objetos de distinto tipo, por lo cual $\emptyset$,
$\Diamond$ y $\varepsilon$ son tres objetos matematicos diferentes.

Si $\alpha_{1},...,\alpha_{n}\in\Sigma^{\ast}$, con $n\geq0$, usaremos
$\alpha_{1}...\alpha_{n}$ para denotar la \textit{concatenacion} de las
palabras $\alpha_{1},...,\alpha_{n}$ (notese que cuando $n=0$, resulta que
$\alpha_{1}...\alpha_{n}=\varepsilon$). Si $\alpha_{1}=...=\alpha_{n}=\alpha$,
entonces escribiremos $\alpha^{n}$ en lugar de $\alpha_{1}...\alpha_{n}$. O
sea que $\alpha^{0}=\varepsilon$.

Diremos que $\alpha$ \textit{es subpalabra (propia) de }$\beta$ cuando
($\alpha\notin\{\varepsilon,\beta\}$ y) existan palabras $\delta,\gamma$ tales
que $\beta=\delta\alpha\gamma$. Diremos que $\beta$ es un \textit{tramo
inicial (propio) }de $\alpha$ si hay una palabra $\gamma$ tal que
$\alpha=\beta\gamma$ (y $\beta\notin\{\varepsilon,\alpha\}$). En forma similar
se define \textit{tramo final (propio).}

Dados $i\in\omega$ y $\alpha\in\Sigma^{\ast}$ definamos%
\[
\left[  \alpha\right]  _{i}=\left\{
\begin{array}
[c]{lll}%
i\text{-esimo elemento de }\alpha &  & \text{si }1\leq i\leq\left\vert
\alpha\right\vert \\
\varepsilon &  & \text{caso contrario}%
\end{array}
\right.
\]
Dada $\gamma\in\Sigma^{\ast}$, definamos%
\[
\gamma^{R}=\left\{
\begin{array}
[c]{lll}%
\lbrack\gamma]_{\left\vert \gamma\right\vert }[\gamma]_{\left\vert
\gamma\right\vert -1}...[\gamma]_{1} &  & \text{si }\left\vert \gamma
\right\vert \geq1\\
\varepsilon &  & \text{caso contrario}%
\end{array}
\right.
\]
La palabra $\gamma^{R}$ es llamada la \textit{resiproca} de $\gamma$.

\subsubsection{Ocurrencias}

Dadas palabras $\alpha,\beta\in\Sigma^{\ast}$, con $\left\vert \alpha
\right\vert ,\left\vert \beta\right\vert \geq1$, y un natural $i\in
\{1,...,\left\vert \beta\right\vert \}$, se dice que $\alpha$ \textit{ocurre a
partir de }$i$ \textit{en }$\beta$ cuando se de que existan palabras
$\delta,\gamma$ tales que $\beta=\delta\alpha\gamma$ y $\left\vert
\delta\right\vert =i-1$. Intuitivamente hablando $\alpha$ ocurre a partir de
$i$ en $\beta$ cuando se de que si comensamos a leer desde el lugar $i$-esimo
de $\beta$ en adelante, leeremos la palabra $\alpha$ completa y luego
posiblemente seguiran otros simbolos.

Notese que una palabra $\alpha$ puede ocurrir en $\beta$, a partir de $i$, y
tambien a partir de $j$, con $i\neq j$. En virtud de esto, hablaremos de las
distintas ocurrencias de $\alpha$ en $\beta$. Por ejemplo hay dos ocurrencias
de la palabra $aba$\ en la palabra%
\[
cccccccabaccccabaccccc
\]
y tambien hay dos ocurrencias de la palabra $aba$\ en la palabra%
\[
cccccccababacccccccccc
\]
En el primer caso diremos que dichas ocurrencias de $aba$ son
\textit{disjuntas} ya que ocupan espacios disjuntos dentro de la palabra. En
cambio en el segundo caso puede apreciarse que las dos ocurrencias se
superponen en una posicion. A veces diremos que una ocurrencia esta
\textit{contenida} o \textit{sucede} dentro de otra. Por ejemplo la segunda
ocurrencia de $ab$ en $babbbfabcccfabccc$ esta contenida en la primer
ocurrencia de $fabc$ en $babbbfabcccfabccc$.

No definiremos en forma matematica precisa el concepto de ocurrencia pero el
lector no tendra problemas en comprenderlo y manejarlo en forma
correcta.\bigskip

\paragraph*{Reemplazos de ocurrencias}

Tambien haremos \textit{reemplazos} de ocurrencias por palabras. Por ejemplo
el resultado de reemplazar la primer ocurrencia de $abb$ en $ccabbgfgabbgg$
por $oolala$ es la palabra $ccoolalagfgabbgg$. Cuando todas las ocurrencias de
una palabra $\alpha$ en una palabra $\beta$ sean disjuntas entre si, podemos
hablar del resultado de \textit{reeplazar simultaneamente cada ocurrencia de
}$\alpha$ \textit{en} $\beta$ \textit{por} $\gamma$. Por ejemplo si tenemos%
\begin{align*}
\alpha & =yet\\
\beta & =ghsyetcjjjyetbcpyeteabc\\
\gamma & =\%\%
\end{align*}
entonces $ghs\%\%cjjj\%\%bcp\%\%eabc$ es el resultado de reemplazar
simultaneamente cada ocurrencia de $\alpha$ en $\beta$ por $\gamma$. Es
importante notar que los reemplazos se hacen simultaneamente y no
secuencialmente (i.e. reemplazando la primer ocurrencia de $\alpha$ por
$\gamma$ y luego al resultado reemplazarle la primer ocurrencia de $\alpha$
por $\gamma$ y asi sucesivamente). Obviamente el reemplazo secuencial puede
dar un resultado distinto al simultaneo (que es el que usaremos en general) e
incluso puede suceder que en el reemplazo secuencial el proceso se pueda
iterar indefinidamente. Dejamos al lector armar ejemplos de estas cituaciones.

Tambien se pueden hacer reemplazos simultaneos de distintas palabras en una
palabra dada. Supongamos tenemos palabras $\alpha_{1},...,\alpha_{n}$ (con
$\alpha_{i}\neq\alpha_{j}$, para $i\neq j$) las cuales tienen la propiedad de
que las distintas ocurrencias de ellas en la palabra $\beta$ son siempre
disjuntas de a pares, y tenemos ademas palabras $\gamma_{1},...,\gamma_{n}$.
Entonces hablaremos del resultado de reemplazar simultaneamente:

\begin{enumerate}
\item[-] cada ocurrencia de $\alpha_{1}$ en $\beta$, por $\gamma_{1}$

\item[-] cada ocurrencia de $\alpha_{2}$ en $\beta$, por $\gamma_{2}$

\item[ ] $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \vdots$

\item[-] cada ocurrencia de $\alpha_{n}$ en $\beta$, por $\gamma_{n}$
\end{enumerate}

Por ejemplo si tomamos%
\begin{align*}
\alpha_{1}  & =gh\\
\alpha_{2}  & =yet\\
\alpha_{3}  & =ana\\
\beta & =ghbbbyetbbgh\%\%ana\#\#ana!!!ana\\
\gamma_{1}  & =AA\\
\gamma_{2}  & =BBBB\\
\gamma_{3}  & =CCC
\end{align*}
entonces $AAbbbBBBBbbAA\%\%CCC\#\#CCC!!!CCC$ es el resultado de reemplazar simultaneamente:

\begin{enumerate}
\item[-] cada ocurrencia de $\alpha_{1}$ en $\beta$, por $\gamma_{1}$

\item[-] cada ocurrencia de $\alpha_{2}$ en $\beta$, por $\gamma_{2}$

\item[ -] cada ocurrencia de $\alpha_{3}$ en $\beta$, por $\gamma_{3}$
\end{enumerate}

\bigskip

\subsection{Matematica orientada a objetos}

Nuestro estilo o enfoque matematico pondra enfasis en los objetos, es decir
haremos matematica prestando atencion a los distintos objetos matematicos
involucrados, los cuales siempre seran definidos en forma precisa en terminos
de objetos mas primitivos. Hay ciertos objetos matematicos los cuales no
definiremos y supondremos que el lector tiene una idea clara y precisa de los
mismos. Por ejemplo un tipo de objeto matematico, quizas el mas famoso, son
los \textit{numeros}. No diremos que es un numero pero supondremos que el
lector tiene una intuicion clara acerca de este tipo de objetos y de sus
propiedades basicas. Otro tipo de objeto que no definiremos y que sera clave
para nuestro enfoque son los \textit{conjuntos}. Nuevamente, no diremos que es
un conjunto pero supondremos que el lector tiene una intuicion clara acerca de
estos objetos y sus propiedades basicas. Es importante que en nuestro enfoque,
numeros y conjuntos son objetos de distinta naturaleza por lo cual nunca un
numero es un conjunto ni un conjunto es un numero. En particular esto nos dice
que el numero $0$ y el conjunto $\emptyset$ son objetos distintos. Otro tipo
de objeto matematico muy importante para la matematica discreta son los
\textit{simbolos}. No discutiremos que es un simbolo sino que aceptaremos este
concepto en forma primitiva. Tambien constituyen un tipo de objeto matematico
las \textit{palabras}, las cuales intuitivamente hablando son juxtaposiciones
de simbolos. Otro tipo de objeto matematico muy importante son los
\textit{pares ordenados} o 2-\textit{uplas}, es decir los objetos de la forma
$(a,b)$, donde $a$ y $b$ son objetos matematicos cualesquiera. Tambien son
objetos matematicos y de distinta naturaleza las 3-\textit{uplas}, las
4-\textit{uplas} y en general las $n$-\textit{uplas} para $n$ un numero
natural mayor o igual a $2$. Cabe destacar que en nuestro enfoque no habra
1-uplas. Sin envargo, si bien hay una sola 0-\textit{upla}, ella constituye un
tipo de objeto matematico distinto a los antes mensionados. El ultimo tipo de
objeto matematico que consideraremos es aquel de las \textit{infinituplas}.

Tenemos entonces dividido nuestro universo matematico en las distintas
categorias de objetos:%

\begin{align*}
& \mathrm{NUMERO}\\
& \mathrm{CONJUNTO}\\
& \mathrm{PALABRA}\\
& 0\mathrm{-UPLA}\\
& 2\mathrm{-UPLA}\\
& 3\mathrm{-UPLA}\\
& \ \ \ \ \ \ \ \ \vdots\\
& \mathrm{INFINITUPLA}%
\end{align*}
(Notar que los simbolos quedan contenidos en la categoria de las palabras). Es
importante entender que las anteriores categorias o tipos de objetos son
disjuntas entre si, es decir nunca un numero sera una palabra o una palabra
sera una 3-upla etc. Esto nos permite definir una funcion $Ti$ la cual a un
objeto matematico le asigna su tipo de objeto matematico segun la lista
anterior. Por ejemplo:%

\begin{align*}
Ti(\pi)  & =\mathrm{NUMERO}\\
Ti(\mathbf{N})  & =\mathrm{CONJUNTO}\\
Ti(\mathcal{P}(\mathbf{N}))  & =\mathrm{CONJUNTO}\\
Ti((1,2,3))  & =3\mathrm{-UPLA}\\
Ti(\mathbf{\emptyset})  & =\mathrm{CONJUNTO}\\
Ti(\varepsilon)  & =\mathrm{PALABRA}\\
Ti(\Diamond)  & =0\mathrm{-UPLA}\\
Ti(\alpha)  & =\mathrm{PALABRA}\text{, si }\alpha\text{ es un simbolo}\\
Ti(f)  & =\mathrm{CONJUNTO}\text{, si }f\text{ es una funcion}%
\end{align*}


\subsection{El concepto de funcion}

Asumiremos que el lector tiene una idea intuitiva del concepto de funcion.
Daremos aqui una definicion matematica de dicho concepto. Una \textit{funcion}
es un conjunto $f$ de pares ordenados con la siguiente propiedad

\begin{enumerate}
\item[(F)] Si $(x,y)\in f$ y $(x,z)\in f$, entonces $y=z$.
\end{enumerate}

Por ejemplo, si tomamos $f=\{(x,x^{2}):x\in\omega\}$ se puede ver facilmente
que $f$ cumple la propiedad (F). Dada una funcion $f$, definamos%
\begin{align*}
D_{f}  & =\text{ dominio de }f=\{x:(x,y)\in f\text{ para algun }y\}\\
I_{f}  & =\text{ imagen de }f=\{y:(x,y)\in f\text{ para algun }x\}
\end{align*}
A veces escribiremos $\mathrm{Dom}(f)$ y $\operatorname{Im}(f)$ para denotar,
respectivamente, el dominio y la imagen de una funcion $f$. Como es usual dado
$x\in D_{f}$, usaremos $f(x)$ para denotar al unico $y\in I_{f}$ tal que
$(x,y)\in f$. Notese que $\emptyset$ es una funcion y que $D_{\emptyset
}=I_{\emptyset}=\emptyset$. Por ejemplo para $f=\{(x,x^{2}):x\in\omega\}$ se
tiene que $D_{f}=\omega$ y $I_{f}=\{y:y=x^{2}$ para algun $x\in\omega\} $.
Ademas notese que $f(x)=x^{2}$, para cada $x\in D_{f}$.

Escribiremos $f:S\subseteq A\rightarrow B$ para expresar que $f$ es una
funcion tal que $D_{f}=S\subseteq A$ y $I_{f}\subseteq B$. Tambien
escribiremos $f:A\rightarrow B$ para expresar que $f$ es una funcion tal que
$D_{f}=A$ y $I_{f}\subseteq B$. En tal contexto llamaremos a $B$
\textit{conjunto de llegada}. Por supuesto $B$ no esta determinado por $f$ ya
que solo debe cumplir $I_{f}\subseteq B$.

Muchas veces para definir una funcion $f$, lo haremos dando su dominio y su
regla de asignacion, es decir especificaremos en forma precisa que conjunto es
el dominio de $f$ y ademas especificaremos en forma presisa quien es $f(x) $
para cada $x$ de dicho dominio. Obviamente esto determina por completo a la
funcion $f$ ya que $f=\{(x,f(x)):x\in D_{f}\}$. Por ejemplo si decimos que $f$
es la funcion dada por:%
\begin{align*}
D_{f}  & =\omega\\
f(x)  & =23x^{2}%
\end{align*}
nos estaremos refiriendo a la funcion $\{(x,23x^{2}):x\in\omega\}$. Tambien
escribiremos%
\[%
\begin{array}
[c]{rll}%
f:\omega & \rightarrow & \omega\\
x & \rightarrow & 23x^{2}%
\end{array}
\]
para describir a $f$. Es decir, a veces para hacer mas intuitiva aun la
descripcion de la funcion, tambien incluiremos un conjunto de llegada de dicha
funcion y a la regla de asignacion la escribiremos usando una flecha. Para dar
otro ejemplo, si escribimos sea $f$ dada por:%
\[%
\begin{array}
[c]{rll}%
f:\mathbf{N} & \rightarrow & \omega\\
x & \rightarrow & \left\{
\begin{array}
[c]{ccc}%
x+1 &  & \text{si }x\text{ es par}\\
x^{2} &  & \text{si }x\text{ es impar}%
\end{array}
\right.
\end{array}
\]
estaremos diciendo que $f$ es la funcion%
\[
\{(x,x+1):x\text{ es par y }x\in\mathbf{N}\}\cup\{(x,x^{2}):x\text{ es impar y
}x\in\mathbf{N}\}
\]


\subsubsection*{Igualdad de funciones}

Sean $f$ y $g$ dos funciones. Ya que las mismas son conjuntos, tendremos que
$f$ sera igual a $g$ si y solo si para cada par $(a,b)$, se tiene que
$(a,b)\in f$ sii $(a,b)\in g$. Muchas veces sera util el siguiente criterio de
igualdad de funciones:

\begin{lemma}
Sean $f$ y $g$ funciones. Entonces $f=g$ sii $D_{f}=D_{g}$ y para cada $x\in
D_{f}$ se tiene que $f(x)=g(x)$
\end{lemma}

\bigskip

\subsubsection{Funcion identidad}

Dado un conjunto $A$, a la funcion%
\[%
\begin{array}
[c]{rll}%
A & \rightarrow & A\\
a & \rightarrow & a
\end{array}
\]
La denotaremos con $Id_{A}$ y la llamaremos la funcion \textit{identidad sobre
}$A$. Notese que $Id_{A}=\{(a,a):a\in A\}$.

\subsubsection{Composicion de funciones}

Dadas funciones $f$ y $g$ definamos la funcion $f\circ g$ de la siguiente
manera:%
\begin{align*}
D_{f\circ g}  & =\{e\in D_{g}:g(e)\in D_{f}\}\\
f\circ g(e)  & =f(g(e))
\end{align*}
Notar que $f\circ g=\{(u,v):$ existe $z$ tal que $(u,z)\in g$ y $(z,v)\in f\}
$. Notese que $f\circ g\neq\emptyset$ si y solo si $I_{g}\cap D_{f}%
\neq\emptyset$, lo cual nos dice que muchas veces sucedera que $f\circ
g=\emptyset$

\bigskip

\subsubsection{Funciones inyectivas, suryectivas y biyectivas}

Una funcion $f$ es \textit{inyectiva} cuando no se da que $f(a)=f(b)$ para
algun par de elementos distintos $a,b\in D_{f}$. Dada una funcion
$f:A\rightarrow B$ diremos que $f$ es \textit{suryectiva} cuando $I_{f}=B$.
Debe notarse que el concepto de suryectividad depende de un conjunto de
llegada previamente fijado, es decir que no tiene sentido hablar de la
suryectividad de una funcion $f$ si no decimos respecto de que conjunto de
llegada lo es. Muchas veces diremos que una funcion $f$ es \textit{sobre} para
expresar que es suryectiva.

Dada una funcion $f:A\rightarrow B$ diremos que $f$ es \textit{biyectiva}
cuando $f$ sea inyectiva y suryectiva. Notese que si $f:A\rightarrow B$ es
biyectiva, entonces podemos definir una nueva funcion $f^{-1}:B\rightarrow A$,
de la siguiente manera:%
\[
f^{-1}(b)=\text{ unico }a\in A\text{ tal que }f(a)=b
\]
La funcion $f^{-1}$ sera llamada la \textit{inversa de }$f$. Notese que
$f\circ f^{-1}=Id_{B}$ y $f^{-1}\circ f=Id_{A}$. El siguiente lema muestra que
esta ultima propiedad caracteriza la inversa.

\begin{lemma}
\label{mutuamente inversas}Supongamos $f:A\rightarrow B$ y $g:B\rightarrow A$
son tales que $f\circ g=Id_{B}$ y $g\circ f=Id_{A}$. Entonces $f$ y $g$ son
biyectivas, $f^{-1}=g$ y $g^{-1}=f$.
\end{lemma}

\bigskip

\subsubsection{El nucleo de una funcion}

Dada una funcion $f:A\rightarrow B$, definamos:%
\[
\ker(f)=\{(a,b)\in A^{2}:f(a)=f(b)\}
\]
El conjunto $\ker(f)$ sera llamado el \textit{nucleo} de $f$. Notese que $f$
es inyectiva si y solo si $\ker(f)=\{(a,a):a\in A\}$.

\subsubsection{Funcion caracteristica de un subconjunto}

Sea $X$ un conjunto cualquiera y sea $S\subseteq X$. Usaremos $\chi_{S}^{X}$
para denotar la funcion%
\[%
\begin{array}
[c]{rcl}%
\chi_{S}^{X}:X & \rightarrow & \omega\\
x & \rightarrow & \left\{
\begin{array}
[c]{c}%
1\text{ si }x\in S\\
0\text{ si }x\notin S
\end{array}
\right.
\end{array}
\]
Llamaremos a $\chi_{S}^{X}$ la \textit{funcion caracteristica de }%
$S$\textit{\ con respecto a }$X$. Muchas veces cuando el conjunto $X$ este
fijo y sea claro el contexto, escribiremos $\chi_{S}$ en lugar de $\chi
_{S}^{X}$.

\subsubsection{Restriccion de una funcion}

Dada una funcion $f$ y un conjunto $S\subseteq D_{f}$, usaremos $f|_{S}$ para
denotar la \textit{restriccion} de $f$ al conjunto $S$, i.e. $f|_{S}%
=f\cap(S\times I_{f})$. Notese que $f|_{S}$ es la funcion dada por%
\begin{align*}
D_{f|_{S}}  & =S\\
f|_{S}(e)  & =f(e)\text{, para cada }e\in S
\end{align*}
Notese que cualesquiera sea la funcion $f$ tenemos que $f|_{\emptyset
}=\emptyset$ y $f|_{D_{f}}=f$.

\subsubsection{Funciones de la forma $[f_{1},...,f_{n}]$}

Dadas funciones $f_{1},...,f_{n}$, con $n\geq2$, definamos la funcion
$[f_{1},...,f_{n}]$ de la siguiente manera:%
\begin{align*}
D_{[f_{1},...,f_{n}]}  & =D_{f_{1}}\cap...\cap D_{f_{n}}\\
\lbrack f_{1},...,f_{n}](e)  & =(f_{1}(e),...,f_{n}(e))
\end{align*}
Notese que $I_{[f_{1},...,f_{n}]}\subseteq I_{f_{1}}\times\cdots\times
I_{f_{n}}$. Por conveniencia notacional (que el lector entendera mas adelante)
definiremos $[f_{1}]=f_{1}$. Es decir que hemos definido para cada sucecion de
funciones $f_{1},...,f_{n}$, con $n\geq1$, una nueva funcion la cual denotamos
con $[f_{1},...,f_{n}]$.

\subsubsection{Union de funciones con dominios disjuntos}

Una observacion interesante es que si $f_{i}:A_{i}\rightarrow B_{i}$,
$i=1,...,k$, son funciones tales que $A_{i}\cap A_{j}=\emptyset$ para $i\neq
j$, entonces $f_{1}\cup...\cup f_{k}$ es la funcion%
\[%
\begin{array}
[c]{rll}%
A_{1}\cup...\cup A_{k} & \rightarrow & B_{1}\cup...\cup B_{k}\\
e & \rightarrow & \left\{
\begin{array}
[c]{clc}%
f_{1}(e) &  & \text{si }e\in A_{1}\\
\vdots &  & \vdots\\
f_{k}(e) &  & \text{si }e\in A_{k}%
\end{array}
\right.
\end{array}
\]


\subsection{Relaciones binarias}

Sea $A$ un conjunto. Por una \textit{relacion binaria sobre }$A$ entenderemos
un subconjunto de $A^{2}$. Algunos ejemplos:

\begin{enumerate}
\item[(E1)] Sea $R=\{(1,2),(2,3)\}$. Entonces $R$ es una relacion binaria
sobre $\mathbf{N}$.

\item[(E2)] Sea $R=\{(x,y)\in\omega^{2}:$ $x$ divide a $y\}$. Entonces $R$ es
una relacion binaria sobre $\omega$.

\item[(E3)] Sea $R=\{(r,t)\in\mathbf{R}^{2}:r\leq t\}$. Entonces $R$ es una
relacion binaria sobre $\mathbf{R}$

\item[(E4)] $\emptyset$ es una relacion binaria sobre $A$, cualesquiera sea el
conjunto $A$.

\item[(E5)] Sea $R=\{(x,y)\in\omega^{2}:x<y$ o $y=0\}$. Entonces $R$ es una
relacion binaria sobre $\omega$
\end{enumerate}

Notese que si $R$ es una relacion binaria sobre $A$ y $A\subseteq B$ entonces
$R$ es una relacion binaria sobre $B$. Por ejemplo las relaciones dadas en los
ejemplos (E1), (E2), (E4) y (E5) tambien son relaciones binarias sobre
$\mathbf{R}$. Sin envargo si $R$ es una relacion binaria sobre $B$ y
$A\subseteq B$ entonces no necesariamente $R$ sera una relacion binaria sobre
$A$ (por que?).

Como es usual, cuando $R$ sea una relacion binaria sobre un conjunto $A$,
algunas veces escribiremos $aRb$ en lugar de $(a,b)\in R$.

\subsubsection{Propiedades notables de relaciones binarias}

Hay algunas propiedades que pueden tener o no las relaciones binarias sobre un
conjunto $A$, las cuales son muy importantes en matematica. Algunas de estas son:

\begin{enumerate}
\item[Reflexividad] $xRx$, cualesquiera sea $x\in A$

\item[Transitividad] $xRy$ y $yRz$ implica $xRz$, cualesquiera sean $x,y,z\in
A$

\item[Simetria] $xRy$ implica $yRx$, cualesquiera sean $x,y\in A$

\item[Antisimetria] $xRy$ y $yRx$ implica $x=y$, cualesquiera sean $x,y\in A$
\end{enumerate}

Cuando $R$ cumpla la primer propiedad diremos que $R$ es \textit{reflexiva,
con respecto a }$A$. Analogamente diremos que $R$ es \textit{transitiva,
simetrica o antisimetrica, con respecto a }$A$, cuando se den, respectivamente
las otras propiedades. Notese que estas propiedades dependen del conjunto $A$,
por ejemplo si tomamos $R=\{(r,t)\in\mathbf{N}^{2}:r\leq t\}$ entonces $R$ es
una relacion binaria sobre $\mathbf{N}$ y tambien es una relacion binaria
sobre $\omega$, pero es relexiva con respepcto a $\mathbf{N}$ y no lo es con
respecto a $\omega$ ya que $(0,0)$ no pertenece a $R$. Sin envargo $R$ es
transitiva con respecto a $\mathbf{N}$ y tambien lo es con respecto a $\omega$.

\subsubsection{\label{ordenes parciales}Ordenes parciales}

Una relacion binaria $R$ sobre un conjunto $A$ sera llamada un \textit{orden
parcial sobre }$A$\textit{\ }si es reflexiva, transitiva y antisimetrica
respecto de $A$. Algunos ejemplos:

\begin{enumerate}
\item[(E1)] Sea $R=\{(r,t)\in\mathbf{R}^{2}:r\leq t\}$. Entonces $R$ es un
orden parcial sobre $\mathbf{R}$, llamado el orden usual de $\mathbf{R}$

\item[(E2)] Sea $R=\{(1,2),(1,3),(1,1),(2,2),(3,3)\}$. Entonces $R$ es un
orden parcial sobre $\{1,2,3\}$

\item[(E3)] Sea $R=\{(S,T)\in\mathcal{P}(\omega)^{2}:S\subseteq T\}$. Entonces
$R$ es un orden parcial sobre $\mathcal{P}(\omega)$

\item[(E4)] Sea $R=\{(x,y)\in\omega^{2}:$ $x\leq y\}$. Entonces $R$ es un
orden parcial sobre $\omega$.

\item[(E5)] Sea $R=\{(1,1)\}$. Entonces $R$ es un orden parcial sobre $\{1\}$.

\item[(E6)] $\{(a,b):a=b\}$ es un orden parcial sobre $A$, cualesquira sea el
conjunto $A$

\item[(E7)] Sea $\mathrm{\leq}=\{(n,m)\in\mathbf{N}^{2}:n\mid m\}$. Es facil
ver que $\leq$ es un orden parcial sobre $\mathbf{N}$
\end{enumerate}

Notese que las relaciones dadas en (E1) y (E4) son distintas, ademas la
relacion dada en (E4) no es un orden parcial sobre $\mathbf{R}$ (por que?).

Muchas veces denotaremos con $\leq$ a una relacion binaria que sea un orden
parcial. Esto hace mas intuitiva nuestra escritura pero siempre hay que tener
en cuenta que $\leq$ en estos casos esta denotando cierto conjunto de pares
ordenados previamente definido.

Usaremos la siguiente

\begin{enumerate}
\item[Convencion notacional] Si hemos denotado con $\leq$ a cierto orden
parcial sobre un conjunto $A$, entonces

\begin{enumerate}
\item Denotaremos con $<$ a la relacion binaria $\{(a,b)\in A^{2}:a\leq b$ y
$a\neq b\}$. Es decir que $\mathrm{<}=\{(a,b)\in A^{2}:a\leq b$ y $a\neq b\}$.
Cuando se de $a<b$ diremos que $a$ \textit{es menor que }$b$ o que $b$
\textit{es mayor que }$a$ (\textit{respecto de }$\leq$)

\item Denotaremos con $\prec$ a la relacion binaria%
\[
\{(a,b)\in A^{2}:a<b\text{ y no existe }z\text{ tal que }a<z<b\}
\]
Cuando se de $a\prec b$ diremos que $a$ \textit{es cubierto por }$b$ o que $b
$ \textit{cubre a }$a$ (\textit{respecto de }$\leq$)
\end{enumerate}
\end{enumerate}

Algunos ejemplos:

\begin{enumerate}
\item[(E1)] Si $A=\mathbf{R}$ y $\mathrm{\leq}=\{(r,t)\in\mathbf{R}^{2}%
:r=t\}$, entonces $\mathrm{<}=\emptyset$

\item[(E2)] Si $A=\{1,2,3,4\}$ y $\mathrm{\leq}%
=\{(1,2),(2,3),(1,3),(1,1),(2,2),(3,3),(4,4)\}$, entonces $\mathrm{<}%
=\{(1,2),(2,3),(1,3)\}$ y $\mathrm{\prec}=\{(1,2),(2,3)\}$. En particular
tenemos que $1\prec2$, $1<3$ pero no se da que $1\prec3$.

\item[(E3)] Si $A=\mathcal{P}(\omega)$ y $\mathrm{\leq}=\{(S,T)\in
\mathcal{P}(\omega)^{2}:S\subseteq T\}$, entonces $\mathrm{<}=\{(S,T)\in
\mathcal{P}(\omega)^{2}:S\subsetneq T\}$ y $S\prec T$ sii hay un $n\in T-S$
tal que $T=S\cup\{n\}$
\end{enumerate}

\bigskip{}

\paragraph{Ordenes totales sobre un conjunto}

Sea $A$ un conjunto cualquiera. Por un \textit{orden total sobre} $A$
entenderemos un orden parcial $\leq$ sobre $A$ el cual cumpla:

\begin{enumerate}
\item[(C)] $a\leq b$ o $b\leq a$, cualesquiera sean $a,b\in A$
\end{enumerate}

Supongamos $A$ es finito no vacio y $\leq$ es un orden total sobre $A$. La
propiedad (C) nos permite probar que para cada conjunto no vacio $S\subseteq
A$, hay un elemento $s\in S$ el cual cumple $s\leq s^{\prime}$ para cada
$s^{\prime}\in S$. Por supuesto, $s$ es unico (por que?) y habitualmente es
llamado el \textit{menor elemento de }$S$, ya que es menor que todo otro
elemento de $S$.

Si $A$ es finito no vacio y $\leq$ es un orden total sobre $A$, podemos
definir recursivamente una funcion $f:\{1,...,\left\vert A\right\vert
\}\rightarrow A$ de la siguiente manera:

\begin{enumerate}
\item[-] $f(1)=$ menor elemento de $A$

\item[-] Si $i\in\{1,...,\left\vert A\right\vert -1\}$, entonces

\begin{enumerate}
\item[-] $f(i+1)=$ menor elemento de $A-\{f(1),...,f(i)\}$
\end{enumerate}
\end{enumerate}

Como es habitual, $f(i)$ es llamado el $i$\textit{-esimo elemento de }$A$.

Muchas veces para dar un orden total sobre un conjunto finito $A$, daremos
simplemente sus elementos en forma creciente ya que esto determina el orden
por completo. Por ejemplo si $A=\{1,2,3\}$, el orden total dado por $2<1<3$ es
la relacion $\mathrm{\leq}=\{(2,1),(1,3),(2,3),(1,1),(2,2),(3,3)\}$.

Un concepto importante relativo a los ordenes totales es el de
\textit{sucesor}. Si $\leq$ es un orden total sobre $A$ y $a,b\in A$, diremos
que $b $ \textit{es el sucesor de} $a$ cuando se de que $a<b$ y $b\leq c$,
para cada $c\in A$ tal que $a<c$, i.e., $b$ es el menor elemento del conjunto
$\{c\in A:$ tal que $a<c\}$. No siempre existe el sucesor de un elemento. Por
ejemplo si $\leq$ es el orden usual de $\mathbf{R}$, entonces ningun elemento
tiene sucesor (justifique).

\bigskip

\paragraph{Diagramas de Hasse}

Dado un orden parcial $\leq$ sobre un conjunto finito $A$ podemos realizar un
diagrama de $\leq$, llamado \textit{diagrama de Hasse,} siguiendo las
siguientes instrucciones:

\begin{enumerate}
\item[(1)] Asociar en forma inyectiva, a cada $a\in$ $A$ un punto $p_{a}$ del plano

\item[(2)] Trazar un segmento de recta uniendo los puntos $p_{a}$ y $p_{b}$,
cada vez que $a\prec b$

\item[(3)] Realizar lo indicado en los puntos (1) y (2) en tal forma que

\begin{enumerate}
\item[(i)] Si $a\prec b$, entonces $p_{a}$ esta por debajo de $p_{b}$

\item[(ii)] Si un punto $p_{a}$ ocurre en un segmento del diagrama entonces lo
hace en alguno de sus extremos.
\end{enumerate}
\end{enumerate}

\noindent La relacion de orden $\leq$ puede ser facilmente obtenida de su
diagrama, a saber, $a\leq b$ sucedera si y solo si $p_{a}=p_{b}$ o hay una
sucesion de segmentos ascendentes desde $p_{a}$ hasta $p_{b}$.

\bigskip

Ejemplos:

\bigskip

\subsubsection{Relaciones de equivalencia}

Sea $A$ un conjunto cualquiera. Por una \textit{relacion de equivalencia sobre
}$A$ entenderemos una relacion binaria sobre $A$ la cual es reflexiva,
transitiva y simetrica, con respecto a $A$, es decir, la cual cumple:

\begin{enumerate}
\item[Reflexividad] $xRx$, cualesquiera sea $x\in A$

\item[Transitividad] $xRy$ y $yRz$ implica $xRz$, cualesquiera sean $x,y,z\in
A$

\item[Simetria] $xRy$ implica $yRx$, cualesquiera sean $x,y\in A$
\end{enumerate}

Algunos ejemplos:

\begin{enumerate}
\item[(E1)] Sea $R=\{(r,t)\in\mathbf{R}^{2}:r=t\}$. Entonces $R$ es una
relacion de equivalencia sobre $\mathbf{R}$

\item[(E2)] Dada una funcion $f:A\rightarrow B$, el nucleo de $f$, i.e.
$\ker(f)=\{(a,b)\in A^{2}:f(a)=f(b)\}$ es una relacion de equivalencia sobre
$A$.

\item[(E3)] Sea $R=\{(1,1),(2,2),(3,3),(1,2),(2,1)\}$. Entonces $R$ es una
relacion de equivalencia sobre $\{1,2,3\}$

\item[(E4)] Sea $R=\{(x,y)\in\omega^{2}:x=y\}$. Entonces $R$ es una relacion
de equivalencia sobre $\omega$

\item[(E5)] Sea $R=\{(S,T)\in\mathcal{P}(\omega)^{2}:(S-T)\cup(T-S)$ es
finito$\}$. Entonces $R$ es una relacion de equivalencia sobre $\mathcal{P}%
(\omega)$

\item[(E7)] Sea $R=\{(1,1)\}$. Entonces $R$ es una relacion de equivalencia
sobre $\{1\}$.

\item[(E8)] Sea $R=\{(x,y)\in\mathbf{Z}^{2}:x-y$ es multiplo de $2\}$.
Entonces $R$ es una relacion de equivalencia sobre $\mathbf{Z}$.
\end{enumerate}

Dada una relacion de equivalencia $R$ sobre $A$ y $a\in A$, definimos:%

\[
a/R=\{b\in A:aRb\}
\]
El conjunto $a/R$ sera llamado la \textit{clase de equivalencia de }%
$a$\textit{, con respecto a }$R$. Ejemplos:

\begin{enumerate}
\item[(E1)] Si $R=\{(r,t)\in\mathbf{R}^{2}:r=t\}$, entonces $r/R=\{r\}$,
cualesquier sea $r\in\mathbf{R}$

\item[(E2)] Si $R=\{(1,1),(2,2),(3,3),(1,2),(2,1)\}$, entonces
$1/R=2/R=\{1,2\}$ y $3/R=\{3\}$

\item[(E3)] Si $R=\{(x,y)\in\mathbf{Z}^{2}:x-y$ es multiplo de $2\}$, entonces
$0/R=\{t\in\mathbf{Z}:t$ es par$\}$, $1/R=\{t\in\mathbf{Z}:t$ es impar$\}$ y
en general notese que $n/R=\{t\in\mathbf{Z}:t$ es par$\}$ si $n$ es par y
$n/R=\{t\in\mathbf{Z}:t$ es impar$\}$ si $n$ es impar. Es decir que hay solo
dos clases de equivalencia con respecto a $R$
\end{enumerate}

Algunas propiedades basicas son:

\begin{lemma}
\label{basicas a/R}Sea $R$ una relacion de equivalencia sobre $A$. Sean
$a,b\in A$.

\begin{enumerate}
\item[(1)] $a\in a/R$

\item[(2)] $aRb$ si y solo si $a/R=b/R$. Es decir que $b\in a/R$ implica
$b/R=a/R$

\item[(3)] $a/R\cap b/R=\emptyset$ o $a/R=b/R$
\end{enumerate}
\end{lemma}

\begin{proof}
(1) es muy facil.

(2). Supongamos $aRb$. Veremos que $a/R\subseteq b/R$. Supongamos $c\in a/R$.
Entonces $aRc$. Como $aRb$, tenemos que $bRa$, por lo cual hemos probado que
$bRa$ y $aRc$, lo cual implica que $bRc$. O sea que $cRb$, lo cual nos dice
que $c\in b/R$. Esto prueba que $a/R\subseteq b/R$. Similarmente se prueba que
$b/R\subseteq a/R$, con lo cual se tiene que $a/R=b/R$.

Reciprocamente, si $a/R=b/R$, entonces $b\in a/R$ ya que $b\in b/R$. Pero esto
nos dice que $aRb$.

o=(3). Supongamos que $a/R\cap b/R$ no es vacio, es decir hay un $c\in a/R\cap
b/R$. Entonces es facil ver que $aRb$. Pero entonces por (2) tenemos que
$a/R=b/R$.
\end{proof}

Denotaremos con $A/R$ al conjunto $\{a/R:a\in A\}$. Llamaremos a $A/R$ el
\textit{cociente de }$A$ \textit{por }$R$. Ejemplos:

\begin{enumerate}
\item[(E1)] Si $R=\{(r,t)\in\mathbf{R}^{2}:r=t\}$, entonces $\mathbf{R}%
/R=\{\{r\}:r\in\mathbf{R}\}$

\item[(E2)] Si $R=\{(1,1),(2,2),(3,3),(1,2),(2,1)\}$, entonces
$\{1,2,3\}/R=\{\{1,2\},\{3\}\}$

\item[(E3)] Si $R=\{(x,y)\in\mathbf{Z}^{2}:x-y$ es multiplo de $2\}$, ya vimos
que $\mathbf{Z}/R=\{\{t\in\mathbf{Z}:t$ es par$\},\{t\in\mathbf{Z}:t$ es
impar$\}\}$
\end{enumerate}

Si $R$ es una relacion de equivalencia sobre $A$, definamos la funcion
$\pi_{R}:A\rightarrow A/R$ por $\pi_{R}(a)=a/R$, para cada $a\in A$. La
funcion $\pi_{R}$ es llamada la \textit{proyeccion canonica }(\textit{respecto
de }$R$).

\begin{lemma}
Sea $R$ una relacion de equivalencia sobre $A$. Entonces $\ker\pi_{R}=R$. Es
decir que $\pi_{R}$ es inyectiva sii $R=\{(x,y)\in A^{2}:x=y\}$
\end{lemma}

\bigskip

\bigskip

\paragraph{Definicion de funciones con dominio $A/R$}

Supongamos $R$ es una relacion de equivalencia sobre $\mathbf{R}$ y supongamos
definimos una funcion $f:\mathbf{R}/R\rightarrow\mathbf{R}$ de la siguiente
manera:%
\[
f(r/R)=r^{2}%
\]
A priori puede pareser que esta definicion es natural y que no esconde ninguna
posible complicacion. Pero supongamos que $R$ es tal que $2R6$. Entonces
tendriamos que $2/R=6/R$ lo cual produciria la siguiente contradiccion:%
\begin{align*}
4  & =2^{2}\\
& =f(2/R)\\
& =f(6/R)\\
& =6^{2}\\
& =36
\end{align*}
El problema aqui es que la ecuacion $f(r/R)=r^{2}$ no esta definiendo en forma
correcta o inhambigua una funcion ya que el supuesto valor de la funcion en
una clase de equivalencia dada depende de que representante de la clase usamos
para denotarla. Si usamos el 2 la ecuacion nos dice que entonces $f$ debe
valer 4 y si usamos el 6 la ecuacion nos dice que $f$ debe valer 36.
Claramente no estamos definiendo una funcion.

Para dar un ejemplo mas concreto de este fenomeno de ambiguedad, supongamos%
\[
R=\{(x,y)\in\mathbf{Z}^{2}:x-y\text{ es multiplo de }2\}
\]
y definimos una funcion $f:\mathbf{Z}/R\rightarrow\mathbf{R}$ de la siguiente
manera:%
\[
f(n/R)=1/(n^{2}+1)
\]
Como ya vimos $\mathbf{Z}/R=\{\{t\in\mathbf{Z}:t$ es par$\},\{t\in
\mathbf{Z}:t$ es impar$\}\}$, por lo cual facilmente se puede llegar a que la
ecuacion $f(n/R)=1/(n^{2}+1)$ no define correctamente una funcion. Dejamos al
lector explicar esto mas detalladamente.

Sin envargo hay muchos casos en los cuales este tipo de definiciones son
inhambiguas y desde luego muy importantes en el algebra moderna. Como un
primer ejemplo tenemos el siguiente lema el cual es una de las ideas
fundamentales del algebra moderna.

\begin{lemma}
Si $f:A\rightarrow B$ es sobre, entonces la ecuacion $\bar{f}(a/\ker f)=f(a)$
define en forma inhambigua una funcion $\bar{f}:A/\ker f\rightarrow B$ la cual
es biyectiva.
\end{lemma}

\begin{proof}
Que la ecuacion $\bar{f}(a/\ker f)=f(a)$ define sin ambiguedad una funcion
$\bar{f}:A/\ker f\rightarrow B$ es obvio ya que si $a/\ker f=b/\ker f$,
entonces por definicion de $\ker f$ debera suceder que $a=b$. Dejamos al
lector la prueba de que $\bar{f}$ es biyectiva
\end{proof}

\bigskip

\paragraph{Correspondencia entre relaciones de equivalencia y particiones}

Dado un conjunto $A$ por una \textit{particion de }$A$ entenderemos un
conjunto $\mathcal{P}$ tal que:

\begin{enumerate}
\item[-] Cada elemento de $\mathcal{P}$ es un subconjunto no vacio de $A$

\item[-] Si $S_{1},S_{2}\in\mathcal{P}$ y $S_{1}\neq S_{2}$, entonces
$S_{1}\cap S_{2}=\emptyset$

\item[-] $A=\{a:a\in S$, para algun $S\in\mathcal{P}\}$
\end{enumerate}

La ultima condicion dice simplemente que la union de todos los elementos de
$\mathcal{P}$ debe ser $A$. Ejemplos:

\begin{enumerate}
\item[(E1)] Si $A=\{1,2,3,4,5\}$, entonces%
\[
\mathcal{P}=\{\{1,5\},\{2,3\},\{4\}\}
\]


es una particion de $A$

\item[(E2)] $\mathcal{P}=\{\mathbf{N},\mathbf{R}-\mathbf{N}\}$ es una
particion de $\mathbf{R}$

\item[(E3)] $\mathcal{P}%
=\{\{0\},\{1,2\},\{3,4\},\{5,6\},\{7,8\},\{9,10\},...\}$ es una particion de
$\omega$
\end{enumerate}

Una observacion importante es que si $\mathcal{P}$ es una particion de $A$,
entonces para cada $a\in A$ hay un unico $S\in\mathcal{P}$ tal que $a\in S$
(por que?). O sea que podemos hablar de EL elemento de $\mathcal{P}$ que
contiene a $a$.

Dada una particion $\mathcal{P}$ de un conjunto $A$ podemos definir una
relacion binaria asociada a $\mathcal{P}$ de la siguiente manera:%
\[
R_{\mathcal{P}}=\{(a,b)\in A^{2}:a,b\in S\text{, para algun }S\in\mathcal{P}\}
\]


\begin{lemma}
Sea $A$ un conjunto cualquiera. Entonces:

\begin{enumerate}
\item[(1)] Sea $\mathcal{P}$ una particion de $A$. Entonces $R_{\mathcal{P}}$
es una relacion de equivalencia sobre $A$.

\item[(2)] Sea $R$ una relacion de equivalencia sobre $A$. Entonces $A/R$ es
una particion de $A$.
\end{enumerate}
\end{lemma}

\begin{proof}
(1). Es facil ver que $R_{\mathcal{P}}$ es reflexiva y simetrica. Veamos que
es transitiva. Supongamos que $aR_{\mathcal{P}}b$ y $bR_{\mathcal{P}}c$. O sea
que hay $S_{1},S_{2}\in\mathcal{P}$ tales que $a,b\in S_{1}$ y $b,c\in S_{2}$.
Ya que $S_{1}$ y $S_{2}$ tienen un elemento en comun, debera suceder que
$S_{1}=S_{2}$. Pero entonces tenemos que $a,c\in S_{1}$, lo cual nos dice que
$aR_{\mathcal{P}}c$.

(2). Sigue facilmente del Lema \ref{basicas a/R}.
\end{proof}

\bigskip

El siguiente teorema da una correspondencia natural entre relaciones de
equivalencia sobre $A$ y particiones de $A$.

\begin{theorem}
Sea $A$ un conjunto cualquiera. Sean%
\begin{align*}
Part  & =\{\text{particiones de }A\}\\
ReEq  & =\{\text{relaciones de equivalencia sobre }A\}
\end{align*}
Entonces las funciones:%
\[%
\begin{array}
[c]{rll}%
Part & \rightarrow & ReEq\\
\mathcal{P} & \rightarrow & R_{\mathcal{P}}%
\end{array}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\begin{array}
[c]{rll}%
ReEq & \rightarrow & Part\\
R & \rightarrow & A/R
\end{array}
\]
son biyecciones una inversa de la otra.
\end{theorem}

\begin{proof}
Notese que por el Lema \ref{mutuamente inversas} basta con probar:

\begin{enumerate}
\item[(1)] $A/R_{\mathcal{P}}=\mathcal{P}$, cualesquiera sea la particion
$\mathcal{P}$ de $A$

\item[(2)] $R_{A/R}=R$, cualesquiera sea la relacion de equivalencia $R$ sobre
$A$
\end{enumerate}

Prueba de (1). Primero veamos que $A/R_{\mathcal{P}}\subseteq\mathcal{P}$. Sea
$a\in A$, veremos que $a/R_{\mathcal{P}}=\{b:aR_{\mathcal{P}}b\}\in
\mathcal{P}$. Sea $S$ el unico elemento de $\mathcal{P}$ que contiene a $a$.
Es facil ver de la definicion de $R_{\mathcal{P}}$ que $a/R_{\mathcal{P}}=S$
por lo cual $a/R_{\mathcal{P}}\in\mathcal{P}$. Veamos ahora que $\mathcal{P}%
\subseteq A/R_{\mathcal{P}}$. Sea $S\in\mathcal{P}$. Sea $a\in S$. Es facil
ver de la definicion de $R_{\mathcal{P}}$ que $a/R_{\mathcal{P}}=S$ por lo
cual $S\in A/R_{\mathcal{P}}$.

Prueba de (2). Primero veamos que $R_{A/R}\subseteq R$. Supongamos $aR_{A/R}b
$. Entonces $a,b\in c/R$, para algun $c\in A$. Es claro que entonces $aRb$.
Veamos ahora que $R\subseteq R_{A/R}$. Supongamos que $aRb$. Entonces $a,b\in
a/R$, lo cual nos dice que $aR_{A/R}b$.
\end{proof}

\bigskip

El teorema anterior muestra que a nivel de informacion es lo mismo tener una
relacion de equivalencia sobre $A$ que tener una particion de $A$. Esto es muy
util ya que muchas veces es mas facil especificar una relacion de equivalencia
via su particion asociada. Por ejemplo si hablamos de la relacion de
equivalencia sobre $\{1,2,3,4,5\}$ dada por la particion%
\[
\mathcal{P}=\{\{1,5\},\{4\},\{2,3\}\}
\]
nos estaremos refiriendo a $R_{\mathcal{P}}$, es decir a la relacion:%

\[
\{(1,1),(2,2),(3,3),(4,4),(5,5),(1,5),(5,1),(2,3),(3,2)\}
\]


\subsection{Operaciones $n$-arias sobre un conjunto}

Sea $A$ un conjunto. Dado $n\in\omega$, por una \textit{operacion }%
$n$\textit{-aria sobre }$A$ entenderemos una funcion cuyo dominio es $A^{n}$ y
cuya imagen esta contenida en $A$. A las operaciones $2$-arias (resp.
$3$-arias, $4$-arias) tambien las llamaremos \textit{operacion binarias}
(resp. \textit{ternarias, cuaternarias}). Algunos ejemplos:

\begin{enumerate}
\item[(E1)] Sea $f:\mathbf{R\times R}\rightarrow\mathbf{R}$ dada por
$f(x,y)=x+y$. Entonces $f$ es una operacion $2$-aria sobre $\mathbf{R}$

\item[(E2)] Sea $f:\{\Diamond\}\rightarrow\omega$, dada por $f(\Diamond)=5$.
Entonces $f$ es una operacion $0$-aria sobre $\omega$.

\item[(E3)] Sea $f:\mathbf{N\times N\times N\times N\times N}\rightarrow
\mathbf{N}$, dada por $f(x_{1},x_{2},x_{3},x_{4},x_{5})=(x_{1}.x_{2})+x_{3}$.
Entonces $f$ es una operacion $5$-aria sobre $\mathbf{N}$.
\end{enumerate}

Si $f$ es una operacion $n$-aria sobre $A$ y $S\subseteq A$, entonces diremos
que $S$ es \textit{cerrado bajo} $f$\bigskip\ cuando se de que $f(a_{1}%
,...,a_{n})\in S$, cada ves que $a_{1},...,a_{n}\in S$. Notese que si $n=0$,
entonces $S$ es cerrado bajo $f$ si y solo si $f(\Diamond)\in S$.

\bigskip

\subsection{Relaciones $n$-arias sobre un conjunto}

Sea $A$ un conjunto. Dado $n\in\omega$, por una \textit{relacion }%
$n$\textit{-aria sobre }$A$ entenderemos un subconjunto de $A^{n}$. A las
relaciones $2$-arias (resp. $3$-arias, $4$-arias) tambien las llamaremos
\textit{relaciones binarias} (resp. \textit{ternarias, cuaternarias}). Algunos ejemplos:

\begin{enumerate}
\item[(E1)] Sea $R=\{(r,t)\in\mathbf{R\times R}:r\leq t\}$. Entonces $R$ es
una relacion $2$-aria sobre $\mathbf{R}$

\item[(E2)] Hay exactamente dos relaciones $0$-arias sobre $A$, a saber:
$\emptyset$ y $\{\Diamond\}$.

\item[(E3)] Sea $R=\{(x_{1},x_{2},x_{3},x_{4},x_{5})\in\mathbf{N}^{5}%
:x_{5}=x_{4}\}$. Entonces $R$ es una relacion $5$-aria sobre $\mathbf{N}$.
Notese que tambien $R$ es una relacion $5$-aria sobre $\mathbf{R}$

\item[(E4)] $\emptyset$ es una relacion $n$-aria sobre $A$, cualesquiera sea
$n\in\omega$ y $A$.
\end{enumerate}

\bigskip

\subsection{Funciones $\Sigma$-mixtas}

Sea $\Sigma$ un alfabeto finito. Dados $n,m\in\omega$, usaremos $\omega
^{n}\times\Sigma^{\ast m}$ para abreviar la expresion%
\[
\overset{n\text{ veces}}{\overbrace{\omega\times...\times\omega}}%
\times\overset{m\text{ veces}}{\overbrace{\Sigma^{\ast}\times...\times
\Sigma^{\ast}}}%
\]
Por ejemplo, $\omega^{3}\times\Sigma^{\ast4}$ sera una forma abreviada de
escribir $\omega\times\omega\times\omega\times\Sigma^{\ast}\times\Sigma^{\ast
}\times\Sigma^{\ast}\times\Sigma^{\ast}$. Debe quedar claro que estamos
haciendo cierto abuso notacional ya que en principio si no hacemos esta
convencion notacional, $\omega^{3}\times\Sigma^{\ast4}$ denota un conjunto de
pares y $\omega\times\omega\times\omega\times\Sigma^{\ast}\times\Sigma^{\ast
}\times\Sigma^{\ast}\times\Sigma^{\ast}$ es un conjunto de $7$-uplas.

Notese que cuando $n=m=0$, tenemos que $\omega^{n}\times\Sigma^{\ast m}$
denota el conjunto $\{\Diamond\}$ y si $m=0$, entonces $\omega^{n}\times
\Sigma^{\ast m}$ denota el conjunto $\omega^{n}$.

Con esta convencion notacional, un elemento generico de $\omega^{n}%
\times\Sigma^{\ast m}$ es una $(n+m)$-upla de la forma $(x_{1},...,x_{n}%
,\alpha_{1},...,\alpha_{m})$. Para abreviar, escribiremos $(\vec{x}%
,\vec{\alpha})$ en lugar de $(x_{1},...,x_{n},\alpha_{1},...,\alpha_{m})$.

\bigskip

\paragraph*{Definicion de funcion $\Sigma$-mixta}

Sea $\Sigma$ un alfabeto finito. Dada una funcion $f$, diremos que $f$ es
$\Sigma$-\textit{mixta} si cumple las siguientes propiedades

\begin{enumerate}
\item[(M1)] Existen $n,m\geq0$, tales que $D_{f}\subseteq\omega^{n}%
\times\Sigma^{\ast m}$

\item[(M2)] Ya sea $I_{f}\subseteq\omega$ o $I_{f}\subseteq\Sigma^{\ast} $
\end{enumerate}

\bigskip

Algunos ejemplos:

\begin{enumerate}
\item[E$_{1}$] Sea $\Sigma=\{\square,\%,\blacktriangle\}$. La funcion%
\[%
\begin{array}
[c]{rll}%
f:\omega\times\{\square,\%,\blacktriangle\}^{\ast} & \rightarrow & \omega\\
(x,\alpha) & \rightarrow & x+\left\vert \alpha\right\vert
\end{array}
\]
es $\Sigma$-mixta ya que se cumple (M1) con $n=m=1$ y (M2). Notese que $f$ no
es $\{\square,\%\}$-mixta ya que no cumple (M1) respecto del alfabeto
$\{\square,\%\}$. Sin envargo note que $f$ es $\{\square,\%,\blacktriangle
,@\}$-mixta

\item[E$_{2}$] La funcion%
\[%
\begin{array}
[c]{rll}%
\omega^{4} & \rightarrow & \omega\\
(x,y,z,w) & \rightarrow & x+y
\end{array}
\]
es $\Sigma$-mixta cualesquiera sea el alfabeto $\Sigma$

\item[E$_{3}$] Sea $\Sigma=\{\square,@\}$. La funcion%
\[%
\begin{array}
[c]{rll}%
\{\square\square\square,@@\} & \rightarrow & \omega\\
\alpha & \rightarrow & \left\vert \alpha\right\vert
\end{array}
\]


es $\Sigma$-mixta ya que se cumple (M1) (con $n=0$ y $m=1$) y (M2)

\item[E$_{4}$] Supongamos $\Sigma=\emptyset$. Tenemos entonces que
$\Sigma^{\ast}=\{\varepsilon\}$. Por ejemplo%
\[%
\begin{array}
[c]{rll}%
D & \rightarrow & \omega\\
(x,\varepsilon,\varepsilon,\varepsilon) & \rightarrow & x^{2}%
\end{array}
\]
donde $D=\{(x,\varepsilon,\varepsilon,\varepsilon):x$ es impar$\}$, es
$\Sigma$-mixta (con $n=1$ y $m=3$ en (M1)). Tambien notese que%
\[%
\begin{array}
[c]{rll}%
\{(\varepsilon,\varepsilon)\} & \rightarrow & \{\varepsilon\}\\
(\varepsilon,\varepsilon) & \rightarrow & \varepsilon
\end{array}
\]
es $\Sigma$-mixta.
\end{enumerate}

\bigskip

Dejamos al lector la facil prueba del siguiente resultado basico.

\begin{lemma}
Supongamos $\Sigma\subseteq\Gamma$ son alfabetos finitos. Entonces si $f$ es
una funcion $\Sigma$-mixta, $f$ es $\Gamma$-mixta
\end{lemma}

\bigskip

Una funcion $\Sigma$-mixta $f$ es $\Sigma$-\textit{total} cuando haya
$n,m\in\omega$ tales que $D_{f}=\omega^{n}\times\Sigma^{\ast m}$. El lema
anterior nos dice que si $\Sigma\subseteq\Gamma$, entonces toda funcion
$\Sigma$-mixta es $\Gamma$-mixta. Sin envargo una funcion puede ser $\Sigma
$-total y no ser $\Gamma$-total, cuando $\Sigma\subseteq\Gamma$. Por ejemplo
tomemos $\Sigma=\{\square,\%,\blacktriangle\}$ y $\Gamma=\{\square
,\%,\blacktriangle,!\}$, y consideremos la funcion%

\[%
\begin{array}
[c]{rll}%
f:\omega\times\Sigma^{\ast} & \rightarrow & \omega\\
(x,\alpha) & \rightarrow & x+\left\vert \alpha\right\vert
\end{array}
\]
Es claro que $f$ es $\Sigma$-mixta y $\Sigma$-total. Tambien es $\Gamma$-mixta
ya que $D_{f}\subseteq\omega\times\Gamma^{\ast}$ y $I_{f}\subseteq\omega$, por
lo cual cumple (M1) y (M2). Sin envargo $f$ no es $\Gamma$-total ya que
$D_{f}$ no es igual a $\omega^{n}\times\Gamma^{\ast m}$, cualesquiera sean $n$
y $m$.

Como hemos visto recien, una funcion $f$ puede ser $\Sigma$-mixta y $\Gamma
$-mixta para dos alfabetos distintos $\Sigma$ y $\Gamma$ e incluso es facil
construir un ejemplo en el cual $\Sigma$ y $\Gamma$ sean incomparables como
conjuntos, es decir que ninguno incluya al otro. Dejamos al lector convencerse
de que si $f$ es una funcion que es $\Sigma$-mixta para algun alfabeto
$\Sigma$, entonces hay un alfabeto $\Sigma_{0}$ el cual es el menor de todos
los alfabetos respecto de los cuales $f$ es mixta, es decir $\Sigma_{0}$
cumple que $f$ es $\Sigma_{0}$-mixta y si $\Gamma$ es tal que $f$ es $\Gamma
$-mixta, entonces $\Sigma_{0}\subseteq\Gamma$.

A continuacion daremos algunas funciones $\Sigma$-mixtas basicas las cuales
seran frecuentemente usadas.

\subsubsection{Funciones $Suc$ y $Pred$}

La \textit{funcion sucesor} es definida por%
\[%
\begin{array}
[c]{rll}%
Suc:\omega & \rightarrow & \omega\\
n & \rightarrow & n+1
\end{array}
\]
La \textit{funcion predecesor} es definida por%
\[%
\begin{array}
[c]{rll}%
Pred:\mathbf{N} & \rightarrow & \omega\\
n & \rightarrow & n-1
\end{array}
\]


\subsubsection{Las funciones $d_{a}$}

Sea $\Sigma$ un alfabeto no vacio. Para cada $a\in\Sigma$, definamos%
\[%
\begin{array}
[c]{rll}%
d_{a}:\Sigma^{\ast} & \rightarrow & \Sigma^{\ast}\\
\alpha & \rightarrow & \alpha a
\end{array}
\]
La funcion $d_{a}$ es llamada la funcion \textit{derecha sub }$a$, respecto
del alfabeto $\Sigma$.

\subsubsection{Las funciones $p_{i}^{n,m}$}

Sea $\Sigma$ un alfabeto. Para $n,m,i\in\omega$ tales que $1\leq i\leq n$,
definamos%
\[%
\begin{array}
[c]{rll}%
p_{i}^{n,m}:\omega^{n}\times\Sigma^{\ast m} & \rightarrow & \omega\\
(\vec{x},\vec{\alpha}) & \rightarrow & x_{i}%
\end{array}
\]
Para $n,m,i\in\omega$ tales que $n+1\leq i\leq n+m$, definamos%
\[%
\begin{array}
[c]{rll}%
p_{i}^{n,m}:\omega^{n}\times\Sigma^{\ast m} & \rightarrow & \Sigma^{\ast}\\
(\vec{x},\vec{\alpha}) & \rightarrow & \alpha_{i-n}%
\end{array}
\]
Las funciones $p_{i}^{n,m}$ son llamadas \textit{proyecciones}. La funcion
$p_{i}^{n,m}$ es llamada la \textit{proyeccion }$n,m,i$, respecto del alfabeto
$\Sigma$. Notese que esta definicion requiere que $n+m\geq1$ ya que $i$ debe
cumplir $1\leq i\leq n+m$.

\subsubsection{Las funciones $C_{k}^{n,m}$ y $C_{\alpha}^{n,m}$}

Sea $\Sigma$ un alfabeto. Para $n,m,k\in\omega$, y $\alpha\in\Sigma^{\ast}$,
definamos%
\[%
\begin{array}
[c]{rll}%
C_{k}^{n,m}:\omega^{n}\times\Sigma^{\ast m} & \rightarrow & \omega\\
(\vec{x},\vec{\alpha}) & \rightarrow & k
\end{array}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\begin{array}
[c]{rll}%
C_{\alpha}^{n,m}:\omega^{n}\times\Sigma^{\ast m} & \rightarrow & \Sigma^{\ast
}\\
(\vec{x},\vec{\alpha}) & \rightarrow & \alpha
\end{array}
\]
Notese que $C_{k}^{0,0}:\{\Diamond\}\rightarrow\{k\}$ y que $C_{\alpha}%
^{0,0}:\{\Diamond\}\rightarrow\{\alpha\}$.

\subsubsection{La funcion $pr$}

Definamos%
\[%
\begin{array}
[c]{rll}%
pr:\mathbf{N} & \rightarrow & \omega\\
n & \rightarrow & n\text{-esimo numero primo}%
\end{array}
\]
Notese que $pr(1)=2$, $pr(2)=3$, $pr(3)=5$, etc.

\subsubsection{El tipo de una funcion mixta}

Dada una funcion $\Sigma$-mixta $f$, si $n,m\in\omega$ son tales que
$D_{f}\subseteq\omega^{n}\times\Sigma^{\ast m}$ y ademas $I_{f}\subseteq
\omega$, entonces diremos que $f$ \textit{es una funcion de tipo }$(n,m,\#)$.
Similarmente si $n,m\in\omega$ son tales que $D_{f}\subseteq\omega^{n}%
\times\Sigma^{\ast m}$ y ademas $I_{f}\subseteq\Sigma^{\ast}$, entonces
diremos que $f$ \textit{es una funcion de tipo }$(n,m,\ast)$. Notese que si
$f\neq\emptyset$, entonces hay unicos $n,m\in\omega$ y $s\in\{\#,\ast\}$ tales
que $f$ es una funcion de tipo $(n,m,s)$. Sin envargo $\emptyset$ es una
funcion de tipo $(n,m,s)$ cualesquiera sean $n,m\in\omega$ y $s\in\{\#,\ast
\}$. De esta forma, cuando $f\neq\emptyset$ hablaremos de "el tipo de $f$"
para refererirnos a esta unica terna $(n,m,s)$. Notese que $Suc$ es de tipo
$(1,0,\#)$ y $d_{a}$ es de tipo $(0,1,\ast)$.

Tambien notese que la relacion "$f$ es una funcion de tipo $(n,m,s)$" no
depende del alfabeto $\Sigma$ (que significa esto?).

\subsubsection{Funciones con imagen contenida en $\omega^{n}\times\Sigma^{\ast
m}$}

Supongamos que $k,l,n,m\in\omega$ y que $F:D_{F}\subseteq\omega^{k}%
\times\Sigma^{\ast l}\rightarrow\omega^{n}\times\Sigma^{\ast m}$. Supongamos
ademas que $n+m\geq1$. Entonces denotaremos con $F_{(i)}$ a la funcion
$p_{i}^{n,m}\circ F$. Notar que%
\begin{align*}
F_{(i)}  & :D_{F}\subseteq\omega^{k}\times\Sigma^{\ast l}\rightarrow
\omega\text{, para cada }i=1,...,n\\
F_{(i)}  & :D_{F}\subseteq\omega^{k}\times\Sigma^{\ast l}\rightarrow
\Sigma^{\ast}\text{, para cada }i=n+1,...,n+m
\end{align*}
Por lo cual cada una de las funciones $F_{(i)}$ son $\Sigma$-mixtas. Ademas
notese que%
\[
F=[F_{(1)},...,F_{(n+m)}]
\]


\label{wiki-predicado._sigma._-mixto}

\label{wiki-predicados._sigma._-mixtos}

\subsubsection{Predicados $\Sigma$-mixtos}

Un \textit{predicado }$\Sigma$-\textit{mixto }es una funcion $f$ la cual es
$\Sigma$-mixta y ademas cumple que $I_{f}\subseteq\{0,1\}$. Por ejemplo%
\[%
\begin{array}
[c]{rll}%
\omega\times\omega & \rightarrow & \omega\\
(x,y) & \rightarrow & \left\{
\begin{array}
[c]{l}%
1\text{ si }x=y\\
0\text{ si }x\neq y
\end{array}
\right.
\end{array}
\ \ \ \ \ \ \ \ \ \ \
\begin{array}
[c]{rll}%
\{1,2,3,4,5\}\times\Sigma^{\ast} & \rightarrow & \omega\\
(x,\alpha) & \rightarrow & \left\{
\begin{array}
[c]{l}%
1\text{ si }x=\left\vert \alpha\right\vert \\
0\text{ si }x\neq\left\vert \alpha\right\vert
\end{array}
\right.
\end{array}
\]


\paragraph{Operaciones logicas entre predicados}

Dados predicados $P:S\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow
\{0,1\}$ y $Q:S\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow\{0,1\}$,
con el mismo dominio, definamos nuevos predicados $(P\vee Q)$, $(P\wedge Q)$ y
$\lnot P$ de la siguiente manera%
\[%
\begin{array}
[c]{rll}%
(P\vee Q):S & \rightarrow & \omega\\
(\vec{x},\vec{\alpha}) & \rightarrow & \left\{
\begin{array}
[c]{lll}%
1 &  & \text{si }P(\vec{x},\vec{\alpha})=1\text{ o }Q(\vec{x},\vec{\alpha
})=1\\
0 &  & \text{caso contrario}%
\end{array}
\right.
\end{array}
\]%
\[%
\begin{array}
[c]{rll}%
(P\wedge Q):S & \rightarrow & \omega\\
(\vec{x},\vec{\alpha}) & \rightarrow & \left\{
\begin{array}
[c]{lll}%
1 &  & \text{si }P(\vec{x},\vec{\alpha})=1\text{ y }Q(\vec{x},\vec{\alpha
})=1\\
0 &  & \text{caso contrario}%
\end{array}
\right.
\end{array}
\]%
\[%
\begin{array}
[c]{rll}%
\lnot P:S & \rightarrow & \omega\\
(\vec{x},\vec{\alpha}) & \rightarrow & \left\{
\begin{array}
[c]{lll}%
1 &  & \text{si }P(\vec{x},\vec{\alpha})=0\\
0 &  & \text{si }P(\vec{x},\vec{\alpha})=1
\end{array}
\right.
\end{array}
\]


\subsubsection{Familias $\Sigma$-indexadas de funciones}

Dado un alfabeto $\Sigma$, una \textit{familia }$\Sigma$-\textit{indexada de
funciones} sera una funcion $\mathcal{G}$ tal que $D_{\mathcal{G}}=\Sigma$ y
para cada $a\in D_{\mathcal{G}}$ se tiene que $\mathcal{G}(a)$ es una funcion.
Algunos ejemplos:

\begin{enumerate}
\item[E$_{1}$] Sea $\mathcal{G}$ dada por%
\[%
\begin{array}
[c]{rcl}%
\mathcal{G}:\{\square,\%,\blacktriangle\} & \rightarrow & \{Suc,Pred\}\\
\square & \rightarrow & Suc\\
\% & \rightarrow & Suc\\
\blacktriangle & \rightarrow & Pred
\end{array}
\]
Claramente $\mathcal{G}$ es una familia $\{\square,\%,\blacktriangle
\}$-indexada de funciones. Notar que%
\[
\mathcal{G}=\{(\square,Suc),(\%,Suc),(\blacktriangle,Pred)\}
\]
Se tiene tambien por ejemplo que $\mathcal{G}(\%)=Suc$ por lo cual tambien es
cierto que $\mathcal{G}(\%)(22)=23$, etc.

\item[E$_{2}$] Si $\Sigma$ es un alfabeto no vacio, la funcion%
\[%
\begin{array}
[c]{rcl}%
\mathcal{G}:\Sigma & \rightarrow & \{f:f\text{ es una funcion de }\Sigma
^{\ast}\text{ en }\Sigma^{\ast}\}\\
a & \rightarrow & d_{a}%
\end{array}
\]
es una familia $\Sigma$-indexada de funciones. Notar que%
\[
\mathcal{G}=\{(a,d_{a}):a\in\Sigma\}
\]


\item[E$_{3}$] $\emptyset$ es una flia $\emptyset$-indexada de funciones
\end{enumerate}

Si $\mathcal{G}$ es una familia $\Sigma$-indexada de funciones, entonces para
$a\in\Sigma$, escribiremos $\mathcal{G}_{a}$ en lugar de $\mathcal{G}(a)$.

\label{wiki-conjunto._:sigma._-mixto}

\label{wiki-conjuntos._:sigma._-mixtos}

\subsection{Conjuntos $\Sigma$-mixtos}

Un conjunto $S$ es llamado $\Sigma$\textit{-mixto} si hay $n,m\in\omega$ tales
que $S\subseteq\omega^{n}\times\Sigma^{\ast m}$. Por ejemplo,%
\[
\{(x,\alpha)\in\omega\times\{\blacktriangle,!\}^{\ast}:\left\vert
\alpha\right\vert =x\}
\]%
\[
\{(0,\blacktriangle\blacktriangle\blacktriangle,\varepsilon
),(1,\%\blacktriangle\%,\blacktriangle\blacktriangle)\}
\]
son conjuntos $\{\blacktriangle,\%,!\}$-mixtos. Tambien notese que $\emptyset$
y $\{\Diamond\}$ son conjuntos $\Sigma$-mixtos, cualesquiera sea el alfabeto
$\Sigma$. Por ultimo el conjunto%

\[
\{(x,\varepsilon,\varepsilon,\varepsilon):x\in\omega\text{ y }x\text{ es
impar}\}
\]
es $\emptyset$-mixto (con $n=1$ y $m=3$).

\bigskip

\subsubsection{El tipo de un conjunto mixto}

Dado un conjunto $\Sigma$-mixto $S$, si $n,m\in\omega$ son tales que
$S\subseteq\omega^{n}\times\Sigma^{\ast m}$, entonces diremos que $S$
\textit{es un conjunto de tipo }$(n,m)$. Notese que si $S\neq\emptyset$,
entonces hay unicos $n,m\in\omega$ tales que $S$ es un conjunto de tipo
$(n,m)$. De esta forma, cuando $S\neq\emptyset$ hablaremos de "el tipo de $S
$" para refererirnos a este unico par $(n,m)$. Tambien es importante notar que
de la definicion anterior sale inmediatemante que $\emptyset$ es un conjunto
de tipo $(n,m)$ cualesquiera sean $n,m\in\omega$, por lo cual cuando hablemos
de EL tipo de un comjunto deberemos estar seguros de que dicho conjunto es no vacio.

Notese que $\omega$ es de tipo $(1,0)$ y $\Sigma^{\ast}$ es de tipo $(0,1) $.

\bigskip

\subsubsection{Conjuntos rectangulares}

Un conjunto $\Sigma$-mixto $S$ es llamado \textit{rectangular }si es de la
forma%
\[
S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m},
\]
con cada $S_{i}\subseteq\omega$ y cada $L_{i}\subseteq\Sigma^{\ast}$. Notar
que todo subconjunto de $\omega$ es rectangular (es el caso $n=1$ y $m=0$).
Tambien $\{\Diamond\}$ es rectangular (es el caso $n=m=0$). Otros ejemplos:

\begin{enumerate}
\item[-] $\mathbf{N}\times\{1,2\}\times\{@@,\varepsilon\}$ es rectangular
(aqui $n=2$ y $m=1$)

\item[-] $\{!!!,!!\}\times\{@@,\varepsilon\}$ es rectangular (aqui $n=0$ y
$m=2$)
\end{enumerate}

Tambien notese que $\emptyset=\emptyset\times\emptyset$ por lo cual
$\emptyset$ es un conjunto rectangular.

El concepto de conjunto rectangular es muy importante en nuestro enfoque.
Aunque en general no habra restricciones acerca del dominio de las funciones y
predicados, nuestra filosofia sera tratar en lo posible que los dominios de
las funciones que utilicemos para hacer nuestro analisis de recursividad de
los distintos paradigmas, sean rectangulares. Aunque en principio puede
pareser que todos los conjuntos son rectangulares, el siguiente lema mostrara
cuan ingenua es esta vision.

\begin{lemma}
Sea $S\subseteq\omega\times\Sigma^{\ast}$. Entonces $S$ es rectangular si y
solo si se cumple la siguiente propiedad:

\begin{enumerate}
\item[(R)] Si $(x,\alpha),(y,\beta)\in S$, entonces $(x,\beta)\in S$
\end{enumerate}
\end{lemma}

\begin{proof}
Ejercicio.
\end{proof}

\bigskip

Supongamos $\Sigma=\{\#,\blacktriangle,\%\}$. Notese que podemos usar el lema
anterior para probar por ejemplo que los siguientes conjuntos no son rectangulares

\begin{enumerate}
\item[-] $\{(0,\#\#),(1,\%\%\%)\}$

\item[-] $\{(x,\alpha):\left\vert \alpha\right\vert =x\}$
\end{enumerate}

Dejamos como ejercicio para el lector enunciar un lema analogo al anterior
pero que caracterice cuando $S\subseteq\omega^{2}\times\Sigma^{\ast3}$ es rectangular.

\bigskip

\subsection{Notacion lambda}

Usaremos la notacion lambda de Church en la forma que se explica a
continuacion. Esta notacion siempre depende de un alfabeto finito previamente
fijado. En general en nuestro lenguaje matematico utilizamos diversas
expresiones las cuales involucran variables que una vez fijadas en sus valores
hacen que la expresion tambien represente un determinado valor

En el contexto de la notacion lambda solo se podran utilizar expresiones con
caracteristicas muy especiales por lo cual a continuacion iremos describiendo
que condiciones tienen que cumplir las expresiones para que puedan ser usadas
en la notacion lambda

\begin{enumerate}
\item[(1)] Solo utilizaremos expresiones que involucran variables numericas,
las cuales se valuaran en numeros de $\omega$, y variables alfabeticas, las
cuales se valuaran en palabras del alfabeto previamente fijado. Las variables
numericas seran seleccionadas de la lista%
\begin{align*}
& x,y,z,w,n,m,k,...\\
& x_{1},x_{2},...\\
& y_{1},y_{2},...\\
& etc
\end{align*}
Las variables alfabeticas seran seleccionadas de la lista%
\begin{align*}
& \alpha,\beta,\gamma,\eta,...\\
& \alpha_{1},\alpha_{2},...\\
& \beta_{1},\beta_{2},...\\
& etc
\end{align*}


\item[(2)] Por ejemplo la expresion:%
\[
x+y+1
\]
tiene dos variables numericas $x$ e $y$ (y ninguna alfabetica). Si le
asignamos a $x$ el valor 2 y a $y$ el valor 45, entonces la expresion $x+y+1$
produce o representa el valor $48=2+45+1$.

\item[(3)] Otro ejemplo, consideremos la expresion%
\[
\left\vert \alpha\beta\right\vert +\left\vert \alpha\right\vert ^{x}%
\]
la cual tiene una variable numerica $x$ y dos variables alfabeticas $\alpha$ y
$\beta$. Supongamos ademas que el alfabeto previamente fijado es $\{@,\%\} $.
Si le asignamos a $x$ el valor 2, a $\alpha$ el valor $@@$ y a $\beta$ el
valor $\%\%\%$, entonces la expresion $\left\vert \alpha\beta\right\vert
+\left\vert \alpha\right\vert ^{x}$ produce o representa el valor $\left\vert
@@\%\%\%\right\vert +\left\vert @@\right\vert ^{2}=9$.

\item[(4)] Para ciertas valuaciones de sus variables la expresion puede no
estar definida. Por ejemplo la expresion%
\[
Pred(\left\vert \alpha\right\vert )
\]
no asume valor o no esta definida cuando el valor asignado a $\alpha$ es
$\varepsilon$. Otro ejemplo, consideremos la expresion%
\[
x/(y-\left\vert \alpha\right\vert )^{2}%
\]
Esta expresion no esta definida o no asume valor para aquellas asignaciones de
valores a sus variables en las cuales el valor asignado a $y$ sea igual a la
longitud del valor asignado a $\alpha$.

\item[(5)] En los ejemplos anteriores las expresiones producen valores
numericos pero tambien trabajaremos con expresiones que producen valores
alfabeticos. Por ejemplo la expresion%
\[
\beta^{y}%
\]
tiene una variable numerica, $y$, una variable alfabetica, $\beta$, y una vez
valuadas estas variables produce un valor alfabetico, a saber el resultado de
elevar el valor asignado a la variable $\beta$, a el valor asignado a $y$.

\item[(6)] Una expresion $E$ para poder ser utilizada en la notacion lambda
relativa a un alfabeto $\Sigma$ debera cumplir alguna de las dos siguientes propiedades

\begin{enumerate}
\item los valores que asuma $E$ cuando hayan sido asignados valores de
$\omega$ a sus variables numericas y valores de $\Sigma^{\ast}$ a sus
variables alfabeticas deberan ser siempre elementos de $\omega$

\item los valores que asuma $E$ cuando hayan sido asignados valores de
$\omega$ a sus variables numericas y valores de $\Sigma^{\ast}$ a sus
variables alfabeticas deberan ser siempre elementos de $\Sigma^{\ast}$.
\end{enumerate}

\item[(7)] Por ejemplo la expresion%
\[
x/2
\]
no cumple la propiedad dada en (6) ya que para ciertos valores de $\omega$
asignados a la variable $x$, la expresion da valores numericos que se salen de
$\omega$ por lo cual no cumple ni (a) ni (b).

\item[(8)] Otro ejemplo, si el alfabeto fijado es $\Sigma=\{@,\%\}$, entonces
la expresion%
\[
@^{x}\$^{y}%
\]
no cumple la propiedad dada en (6) ya que por ejemplo cuando le asignamos a
$x$ el valor 2 y a $y$ el valor 6, la expresion nos da la palabra
$@@\$\$\$\$\$\$$ la cual no pertenece a $\Sigma^{\ast}$ por lo cual no cumple
ni (a) ni (b).

\item[(9)] No necesariamente las expresiones que usaremos en la notacion
lambda deben ser hechas como combinacion de operaciones matematicas conocidas.
Muchas veces usaremos expresiones que involucran incluso lenguaje coloquial
castellano. Por ejemplo la expresion%
\[
\mathrm{el\ menor\ numero\ primo\ que\ es\ mayor\ que\ }x
\]
Es claro que esta expresion para cada valor de $\omega$ asignado a la variable
$x$ produce o representa un valor concreto de $\omega$. Otro ejemplo:%
\[
\mathrm{el\ tercer\ simbolo\ de\ }\alpha
\]
notese que esta expresion, una ves fijado un alfabeto $\Sigma$, estara
definida o producira un valor solo cuando le asignamos a $\alpha$ una palabra
de $\Sigma^{\ast}$ de longitud mayor o igual a $3$.

\item[(10)] \textbf{Expresiones Booleanas.} A las expresiones Booleanas tales
como%
\[
x=y+1\text{ y }\left\vert \alpha\right\vert \leq22
\]

\end{enumerate}

las pensaremos que asumen valores del conjunto $\{0,1\}\subseteq\omega$. Por
ejemplo la expresion anterior asume o produce el valor $1$ cuando le asignamos
a $x$ el valor 11, a $y$ el valor 10 y a $\alpha$ la palabra $\varepsilon$.
Las expresiones Booleanas pensadas de esta forma podran ser utilizadas en la
notacion lambda si es que tambien cumplen con las anteriores condiciones.

\begin{enumerate}
\item[(11)] La expresion%
\[
5
\]
no tiene variables por lo cual pensaremos que siempre produce el valor $5$
cualesquiera sean los valores asignados a las variables.
\end{enumerate}

\bigskip

\subsubsection{Expresiones lambdificables con respecto a $\Sigma$}

Dado un alfabeto $\Sigma$ a las expresiones que cumplan las caracteristicas
dadas anteriormente las llamaremos \textit{lambdificables con respecto a
}$\Sigma$. Notese que este concepto es intuitivo y no un concepto
matematicamente definido en forma precisa. Mas aun el concepto de expresion
tampoco ha sido definido matematicamente (aunque obviamente si sabemos que una
expresion es una palabra de cierto alfabeto). Esto no nos traera problemas
para el uso notacional que las utilizaremos. Recien en las secciones de logica
veremos la matematizacion de ciertas expresiones (no las lambdificables) y nos
servira de ejemplo para imaginar como podriamos matematizar el concepto de expresion.

Algunos ejemplos:

\begin{enumerate}
\item[(E1)] $x/2$ no es lambdificable con respecto a $\Sigma$ cualesquiera sea
$\Sigma$

\item[(E2)] $@^{x}\$^{y}$ es lambdificable con respecto a $\{@,\$\}$ y no es
lambdificable con respecto a $\{@,\#,\%\}$

\item[(E3)] $x=y+1$ es lambdificable con respecto a $\Sigma$ cualesquiera sea
$\Sigma$

\item[(E4)] la expresion%
\[
\mathrm{el\ menor\ numero\ primo\ que\ es\ mayor\ que\ }x^{\left\vert
\beta\right\vert }%
\]
es lambdificable con respecto a $\Sigma$ cualesquiera sea $\Sigma$

\item[(E5)] la expresion%
\[
5
\]
es lambdificable con respecto a $\Sigma$ cualesquiera sea $\Sigma$
\end{enumerate}

\bigskip

\subsubsection*{Definicion de $\lambda x_{1}...x_{n}\alpha_{1}...\alpha
_{m}\left[  E\right]  $}

Supongamos ya hemos fijado un alfabeto finito $\Sigma$ y supongamos $E$ es una
expresion la cual es lambdificable con respecto a $\Sigma$. Sea $x_{1}%
,...,x_{n},\alpha_{1},...,\alpha_{m}$ una lista de variables todas distintas
tal que las variables numericas que ocurren en $E$ estan todas contenidas en
la lista $x_{1},...,x_{n}$ y las variables alfabeticas que ocurren en $E$
estan en la lista $\alpha_{1},...,\alpha_{m}$\ (puede suceder que haya
variables de la lista $x_{1},...,x_{n},\alpha_{1},...,\alpha_{m}$ las cuales
no ocurran en $E$). Entonces%
\[
\lambda x_{1}...x_{n}\alpha_{1}...\alpha_{m}\left[  E\right]
\]
denotara la funcion definida por:

\begin{enumerate}
\item[(L1)] El dominio de $\lambda x_{1}...x_{n}\alpha_{1}...\alpha_{m}\left[
E\right]  $ es el conjunto de las $(n+m)$-uplas $(k_{1},...,k_{n},\beta
_{1},...,\beta_{m})\in\omega^{n}\times\Sigma^{\ast m}$ tales que $E$ esta
definida cuando le asignamos a cada $x_{i}$ el valor $k_{i}$ y a cada
$\alpha_{i}$ el valor $\beta_{i}$.

\item[(L2)] $\lambda x_{1}...x_{n}\alpha_{1}...\alpha_{m}\left[  E\right]
(k_{1},...,k_{n},\beta_{1},...,\beta_{m})=$ valor que asume o representa $E $
cuando le asignamos a cada $x_{i}$ el valor $k_{i}$ y a cada $\alpha_{i} $ el
valor $\beta_{i}$.
\end{enumerate}

\bigskip

Notese que por tener $E$ la propiedad (6) de mas arriba, la funcion $\lambda
x_{1}...x_{n}\alpha_{1}...\alpha_{m}\left[  E\right]  $ es $\Sigma$-mixta de
tipo $(n,m,s)$ para algun $s\in\{\#,\ast\}$. Algunos ejemplos:

\begin{enumerate}
\item[(a)] Supongamos fijamos el alfabeto $\Sigma=\{@,?,$\textexclamdown $\}
$. Entonces $\lambda x\alpha\left[  \alpha^{2x}\right]  $ es la funcion%
\[%
\begin{array}
[c]{rll}%
\omega\times\{@,?,\text{\textexclamdown }\}^{\ast} & \rightarrow &
\{@,?,\text{\textexclamdown }\}^{\ast}\\
(x,\alpha) & \rightarrow & \alpha^{2x}%
\end{array}
\]
Aqui el lector puede notar la dependencia de la notacion lambda respecto del
alfabeto fijado. Si en lugar de fijar $\Sigma=\{@,?,$\textexclamdown $\}$
hubieramos fijado $\Sigma=\{\%\}$, entonces $\lambda x\alpha\left[
\alpha^{2x}\right]  $ denotaria otra funcion, a saber%
\[%
\begin{array}
[c]{rll}%
\omega\times\{\%\}^{\ast} & \rightarrow & \{\%\}^{\ast}\\
(x,\alpha) & \rightarrow & \alpha^{2x}%
\end{array}
\]


\item[(b)] Supongamos fijamos el alfabeto $\Sigma=\{@,?,$\textexclamdown $\}
$. Entonces $\lambda x\alpha\left[  5\right]  $ es la funcion%
\[%
\begin{array}
[c]{rll}%
\omega\times\{@,?,\text{\textexclamdown }\}^{\ast} & \rightarrow & \omega\\
(x,y,z,\alpha) & \rightarrow & 5
\end{array}
\]


\item[(c)] Supongamos fijamos el alfabeto $\Sigma=\{\%,!\}$. Entonces
$\lambda\alpha\beta\left[  \alpha\beta\right]  $ es la funcion%
\[%
\begin{array}
[c]{rll}%
\{\%,!\}^{\ast}\times\{\%,!\}^{\ast} & \rightarrow & \{\%,!\}^{\ast}\\
(\alpha,\beta) & \rightarrow & \alpha\beta
\end{array}
\]


Tambien tenemos que $\lambda\beta\alpha\left[  \alpha\beta\right]  $ es la
funcion%
\[%
\begin{array}
[c]{rll}%
\{\%,!\}^{\ast}\times\{\%,!\}^{\ast} & \rightarrow & \{\%,!\}^{\ast}\\
(\beta,\alpha) & \rightarrow & \alpha\beta
\end{array}
\]
Notese que estas funciones son distintas. Por ejemplo $\lambda\alpha
\beta\left[  \alpha\beta\right]  (\%,!)=\%!$ y $\lambda\beta\alpha\left[
\alpha\beta\right]  (\%,!)=!\%$

\item[(d)] Independientemente de quien sea $\Sigma$ el alfabeto previamente
fijado, tenemos que $\lambda xy[x+y]$ es la funcion%
\[%
\begin{array}
[c]{rll}%
\omega^{2} & \rightarrow & \omega\\
(x,y) & \rightarrow & x+y
\end{array}
\]
Tambien $\lambda xyzw[x+w]$ es la funcion%
\[%
\begin{array}
[c]{rll}%
\omega^{4} & \rightarrow & \omega\\
(x,y,z,w) & \rightarrow & x+w
\end{array}
\]


\item[(e)] Supongamos fijamos el alfabeto $\Sigma=\{@,?,$\textexclamdown $\}
$. Entonces por la clausula (L1) tenemos que el dominio de la funcion $\lambda
xy\alpha\beta\left[  Pred(\left\vert \alpha\right\vert )+Pred(y)\right]  $ es%
\[
D=\left\{  (x,y,\alpha,\beta)\in\omega^{2}\times\Sigma^{\ast2}:\left\vert
\alpha\right\vert \geq1\text{ y }y\geq1\right\}
\]
Es decir que $\lambda xy\alpha\beta\left[  Pred(\left\vert \alpha\right\vert
)+Pred(y)\right]  $ es la funcion%
\[%
\begin{array}
[c]{rll}%
D & \rightarrow & \omega\\
(x,y,\alpha,\beta) & \rightarrow & Pred(\left\vert \alpha\right\vert )+Pred(y)
\end{array}
\]


\item[(f)] Atentos a (10) de mas arriba, la funcion $\lambda xy\left[
x=y\right]  $ es el predicado%
\[%
\begin{array}
[c]{rll}%
\omega\times\omega & \rightarrow & \omega\\
(x,y) & \rightarrow & \left\{
\begin{array}
[c]{l}%
1\text{ si }x=y\\
0\text{ si }x\neq y
\end{array}
\right.
\end{array}
\]
y $\lambda x\alpha\left[  Pred(x)=\left\vert \alpha\right\vert \right]  $ es
el predicado%
\[%
\begin{array}
[c]{rll}%
\mathbf{N}\times\Sigma^{\ast} & \rightarrow & \omega\\
(x,\alpha) & \rightarrow & \left\{
\begin{array}
[c]{l}%
1\text{ si }Pred(x)=\left\vert \alpha\right\vert \\
0\text{ si }Pred(x)\neq\left\vert \alpha\right\vert
\end{array}
\right.
\end{array}
\]
Tambien $\lambda\alpha\beta\left[  \alpha=\beta\right]  $ es el predicado%
\[%
\begin{array}
[c]{rll}%
\Sigma^{\ast}\times\Sigma^{\ast} & \rightarrow & \omega\\
(\alpha,\beta) & \rightarrow & \left\{
\begin{array}
[c]{l}%
1\text{ si }\alpha=\beta\\
0\text{ si }\alpha\neq\beta
\end{array}
\right.
\end{array}
\]


\item[(g)] Notar que para $S\subseteq\omega^{n}\times\Sigma^{\ast m}$ se tiene
que $\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}}=\lambda x_{1}...x_{n}%
\alpha_{1}...\alpha_{m}\left[  (\vec{x},\vec{\alpha})\in S\right]  $

\item[(h)] Como dijimos, la notacion lambda depende del alfabeto previamnete
fijado, aunque para el caso en que la lista de variables que sigue a la letra
$\lambda$ no tenga variables alfabeticas, la funcion representada no depende
del alfabeto
\end{enumerate}

\bigskip

\textbf{Un par de ejemplos sutiles}

\begin{enumerate}
\item[(a)] La expresion%
\[
Suc
\]
no es lambdificable respecto de cualquier alfabeto $\Sigma$. Esto es porque si
bien cualesquiera sea el valor asignado a las variables, ella asume el valor
$Suc$, no cumple (6) de mas arriba ya que $Suc$ no es un elemento de $\omega$
ni tampoco una palabra (es una funcion!)

\item[(b)] La expresion%
\[
Suc+(\left\vert \beta\right\vert +1)
\]
es lambdificable con respecto a $\Sigma$ cualesquiera sea $\Sigma$. Por
ejemplo $\lambda x\beta\lbrack Suc+(\left\vert \beta\right\vert +1)]$ es la
funcion $\emptyset$, ya que la expresion $Suc+(\left\vert \beta\right\vert
+1)$ cualesquiera sean los valores de $x$ y $\beta$ no esta definida.
\end{enumerate}

\bigskip

\subsection{Ordenes naturales sobre $\Sigma^{\ast}$}

En esta seccion daremos biyecciones naturales entre $\Sigma^{\ast}$ y $\omega
$, para cada alfabeto no vacio $\Sigma$. Dichas biyecciones dependen de tener
asociado a $\Sigma$ un orden total. Primero haremos un caso particular pero
que tiene un interes extra ya que esta emparentado con nuestra notacion
decimal clasica de los numeros de $\omega$.

\subsubsection{Notacion decimal sin $0$}

Llamaremos \textit{numerales} a los siguientes simbolos%
\[
0\ 1\ 2\ 3\ 4\ 5\ 6\ 7\ 8\ 9
\]
Usaremos $Num$ para denotar el conjunto de numerales. Notese que
$Num\cap\omega=\emptyset$. Es decir, no debemos confundir los simbolos que
usualmente denotan los primeros diez numeros enteros con los numeros que ellos
denotan. De hecho en china o japon los primeros diez numeros enteros se
denotan con otros simbolos. Similarmente las palabras pertenecientes a
$Num^{\ast}$ denotan (notacion decimal) a los numeros de $\omega$ pero debemos
tener en cuenta que $Num^{\ast}\cap\omega=\emptyset$. Cuando tratamos con
palabras de $Num^{\ast}$, debemos ser cuidadosos ya que muchas veces en
nuestro discurso matematico (es decir las guias, el apunte, lo que escriben
los profesores en el pizarron, etc) representamos dos objetos diferentes de la
misma forma. Por ejemplo $45$ puede estar denotando al numero entero cuarenta
y cinco o tambien $45$ puede estar denotando la palabra de longitud $2$ cuyo
primer simbolo es el numeral $4$ y cuyo segundo simbolo es el numeral $5$, es
decir ella misma. Por dar otro ejemplo, el simbolo $1$ en nuestro discurso
algunas veces se denotara a si mismo y otras veces denotara al numero uno.

Es bien conocido que, en notacion decimal, las siguientes palabras del
alfabeto $Num$, denotan, de menor a mayor, a los numeros de $\omega$%
\[
0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,...
\]
Por supuesto esta lista de palabras es infinita pero asumimos que el lector
sabe como obtener la palabra siguiente a cada miembro de la lista (i.e. sumar
1 en notacion decimal), lo cual determina por completo la lista conociendo que
la misma comienza con la palabra $0$.

Cabe destacar que debido a la presencia del numeral $0$ en la lista, la
$n$-esima palabra representa o denota al numero $n-1$ o, dicho de otra forma,
el numero $n\in\omega$ es representado por la $(n+1)$-esima palabra de la lista.

Un detalle de la representacion decimal de numeros de $\omega$ mediante
palabras de $Num^{\ast}$ es que la misma no nos da una biyeccion entre
$Num^{\ast}$ y $\omega$ ya que por ejemplo las palabras $00016$ y $16$
representan el mismo numero. Dicho de otra forma en la lista anterior no
figuran todas las palabras de $Num^{\ast}$, a saber estan omitidas todas las
palabras que comienzan con el simbolo $0$ y tienen longitud mayor que uno. A
continuacion daremos una representacion de los numeros de $\omega$ mediante
palabras, la cual no tendra este problema. El alfabeto que usaremos tendra
todos los numerales menos el $0$ y ademas tendra un simbolo para denotar al
numero diez, a saber el simbolo $d$. Es decir%
\[
\widetilde{Num}=\{1,2,3,4,5,6,7,8,9,d\}
\]
Representaremos a los numeros de $\omega$ con la siguiente lista infinita de
palabras de $\widetilde{Num}$

$\bigskip$

$\varepsilon,1,2,3,4,5,6,7,8,9,d,$

$11,12,...,1d,21,22,...,2d,...,91,92,...,9d,d1,d2,...,dd,$

$111,112,...,11d,121,122,...,12d,...$

\bigskip

\noindent El lector ya se habra dado cuenta de que el siguiente a una palabra
$\alpha$ de la lista anterior se obtiene aplicando las siguientes clausulas

\begin{enumerate}
\item[C$_{1}$] si $\alpha=d^{n}$, con $n\geq0$ entonces el siguiente de
$\alpha$ es $1^{n+1}$

\item[C$_{2}$] si $\alpha$ no es de la forma $d^{n}$, con $n\geq0$, entonces
el siguiente de $\alpha$ se obtiene de la siguiente manera:

\begin{enumerate}
\item buscar de derecha a izquierda el primer simbolo no igual a $d$

\item reemplazar dicho simbolo por su siguiente en la lista
$1,2,3,4,5,6,7,8,9,d$

\item reemplazar por el simbolo $1$ a todos los simbolos iguales a $d$ que
ocurrian a la derecha del simbolo reemplazado
\end{enumerate}
\end{enumerate}

\noindent Notese que

\begin{enumerate}
\item[-] El numero $0$ es representado en la lista anterior con la palabra
$\varepsilon$

\item[-] El numero $1$ es representado en la lista anterior con la palabra $1
$

$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \vdots$

\item[-] El numero $9$ es representado en la lista anterior con la palabra $9
$

\item[-] El numero $10$ es representado en la lista anterior con la palabra
$d$

\item[-] El numero $11$ es representado en la lista anterior con la palabra
$11$

$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \vdots$

\item[-] El numero $19$ es representado en la lista anterior con la palabra
$19$

\item[-] El numero $20$ es representado en la lista anterior con la palabra
$1d$

\item[-] El numero $21$ es representado en la lista anterior con la palabra
$21$

\item[-] El numero $22$ es representado en la lista anterior con la palabra
$22$

$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \vdots$
\end{enumerate}

\noindent Como puede notarse en estos primeros veinte y pico numeros solo dos
(el $0$ y el $20$) se representan en forma distinta a la reprentacion decimal
clasica. Es natural que $\varepsilon$ denote al numero $0$ y ademas notese que
la palabra $1d$ (que en la lista representa el $20$) puede leerse como
"diecidiez" (es decir la palabra que sigue a "diecinueve") que justamente es
$20$. Por supuesto con esta manera de pensar la palabra $2d$ deberiamos leerla
como "ventidiez" y si nos fijamos en la lista ella representa al numero
treinta lo cual nuevamente es muy natural. Otro ejemplo: a $6d$ deberiamos
leerla como "sesentidiez" y es natural ya que en la lista representa al
setenta. Tambien, la palabra $9d$ puede leerse noventidiez ya que representa
en la lista al numero $100$.

La lista anterior va representando los numeros de $\omega$ en forma muy
natural pero aunque nuestra intuicion nos diga que no, en principio podria
pasar que una misma palabra del alfabeto $\widetilde{Num}$ ocurra dos veces en
la lista y esto nos diria que una misma palabra estaria representando a dos
numeros distintos. Tambien, en principio podria suceder que haya una palabra
del alfabeto $\widetilde{Num}$ la cual nunca figure en la lista. Mas abajo
probaremos que estas dos posibilidades no suceden, es decir muestran que

\begin{enumerate}
\item[(S)] Toda palabra de $\widetilde{Num}^{\ast}$ aparece en la lista

\item[(I)] Ninguna palabra de $\widetilde{Num}^{\ast}$ aparece mas de una ves
\end{enumerate}

\noindent Notese que la propiedad (S) nos dice que la funcion%
\[%
\begin{array}
[t]{rll}%
\ast:\omega & \rightarrow & \widetilde{Num}^{\ast}\\
n & \rightarrow & (n+1)\text{-esimo elemento de la lista}%
\end{array}
\]
es sobreyectiva y la propiedad (I) nos garantiza que dicha funcion es
inyectiva, por lo cual entre las dos nos garantizan que dicha representacion
establece una biyeccion entre $\omega$ y $\widetilde{Num}^{\ast}$.

Por supuesto, la pregunta que inmediatamente surge es como calcular la inversa
de $\ast$. Llamemos $\#$ a la inversa de $\ast$. Notese que dada una palabra
$\alpha\in\widetilde{Num}^{\ast}$, el numero $\#(\alpha)$ es justamente el
numero representado por la palabra $\alpha$, o dicho de otra forma
$\#(\alpha)$ es la posicion que ocupa $\alpha$ en la lista, contando desde el
$0$ (es decir $\alpha$ es la $(\#(\alpha)+1)$-esima palabra de la lista). Por
ejemplo:%
\begin{gather*}
\#(\varepsilon)=0\\
\#(1)=1\\
\vdots\\
\#(9)=9\\
\#(d)=10\\
\#(11)=11\\
\#(12)=12\\
\vdots\\
\#(19)=19\\
\#(1d)=20
\end{gather*}
Aqui hay que tener cuidado como leemos las igualdades anteriores. Por ejemplo
en la igualdad%
\[
\#(1)=1
\]
la primera ocurrencia del simbolo $1$ se refiere al numeral uno, es decir
denota una palabra y la segunda ocurrencia se esta refiriendo al numero uno,
es decir denota un numero.

Dejamos al lector el ejercicio de ganar intuicion con ejemplos hasta que se
convensa de que tal como en el caso de la notacion decimal, el numero
$\#(\alpha)$ se expresa como una suma de potencias de $10$, con los
coeficientes dados en funcion de los simbolos de $\alpha$. Mas concretamente
si $\alpha=s_{1}s_{2}...s_{k}$ con $k\geq1$ y $s_{1},s_{2},...,s_{k}%
\in\widetilde{Num}$, entonces%
\[
\#(\alpha)=\#(s_{1}).10^{k-1}+\#(s_{2}).10^{k-2}+...+\#(s_{k}).10^{0}%
\]
No daremos aqui una prueba de este hecho ya que lo probaremos abajo para el
caso general. Para ganar intuicion sobre el mismo el lector puede ver mas
abajo la prueba de las propiedades (S) e (I), desde donde se ve con mas
claridad como va aumentando la funcion $\#$ a medida que recorremos la lista
de izquierda a derecha. Algunos ejemplos%
\begin{align*}
\#(1d)  & =1.10^{1}+10.10^{0}=10+10=20\\
\#(dd)  & =10.10^{1}+10.10^{0}=100+10=110\\
\#(111)  & =1.10^{2}+1.10^{1}+1.10^{0}=100+10+1=111\\
\#(1d3d)  & =1.10^{3}+10.10^{2}+3.10^{1}+10.10^{0}%
\end{align*}


Ahora que sabemos que las palabras de $\widetilde{Num}$ representan los
numeros como suma de potencias de diez, en forma analoga a la notacion decimal
clasica, podemos refozar aun mas la analogia poniendo nombres adecuados que,
tal como en el caso clasico, nos permitan leer las palabras de $\widetilde
{Num}$ describiendo su suma de potencias asociada. Por ejemplo podriamos
llamar "decenta" al numero $100$, por analogia a "treinta",
"cuarenta",...,"noventa". O sea una decenta es diez veces diez. De esta forma
la palabra $d1$ se leera "decenta y uno" y esto es natural ya que en la lista
representa al $101$. La palabra $dd$ se leera "decenta y diez" y esto describe
a la perfeccion el numero que representa, i.e. el $10.10+10=110 $. La palabra
que sigue en la lista a $dd$ es $111$ la cual representa al $111$, es decir
aqui como en los otros casos vistos en los cuales no hay ocurrencias del
simbolo $d$ la palabra representa al mismo numero que representa en la
notacion decimal clasica. Por dar otro ejemplo, la palabra $59d3$ se leera
"cinco mil novecientos decenta y tres" y representara al numero $6003$.

Para seguir debemos ponerle nombre a "diez veces cien", es decir, "decientos"
(por analogia con "novecientos = nueve veces cien") denotara al numero
$1000=10.100$. De esta forma la palabra $d51$ se leera "decientos cincuenta y
uno" y esto es natural ya que pensando un rato se puede ver que ella
representa al $1051$. Tambien, la palabra $ddd$ se leera "decientos decenta y
diez" y representara al numero $1110$.

\bigskip

\paragraph{Prueba de las propiedades (S) e (I)}

Dado que el siguiente a un elemento $\alpha$ de la lista es de la misma
longitud que $\alpha$ o tiene longitud igual a $\left\vert \alpha\right\vert
+1$, podemos representar la lista anterior de la siguiente manera:%
\[
B_{0};B_{1};B_{2};B_{3};B_{4};...
\]
donde cada $B_{n}$ es, por definicion, la parte de la lista en la cual las
palabras tienen longitud exactamente $n$. Por ejemplo:

\begin{enumerate}
\item[-] $B_{0}$ es $\varepsilon$

\item[-] $B_{1}$ es $1,2,3,4,5,6,7,8,9,d$

\item[-] $B_{2}$ es $11,12,...,1d,21,22,...,2d,...,91,92,...,9d,d1,d2,...,dd$
\end{enumerate}

Notese que hasta el momento nada nos asegura que no suceda que para algun $n$
se de que $B_{n}$ sea una lista infinita, lo cual ademas nos diria que los
bloques $B_{n+1},B_{n+2},...$ son todos vacios. Es decir podria pasar que la
lista se estanque en una longitud $n$ y nunca aparezca una palabra de longitud
mayor que $n$. Esto por supuesto obligaria a que se repitan muchas veces
palabras de dicha longitud $n$ ya que hay una cantidad finita de las mismas
($10^{n}$).

Por supuesto nuestra intuicion nos dice que en el bloque $B_{n}$ estan
listadas sin repeticion todas las palabras de $\widetilde{Num}^{\ast}$ de
longitud $n$, pero debemos justificar esto con argumentos solidos. Algunas
propiedades basicas que se pueden probar facilmente son:

\begin{enumerate}
\item[(1)] Si $B_{n}=\alpha_{1},...,\alpha_{k}$, entonces $\alpha_{1}=1^{n}$ y
$\alpha_{k}=d^{n}$

\item[(2)] Si $d^{n}$ ocurre en $B_{n}$ lo hace en la ultima posicion
\end{enumerate}

\noindent estas propiedades son consecuencias inmediatas de como se calcula el
elemento siguiente a uno dado en la lista y son dejadas como ejercicio. Otra
propiedad importante es la siguiente

\begin{enumerate}
\item[(3)] Si $B_{n}=\alpha_{1},...,\alpha_{k}$, entonces $B_{n+1}=1\alpha
_{1},...,1\alpha_{k},2\alpha_{1},...,2\alpha_{k},...,d\alpha_{1}%
,...,d\alpha_{k}$
\end{enumerate}

Para probar (3) es muy util el siguiente resultado obvio

\begin{lemma}
Sea $\sigma\in\widetilde{Num}$ y supongamos $\alpha\in\widetilde{Num}^{\ast}$
no es de la forma $d^{n}$. Entonces el siguiente a $\sigma\alpha$ es
$\sigma\beta$ donde $\beta$ es el siguiente a $\alpha$
\end{lemma}

Dejamos como ejercicio al lector hacer la prueba de (3) usando el lema
anterior y las propiedades (1) y (2). Ahora es facil usando (3) probar
inductivamente que

\begin{enumerate}
\item[(4)] $B_{n}$ es una lista sin repeticiones de todas las palabras de
longitud $n$
\end{enumerate}

Pero claramente de (4) se desprenden en forma obvia las propiedades (S) y (I).

\subsubsection{El caso general}

Sea $\Sigma$ un alfabeto no vacio y supongamos $\leq$ es un orden total sobre
$\Sigma$. Supongamos que $\Sigma=\{a_{1},...,a_{n}\}$, con $a_{1}%
<a_{2}<...<a_{n}$. Inspirados en la lista dada anteriormente de las palabras
de $\widetilde{Num}^{\ast}$, podemos dar la siguiente lista de palabras de
$\Sigma^{\ast}$

\bigskip

${\small \varepsilon,a}_{1}{\small ,a}_{2}{\small ,...,a}_{n}{\small ,}$

${\small a}_{1}{\small a}_{1}{\small ,a}_{1}{\small a}_{2}{\small ,...,a}%
_{1}{\small a}_{n}{\small ,a}_{2}{\small a}_{1}{\small ,a}_{2}{\small a}%
_{2}{\small ,...,a}_{2}{\small a}_{n}{\small ,...,a}_{n}{\small a}%
_{1}{\small ,a}_{n}{\small a}_{2}{\small ,...,a}_{n}{\small a}_{n}{\small ,}$

${\small a}_{1}{\small a}_{1}{\small a}_{1}{\small ,a}_{1}{\small a}%
_{1}{\small a}_{2}{\small ,...,a}_{1}{\small a}_{1}{\small a}_{n}%
{\small ,a}_{1}{\small a}_{2}{\small a}_{1}{\small ,a}_{1}{\small a}%
_{2}{\small a}_{2}{\small ,...,a}_{1}{\small a}_{2}{\small a}_{n}%
{\small ,...,a}_{1}{\small a}_{n}{\small a}_{1}{\small ,a}_{1}{\small a}%
_{n}{\small a}_{2}{\small ,a}_{1}{\small a}_{n}{\small a}_{n}{\small ,}$

${\small a}_{2}{\small a}_{1}{\small a}_{1}{\small ,a}_{2}{\small a}%
_{1}{\small a}_{2}{\small ,...,a}_{2}{\small a}_{1}{\small a}_{n}%
{\small ,a}_{2}{\small a}_{2}{\small a}_{1}{\small ,a}_{2}{\small a}%
_{2}{\small a}_{2}{\small ,...,a}_{2}{\small a}_{2}{\small a}_{n}%
{\small ,...,a}_{2}{\small a}_{n}{\small a}_{1}{\small ,a}_{2}{\small a}%
_{n}{\small a}_{2}{\small ,a}_{2}{\small a}_{n}{\small a}_{n}{\small ,}$

${\small \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }%
\vdots$

${\small a}_{n}{\small a}_{1}{\small a}_{1}{\small ,a}_{n}{\small a}%
_{1}{\small a}_{2}{\small ,...,a}_{n}{\small a}_{1}{\small a}_{n}%
{\small ,a}_{n}{\small a}_{2}{\small a}_{1}{\small ,a}_{n}{\small a}%
_{2}{\small a}_{2}{\small ,...,a}_{n}{\small a}_{2}{\small a}_{n}%
{\small ,...,a}_{n}{\small a}_{n}{\small a}_{1}{\small ,a}_{n}{\small a}%
_{n}{\small a}_{2}{\small ,a}_{n}{\small a}_{n}{\small a}_{n}{\small ,}$

${\small a}_{1}{\small a}_{1}{\small a}_{1}{\small a}_{1}{\small ,a}%
_{1}{\small a}_{1}{\small a}_{1}{\small a}_{2}{\small ,...}$

\bigskip

\noindent El objetivo es probar que la lista anterior enumera sin repeticiones
todas las palabras de $\Sigma^{\ast}$, i.e. produce naturalmente una biyeccion
entre $\omega$ y $\Sigma^{\ast}$. Pero antes debemos definir mas formalmente
la lista. Para esto definamos $s^{\leq}:\Sigma^{\ast}\rightarrow\Sigma^{\ast}$
de la siguiente manera

\begin{enumerate}
\item[-] $s^{\leq}((a_{n})^{m})=(a_{1})^{m+1}$, para cada $m\geq0$

\item[-] $s^{\leq}(\alpha a_{i}(a_{n})^{m})=\alpha a_{i+1}(a_{1})^{m}$, cada
vez que $\alpha\in\Sigma^{\ast}$, $1\leq i<n$ y $m\geq0$
\end{enumerate}

Notese que la definicion de $s^{\leq}$ es correcta ya que una palabra de
$\Sigma^{\ast}$ ya sea es de la forma $(a_{n})^{m}$, con $m\geq0$, o es de la
forma $\alpha a_{i}(a_{n})^{m}$, con $\alpha\in\Sigma^{\ast}$, $1\leq i<n$ y
$m\geq0$; y estos dos casos posibles son mutuamente excluyentes.

Claramente se tiene entonces que la lista anterior puede ser escrita de la
siguiente manera%
\[
\varepsilon,s^{\leq}(\varepsilon),s^{\leq}(s^{\leq}(\varepsilon)),s^{\leq
}(s^{\leq}(s^{\leq}(\varepsilon))),s^{\leq}(s^{\leq}(s^{\leq}(s^{\leq
}(\varepsilon)))),...
\]
Con esta definicion formal de la lista, podemos probar de la misma forma en la
que lo hicimos arriba para el caso $\Sigma=\widetilde{Num}$ que:

\begin{enumerate}
\item[(S)] Toda palabra de $\Sigma^{\ast}$ aparece en la lista

\item[(I)] Ninguna palabra de $\Sigma^{\ast}$ aparece mas de una ves en la lista
\end{enumerate}

\noindent(dejamos al lector los detalles por tratarse de un argumento
completamente similar).

\bigskip

Definamos $\ast^{\leq}:\omega\rightarrow\Sigma^{\ast}$ recursivamente de la
siguiente manera:

\begin{enumerate}
\item[-] $\ast^{\leq}(0)=\varepsilon$

\item[-] $\ast^{\leq}(i+1)=s^{\leq}(\ast^{\leq}(i))$
\end{enumerate}

\noindent Es claro que entonces $\ast^{\leq}(i)$ nos da el $(i+1)$-esimo
elemento de la lista, o lo que es lo mismo, el $i$-esimo elemento de la lista
contando desde el $0$. O sea que las propiedades (S) y (I) nos garantizan que
la funcion $\ast^{\leq}$ es biyectiva. A continuacion describiremos su
inversa. Primero un lema facil pero muy importante.

\begin{lemma}
Sea $\Sigma$ un alfabeto no vacio y supongamos $\leq$ es un orden total sobre
$\Sigma$. Supongamos que $\Sigma=\{a_{1},...,a_{n}\}$, con $a_{1}%
<a_{2}<...<a_{n}$. Entonces para cada $\alpha\in\Sigma^{\ast}-\{\varepsilon\}$
hay unicos $k\in\omega$ y $i_{0},i_{1},...,i_{k}\in\{1,...,n\}$ tales que%
\[
\alpha=a_{i_{k}}...a_{i_{0}}%
\]

\end{lemma}

\bigskip

Notar que $k$ del lema anterior es $\left\vert \alpha\right\vert -1$ y los
numeros $i_{k},...,i_{0}$ van dando el numero de orden de cada simbolo de
$\alpha$ yendo de izquierda a derecha. Por ejemplo si $\Sigma=\{\%,!,@\}$ y
$\leq$ es el orden total sobre $\Sigma$ dado por $\%<!<@$ (es decir que aqui
$a_{1}=\%$, $a_{2}=!$ y $a_{3}=@$) entonces para la palabra $!\%@\%@$ tenemos
$k=4$ y $i_{4}=2$, $i_{3}=1$, $i_{2}=3$, $i_{1}=1$ y $i_{0}=3$. Sin envargo si
hubieramos tomado el orden dado por $@<\%<!$, para la misma palabra hubieramos
tenido $i_{4}=3$, $i_{3}=2$, $i_{2}=1$, $i_{1}=2$ y $i_{0}=1$.

Ahora podemos definir la funcion $\#^{\leq}$ de la siguiente manera%
\[%
\begin{array}
[t]{rll}%
\#^{\leq}:\Sigma^{\ast} & \rightarrow & \omega\\
\varepsilon & \rightarrow & 0\\
a_{i_{k}}...a_{i_{0}} & \rightarrow & i_{k}n^{k}+...+i_{0}n^{0}%
\end{array}
\]


\begin{lemma}
\label{numeral es la inversa de estrella}La funcion $\#^{\leq}$ es la inversa
de $\ast^{\leq}$
\end{lemma}

\begin{proof}
Primero probaremos por induccion en $x$ que

\begin{enumerate}
\item[(a)] Para cada $x\in\omega$, se tiene que $\#^{\leq}(\ast^{\leq}(x))=x$
\end{enumerate}

\noindent El caso $x=0$ es trivial. Supongamos que $\#^{\leq}(\ast^{\leq
}(x))=x$, veremos entonces que $\#^{\leq}(\ast^{\leq}(x+1))=x+1$. Sean
$k\geq0$ y $i_{k},...,i_{0}$ tales que $\ast^{\leq}(x)=a_{i_{k}}...a_{i_{0}}$.
Ya que $\#^{\leq}(\ast^{\leq}(x))=x$ tenemos que $x=i_{k}n^{k}+...+i_{0}n^{0}%
$. Hay varios casos.

\noindent Caso $i_{0}<n$. Entonces $\ast^{\leq}(x+1)=s^{\leq}(\ast^{\leq
}(x))=a_{i_{k}}...a_{i_{0}+1}$ por lo cual
\[%
\begin{array}
[c]{ll}%
\#^{\leq}(\ast^{\leq}(x+1)) & =i_{k}n^{k}+i_{k-1}n^{k-1}+...+(i_{0}+1)n^{0}\\
& =\left(  i_{k}n^{k}+i_{k-1}n^{k-1}+...+i_{0}n^{0}\right)  +1\\
& =x+1
\end{array}
\]
Caso $i_{k}=i_{k-1}=...=i_{0}=n$. Entonces $\ast^{\leq}(x+1)=s^{\leq}%
(\ast^{\leq}(x))=(a_{1})^{k+2}$ por lo cual%
\[%
\begin{array}
[c]{ll}%
\#^{\leq}(\ast^{\leq}(x+1)) & =1n^{k+1}+1n^{k}+...+1n^{1}+1n^{0}\\
& =\left(  nn^{k}+nn^{k-1}+...+nn^{0}\right)  +1\\
& =x+1
\end{array}
\]
Caso $i_{0}=i_{1}=...=i_{h}=n$, $\;i_{h+1}\not =n$, \ para algun $0\leq h<k$.
Entonces $\ast^{\leq}(x+1)=s^{\leq}(\ast^{\leq}(x))=a_{i_{k}}...a_{i_{h+2}%
}a_{i_{h+1}+1}(a_{1})^{h}$ por lo cual%
\[%
\begin{array}
[c]{ll}%
\#^{\leq}(\ast^{\leq}(x+1)) & =i_{k}n^{k}+...+i_{h+2}n^{h+2}+(i_{h+1}%
+1)n^{h+1}+1n^{h}+...+1n^{1}+1n^{0}\\
& =\left(  i_{k}n^{k}+...+i_{h+2}n^{h+2}+i_{h+1}n^{h+1}+n^{h+1}+n^{h}%
+...+n^{1}\right)  +1\\
& =\left(  i_{k}n^{k}+...+i_{h+2}n^{h+2}+i_{h+1}n^{h+1}+nn^{h}+...+nn^{0}%
\right)  +1\\
& =x+1
\end{array}
\]
De esta forma hemos probado (a).

Por definicion la inversa de $\ast^{\leq}$ es la funcion con dominio
$\Sigma^{\ast}$ que a una palabra $\alpha$ le asocia el unico $x\in\omega$ tal
que $\ast^{\leq}(x)=\alpha$. Es decir debemos probar que

\begin{enumerate}
\item[(b)] $\#^{\leq}(\alpha)=$ unico $x\in\omega$ tal que $\ast^{\leq
}(x)=\alpha$, para cada $\alpha\in\Sigma^{\ast}$
\end{enumerate}

\noindent Pero (b) es una concecuencia inmediata de (a).
\end{proof}

\bigskip

Cabe destacar que dada una palabra $\alpha$, el numero $\#^{\leq}(\alpha)$ nos
dice en que posicion se hubica $\alpha$ en la lista, es decir $\alpha$ es la
($\#^{\leq}(\alpha)+1$)-esima palabra de la lista.

De los desarrollos hechos se desprende el interesante resultado. Dejamos al
lector la prueba como ejercicio.

\begin{lemma}
Sea $n\geq1$ fijo. Entonces cada $x\geq1$ se escribe en forma unica de la
siguiente manera:%
\[
x=i_{k}n^{k}+i_{k-1}n^{k-1}+...+i_{0}n^{0},
\]
con $k\geq0$ y $1\leq i_{k},i_{k-1},...,i_{0}\leq n$.
\end{lemma}

\bigskip

Como hemos visto las biyecciones dadas producen una "identificacion" entre
numeros de $\omega$ y palabras del alfabeto $\Sigma$. Es decir, en algun
sentido identificamos palabras y numeros ya que se corresponden
biunivocamente. Supongamos que $\alpha$ es una palabra de $\Sigma^{\ast
}-\{\varepsilon\}$ y queremos "verla como un numero". Entonces en ves de ver
sus simbolos vemos los ordenes de aparicion en $\Sigma$ de los mismos y
miramos la suma de potencias asociada.

Supongamos ahora que $x$ es un numero de $\omega-\{0\}$ y ademas supongamos
que somos super inteligentes y que cuando vemos a $x$ vemos la secuencia unica
de numeros $i_{k},i_{k-1},...,i_{0}$ que nos permite expresarlo como suma de
potencias segun el lema anterior. Entonces si queremos ver a $x$ como una
palabra simplemente miramos la secuencia $i_{k},i_{k-1},...,i_{0}$ como
palabra, reemplazando cada $i_{j}$ por el simbolo $i_{j}$-esimo de $\Sigma$.

\paragraph{Caracter recursivo de las funciones $s^{\leq}$, $\ast^{\leq}$ y
$\#^{\leq}$}

Es un ejercicio (dejado al lector) probar que cualquiera sea $\alpha\in
\Sigma^{\ast}$, se tiene que%
\begin{align*}
s^{\leq}(\varepsilon)  & =a_{1}\\
s^{\leq}(\alpha a_{i})  & =\alpha a_{i+1}\text{, }i<n\\
s^{\leq}(\alpha a_{n})  & =s^{\leq}(\alpha)a_{1}%
\end{align*}
Notese que esto nos permite calcular recursivamente el valor de $s^{\leq}$ ya
que las ecuaciones anteriores nos muestran como obtener rapidamente $s^{\leq
}(\alpha a)$ en terminos de $s^{\leq}(\alpha)$ y $a$, donde $a$ es un elemento
cualquiera de $\Sigma$. Por supuesto, en algun momento deberemos usar el dato
inicial $s^{\leq}(\varepsilon)=a_{1}$. En un lenguaje de programacion
funcional, las tres ecuaciones anteriores son directamente un programa para
computar $s^{\leq}$ o si se quiere una definicion de dicha funcion. Dejamos al
lector que intente usar las ecuaciones anteriores para dar un programa
imperativo que compute $s^{\leq}$ (esto esta hecho mas adelante en la primera
lista de funciones $\Sigma$-efectivamente computables).

Lo mismo sucede con la funcion $\ast^{\leq}$ la cual fue directamente definida
en forma recursiva por las ecuaciones%
\begin{align*}
\ast^{\leq}(0)  & =\varepsilon\\
\ast^{\leq}(i+1)  & =s^{\leq}(\ast^{\leq}(i))
\end{align*}
Dejamos al lector corroborar que la funcion $\#^{\leq}$ verifica las
siguientes ecuaciones, las cuales obviamente pueden ser usadas para calcular
recursivamente sus valores%

\begin{align*}
\#^{\leq}(\varepsilon)  & =0\\
\#^{\leq}(\alpha a_{i})  & =\#^{\leq}(\alpha).n+i
\end{align*}


\bigskip

\paragraph{Extension del orden total de $\Sigma$ a $\Sigma^{\ast}$}

Podemos extender el orden de $\Sigma$ a $\Sigma^{\ast}$ de la siguiente manera

\begin{enumerate}
\item[-] $\alpha\leq\beta$ sii $\#^{\leq}(\alpha)\leq\#^{\leq}(\beta) $
\end{enumerate}

Es decir $\alpha\leq\beta$ sii $\alpha=\beta$ o $\alpha$ ocurre antes que
$\beta$ en la lista. Dejamos como ejercicio para el lector probar que $\leq$
es un orden total sobre $\Sigma^{\ast}$.

\bigskip

Deberia ser intuitivamente claro que el orden recien definido sobre
$\Sigma^{\ast}$ posee las mismas propiedades matematicas que el orden usual de
$\omega$. Esto se entendera en forma mas profunda cuando veamos el concepto de
isomorfismo de posets en los capitulos de logica. Veamos un ejemplo:

\begin{lemma}
Si $S\subseteq\Sigma^{\ast}$ es no vacio, entonces existe $\alpha\in S$ tal
que $\alpha\leq\beta$, para cada $\beta\in S$.
\end{lemma}

\bigskip

\subsection{Codificacion de infinituplas de numeros}

Usaremos $\omega^{\mathbf{N}}$ para denotar el conjunto de todas las
infinituplas con coordenadas en $\omega$. Es decir%
\[
\omega^{\mathbf{N}}=\left\{  (s_{1},s_{2},...):s_{i}\in\omega\text{, para cada
}i\geq1\right\}  \text{.}%
\]
Definamos el siguiente subconjunto de $\omega^{\mathbf{N}}$%
\[
\omega^{\left[  \mathbf{N}\right]  }=\left\{  (s_{1},s_{2},...)\in
\omega^{\mathbf{N}}:\text{ hay un }n\in\mathbf{N}\text{ tal que }%
s_{i}=0,\text{para }i\geq n\right\}  \text{.}%
\]
Notese que $\omega^{\mathbf{N}}\neq\omega^{\left[  \mathbf{N}\right]  }$, por
ejemplo las infinituplas%

\begin{align*}
& (10,20,30,40,50,...)\\
& (1,0,1,0,1,0,1,0,...)
\end{align*}
no pertenecen a $\omega^{\left[  \mathbf{N}\right]  }$. Notese que
$(s_{1},s_{2},...)\in\omega^{\left[  \mathbf{N}\right]  }$ si y solo si solo
una cantidad finita de coordenadas de $(s_{1},s_{2},...)$ son no nulas (i.e.
$\{i:s_{i}\neq0\}$ es finito).

Definamos%
\[%
\begin{array}
[c]{rll}%
pr:\mathbf{N} & \rightarrow & \omega\\
n & \rightarrow & n\text{-esimo numero primo}%
\end{array}
\]
N\'{o}tese que $pr(1)=2$, $pr(2)=3$, $pr(3)=5$, etc.

Es bien conocido que todo numero natural es expresable como producto de
primos. Por ejemplo si tomamos $x=57596$ tenemos que $x=2.2.7.11.11.17$.
Tambien es un hecho conocido que dicha representacion en producto de primos es
unica, si escribimos a los factores primos de menor a mayor, tal como lo
hicimos recien con el numero $57596$. El Teorema Fundamental de la Aritmetica
justamente acevera esta propiedad de factorisacion unica de todo numero
natural. Trataremos de escribir este teorema de una forma un poco mas "cheta".

Ya que $57596=2.2.7.11.11.17$, podemos escribir%
\[
57596=pr(1)^{2}.pr(4)^{1}.pr(5)^{2}.pr(7)^{1}%
\]
Notese que ahora cada primo que interviene en la factorizacion de $57596$
figura con un exponente que nos dice cuantas veces ocurre en dicha
factorizacion. Hay muchos primos que no ocurren en esta factorizacion, es
decir ocurren $0$ veces en la misma. Pero podemos escribir%
\[
57596=pr(1)^{2}.pr(2)^{0}.pr(3)^{0}.pr(4)^{1}.pr(5)^{2}.pr(6)^{0}%
.pr(7)^{1}.pr(8)^{0}.pr(9)^{0}.pr(10)^{0}....
\]
y la igualdad no se altera ya que agregamos factores iguales a $1$ (una
cantidad infinita!). De esta manera cada primo interviene en la factorizacion.
Ademas si vemos la infinitupla de exponentes de dicha factorizacion, es decir%
\[
(2,0,0,1,2,0,1,0,0,0,...)
\]
obtenemos un elemento de $\omega^{\lbrack\mathbf{N}]}$.

Por supuesto esto lo podemos hacer con cualquier numero natural y siempre la
infinitupla de exponentes sera un elemento de $\omega^{\lbrack\mathbf{N}]}$.
Ademas es facil notar que estas representaciones "chetas" tambien resultan unicas.

Para probar nuestra version del Teorema Fundamental de la Aritmetica
necesitaremos el siguiente lema el cual aceptaremos sin demostracion.

\bigskip

\begin{lemma}
\label{primos}Si $p,p_{1},...,p_{n}$ son numeros primos y $p$ divide a
$p_{1}.p_{2}.\ldots.p_{n}$, entonces $p=p_{i}$, para algun $i$.
\end{lemma}

\bigskip

\begin{theorem}
Para cada $x\in\mathbf{N}$, hay una unica infinitupla $(s_{1},s_{2}%
,...)\in\omega^{\left[  \mathbf{N}\right]  }$ tal que%
\[
x=\underset{i=1}{\overset{\infty}{\Pi}}pr(i)^{s_{i}}%
\]
(Tiene sentido escribir $\underset{i=1}{\overset{\infty}{\Pi}}pr(i)^{s_{i}} $,
ya que en esta productoria solo una cantidad finita de factores son no iguales
a $1$.)
\end{theorem}

\begin{proof}
Primero probaremos la existencia por induccion en $x$. Claramente
$1=\underset{i=1}{\overset{\infty}{\Pi}}pr(i)^{0}$, con lo cual tomando
$(s_{1},s_{2},...)=(0,0,0,...)$ el caso $x=1$ esta probado. Fijemos ahora un
$x>1$ y supongamos la existencia vale para cada $y$ menor que $x$. Veremos que
entonces vale para $x$. Si $x$ es primo, entonces $x=pr(i_{0})$ para algun
$i_{0}$ por lo cual tenemos que $x=\underset{i=1}{\overset{\infty}{\Pi}%
}pr(i)^{s_{i}}$, tomando $s_{i}=0$ si $i\neq i_{0}$ y $s_{i_{0}}=1$. Si $x$ no
es primo, entonces $x=y_{1}.y_{2}$, con $y_{1},y_{2}<x$. Por hipotesis
inductiva tenemos que hay $(s_{1},s_{2},...),(t_{1},t_{2},...)\in
\omega^{\left[  \mathbf{N}\right]  }$ tales que $y_{1}=\underset{i=1}%
{\overset{\infty}{\Pi}}pr(i)^{s_{i}}$ y $y_{2}=\underset{i=1}{\overset{\infty
}{\Pi}}pr(i)^{t_{i}}$. Tenemos entonces que $x=\underset{i=1}{\overset{\infty
}{\Pi}}pr(i)^{s_{i}+t_{i}}$ lo cual concluye la prueba de la existencia.

Veamos ahora la unicidad. Suponganos que las infinituplas $(s_{1}%
,s_{2},...),(t_{1},t_{2},...)\in\omega^{\left[  \mathbf{N}\right]  }$ son
tales que%
\[
\underset{i=1}{\overset{\infty}{\Pi}}pr(i)^{s_{i}}=\underset{i=1}%
{\overset{\infty}{\Pi}}pr(i)^{t_{i}}%
\]
y ademas $s_{i}\neq t_{i}$ para algun $i$. Si $s_{i}>t_{i}$ entonces
dividiendo ambos miembros por $pr(i)^{t_{i}}$ obtenemos que $pr(i)$ divide a
un producto de primos todos distintos de el, lo cual es absurdo por el lema
anterior. Analogamente llegamos a un absurdo si suponemos que $t_{i}>s_{i}$,
lo cual nos dice que vale la unicidad.
\end{proof}

\bigskip

Como podra notarse la existencia en el teorema anterior es facil e
intuitivamente clara de probar. En realidad la potencia del Teorema
Fundamental de la Aritm\'{e}tica radica en el hecho de que dicha factorizacion
es unica.

A continuacion un poco de notacion. Dada una infinitupla $(s_{1},s_{2}%
,...)\in\omega^{\left[  \mathbf{N}\right]  }$ usaremos $\left\langle
s_{1},s_{2},...\right\rangle $ para denotar al numero $\underset{i=1}%
{\overset{\infty}{\Pi}}pr(i)^{s_{i}}$. Dado $x\in\mathbf{N}$, usaremos $(x)$
para denotar a la unica infinitupla $(s_{1},s_{2},...)\in\omega^{\left[
\mathbf{N}\right]  }$ tal que%
\[
x=\left\langle s_{1},s_{2},...\right\rangle =\underset{i=1}{\overset{\infty
}{\Pi}}pr(i)^{s_{i}}%
\]
Ademas para $i\in\mathbf{N}$, usaremos $(x)_{i}$ para denotar a $s_{i}$ de
dicha unica infinitupla. Es decir que

\begin{enumerate}
\item[(1)] $(x)=((x)_{1},(x)_{2},...)$

\item[(2)] $(x)_{i}$ es el exponente de $pr(i)$ en la (unica posible)
factorizacion de $x$ como producto de primos
\end{enumerate}

\bigskip

Claramente entonces

\begin{enumerate}
\item[(3)] $\left\langle (x)_{1},(x)_{2},...\right\rangle =x$, para cada
$x\in\mathbf{N}$

\item[(4)] Para cada $(s_{1},s_{2},...)\in\omega^{\left[  \mathbf{N}\right]
}$, se tiene que%
\[
(\left\langle s_{1},s_{2},...\right\rangle )_{i}=s_{i}\text{, para }%
i\in\mathbf{N}%
\]
Es decir que%
\[
(\left\langle s_{1},s_{2},...\right\rangle )=(s_{1},s_{2},...)
\]

\end{enumerate}

\bigskip

(Justifique con palabras las propiedades (3) y (4)). Tenemos entonces el
siguiente resultado fundamental

\begin{theorem}
Las funciones%
\[%
\begin{array}
[c]{lll}%
\mathbf{N} & \rightarrow & \omega^{\left[  \mathbf{N}\right]  }\\
x & \rightarrow & (x)=((x)_{1},(x)_{2},...)
\end{array}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\begin{array}
[c]{rll}%
\omega^{\left[  \mathbf{N}\right]  } & \rightarrow & \mathbf{N}\\
(s_{1},s_{2},...) & \rightarrow & \left\langle s_{1},s_{2},...\right\rangle
\end{array}
\]
son biyecciones una inversa de la otra.
\end{theorem}

\begin{proof}
Llamemos $f$ a la funcion de la izquierda y $g$ a la de la derecha. Notese que
el Lema \ref{mutuamente inversas} nos dice que basta con probar que $f\circ
g=Id_{\omega^{\left[  \mathbf{N}\right]  }}$ y $g\circ f=Id_{\mathbf{N}}$.
Pero (3) justamente nos dice que $g\circ f=Id_{\mathbf{N}}$ y (4) nos dice que
$f\circ g=Id_{\omega^{\left[  \mathbf{N}\right]  }}$.
\end{proof}

\bigskip

Tal como se hace en la escuela primaria, el siguiente lema nos permite
calcular $(x)_{i}$.

\begin{lemma}
Dados $x,i\in\mathbf{N}$, se tiene que%
\[
(x)_{i}=\max_{t}\left(  pr(i)^{t}\text{ divide a }x\right)
\]

\end{lemma}

\begin{proof}
Ejercicio (aplique el Lema \ref{primos}).
\end{proof}

\bigskip

Definamos la funcion $Lt:\mathbf{N}\rightarrow\omega$ de la siguiente manera:%
\[
Lt(x)=\left\{
\begin{array}
[c]{lll}%
\max_{i}\;(x)_{i}\neq0 &  & \text{si }x\neq1\\
0 &  & \text{si }x=1
\end{array}
\right.
\]
Se tienen las siguientes propiedades basicas

\begin{lemma}
Para cada $x\in\mathbf{N}$:

\begin{enumerate}
\item $Lt(x)=0$ sii $x=1$

\item $x=\prod\nolimits_{i=1}^{Lt(x)}pr(i)^{(x)_{i}}$
\end{enumerate}
\end{lemma}

\bigskip

\section{\label{ParadigmaFilosofico}Procedimientos efectivos}

UUn concepto importante en ciencias de la computacion es el de
\textit{procedimiento} o \textit{metodo} para realizar alguna tarea
determinada. Nos interesan los procedimientos que estan definidos en forma
precisa e inambigua, es decir aquellos en los cuales en cada paso a seguir, la
tarea a realizar esta objetivamente descripta. Tambien deben ser repetibles,
en el sentido de que si realizamos un procedimiento dos veces con el mismo
dato de entrada, entonces ambas ejecuciones deben ser identicas, es decir se
realizaran las mismas tareas y en el mismo orden.

Nos interesan los procedimientos $\mathbb{P}$ que posean las siguientes caracteristicas:

\begin{enumerate}
\item Siempre supondremos que el interprete o ejecutante de $\mathbb{P}$ es
una persona que trabajara con papel y lapiz (ambos recursos disponibles en
forma ilimitada).

\item Cada paso o tarea que $\mathbb{P}$ encomiende a realizar debe ser simple
y facil de realizar en forma \textit{efectiva} por cualquier persona.

\item El procedimiento $\mathbb{P}$ comienza a funcionar siempre a partir de
cierto dato de entrada y una ves que haya comensado, siempre sucedera una de
las dos siguientes posibilidades

\begin{enumerate}
\item $\mathbb{P}$ se detiene y da cierto dato de salida

\item $\mathbb{P}$ nunca se detiene, es decir a medida que se van realizando
las instrucciones o tareas, $\mathbb{P}$ siempre direcciona a realizar nuevas
tareas y lo hace sucesiva e indefinidamente.
\end{enumerate}

En el caso a. diremos que $\mathbb{P}$ se detiene partiendo del dato de
entrada en cuestion y en el caso b. diremos que $\mathbb{P}$ no se detiene
partiendo de dicho dato

\item Hay $n,m\in\omega$ y un alfabeto $\Sigma$ tales que el conjunto de datos
de entrada de $\mathbb{P}$ es $\omega^{n}\times\Sigma^{\ast m}$. Cabe aclarar
que para ciertas $(n+m)$-uplas de $\omega^{n}\times\Sigma^{\ast m}$ el
procedimiento $\mathbb{P}$ se detendra y para ciertas otras no lo hara.
\end{enumerate}

\bigskip

Llamaremos \textit{procedimientos efectivos} a aquellos procedimientos que
posean las caracteristicas arriba mencionadas.

El \textit{conjunto de datos de salida de }$\mathbb{P}$ es el conjunto de
todos los datos que el procedimiento $\mathbb{P}$ dara como salida en alguna
de las posibles ejecuciones al variar todos los datos de entrada posibles. Si
bien siempre el conjunto de datos de entrada sera de la forma $\omega
^{n}\times\Sigma^{\ast m}$, puede ser muy dificil o imposible, en general,
conocer con precision el conjunto de datos de salida de un procedimiento (esto
lo justificaremos mas adelante).

Ya que el interprete de $\mathbb{P}$ es una persona dotada de lapiz y papel,
supondremos que los elementos de $\omega$ que intervienen en los datos de
entrada y de salida estaran representados por palabras de $Num$ usando la
notacion decimal.

Quisas el procedimiento efectivo mas famoso de la matematica es aquel que se
ense\~{n}a en los colegios para sumar dos numeros naturales expresados en
notacion decimal. Notar que el conjunto de datos de entrada de dicho
procedimiento es $\omega^{2}$ y el conjunto de datos de salida es el conjunto
formado por todas las sumas posibles de pares de elementos de $\omega$, es
decir $\omega$. Por supuesto este procedimiento solo usa lapiz, papel y pasos
extremadamente simples a seguir en cada momento de la computacion, es decir,
en algun sentido, no es necesario "entender que es lo que se esta haciendo"
para llegar al final y obtener la palabra que representa en notacion decimal a
la suma de los numeros iniciales. Dejamos al lector repasar este procedimiento
asi como el que calcula dado un numero $x$ no nulo de $\omega$, al numero
$x-1$, los cuales nos haran falta mas adelante en los ejemplos.

\subsection{Funciones $\Sigma$-efectivamente computables}

Una funcion $\Sigma$-mixta $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast
m}\rightarrow\omega$ sera llamada $\Sigma$\textit{-efectivamente computable}
si hay un procedimiento efectivo $\mathbb{P}$ tal que

\begin{enumerate}
\item[(1)] El conjunto de datos de entrada de $\mathbb{P}$ es $\omega
^{n}\times\Sigma^{\ast m}$

\item[(2)] El conjunto de datos de salida esta contenido en $\omega$.

\item[(3)] Si $(\vec{x},\vec{\alpha})\in D_{f}$, entonces $\mathbb{P}$ se
detiene partiendo de $(\vec{x},\vec{\alpha})$, dando como dato de salida
$f(\vec{x},\vec{\alpha})$.

\item[(4)] Si $(\vec{x},\vec{\alpha})\in(\omega^{n}\times\Sigma^{\ast
m})-D_{f}$, entonces $\mathbb{P}$ no se detiene partiendo desde $(\vec{x}%
,\vec{\alpha})$
\end{enumerate}

Analogamente una funcion $\Sigma$-mixta $f:D_{f}\subseteq\omega^{n}%
\times\Sigma^{\ast m}\rightarrow\Sigma^{\ast}$ sera llamada $\Sigma
$\textit{-efectivamente computable} si hay un procedimiento efectivo
$\mathbb{P}$ tal que

\begin{enumerate}
\item[(1)] El conjunto de datos de entrada de $\mathbb{P}$ es $\omega
^{n}\times\Sigma^{\ast m}$

\item[(2)] El conjunto de datos de salida esta contenido en $\Sigma^{\ast}$.

\item[(3)] Si $(\vec{x},\vec{\alpha})\in D_{f}$, entonces $\mathbb{P}$ se
detiene partiendo de $(\vec{x},\vec{\alpha})$, dando como dato de salida
$f(\vec{x},\vec{\alpha})$.

\item[(4)] Si $(\vec{x},\vec{\alpha})\in(\omega^{n}\times\Sigma^{\ast
m})-D_{f}$, entonces $\mathbb{P}$ no se detiene partiendo desde $(\vec{x}%
,\vec{\alpha})$
\end{enumerate}

En ambos casos diremos que $\mathbb{P}$ \textit{computa} a la funcion $f$.

Notese que esta definicion para el caso $f=\emptyset$ tiene a priori cierta
ambiguedad ya que cualesquiera sean $n,m\in\omega$ y $O\in\{\omega
,\Sigma^{\ast}\}$ tenemos que $\emptyset:\emptyset\subseteq\omega^{n}%
\times\Sigma^{\ast m}\rightarrow O$ ya que $D_{\emptyset}=\emptyset$ y
$I_{\emptyset}=\emptyset$. De todas maneras, cualesquiera sean los $n,m$ y $O$
elejidos, siempre hay un procedimiento efectivo que computa a $f=\emptyset$,
i.e. un procedimiento que nunca se detiene, cualesquiera sea el dato de
entrada de $\omega^{n}\times\Sigma^{\ast m}$. Es decir que la funcion
$\emptyset$ es $\Sigma$-efectivamente computable cualesquiera sea el alfabeto
$\Sigma$. Cabe destacar que para el caso de una funcion $f\neq\emptyset$,
nuestra definicion es inambigua ya que hay unicos $n,m\in\omega$ y
$O\in\{\omega,\Sigma^{\ast}\}$ tales que $f:D_{f}\subseteq\omega^{n}%
\times\Sigma^{\ast m}\rightarrow O$.

Veamos algunos ejemplos:

\begin{enumerate}
\item[$($E$1)$] La funcion $\lambda xy\left[  x+y\right]  $ es $\Sigma
$-efectivamente computable, cualquiera sea el alfabeto $\Sigma$ ya que el
procedimiento ense\~{n}ado en la escuela primaria para sumar numeros en
notacion decimal es efectivo y computa esta funcion. Tambien las funciones
$\lambda xy\left[  x.y\right]  $ y $\lambda xy\left[  x^{y}\right]  $ son
$\Sigma$-efectivamente computables via los procedimientos clasicos
ense\~{n}ados en la escuela primaria.

\item[$($E$2)$] La funcion $C_{3}^{1,2}$ es $\Sigma$-efectivamente computable
ya que el siguiente procedimiento $\mathbb{P}$ con conjunto de datos de
entrada $\omega\times\Sigma^{\ast2}$ la computa:

\begin{enumerate}
\item[-] Independientemente de quien sea el dato de entrada $(x_{1},\alpha
_{1},\alpha_{2})$, terminar y dar como salida el numero $3$
\end{enumerate}

\item[$($E$3)$] La funcion $p_{3}^{2,3}$ es $\Sigma$-efectivamente computable
ya que el siguiente procedimiento con conjunto de datos de entrada $\omega
^{2}\times\Sigma^{\ast3}$ la computa:

\begin{enumerate}
\item[-] Dado el dato de entrada $(x_{1},x_{2},\alpha_{1},\alpha_{2}%
,\alpha_{3})$, terminar y dar como salida la palabra $\alpha_{1}$
\end{enumerate}

\item[$($E$4)$] $Pred$ es $\Sigma$-efectivamente computable. Para realizar el
procedimiento efectivo que compute a $Pred$ necesitaremos el procedimiento de
la escuela primaria que dado un numero no nulo $x$, expresado en notacion
decimal, calcula el numero $x-1$, en notacion decimal. Llamemos $\mathbb{P}%
_{-1}$ a dicho procedimiento. El siguiente procedimiento (con conjunto de
datos de entrada igual a $\omega$) computa a $Pred$:

Dado como dato de entrada un elemento $x\in\omega$, realizar lo siguiente:

Etapa 1

\noindent Si $x=0$, entonces ir a Etapa 3, en caso contrario ir a Etapa 2.

Etapa 2

\noindent Correr $\mathbb{P}_{-1}$ con dato de entrada $x$ obteniendo $y$ como
dato de salida. Detenerse y dar $y$ como dato de salida.

Etapa 3

\noindent Si $x=0$, entonces ir a Etapa 1.

Como puede notarse el procedimiento anterior es efectivo ya que debemos
entender que en la Etapa 2, los sucesivos pasos efectuados al correr
$\mathbb{P}_{-1}$ son todos simples y efectivamente realizables ya que
$\mathbb{P}_{-1}$ es efectivo. Por supuesto si uno quisiera ser mas prolijo,
deberia reemplazar la Etapa 2 por las distintas instrucciones de
$\mathbb{P}_{-1}$, referidas a $x$.

\item[$($E$5)$] El predicado $\lambda xy[x<y]$ es $\Sigma$-efectivamente
computable cualquiera sea el alfabeto $\Sigma$. Describiremos con palabras un
procedimiento que computa a $\lambda xy[x<y]$. Su conjunto de datos de entrada
es $\omega^{2}$. Dado un par $(x,y)\in\omega^{2}$, el procedimiento primero
compara las longitudes de las palabras que en notacion decimal representan a
$x$ y $y$. Por supuesto esto lo hace borrando de a un simbolo y viendo si
alguna se termina primero. Si resultan de distinta longitud, es facil darse
cuenta como sigue. En caso de que las palabras resulten de igual longitud,
entonces se hace el procedimiento clasico de ir comparando digitos de
izquierda a derecha.

\item[(E$6$)] Veamos que la funcion $\lambda\alpha\lbrack\left\vert
\alpha\right\vert ]$ es $\Sigma$-efectivamente computable. Como en los
lenguajes de programacion, usaremos variables y asignaciones para dise\~{n}ar
el procedimiento. Ademas llamemos $\mathbb{P}_{+1}$ a el procedimiento de la
escuela primaria que dado un numero no nulo $x$, expresado en notacion
decimal, calcula el numero $x+1$, en notacion decimal. Sea $\mathbb{P}$ el
siguiente procedimiento.

Dado como dato de entrada un elemento $\alpha\in\Sigma^{\ast}$, realizar lo siguiente:

Etapa 1: Hacer las siguientes asignaciones%
\begin{align*}
A  & \leftarrow\alpha\\
B  & \leftarrow0
\end{align*}
e ir a Etapa 2.

Etapa 2: Si $A$ no es $\varepsilon$, ir a Etapa 3. En caso contrario terminar
y dar como salida $B$.

Etapa 3: Correr $\mathbb{P}_{+1}$ con dato de entrada igual al contenido de
$B$, obteniendo $y$ como salida. Hacer la asignacion%
\begin{align*}
A  & \leftarrow\text{resultado de remover el 1er simbolo de }A\\
B  & \leftarrow y
\end{align*}
e ir a Etapa 2.

Dejamos como ejercicio convenserse que el uso de asignaciones puede realizarse
usando solo lapiz y papel. Imagine como lo haria en este ejemplo y corrobore
que dicho procedimiento es efectivo y ademas computa a $\lambda\alpha
\lbrack\left\vert \alpha\right\vert ]$

\item[(E$7$)] Si $\leq$ es el orden total sobre $\Sigma=\{\blacktriangle,\%\}$
dado por $\blacktriangle<\%$, entonces veremos que la funcion $s^{\leq}$ es
$\Sigma$-efectivamente computable. Por el Lema
\ref{recursion para la sucesor de palabras} tenemos que cualquiera sea
$\alpha\in\Sigma^{\ast}$, se cumple%
\begin{align*}
s^{\leq}(\varepsilon)  & =\blacktriangle\\
s^{\leq}(\alpha\blacktriangle)  & =\alpha\%\\
s^{\leq}(\alpha\%)  & =s^{\leq}(\alpha)\blacktriangle
\end{align*}
Tal como lo explica dicho lema el valor de $s^{\leq}$ queda determinado por
las tres ecuaciones anteriores. Usaremos esta idea para dar un procedimiento
efectivo (con conjunto de datos de entrada igual a $\Sigma^{\ast}$) que
compute a $s^{\leq}$.

Etapa 1: Dado el dato de entrada $\alpha\in\Sigma^{\ast}$, hacer las
siguientes asignaciones%
\begin{align*}
A  & \leftarrow\alpha\\
B  & \leftarrow\varepsilon\\
F  & \leftarrow\blacktriangle
\end{align*}
e ir a Etapa 2.

Etapa 2: Si $A$ comiensa con $\blacktriangle$, entonces hacer las siguientes
asignaciones%
\begin{align*}
A  & \leftarrow\text{resultado de remover el 1er simbolo de }A\\
F  & \leftarrow B\%\\
B  & \leftarrow B\blacktriangle
\end{align*}
e ir a la Etapa 2. En caso contrario ir a la Etapa 3.

Etapa 3: Si $A$ comiensa con $\%$, entonces hacer las siguientes asignaciones%
\begin{align*}
A  & \leftarrow\text{resultado de remover el 1er simbolo de }A\\
F  & \leftarrow F\blacktriangle\\
B  & \leftarrow B\%
\end{align*}
e ir a la Etapa 2. En caso contrario ir a la Etapa 4.

Etapa 4: Dar como salida $F$

\item[$($E$8)$] Usando que $s^{\leq}$ es $\Sigma$-efectivamente computable
podemos ver que $\ast^{\leq}:\omega\rightarrow\Sigma^{\ast}$ tambien lo es ya
que $\ast^{\leq}$ es definida con las ecuaciones%
\begin{align*}
\ast^{\leq}(0)  & =\varepsilon\\
\ast^{\leq}(x+1)  & =s^{\leq}(\ast^{\leq}(x))
\end{align*}

\end{enumerate}

Dejamos como ejercico para el lector dise\~{n}ar procedimientos efectivos que
computen las funciones:

\begin{enumerate}
\item[-] $\lambda xy[x$ divide a $y]$

\item[-] $\lambda x[pr(x)]$

\item[-] $\lambda ix[(x)_{i}]$
\end{enumerate}

\noindent(Utilice en el dise\~{n}o de los respectivos procedimientos a los
procedimientos que computan las funciones $\lambda xy\left[  x+y\right]  $,
$\lambda xy\left[  x.y\right]  $ y $\lambda xy\left[  x^{y}\right]  $)

\bigskip

\textbf{Nota Importante:} en lo que sigue muchas veces daremos procedimientos
que son efectivos en terminos de otros que ya se han dado, es decir daremos un
procedimiento que en principio no es claro que sea efectivo pero el cual se
volveria efectivo si reemplazaramos ciertas instrucciones por la manera
efectiva de simularlas. Para hacer mas dinamico el discurso no distinguiremos
entre este tipo de procedimientos (efectivisables) y los efectivos propiamente dichos.

\bigskip

\subsubsection{Constructores que preservan computabilidad efectiva}

Hay muchos procesos constructivos que nos sirven para definir o construir una
funcion en terminos de otras funciones dadas. Un ejemplo de esto es la
composicion de funciones, la cual dadas dos funciones $f,g$ nos permite
construir su composicion, a saber $f\circ g$. Otro ejemplo es el contructor de
predicados que dados dos predicados $\Sigma$-mixtos $P:S\subseteq\omega
^{n}\times\Sigma^{\ast m}\rightarrow\{0,1\}$ y $Q:S\subseteq\omega^{n}%
\times\Sigma^{\ast m}\rightarrow\{0,1\}$, con el mismo dominio, nos define el
predicado%
\[%
\begin{array}
[c]{rll}%
(P\vee Q):S & \rightarrow & \omega\\
(\vec{x},\vec{\alpha}) & \rightarrow & \left\{
\begin{array}
[c]{lll}%
1 &  & \text{si }P(\vec{x},\vec{\alpha})=1\text{ o }Q(\vec{x},\vec{\alpha
})=1\\
0 &  & \text{caso contrario}%
\end{array}
\right.
\end{array}
\]
Otro constructor muy importante que utilizaremos mucho es aquel que a partir
de funciones $f_{i}:D_{f_{i}}\rightarrow O$, $i=1,...,k$, tales que $D_{f_{i}%
}\cap D_{f_{j}}=\emptyset$ para $i\neq j$, nos da la nueva funcion $f_{1}%
\cup...\cup f_{k}$, la cual cumple%
\[%
\begin{array}
[c]{rll}%
D_{f_{1}}\cup...\cup D_{f_{k}} & \rightarrow & O\\
e & \rightarrow & \left\{
\begin{array}
[c]{clc}%
f_{1}(e) &  & \text{si }e\in D_{f_{1}}\\
\vdots &  & \vdots\\
f_{k}(e) &  & \text{si }e\in D_{f_{k}}%
\end{array}
\right.
\end{array}
\]


\bigskip

\bigskip

\bigskip

\bigskip

\bigskip

\bigskip

\bigskip

\begin{lemma}
\label{boolean op de predicados e.c.}Si $P:S\subseteq\omega^{n}\times
\Sigma^{\ast m}\rightarrow\omega$ y $Q:S\subseteq\omega^{n}\times\Sigma^{\ast
m}\rightarrow\omega$ son predicados $\Sigma$-efectivamente computables,
entonces $(P\vee Q)$, $(P\wedge Q)$ y $\lnot P$ lo son tambien.
\end{lemma}

\subsubsection{Composicion}

Dadas funciones $\Sigma$-mixtas $f,f_{1},...,f_{r}$, con $r\geq1$, diremos que
la funcion $f\circ\lbrack f_{1},...,f_{r}]$ es \textit{obtenida por
composicion a partir de las funciones }$f,f_{1},...,f_{r}$. Para probar que la
composicion preserva la computabilidad efectiva necesitaremos el siguiente lema.

\begin{lemma}
Supongamos que $f,f_{1},...,f_{r}$ son funciones $\Sigma$-mixtas, con $r\geq
1$. Supongamos ademas que $f\circ\lbrack f_{1},...,f_{r}]\neq\emptyset$.
Entonces hay $n,m,k,l\in\omega$ y $s\in\{\#,\ast\}$ tales que

\begin{enumerate}
\item[-] $r=n+m$

\item[-] $f$ es de tipo $(n,m,s)$

\item[-] $f_{i}$ es de tipo $(k,l,\#)$, para cada $i=1,...,n$

\item[-] $f_{i}$ es de tipo $(k,l,\ast)$, para cada $i=n+1,...,n+m$
\end{enumerate}

Mas aun, en tal caso la funcion $f\circ\lbrack f_{1},...,f_{n+m}]$ es de tipo
$(k,l,s)$ y:%
\begin{align*}
D_{f\circ\lbrack f_{1},...,f_{n+m}]}  & =\left\{  (\vec{x},\vec{\alpha}%
)\in\bigcap_{i=1}^{n+m}D_{f_{i}}:(f_{1}(\vec{x},\vec{\alpha}),...,f_{n+m}%
(\vec{x},\vec{\alpha}))\in D_{f}\right\} \\
f\circ\lbrack f_{1},...,f_{n+m}](\vec{x},\vec{\alpha})  & =f(f_{1}(\vec
{x},\vec{\alpha}),...,f_{n+m}(\vec{x},\vec{\alpha})).
\end{align*}

\end{lemma}

\begin{proof}
Notese que $f\neq\emptyset$ y $[f_{1},...,f_{r}]\neq\emptyset$ (por que?). Ya
que $f\neq\emptyset$ tenemos que hay unicos $n,m\in\omega$ y $s\in\{\#,\ast\}$
tales que $f$ es de tipo $(n,m,s)$. Ya que $f\circ\lbrack f_{1},...,f_{r}%
]\neq\emptyset$ y $I_{[f_{1},...,f_{r}]}\subseteq I_{f_{1}}\times...\times
I_{f_{r}}$, tenemos que

\begin{enumerate}
\item[-] $r=n+m$

\item[-] $I_{f_{i}}\subseteq\omega$, para cada $i=1,...,n$

\item[-] $I_{f_{i}}\subseteq\Sigma^{\ast}$, para cada $i=n+1,...,n+m$
\end{enumerate}

Ya que $[f_{1},...,f_{r}]\neq\emptyset$ tenemos que $D_{[f_{1},...,f_{r}%
]}=\bigcap_{i=1}^{r}D_{f_{i}}\neq\emptyset$, por lo cual los conjuntos
$D_{f_{1}},...,D_{f_{n+m}}$ deberan ser todos de un mismo tipo, digamos de
tipo $(k,l)$. Es decir que $f_{i}$ es de tipo $(k,l,\#)$, para cada
$i=1,...,n$ y $f_{i}$ es de tipo $(k,l,\ast)$, para cada $i=n+1,...,n+m$.

Las ultimas observaciones del lema son directas de las definiciones de
$[f_{1},...,f_{n+m}]$ y de composicion de funciones
\end{proof}

\bigskip

Ahora si podemos probar facilmente que se preserva la computabilidad efectiva
cuando componemos

\begin{lemma}
Si $f,f_{1},...,f_{r}$, con $r\geq1$, son $\Sigma$-efectivamente computables,
entonces $f\circ\lbrack f_{1},...,f_{r}]$ lo es.
\end{lemma}

\begin{proof}
Si $f\circ\lbrack f_{1},...,f_{r}]=\emptyset$, entonces claramente es $\Sigma
$-efectivamente computable. Supongamos entonces que $f\circ\lbrack
f_{1},...,f_{r}]\neq\emptyset$. Por el lema anterior hay $n,m,k,l\in\omega$ y
$s\in\{\#,\ast\}$ tales que

\begin{enumerate}
\item[-] $r=n+m$

\item[-] $f$ es de tipo $(n,m,s)$

\item[-] $f_{i}$ es de tipo $(k,l,\#)$, para cada $i=1,...,n$

\item[-] $f_{i}$ es de tipo $(k,l,\ast)$, para cada $i=n+1,...,n+m$
\end{enumerate}

Sean $\mathbb{P},\mathbb{P}_{1},...,\mathbb{P}_{n+m}$ procedimientos efectivos
los cuales computen las funciones $f,f_{1},...,f_{n+m}$, respectivamente.
Usando estos procedimientos es facil definir un procedimiento efectivo el cual
compute a $f\circ\lbrack f_{1},...,f_{n+m}]$.
\end{proof}

\bigskip

\subsubsection{Lema de division por casos para funciones $\Sigma
$-efectivamente computables}

Recordemos que si $f_{i}:D_{f_{i}}\rightarrow O$, $i=1,...,k$, son funciones
tales que $D_{f_{i}}\cap D_{f_{j}}=\emptyset$ para $i\neq j$, entonces
$f_{1}\cup...\cup f_{k}$ es la funcion%
\[%
\begin{array}
[c]{rll}%
D_{f_{1}}\cup...\cup D_{f_{k}} & \rightarrow & O\\
e & \rightarrow & \left\{
\begin{array}
[c]{clc}%
f_{1}(e) &  & \text{si }e\in D_{f_{1}}\\
\vdots &  & \vdots\\
f_{k}(e) &  & \text{si }e\in D_{f_{k}}%
\end{array}
\right.
\end{array}
\]


\begin{lemma}
\label{dpc para efectivamente computables}Sean $n,m\in\omega$ y $O\in
\{\omega,\Sigma^{\ast}\}$. Supongamos $f_{i}:D_{f_{i}}\subseteq\omega
^{n}\times\Sigma^{\ast m}\rightarrow O$, $i=1,...,k$, son funciones $\Sigma
$-efectivamente computables tales que $D_{f_{i}}\cap D_{f_{j}}=\emptyset$ para
$i\neq j$. Entonces $f_{1}\cup...\cup f_{k}$ es $\Sigma$-efectivamente computable.
\end{lemma}

\begin{proof}
Haremos el caso $O=\Sigma^{\ast}$ y $k=2$. Sean $\mathbb{P}_{1}$ y
$\mathbb{P}_{2}$ procedimientos efectivos que computen a $f_{1}$ y $f_{2}$,
respectivamente. Sea $\mathbb{P}$ el procedimiento efectivo siguiente:

- Conjunto de datos de entrada de $\mathbb{P}$ igual a $\omega^{n}\times
\Sigma^{\ast m}$

- Conjunto de datos de salida de $\mathbb{P}$ contenido en $\Sigma^{\ast}$

- Funcionamiento:

Etapa 1

\noindent Hacer $T=1$

Etapa 2

\noindent Correr el procedimiento $\mathbb{P}_{1}$ una cantidad $T$ de pasos.
En caso de que termine guardar la salida en la variable $X$ e ir a Etapa 5. Si
no termina ir a Etapa 3.

Etapa 3

\noindent Correr el procedimiento $\mathbb{P}_{2}$ una cantidad $T$ de pasos.
En caso de que termine guardar la salida en la variable $X$ e ir a Etapa 6. Si
no termina ir a Etapa 4.

Etapa 4

\noindent Hacer $T=T+1$ e ir a Etapa 2

Etapa 5

\noindent Dar como salida el contenido de $X$ y terminar.

Dejamos al lector corroborar que el procedimiento $\mathbb{P}$ computa a la
funcion $f_{1}\cup f_{2}$.
\end{proof}

\bigskip

\subsection{\label{conjuntos sigma-efectivamente computables}Conjuntos
$\Sigma$-efectivamente computables}

\noindent Un conjunto $S\subseteq\omega^{n}\times\Sigma^{\ast m}$ sera llamado
$\Sigma$\textit{-efectivamente computable} cuando la funcion $\chi_{S}%
^{\omega^{n}\times\Sigma^{\ast m}}$ sea $\Sigma$-efectivamente computable.
Notese que $S$ es $\Sigma$-efectivamente computable sii hay un procedimiento
efectivo $\mathbb{P}$, el cual computa $\chi_{S}^{\omega^{n}\times\Sigma^{\ast
m}}$, es decir:

\begin{enumerate}
\item[-] El conjunto de datos de entrada de $\mathbb{P}$ es $\omega^{n}%
\times\Sigma^{\ast m}$, siempre termina y da como dato de salida un elemento
de $\{0,1\}$.

\item[-] Dado $(\vec{x},\vec{\alpha})\in\omega^{n}\times\Sigma^{\ast m}$,
$\mathbb{P}$ da como salida al numero $1$ si $(\vec{x},\vec{\alpha})\in S$ y
al numero $0$ si $(\vec{x},\vec{\alpha})\notin S$.
\end{enumerate}

Si $\mathbb{P}$ es un procedimiento efectivo el cual computa a $\chi
_{S}^{\omega^{n}\times\Sigma^{\ast m}}$, diremos que $\mathbb{P}$
\textit{decide la pertenecia a }$S$, con respecto al conjunto $\omega
^{n}\times\Sigma^{\ast m}$.

Notese que esta definicion para el caso $S=\emptyset$ tiene a priori cierta
ambiguedad ya que cualesquiera sean $n,m\in\omega$ tenemos que $\emptyset
\subseteq\omega^{n}\times\Sigma^{\ast m}$. De todas maneras, cualesquiera sean
los $n,m$ elejidos, siempre hay un procedimiento efectivo que computa a
$\chi_{\emptyset}^{\omega^{n}\times\Sigma^{\ast m}}$, i.e. un procedimiento
que siempre da como salida $0$, cualesquiera sea el dato de entrada de
$\omega^{n}\times\Sigma^{\ast m}$. Es decir que el conjunto $\emptyset$ es
$\Sigma$-efectivamente computable cualesquiera sea el alfabeto $\Sigma$. Cabe
destacar que para el caso de un conjunto $S\neq\emptyset$, nuestra definicion
es inambigua ya que hay unicos $n,m\in\omega$ tales que $S\subseteq\omega
^{n}\times\Sigma^{\ast m}$.

\bigskip

Dejamos al lector la facil prueba del siguiente resultado.

\begin{lemma}
Sean $S_{1},S_{2}\subseteq\omega^{n}\times\Sigma^{\ast m}$ conjuntos $\Sigma
$-efectivamente computables. Entonces $S_{1}\cup S_{2},S_{1}\cap S_{2} $ y
$(\omega^{n}\times\Sigma^{\ast m})-S_{1}$ son $\Sigma$-efectivamente computables.
\end{lemma}

\bigskip

El siguiente lema caracteriza cuando un conjunto rectangular es $\Sigma
$-efectivamente computable

\begin{lemma}
Supongamos $S_{1},...,S_{n}\subseteq\omega$, $L_{1},...,L_{m}\subseteq
\Sigma^{\ast}$ son conjuntos no vacios. Entonces $S_{1}\times...\times
S_{n}\times L_{1}\times...\times L_{m}$ es $\Sigma$-efectivamente computable
sii $S_{1},...,S_{n},L_{1},...,L_{m}$ son $\Sigma$-efectivamente computables
\end{lemma}

\begin{proof}
Notese que si $n=m=0$, entonces $S_{1}\times...\times S_{n}\times L_{1}%
\times...\times L_{m}=\{\Diamond\}$ el cual es $\Sigma$-efectivamente
computable por lo cual el lema se cumple. Vemos entonces el caso $n+m\geq1$.
Para hacer mas lejible la prueba haremos el caso $n=m=1$. La prueba general es
completamente analoga.

($\Rightarrow$) Ya que $S_{1}$ y $L_{1}$ son conjuntos no vacios, hay
$x_{0}\in S_{1}$ y $\alpha_{0}\in L_{1}$. Ya que $S_{1}\times L_{1}$ es
$\Sigma$-efectivamente computable, tenemos que $\chi_{S_{1}\times L_{1}%
}^{\omega\times\Sigma^{\ast}}$ es $\Sigma$-efectivamente computable. Notese
que:%
\[
x\in S_{1}\text{ sii }(x,\alpha_{0})\in S_{1}\times L_{1}\text{ sii }%
\chi_{S_{1}\times L_{1}}^{\omega\times\Sigma^{\ast}}(x,\alpha_{0})=1\text{ }%
\]
Por lo tanto, es facil usando un procedimiento efectivo que compute a
$\chi_{S_{1}\times L_{1}}^{\omega\times\Sigma^{\ast}}$ dise\~{n}ar un
procedimiento efectivo que compute a $\chi_{S_{1}}^{\omega}$. En forma similar
se prueba que $L_{1}$ es $\Sigma$-efectivamente computable.

($\Leftarrow$) Es facil, usando procedimientos efectivos que computen a
$\chi_{S_{1}}^{\omega}$ y $\chi_{L_{1}}^{\Sigma^{\ast}}$, armar un
procedimiento efectivo que compute a $\chi_{S_{1}\times L_{1}}^{\omega
\times\Sigma^{\ast}}$.
\end{proof}

\bigskip

\subsection{\label{conjuntos sigma-efectivamente enumerables}Conjuntos
$\Sigma$-efectivamente enumerables}

\noindent Un conjunto $S\subseteq\omega^{n}\times\Sigma^{\ast m}$ sera llamado
$\Sigma$\textit{-efectivamente enumerable} cuando sea vacio o haya una funcion
$F:\omega\rightarrow\omega^{n}\times\Sigma^{\ast m}$ tal que $I_{F}=S$ y
$F_{(i)}$ sea $\Sigma$-efectivamente computable, para cada $i\in
\{1,...,n+m\}$. Notese que para el caso $n=m=0$, la condicion de que $F_{(i)}$
sea $\Sigma$-efectivamente computable, para cada $i\in\{1,...,n+m\}$ se cumple
vacuamente y por lo tanto la definicion anterior nos dice que un conjunto
$S\subseteq\omega^{0}\times\Sigma^{\ast0}=\{\Diamond\}$ sera $\Sigma
$-efectivamente enumerable sii es vacio o hay una funcion $\Sigma
$-efectivamente computable $F:\omega\rightarrow\{\Diamond\}$, tal que
$I_{F}=S$. Por supuesto, esto nos dice que $\emptyset$ y $\{\Diamond\}$ son
$\Sigma$-efectivamente enumerables.

El siguiente resultado nos permite entender mejor la idea subyacente a esta definicion.

\begin{lemma}
Un conjunto no vacio $S\subseteq\omega^{n}\times\Sigma^{\ast m}$ es $\Sigma
$-efectivamente enumerable sii hay un procedimiento efectivo $\mathbb{P}$ tal que

\begin{enumerate}
\item[(1)] El conjunto de datos de entrada de $\mathbb{P}$ es $\omega$

\item[(2)] $\mathbb{P}$ se detiene para cada $x\in\omega$

\item[(3)] El conjunto de datos de salida de $\mathbb{P}$ es igual a $S$. (Es
decir, siempre que $\mathbb{P}$ se detiene, da como salida un elemento de $S$,
y para cada elemento $(\vec{x},\vec{\alpha})\in S$, hay un $x\in\omega$ tal
que $\mathbb{P}$ da como salida a $(\vec{x},\vec{\alpha})$ cuando lo corremos
con $x$ como dato de entrada)
\end{enumerate}
\end{lemma}

\begin{proof}
El caso $n=m=0$ es facil y es dejado al lector. Supongamos entonces que
$n+m\geq1$.

($\Rightarrow$) Supongamos que $S\subseteq\omega^{n}\times\Sigma^{\ast m}$ es
$\Sigma$-efectivamente enumerable. Ya que $S$ es no vacio, por definicion hay
una funcion $F:\omega\rightarrow\omega^{n}\times\Sigma^{\ast m}$ tal que
$I_{F}=S$ y cada $F_{(i)}$ es $\Sigma$-efectivamente computable. Para cada
$i\in\{1,...,n+m\}$ sea $\mathbb{P}_{i}$ un procedimiento efectivo que compute
a $F_{(i)}$. Notar que cada $\mathbb{P}_{i}$ tiene a $\omega$ como conjunto de
datos de entrada y siempre termina. Sea $\mathbb{P}$ el siguiente
procedimiento efectivo, con conjunto de datos de entrada igual a $\omega$ y
conjunto de datos de salida contenido en $\omega^{n}\times\Sigma^{\ast m}$.

Etapa 1: Correr $\mathbb{P}_{1}$ con dato de entrada $x$ y alojar el dato de
salida en la variable $X_{1}$

Etapa 2: Correr $\mathbb{P}_{2}$ con dato de entrada $x$ y alojar el dato de
salida en la variable $X_{2}$

$\ \ \ \ \vdots$

Etapa $n$: Correr $\mathbb{P}_{n}$ con dato de entrada $x$ y alojar el dato de
salida en la variable $X_{n}$

Etapa $n+1$: Correr $\mathbb{P}_{n+1}$ con dato de entrada $x$ y alojar el
dato de salida en la variable $A_{1}$

Etapa $n+2$: Correr $\mathbb{P}_{n+2}$ con dato de entrada $x$ y alojar el
dato de salida en la variable $A_{2}$

$\ \ \ \ \vdots$

Etapa $n+m$: Correr $\mathbb{P}_{n+m}$ con dato de entrada $x$ y alojar el
dato de salida en la variable $A_{m}$

Etapa $n+m+1$: Detenerse y dar $(X_{1},...,X_{n},A_{1},...,A_{m})$ como dato
de salida

\noindent Dejamos al lector la verificacion de que el procedimiento
$\mathbb{P}$ es efectivo y cumple las propiedades (1), (2) y (3).

($\Leftarrow$) Supongamos $\mathbb{P}$ es un procedimiento efectivo el cual
cumple las propiedades (1), (2) y (3). Definamos $F:\omega\rightarrow
\omega^{n}\times\Sigma^{\ast m}$, de la siguiente manera:%
\[
F(x)=\text{ dato de salida de }\mathbb{P}\text{ cuando lo corremos desde }x
\]
Notar que para cada $i\in\{1,...,n+m\}$ la funcion $F_{(i)}$ es $\Sigma
$-efectivamente computable ya que el siguiente procedimiento efectivo la computa:

Etapa 1: Correr $\mathbb{P}$ desde $x$ y guardar la salida en la variable $V$

Etapa 2: Dar como salida la coordenada $i$-esima de $V$
\end{proof}

\bigskip

Cuando un procedimiento $\mathbb{P}$ cumpla (1), (2) y (3) del lema anterior,
diremos que $\mathbb{P}$ \textit{enumera} a $S$. O sea que $S$ es $\Sigma
$-efectivamente enumerable sii es vacio o hay un procedimiento efectivo el
cual enumera a $S$.

Dicho de otra forma un conjunto no vacio $S$ es $\Sigma$-efectivamente
enumerable sii hay un procedimiento efectivo $\mathbb{P}$ el cual tiene
conjunto de datos de entrada $\omega$ y ademas para los sucesivos datos de
entrada $0,1,2,3,...$, el procedimiento $\mathbb{P}$ produce respectivamente
los datos de salida $e_{0},e_{1},e_{2},e_{3},...$ de manera que $S=\{e_{0}%
,e_{1},e_{2},...\}$. Cabe destacar aqui que puede suceder que $e_{i}=e_{j}$,
para ciertos $i,j$, con $i\neq j$.

Algunos ejemplos:

\begin{enumerate}
\item[E$_{1}$] El conjunto $S=\{x\in\omega:x$ es par$\}$ es $\Sigma
$-efectivamente enumerable, cualesquiera sea $\Sigma$. El siguiente
procedimiento enumera a $S$:

\begin{enumerate}
\item[-] Calcular $2x$, darlo como dato de salida y terminar.
\end{enumerate}

\item[E$_{2}$] Un procedimiento que enumera $\omega\times\omega$ es el siguiente:

Etapa 1

\noindent Si $x=0$, dar como salida el par $(0,0)$ y terminar. Si $x\neq0$,
calcular $x_{1}=(x)_{1}$ y $x_{2}=(x)_{2}$.

Etapa 2

\noindent Dar como dato de salida el par $(x_{1},x_{2})$ y terminar

Como puede notarse el procedimiento es efectivo y ademas el conjunto de datos
de salida es $\omega\times\omega$ ya que si tomamos un par cualquiera
$(a,b)\in\omega\times\omega$, el procedimiento lo dara como dato de salida
para la entrada $x=2^{a}3^{b}$.

\item[E$_{3}$] Veamos que $\omega^{2}\times\Sigma^{\ast3}$ es $\Sigma
$-efectivamente enumerable cualquiera sea el alfabeto no vacio $\Sigma$. Sea
$\leq$ un orden total para el alfabeto $\Sigma$. Utilisando el orden $<$
podemos dise\~{n}ar el siguiente procedimiento para enumerar $\omega^{2}%
\times\Sigma^{\ast3}$:

Etapa 1

\noindent Si $x=0$, dar como salida $(0,0,\varepsilon,\varepsilon
,\varepsilon)$ y terminar. Si $x\neq0$, calcular

$x_{1}=(x)_{1}$

$x_{2}=(x)_{2}$

$\alpha_{1}=\ast^{\leq}((x)_{3})$

$\alpha_{2}=\ast^{\leq}((x)_{4})$

$\alpha_{3}=\ast^{\leq}((x)_{5})$

Etapa 2

\noindent Dar como dato de salida la 5-upla $(x_{1},x_{2},\alpha_{1}%
,\alpha_{2},\alpha_{3})$.
\end{enumerate}

\bigskip

\begin{lemma}
Sean $S_{1},S_{2}\subseteq\omega^{n}\times\Sigma^{\ast m}$ conjuntos $\Sigma
$-efectivamente enumerables. Entonces $S_{1}\cup S_{2}$ y $S_{1}\cap S_{2}$
son $\Sigma$-efectivamente enumerables.
\end{lemma}

\begin{proof}
El caso en el que alguno de los conjuntos es vacio es trivial. Supongamos que
ambos conjuntos son no vacios y sean $\mathbb{P}_{1}$ y $\mathbb{P}_{2}$
procedimientos que enumeran a $S_{1}$ y $S_{2}$. El siguiente procedimiento
enumera al conjunto $S_{1}\cup S_{2}$:

\begin{enumerate}
\item[-] Si $x$ es par realizar $\mathbb{P}_{1}$ partiendo de $x/2$ y dar el
elemento de $S_{1}$ obtenido como salida. Si $x$ es impar realizar
$\mathbb{P}_{2}$ partiendo de $(x-1)/2$ y dar el elemento de $S_{2}$ obtenido
como salida.
\end{enumerate}

Veamos ahora que $S_{1}\cap S_{2}$ es $\Sigma$-efectivamente enumerable. Si
$S_{1}\cap S_{2}=\emptyset$ entonces no hay nada que probar. Supongamos
entonces que $S_{1}\cap S_{2}$ es no vacio. Sea $e_{0}$ un elemento fijo de
$S_{1}\cap S_{2}$. Sea $\mathbb{P}$ un procedimiento efectivo el cual enumere
a $\omega\times\omega$ (ver el ejemplo de mas arriba). Un procedimiento que
enumera a $S_{1}\cap S_{2}$ es el siguiente

Etapa 1

\noindent Realizar $\mathbb{P}$ con dato de entrada $x$, para obtener un par
$(x_{1},x_{2})\in\omega\times\omega$.

Etapa 2

\noindent Realizar $\mathbb{P}_{1}$ con dato de entrada $x_{1}$ para obtener
un elemento $e_{1}\in S_{1}$

Etapa 3

\noindent Realizar $\mathbb{P}_{2}$ con dato de entrada $x_{2}$ para obtener
un elemento $e_{2}\in S_{2}$

Etapa 4

\noindent Si $e_{1}=e_{2}$, entonces dar como dato de salida $e_{1}.$ En caso
contrario dar como dato de salida $e_{0}$.

Dejamos al lector la prueba de que este procedimiento enumera a $S_{1}\cap
S_{2}$.
\end{proof}

\bigskip

Dejamos al lector la prueba del siguiente resultado.

\begin{lemma}
Supongamos $S_{1},...,S_{n}\subseteq\omega$, $L_{1},...,L_{m}\subseteq
\Sigma^{\ast}$ son conjuntos no vacios. Entonces $S_{1}\times...\times
S_{n}\times L_{1}\times...\times L_{m}$ es $\Sigma$-efectivamente enumerable
sii $S_{1},...,S_{n},L_{1},...,L_{m}$ son $\Sigma$-efectivamente enumerables
\end{lemma}

\bigskip

\begin{lemma}
Si $S\subseteq\omega^{n}\times\Sigma^{\ast m}$ es $\Sigma$-efectivamente
computable entonces $S$ es $\Sigma$-efectivamente enumerable.
\end{lemma}

\begin{proof}
Supongamos $S\neq\emptyset$. Sea $(\vec{z},\gamma)\in S$, fijo. Sea
$\mathbb{P}$ un procedimiento efectivo que compute a $\chi_{S}^{\omega
^{n}\times\Sigma^{\ast m}}$. Ya vimos en el ejemplo anterior que $\omega
^{2}\times\Sigma^{\ast3}$ es $\Sigma$-efectivamente enumerable. En forma
similar se puede ver que $\omega^{n}\times\Sigma^{\ast m}$ lo es. Sea
$\mathbb{P}_{1}$ un procedimiento efectivo que enumere a $\omega^{n}%
\times\Sigma^{\ast m}$. Entonces el siguiente procedimiento enumera a $S$:

Etapa 1

\noindent Realizar $\mathbb{P}_{1}$ con $x$ de entrada para obtener $(\vec
{x},\vec{\alpha})\in\omega^{n}\times\Sigma^{\ast m}$.

Etapa 2

\noindent Realizar $\mathbb{P}$ con $(\vec{x},\vec{\alpha})$ de entrada para
obtener el valor Booleano $e$ de salida.

Etapa 3

\noindent Si $e=1$ dar como dato de salida $(\vec{x},\vec{\alpha}).$ Si $e=0$
dar como dato de salida $(\vec{z},\gamma)$.
\end{proof}

\bigskip

Como veremos mas adelante en la materia (Proposicion \ref{A es EE y no EC}),
hay conjuntos que son $\Sigma$-efectivamente enumerables y no $\Sigma
$-efectivamente computables. Sin envargo tenemos el siguiente interesante resultado:

\begin{theorem}
Sea $S\subseteq\omega^{n}\times\Sigma^{\ast m}$. Son equivalentes

\begin{enumerate}
\item[(a)] $S$ es $\Sigma$-efectivamente computable

\item[(b)] $S$ y $(\omega^{n}\times\Sigma^{\ast m})-S$ son $\Sigma
$-efectivamente enumerables
\end{enumerate}
\end{theorem}

\begin{proof}
(a)$\Rightarrow$(b). Por el lema anterior tenemos que $S$ es $\Sigma
$-efectivamente enumerable. Notese ademas que, dado que $S$ es $\Sigma
$-efectivamente computable, $(\omega^{n}\times\Sigma^{\ast m})-S$ tambien lo
es (por que?). Es decir que aplicando nuevamente el lema anterior tenemos que
$(\omega^{n}\times\Sigma^{\ast m})-S$ es $\Sigma$-efectivamente enumerable.

(b)$\Rightarrow$(a). Si $S=\emptyset$ o $S=\omega^{n}\times\Sigma^{\ast m}$ es
claro que se cumple (a). O sea que podemos suponer que ni $S$ ni $\omega
^{n}\times\Sigma^{\ast m}$ son igual al conjunto vacio. Sea $\mathbb{P}_{1}$
un procedimiento efectivo que enumere a $S$ y sea $\mathbb{P}_{2}$ un
procedimiento efectivo que enumere a $(\omega^{n}\times\Sigma^{\ast m})-S$. Es
facil ver que el siguiente procedimiento computa el predicado $\chi
_{S}^{\omega^{n}\times\Sigma^{\ast m}}$:

Etapa 1

\noindent Darle a la variable $T$ el valor $0$.

Etapa 2

\noindent Realizar $\mathbb{P}_{1}$ con el valor de $T$ como entrada para
obtener de salida la upla $(\vec{y},\vec{\beta})$.

Etapa 3

\noindent Realizar $\mathbb{P}_{2}$ con el valor de $T$ como entrada para
obtener de salida la upla $(\vec{z},\vec{\gamma})$.

Etapa 4

\noindent Si $(\vec{y},\vec{\beta})=(\vec{x},\vec{\alpha})$, entonces
detenerse y dar como dato de salida el valor $1$. Si $(\vec{z},\vec{\gamma
})=(\vec{x},\vec{\alpha})$, entonces detenerse y dar como dato de salida el
valor $0.$ Si no suceden ninguna de las dos posibilidades antes mensionadas,
aumentar en $1$ el valor de la variable $T$ y dirijirse a la Etapa 2.
\end{proof}

\bigskip

Supongamos que $k,l,n,m\in\omega$ y que $F:D_{F}\subseteq\omega^{k}%
\times\Sigma^{\ast l}\rightarrow\omega^{n}\times\Sigma^{\ast m}$. Supongamos
ademas que $n+m\geq1$. Entonces denotaremos con $F_{(i)}$ a la funcion
$p_{i}^{n,m}\circ F$. Notar que%
\begin{align*}
F_{(i)}  & :D_{F}\subseteq\omega^{k}\times\Sigma^{\ast l}\rightarrow
\omega\text{, para cada }i=1,...,n\\
F_{(i)}  & :D_{F}\subseteq\omega^{k}\times\Sigma^{\ast l}\rightarrow
\Sigma^{\ast}\text{, para cada }i=n+1,...,n+m
\end{align*}
Por lo cual cada una de las funciones $F_{(i)}$ son $\Sigma$-mixtas. Ademas
notese que%
\[
F=[F_{(1)},...,F_{(n+m)}]
\]


\begin{theorem}
\label{equivalencias de efectivamente enumerable}Dado $S\subseteq\omega
^{n}\times\Sigma^{\ast m}$, son equivalentes

\begin{enumerate}
\item[(1)] $S$ es $\Sigma$-efectivamente enumerable

\item[(2)] $S=I_{F}$, para alguna $F:D_{F}\subseteq\omega^{k}\times
\Sigma^{\ast l}\rightarrow\omega^{n}\times\Sigma^{\ast m}$ tal que cada
$F_{(i)}$ es $\Sigma$-efectivamente computable.

\item[(3)] $S=D_{f}$, para alguna funcion $f$ la cual es $\Sigma
$-efectivamente computable.
\end{enumerate}
\end{theorem}

\begin{proof}
El caso $n=m=0$ es facil y es dejado al lector. Supongamos entonces que
$n+m\geq1$.

(1)$\Rightarrow$(2) es trivial.

(2)$\Rightarrow$(3). Para $i=1,...,n+m$, sea $\mathbb{P}_{i}$ un procedimiento
el cual computa a $F_{(i)}$ y sea $\mathbb{P}$ un procedimiento el cual
enumere a $\omega\times\omega^{k}\times\Sigma^{\ast l}.$ El siguiente
procedimiento computa la funcion $f:I_{F}\rightarrow\{1\}$:

Etapa 1

\noindent Darle a la variable $T$ el valor 0.

Etapa 2

\noindent Hacer correr $\mathbb{P}$ con dato de entrada $T$ y obtener
$(t,z_{1},...,z_{k},\gamma_{1},...,\gamma_{l})$ como dato de salida.

Etapa 3

\noindent Para cada $i=1,...,n+m$, hacer correr $\mathbb{P}_{i}$ durante $t$
pasos, con dato de entrada $(z_{1},...,z_{k},\gamma_{1},...,\gamma_{l}).$ Si
cada procedimiento $\mathbb{P}_{i}$ al cabo de los $t$ pasos termino y dio
como resultado el valor $o_{i}$, entonces comparar $(\vec{x},\vec{\alpha})$
con $(o_{1},...,o_{n+m})$ y en caso de que sean iguales detenerse y dar como
dato de salida el valor $1$. En el caso en que no son iguales, aumentar en $1$
el valor de la variable $T$ y dirijirse a la Etapa 2. Si algun procedimiento
$\mathbb{P}_{i}$ al cabo de los $t$ pasos no termino, entonces aumentar en $1$
el valor de la variable $T$ y dirijirse a la Etapa 2.

(3)$\Rightarrow$(1). Supongamos $S\neq\emptyset$. Sea $(\vec{z},\vec{\gamma})$
un elemento fijo de $S$. Sea $\mathbb{P}$ un procedimiento el cual compute a
$f$. Sea $\mathbb{P}_{1}$ un procedimiento el cual enumere a $\omega
\times\omega^{n}\times\Sigma^{\ast m}.$ Dejamos al lector el dise\~{n}o de un
procedimiento efectivo el cual enumere $D_{f}$.
\end{proof}

\bigskip

Dejamos como ejercicio la prueba de los dos siguientes lemas.

\begin{lemma}
Sean $n,m\in\omega$ y $O\in\{\omega,\Sigma^{\ast}\}$. Supongamos
$f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow O$ es $\Sigma
$-efectivamente computable y $S\subseteq I_{f}$ es $\Sigma$-efectivamente
enumerable, entonces $f^{-1}(S)=\{(\vec{x},\vec{\alpha}):f(\vec{x},\vec
{\alpha})\in S\}$ es $\Sigma$-efectivamente enumerable
\end{lemma}

\begin{lemma}
Sean $n,m\in\omega$ y $O\in\{\omega,\Sigma^{\ast}\}$. Supongamos
$f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow O$ es $\Sigma
$-efectivamente computable y $S\subseteq D_{f}$ es $\Sigma$-efectivamente
enumerable, entonces $f|_{S}$ es $\Sigma$-efectivamente computable
\end{lemma}

\bigskip

\subsection{Independencia del alfabeto}

Una observacion importante es que los conceptos de funcion $\Sigma
$-efectivamente computable, de conjunto $\Sigma$-efectvamente computable y de
conjunto $\Sigma$-efectivamente enumerable, no dependen del alfabeto $\Sigma$.
Esto lo establecemos formalmente en los dos siguientes lemas.

\begin{lemma}
\label{independencia alfabeto funciones efectivamente computables}Sean
$\Sigma$ y $\Gamma$ alfabetos cualesquiera. Supongamos una funcion $f$ es
$\Sigma$-mixta y $\Gamma$-mixta, entonces $f$ es $\Sigma$-efectivamente
computable sii $f$ es $\Gamma$-efectivamente computable.
\end{lemma}

\bigskip

\begin{lemma}
\label{independencia alfabeto para conjuntos e.e. y e.c.}Sean $\Sigma$ y
$\Gamma$ alfabetos cualesquiera. Supongamos un conjunto $S$ es $\Sigma$-mixto
y $\Gamma$-mixto, entonces $S$ es $\Sigma$-efectivamente computable (resp.
$\Sigma$-efectivamente enumerable) sii $S$ es $\Gamma$-efectivamente
computable (resp. $\Gamma$-efectivamente enumerable).
\end{lemma}

\bigskip

Dejamos al lector los detalles de las rutinarias pruebas de estos dos lemas.

\bigskip

\section{Tres modelos matematicos de la computabilidad efectiva}

Ya que el concepto de procedimiento efectivo es un concepto intuitivo,
impresiso y a priori no expresado en el formalismo matematico, los conceptos de

\begin{enumerate}
\item[-] Funcion $\Sigma$-efectivamente computable

\item[-] Conjunto $\Sigma$-efectivamente computable

\item[-] Conjunto $\Sigma$-efectivamente enumerable
\end{enumerate}

\noindent tambien son impresisos y estan fuera del formalismo matematico,
debido a que los tres se definen en terminos de la existencia de
procedimientos efectivos. Por supuesto, los tres conceptos son fundamentales
en el estudio teorico de la computabilidad por lo que es muy importante poder
dar un modelo o formalizacion matematica de estos conceptos. Pero notese que
los dos ultimos se definen en funcion del primero por lo que una formalizacion
matematica precisa del concepto de funcion $\Sigma$-efectivamente computable,
resuelve el problema de modelizar en forma matematica estos a tres conceptos.

En esta seccion daremos las tres formalizaciones matematicas mas clasicas del
concepto de funcion $\Sigma$-efectivamente computable. La primera y la mas
apegada a la idea intuitiva de procedimiento efectivo es la dada por Alan
Turing via la matematizacion del concepto de maquina. La segunda, es la dada
por Godel en su estudio de sistemas formales de la logica de primer orden. Por
ultimo veremos la formalizacion via un lenguaje de programacion imperativo. En
honor a la influencia que tuvo Von Neumann en el dise\~{n}o de la primer
computadora de caracter universal (i.e. programable de proposito general),
llamaremos a este paradigma el paradigma imperativo de Von Neumann.

\bigskip

\subsection{\label{BasicosMaquinasDeTuring}El paradigma de Turing}

Estudiaremos el concepto de maquina de Turing, el cual fue introducido por
Alam Turing para formalizar o modelizar matematicamente la idea de
procedimiento efectivo. Una vez definidas las maquinas podremos dar una
modelizacion matematica precisa del concepto de funcion $\Sigma$-efectivamente
computable. Llamaremos a estas funciones $\Sigma$-Turing computables y seran
aquellas que (en algun sentido que sera bien precisado matematicamente) pueden
ser computadas por una maquina de Turing. Por supuesto, la fidedignidad de
este concepto, es decir cuan buena es la modelizacion matematica dada por
Turing, puede no ser clara al comienzo pero a medida que vayamos avanzando en
nuestro estudio y conozcamos ademas los otros paradigmas y su relacion,
quedara claro que el modelo de Turing es acertado.

Vivimos en un mundo plagado de maquinas (ascensores, celulares, relojes,
taladros, etc). Una caracteristica comun a todas las maquinas es que tienen
distintos estados posibles. Un estado es el conjunto de caracteristicas que
determinan un momento concreto posible de la maquina cuando esta funcionando.
Por ejemplo un estado posible de un ascensor seria:

\begin{enumerate}
\item[-] esta en el tercer piso, con la primer puerta abierta y la otra
cerrada, esta apretado el boton de ir al sexto piso, etc
\end{enumerate}

\noindent donde ponemos etc porque dependiendo del tipo de ascensor (si es con
memoria, a que pisos puede ir, etc) habra mas datos que especificar para
determinar un estado concreto.

Otra caracteristica comun de las maquinas es que interactuan de distintas
formas con el usuario o mas generalmente su entorno. Dependiendo de que accion
se ejecute sobre la maquina y en que estado este, la maquina realizara alguna
tarea y ademas cambiara de estado. En general las maquinas son
\textit{deterministicas} en el sentido que siempre que esten en determinado
estado y se les aplique determinada accion, realizaran la misma tarea y
pasaran al mismo estado.\bigskip

\subsubsection{Descripcion informal de las maquinas de Turing}

Son un modelo abstracto de maquina con una cantidad finita de estados la cual
trabaja sobre una cinta de papel dividida en cuadros e interactua o recibe
acciones externas por medio de una cabeza lectora que lee de a un cuadro de la
cinta a la ves y ademas puede borrar el contenido del cuadro leido y escribir
en el un simbolo. Tambien la cabeza lectora puede moverse un cuadro hacia la
izquierda o hacia la derecha. La cinta tiene un primer cuadro hacia su
izquierda pero hacia la derecha puede extenderse todo lo necesario. En un
cuadro de la cinta podra haber un simbolo o un cuadro puede simplemente estar
en blanco. Es decir que habra un alfabeto $\Gamma$ el cual consiste de todos
los simbolos que pueden figurar en la cinta. Esto sera parte de los datos o
caracteristicas de cada maquina, es decir, $\Gamma$ puede cambiar dependiendo
de la maquina. La maquina, en funcion del estado en que se encuentre y de lo
que vea su cabeza lectora en el cuadro escaneado, podra moverse a lo sumo un
cuadro (izquierda, derecha o quedarse quieta), modificar lo que encuentre en
dicho cuadro (borrando y escribiendo algun nuevo simbolo) y cambiar de estado
(posiblemente al mismo que tenia). Para simplificar supondremos que hay en
$\Gamma$ un simbolo el cual si aparece en un cuadro de la cinta, significara
que dicho cuadro esta sin escribir o en blanco. Esto nos permitira describir
mas facilmente el funcionamiento de la maquina. En gral llamaremos $B$ a tal
simbolo. Tambien por lo general llamaremos $Q$ al conjunto de estados de la maquina.

Tambien cada maquina tendra un estado especial el cual sera llamado su estado
inicial, generalmente denotado con $q_{0}$, el cual sera el estado en el que
estara la maquina al comenzar a trabajar sobre la cinta. Hay otras
caracteristicas que tendran las maquinas de Turing pero para dar un primer
ejemplo ya nos basta. Describiremos una maquina de Turing $M$ que tendra
$\Gamma=\{@,a,b,B\}$ y tendra dos estados, es decir $Q=\{q_{0},q_{1}\}$.
Obviamente $q_{0}$ sera su estado inicial y ademas el "comportamiento o
personalidad" de $M$ estara dado por las siguientes clausulas:

\begin{enumerate}
\item[-] Estando en estado $q_{0}$ si ve ya sea $b$ o $B$ o $@$, entonces se
queda en estado $q_{0}$ y se mueve a la derecha

\item[-] Estando en estado $q_{0}$ si ve $a$ entonces reescribe $@$, se mueve
a la izquierda y cambia al estado $q_{1}$

\item[-] Estando en estado $q_{1}$ si ve $a$ o $b$ o $B$ o $@$ entonces lo
deja como esta, se mueve a la izquierda y queda en estado $q_{1}$
\end{enumerate}

Supongamos ahora que tomamos una palabra $\alpha\in\Gamma^{\ast}$ y la
distribuimos en la cinta dejando el primer cuadro en blanco y luego poniendo
los simbolos de $\alpha$ en los siguientes cuadros. Supongamos ademas que
ponemos la maquina en estado $q_{0}$ y con su cabeza lectora escaneando el
primer cuadro de la cinta. Esto lo podemos representar graficamente de la
siguiente manera%
\[%
\begin{array}
[c]{cccccccc}%
B & \alpha_{1} & ... & \alpha_{n} & B & B & B & ...\\
\uparrow &  &  &  &  &  &  & \\
q_{0} &  &  &  &  &  &  &
\end{array}
\]
donde $\alpha_{1},...,\alpha_{n}$ son los sucesivos simbolos de $\alpha$.
Supongamos ademas que $a$ ocurre an $\alpha$. Dejamos al lector ir aplicando
las clausulas de $M$ para convencerse que luego de un rato de funcionar $M$,
la situacion sera%
\[%
\begin{array}
[c]{cccccccc}%
B & \beta_{1} & ... & \beta_{n} & B & B & B & ...\\
\uparrow &  &  &  &  &  &  & \\
q_{1} &  &  &  &  &  &  &
\end{array}
\]
donde $\beta_{1}...\beta_{n}$ es el resultado de reemplazar en $\alpha$ la
primer ocurrencia de $a$ por $@$. Dejamos como ejercicio para el lector
averiguar que sucede cuando $a$ no ocurre en $\alpha$

\bigskip

Una cosa que puede pasar es que para un determinado estado $p$ y un $\sigma
\in\Gamma$, la maquina no tenga contemplada ninguna accion posible. Por
ejemplo sea $M$ la maquina de Turing dada por $Q=\{q_{0}\}$, $\Gamma
=\{@,\$,B\}$ y por la siguiente clausula:

\begin{enumerate}
\item[-] Estando en estado $q_{0}$ si ve ya sea $@$ o $B$, entonces se queda
en estado $q_{0}$ y se mueve a la derecha
\end{enumerate}

Es facil ver que si partimos de una situacion%
\[%
\begin{array}
[c]{cccccccc}%
B & \alpha_{1} & ... & \alpha_{n} & B & B & B & ...\\
\uparrow &  &  &  &  &  &  & \\
q_{0} &  &  &  &  &  &  &
\end{array}
\]
donde $\alpha_{1},...,\alpha_{n}\in\Gamma$, entonces si ningun $\alpha_{i}$ es
igual a $\$$, la maquina se movera indefinidamente hacia la derecha y en caso
contrario se movera $i$ pasos a la derecha y se detendra, donde $i$ es el
menor $l$ tal que $\alpha_{l}=\$$.

Otro caso posible de detencion de una maquina de Turing es cuando esta
escaneando el primer cuadro de la cinta y su unica accion posible implica
moverse un cuadro a la izquierda. Tambien en estos casos diremos que la
maquina se detiene ya que la cinta no es extensible hacia la izquierda.

Otra caracteristica de las maquinas de Turing es que poseen un
\textit{alfabeto de entrada} el cual esta contenido en el alfabeto $\Gamma$ y
en el cual estan los simbolos que se usaran para formar la configuracion
inicial de la cinta (exepto $B$). En general lo denotaremos con $\Sigma$ al
alfabeto de entrada y los simbolos de $\Gamma-\Sigma$ son considerados
auxiliares. Tambien habra un conjunto $F$ contenido en el conjunto $Q$ de los
estados de la maquina, cuyos elementos seran llamados \textit{estados finales}.

Diremos que una palabra $\alpha=\alpha_{1}...\alpha_{n}\in\Sigma^{\ast} $ es
\textit{aceptada por }$M$ \textit{por alcance de estado final} si partiendo de%
\[%
\begin{array}
[c]{cccccccc}%
B & \alpha_{1} & ... & \alpha_{n} & B & B & B & ...\\
\uparrow &  &  &  &  &  &  & \\
q_{0} &  &  &  &  &  &  &
\end{array}
\]
en algun momento de la computacion $M$ esta en un estado de $F$. Llamaremos
$L(M)$ al conjunto formado por todas las palabras que son aceptadas por
alcance de estado final

Diremos que una palabra $\alpha=\alpha_{1}...\alpha_{n}\in\Sigma^{\ast} $ es
\textit{aceptada por }$M$ \textit{por detencion} si partiendo de%
\[%
\begin{array}
[c]{cccccccc}%
B & \alpha_{1} & ... & \alpha_{n} & B & B & B & ...\\
\uparrow &  &  &  &  &  &  & \\
q_{0} &  &  &  &  &  &  &
\end{array}
\]
en algun momento $M$ se detiene. Llamaremos $H(M)$ al conjunto formado por
todas las palabras que son aceptadas por detencion

\bigskip

\subsubsection{Definicion matematica de maquina de Turing}

Una \textit{maquina de Turing }es una 7-upla $M=\left(  Q,\Sigma,\Gamma
,\delta,q_{0},B,F\right)  $ donde

\begin{enumerate}
\item[-] $Q$ es un conjunto finito cuyos elementos son llamados
\textit{estados}

\item[-] $\Gamma$ es un alfabeto que contiene a $\Sigma$

\item[-] $\Sigma$ es un alfabeto llamado el \textit{alfabeto de entrada}

\item[-] $B\in\Gamma-\Sigma$ es un simbolo de $\Gamma$ llamado el
\textit{blank symbol}

\item[-] $\delta:D_{\delta}\subseteq Q\times\Gamma\rightarrow Q\times
\Gamma\times\{L,R,K\}$

\item[-] $q_{0}$ es un estado llamado el \textit{estado inicial} de $M$

\item[-] $F\subseteq Q$ es un conjunto de estados llamados \textit{finales}
\end{enumerate}

\bigskip

Notese que la funcion $\delta$ da la "personalidad" de la maquina. Aqui los
simbolos $L,R,K$ serviran para especificar que hace el cabezal. O sea:

\begin{enumerate}
\item[-] $\delta(p,\sigma)=(q,\gamma,L)$ significara que la maquina estando en
estado $p$ y leyendo el simbolo $\sigma$ borrara $\sigma$ y escribira $\gamma$
en su lugar y luego se movera un cuadro a la izquierda (esto en caso que el
cabezal no este en el cuadro de mas a la izquierda, en cuyo caso no podra
realizar dicha tarea y se detendra).

\item[-] $\delta(p,\sigma)=(q,\gamma,K)$ significara que la maquina estando en
estado $p$ y leyendo el simbolo $\sigma$ borrara $\sigma$ y escribira $\gamma$
en su lugar y luego el cabezal se quedara kieto

\item[-] $\delta(p,\sigma)=(q,\gamma,R)$ significara que la maquina estando en
estado $p$ y leyendo el simbolo $\sigma$ borrara $\sigma$ y escribira $\gamma$
en su lugar y luego el cabezal se movera un cuadro a la derecha
\end{enumerate}

\bigskip

Si bien en nuestra definicion de maquina de Turing no hay ninguna restriccion
acerca de la naturaleza de los elementos de $Q$, para continuar nuestro
analisis asumiremos siempre que $Q$ es un alfabeto disjunto con $\Gamma$. Esto
nos permitira dar definiciones matematicas precisas que formalizaran el
funcionamiento de las maquinas de Turing en el contexto de las funciones
mixtas. Deberia quedar claro que el hecho que solo analicemos maquinas en las
cuales $Q$ es un alfabeto disjunto con $\Gamma$, no afectara la profundidad y
generalidad de nuestros resultados.

\paragraph{Descripciones instantaneas}

Una \textit{descripcion instantanea} sera una palabra de la forma $\alpha
q\beta$, donde $\alpha,\beta\in\Gamma^{\ast}$, $\left[  \beta\right]
_{\left\vert \beta\right\vert }\neq B$ y $q\in Q$. La descripcion instantanea
$\alpha_{1}...\alpha_{n}q\beta_{1}...\beta_{m}$, con $\alpha_{1}%
,...,\alpha_{n}$, $\beta_{1},...,\beta_{m}\in\Gamma$, $n,m\geq0$ representara
la siguiente situacion%
\[%
\begin{array}
[c]{cccccccccccc}%
\alpha_{1} & \alpha_{2} & ... & \alpha_{n} & \beta_{1} & \beta_{2} & ... &
\beta_{m} & B & B & B & ...\\
&  &  &  & \uparrow &  &  &  &  &  &  & \\
&  &  &  & q &  &  &  &  &  &  &
\end{array}
\]
Usaremos $Des$ para denotar el conjunto de las descripciones instantaneas.
Definamos la funcion $St:Des\rightarrow Q$, de la siguiente manera%
\[
St(d)=\text{unico simbolo de }Q\text{ que ocurre en }d
\]


\paragraph{La relacion $\vdash$}

Dado $\alpha\in(Q\cup\Gamma)^{\ast}$, definamos $\left\lfloor \alpha
\right\rfloor $ de la siguiente manera%
\begin{align*}
\left\lfloor \varepsilon\right\rfloor  & =\varepsilon\\
\left\lfloor \alpha\sigma\right\rfloor  & =\alpha\sigma\text{, si }\sigma\neq
B\\
\left\lfloor \alpha B\right\rfloor  & =\left\lfloor \alpha\right\rfloor
\end{align*}
Es decir $\left\lfloor \alpha\right\rfloor $ es el resultado de remover de
$\alpha$ el tramo final mas grande de la forma $B^{n}$. Dada cualquier palabra
$\alpha$ definimos%
\[
^{\curvearrowright}\alpha=\left\{
\begin{array}
[c]{lll}%
\left[  \alpha\right]  _{2}...\left[  \alpha\right]  _{\left\vert
\alpha\right\vert } & \text{si} & \left\vert \alpha\right\vert \geq2\\
\varepsilon & \text{si} & \left\vert \alpha\right\vert \leq1
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \alpha^{\curvearrowleft
}=\left\{
\begin{array}
[c]{lll}%
\left[  \alpha\right]  _{1}...\left[  \alpha\right]  _{\left\vert
\alpha\right\vert -1} & \text{si} & \left\vert \alpha\right\vert \geq2\\
\varepsilon & \text{si} & \left\vert \alpha\right\vert \leq1
\end{array}
\right.
\]
Dadas $d_{1},d_{2}\in Des$, escribiremos $d_{1}\vdash d_{2}$ cuando existan
$\sigma\in\Gamma$, $\alpha,\beta\in\Gamma^{\ast}$ y $p,q\in Q$ tales que se
cumple alguno de los siguientes casos

Caso 1.%
\begin{align*}
d_{1}  & =\alpha p\beta\\
\delta\left(  p,\left[  \beta B\right]  _{1}\right)   & =(q,\sigma,R)\\
d_{2}  & =\alpha\sigma q^{\curvearrowright}\beta
\end{align*}


Caso 2.%
\begin{align*}
d_{1}  & =\alpha p\beta\\
\delta\left(  p,\left[  \beta B\right]  _{1}\right)   & =(q,\sigma,L)\text{ y
}\alpha\neq\varepsilon\\
d_{2}  & =\left\lfloor \alpha^{\curvearrowleft}q\left[  \alpha\right]
_{\left\vert \alpha\right\vert }\sigma^{\curvearrowright}\beta\right\rfloor
\end{align*}


Caso 3.%
\begin{align*}
d_{1}  & =\alpha p\beta\\
\delta(p,\left[  \beta B\right]  _{1})  & =(q,\sigma,K)\\
d_{2}  & =\left\lfloor \alpha q\sigma^{\curvearrowright}\beta\right\rfloor
\end{align*}
Escribiremos $d\nvdash d^{\prime}$ para expresar que no se da $d\vdash
d^{\prime}$. Para $d,d^{\prime}\in Des$ y $n\geq0$, escribiremos $d\overset
{n}{\vdash}d^{\prime}$ si existen $d_{1},...,d_{n+1}\in Des$ tales que%
\begin{align*}
d  & =d_{1}\\
d^{\prime}  & =d_{n+1}\\
d_{i}  & \vdash d_{i+1}\text{, para }i=1,...,n.
\end{align*}
Notese que $d\overset{0}{\vdash}d^{\prime}$ sii $d=d^{\prime}$. Finalmente
definamos%
\[
d\overset{\ast}{\vdash}d^{\prime}\text{ sii }(\exists n\in\omega
)\;d\overset{n}{\vdash}d^{\prime}\text{.}%
\]


\paragraph{Detencion}

Dada $d\in Des$, diremos que $M$ \textit{se detiene partiendo de }$d$ si
existe $d^{\prime}\in Des$ tal que

\begin{enumerate}
\item[-] $d\overset{\ast}{\vdash}d^{\prime}$

\item[-] $d^{\prime}\nvdash d^{\prime\prime}$, para cada $d^{\prime\prime}\in
Des$
\end{enumerate}

\noindent Deberia quedar claro que es posible que $\alpha p\beta\nvdash d$,
para cada descripcion instantanea $d$, y que $\delta(p,[\beta B]_{1})$ sea no vacio.

\paragraph{El lenguaje $L(M)$}

Diremos que una palabra $\alpha\in\Sigma^{\ast}$ es \textit{aceptada por }$M$
\textit{por alcance de estado final} cuando%
\[
\left\lfloor q_{0}B\alpha\right\rfloor \overset{\ast}{\vdash}d\text{, con
}d\text{ tal que }St(d)\in F.
\]
El \textit{lenguage aceptado por }$M$ \textit{por alcance de estado final} se
define de la siguiente manera%
\[
L(M)=\{\alpha\in\Sigma^{\ast}:\alpha\text{ es aceptada por }M\text{ por
alcance de estado final}\}\text{.}%
\]


\paragraph{El lenguaje $H(M)$}

Diremos que una palabra $\alpha\in\Sigma^{\ast}$ es \textit{aceptada por }$M$
\textit{por detencion} cuando $M$ se detiene partiendo de $\left\lfloor
q_{0}B\alpha\right\rfloor $. El \textit{lenguage aceptado por }$M$ \textit{por
detencion} se define de la siguiente manera%
\[
H(M)=\{\alpha\in\Sigma^{\ast}:\alpha\text{ es aceptada por }M\text{ por
detencion}\}
\]


\bigskip

\subsubsection{Funciones $\Sigma$-Turing computables}

Para poder computar funciones mixtas con una maquina de Turing necesitaremos
un simbolo para representar numeros sobre la cinta. Llamaremos a este simbolo
\textit{unit} y lo denotaremos con $\shortmid$. Mas formalmente una
\textit{maquina de Turing con unit} es una 8-upla $M=\left(  Q,\Sigma
,\Gamma,\delta,q_{0},B,\shortmid,F\right)  $ tal que $\left(  Q,\Sigma
,\Gamma,\delta,q_{0},B,F\right)  $ es una maquina de Turing y $\shortmid$ es
un simbolo distingido perteneciente a $\Gamma-(\{B\}\cup\Sigma)$.

Diremos que una funcion $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast
m}\rightarrow\Sigma^{\ast}$ es $\Sigma$-\textit{Turing computable} si existe
una maquina de Turing con unit, $M=\left(  Q,\Sigma,\Gamma,\delta
,q_{0},B,\shortmid,F\right)  $ tal que:

\begin{enumerate}
\item[(1)] Si $(\vec{x},\vec{\alpha})\in D_{f}$, entonces hay un $p\in Q$ tal
que%
\[
\left\lfloor q_{0}B\shortmid^{x_{1}}B...B\shortmid^{x_{n}}B\alpha
_{1}B...B\alpha_{m}\right\rfloor \overset{\ast}{\vdash}\left\lfloor
pBf(\vec{x},\vec{\alpha})\right\rfloor
\]
y $\left\lfloor pBf(\vec{x},\vec{\alpha})\right\rfloor \nvdash d$, para cada
$d\in Des$

\item[(2)] Si $(\vec{x},\vec{\alpha})\in\omega^{n}\times\Sigma^{\ast m}-D_{f}%
$, entonces $M$ no se detiene partiendo de%
\[
\left\lfloor q_{0}B\shortmid^{x_{1}}B...B\shortmid^{x_{n}}B\alpha
_{1}B...B\alpha_{m}\right\rfloor .
\]

\end{enumerate}

\noindent En forma similar, una funcion $f:D_{f}\subseteq\omega^{n}%
\times\Sigma^{\ast}{}^{m}\rightarrow\omega$, es llamada $\Sigma$%
-\textit{Turing computable} si existe una maquina de Turing con unit,
$M=\left(  Q,\Sigma,\Gamma,\delta,q_{0},B,\shortmid,F\right)  $, tal que:

\begin{enumerate}
\item[(1)] Si $(\vec{x},\vec{\alpha})\in D_{f}$, entonces hay un $p\in Q$ tal
que%
\[
\left\lfloor q_{0}B\shortmid^{x_{1}}B...B\shortmid^{x_{n}}B\alpha
_{1}B...B\alpha_{m}\right\rfloor \overset{\ast}{\vdash}\left\lfloor
pB\shortmid^{f(\vec{x},\vec{\alpha})}\right\rfloor
\]
y $\left\lfloor pB\shortmid^{f(\vec{x},\vec{\alpha})}\right\rfloor \nvdash d
$, para cada $d\in Des$

\item[(2)] Si $(\vec{x},\vec{\alpha})\in\omega^{n}\times\Sigma^{\ast m}-D_{f}%
$, entonces $M$ no se detiene partiendo de%
\[
\left\lfloor q_{0}B\shortmid^{x_{1}}B...B\shortmid^{x_{n}}B\alpha
_{1}B...B\alpha_{m}\right\rfloor
\]

\end{enumerate}

\noindent Cuando $M$ y $f$ cumplan los items (1) y (2) de la definicion
anterior, diremos que la funcion $f$ es \textit{computada} por $M$.

Por supuesto esta definicion no tendria sentido como modelo matematico del
concepto de funcion $\Sigma$-efectivamente computable si no sucediera que toda
funcion $\Sigma$-Turing computable fuera $\Sigma$-efectivamente computable.
Este hecho es intuitivamente claro y lo expresamos en forma de proposicion.

\begin{proposition}
Sean $n,m\in\omega$ y $O\in\{\omega,\Sigma^{\ast}\}$. Si $f:D_{f}%
\subseteq\omega^{n}\times\Sigma^{\ast}{}^{m}\rightarrow O$ es computada por
una maquina de Turing con unit $M=\left(  Q,\Sigma,\Gamma,\delta
,q_{0},B,\shortmid,F\right)  $, entonces $f$ es $\Sigma$-efectivamente computable.
\end{proposition}

\begin{proof}
Haremos el caso $O=\Sigma^{\ast}$. Sea $\mathbb{P}$ el siguiente procedimiento efectivo.

- Conjunto de datos de entrada de $\mathbb{P}$ igual a $\omega^{n}\times
\Sigma^{\ast}{}^{m}$

- Conjunto de datos de salida de $\mathbb{P}$ contenido en $O$

- Funcionamiento: Hacer funcionar paso a paso la maquina $M$ partiendo de la
descripcion instantanea $\left\lfloor q_{0}B\shortmid^{x_{1}}B...B\shortmid
^{x_{n}}B\alpha_{1}B...B\alpha_{m}\right\rfloor $. Si en alguna instancia $M$
termina, dar como salida el resultado de remover de la descripcion instantanea
final los dos primeros simbolos.

Notese que este procedimiento termina solo en aquelos elementos $(\vec{x}%
,\vec{\sigma})\in\omega^{n}\times\Sigma^{\ast}{}^{m}$ tales que la maquina $M$
termina partiendo desde%
\[
\left\lfloor q_{0}B\shortmid^{x_{1}}B...B\shortmid^{x_{n}}B\alpha
_{1}B...B\alpha_{m}\right\rfloor
\]
por lo cual termina solo en los elementos de $D_{f}$ ya que $M$ computa a $f$.
Ademas es claro que en caso de terminacion el procedimiento da como salida
$f(\vec{x},\vec{\sigma})$.
\end{proof}

\bigskip

Sin envargo el modelo Turingniano podria a priori no ser del todo correcto ya
que podria pasar que haya una funcion que sea computada por un procedimiento
efectivo pero que no exista una maquina de Turing que la compute. En otras
palabras el modelo podria ser incompleto. La completitud de este modelo puede
no ser clara al comienzo pero a medida que vayamos avanzando en nuestro
estudio y conozcamos ademas los otros paradigmas y su relacion, quedara claro
que el modelo de Turing es acertado.

\subsubsection{Conjuntos $\Sigma$-Turing enumerables}

Ya que la nocion de funcion $\Sigma$-Turing computable es el modelo matematico
de Turing del concepto de funcion $\Sigma$-efectivamente computable, nos
podriamos preguntar entonces cual es el modelo matematico de Turing del
concepto de conjunto $\Sigma$-efectivamente enumerable. Si prestamos atencion
a la definicion de conjunto $\Sigma$-efectivamente enumerable, notaremos que
depende de la existencia de ciertas funciones $\Sigma$-efectivamente
computables por lo cual la siguiente definicion cae de maduro:

Diremos que un conjunto $S\subseteq\omega^{n}\times\Sigma^{\ast m}$ sera
llamado $\Sigma$\textit{-Turing enumerable} cuando sea vacio o haya una
funcion $F:\omega\rightarrow\omega^{n}\times\Sigma^{\ast m}$ tal que $I_{F}=S$
y $F_{(i)}$ sea $\Sigma$-Turing computable, para cada $i\in\{1,...,n+m\}$.

Deberia quedar claro que si el concepto de funcion $\Sigma$-Turing computable
modeliza correctamente al concepto de funcion $\Sigma$-efectivamente
computable, entonces el concepto de conjunto $\Sigma$-Turing enumerable recien
definido modeliza correctamente al concepto de conjunto $\Sigma$-efectivamente
enumerable. Notese que segun la definicion que acabamos de escribir, un
conjunto no vacio $S\subseteq\omega^{n}\times\Sigma^{\ast m}$ es $\Sigma
$-Turing enumerable si y solo si hay maquinas de Turing deterministicas con
unit%
\begin{align*}
M_{1}  & =\left(  Q_{1},\Sigma,\Gamma_{1},\delta_{1},q_{01},B,\shortmid
,F_{1}\right) \\
M_{2}  & =\left(  Q_{2},\Sigma,\Gamma_{2},\delta_{2},q_{02},B,\shortmid
,F_{2}\right) \\
& \vdots\\
M_{n+m}  & =\left(  Q_{n+m},\Sigma,\Gamma_{n+m},\delta_{n+m},q_{0n+m}%
,B,\shortmid,F_{n+m}\right)
\end{align*}
tales que

\begin{enumerate}
\item[-] cada $M_{i}$, con $i=1,...,n$, computa una funcion $F_{i}%
:\omega\rightarrow\omega$

\item[-] cada $M_{i}$, con $i=n+1,...,n+m$, computa una funcion $F_{i}%
:\omega\rightarrow\Sigma^{\ast}$

\item[-] $S=\operatorname{Im}[F_{1},...,F_{n+m}]$
\end{enumerate}

\noindent Como puede notarse las maquinas $M_{1},...,M_{n+m}$ puestas en
paralelo a funcionar desde la descripciones instantaneas%
\begin{align*}
& \left\lfloor q_{01}B\shortmid^{x}\right\rfloor \\
& \left\lfloor q_{02}B\shortmid^{x}\right\rfloor \\
& \ \ \ \ \ \ \ \ \ \vdots\\
& \left\lfloor q_{0n+m}B\shortmid^{x}\right\rfloor
\end{align*}
producen en forma natural un procedimiento efectivo (con dato de entrada
$x\in\omega$) que enumera a $S$ \ Por supuesto podemos decir que en tal caso
las maquinas $M_{1},...,M_{n+m}$ enumeran a $S$. La siguiente proposicion
muestra que tambien las cosas se pueden hacer con una sola maquina de Turing.

\bigskip

\begin{proposition}
Sea $S\subseteq\omega^{n}\times\Sigma^{\ast m}$ un conjunto no vacio. Entonces
$S$ es $\Sigma$-Turing enumerable si y solo si hay una maquina de Turing
deterministica con unit $M=\left(  Q,\Sigma,\Gamma,\delta,q_{0},B,\shortmid
,F\right)  $, tal que:

\begin{enumerate}
\item[(1)] Para cada $x\in\omega$, tenemos que $M$ se detiene partiendo de
$\left\lfloor q_{0}B\shortmid^{x}\right\rfloor $ y llega a una descripcion
instantanea de la forma $\left\lfloor qB\shortmid^{x_{1}}B...B\shortmid
^{x_{n}}B\alpha_{1}B...B\alpha_{m}\right\rfloor $, con $(\vec{x},\vec{\alpha
})\in S$.

\item[(2)] Para cada $(\vec{x},\vec{\alpha})\in S$ hay un $x\in\omega$ tal que
$M$ se detiene partiendo de $\left\lfloor q_{0}B\shortmid^{x}\right\rfloor $ y
llega a una descripcion instantanea de la forma $\left\lfloor qB\shortmid
^{x_{1}}B...B\shortmid^{x_{n}}B\alpha_{1}B...B\alpha_{m}\right\rfloor $
\end{enumerate}
\end{proposition}

\begin{proof}
Queda como ejercicio ver como construir la maquina $M$ utilizando las maquinas
$M_{1},...,M_{n+m}$ y reciprocamente ver como a partir de una maquina $M$ con
las propiedades (1) y (2) se pueden construir las maquinas $M_{1},...,M_{n+m}$.
\end{proof}

\bigskip

\subsubsection{Conjuntos $\Sigma$-Turing computables}

La version Turingniana del concepto de conjunto $\Sigma$-efectivamente
computable es facil de dar: un conjunto $S\subseteq\omega^{n}\times
\Sigma^{\ast m}$ sera llamado $\Sigma$\textit{-Turing computable} cuando la
funcion $\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}}$ sea $\Sigma$-Turing
computable. O sea que $S\subseteq\omega^{n}\times\Sigma^{\ast m}$ es $\Sigma
$-Turing computable sii hay una maquina de Turing deterministica con unit
$M=\left(  Q,\Sigma,\Gamma,\delta,q_{0},B,\shortmid,F\right)  $ la cual
computa a $\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}}$, es decir:

\begin{enumerate}
\item[-] Si $(\vec{x},\vec{\alpha})\in S$, entonces hay un $p\in Q$ tal que%
\[
\left\lfloor q_{0}B\shortmid^{x_{1}}B...B\shortmid^{x_{n}}B\alpha
_{1}B...B\alpha_{m}\right\rfloor \overset{\ast}{\vdash}\left\lfloor
pB\shortmid\right\rfloor
\]
y $\left\lfloor pB\shortmid\right\rfloor \nvdash d$, para cada $d\in Des$

\item[-] Si $(\vec{x},\vec{\alpha})\in(\omega^{n}\times\Sigma^{\ast m})-S $,
entonces hay un $p\in Q$ tal que%
\[
\left\lfloor q_{0}B\shortmid^{x_{1}}B...B\shortmid^{x_{n}}B\alpha
_{1}B...B\alpha_{m}\right\rfloor \overset{\ast}{\vdash}\left\lfloor
pB\right\rfloor
\]
y $\left\lfloor pB\right\rfloor \nvdash d$, para cada $d\in Des$
\end{enumerate}

Si $M$ es una maquina de Turing la cual computa a $\chi_{S}^{\omega^{n}%
\times\Sigma^{\ast m}}$, diremos que $M$ \textit{decide la pertenecia a }$S$,
con respecto al conjunto $\omega^{n}\times\Sigma^{\ast m}$.

\bigskip

\subsection{El paradigma de Godel: Funciones $\Sigma$-recursivas}

En esta seccion desarrollaremos el modelo matematico del concepto de funcion
$\Sigma$-efectivamente computable, dado por Godel. Dichas funciones seran
llamadas $\Sigma$-recursivas. La idea es partir de un conjunto inicial de
funciones muy simples y obviamente $\Sigma$-efectivamente computables y luego
obtener nuevas funciones $\Sigma$-efectivamente computables usando
constructores que preservan la computabilidad efectiva. Las funciones $\Sigma
$-recursivas seran las que se obtienen iterando el uso de estos constructores,
partiendo del conjunto inicial de funciones antes mencionado. Nos referiremos
a este paradigma como el paradigma Godeliano o recursivo. A veces tambien lo
llamaremos el paradigma funcional.

La familia de funciones simples y obviamente $\Sigma$-efectivamente
computables de la que partiremos es la siguiente%
\[
\left\{  Suc,Pred,C_{0}^{0,0},C_{\varepsilon}^{0,0}\right\}  \cup\left\{
d_{a}:a\in\Sigma\right\}  \cup\left\{  p_{j}^{n,m}:1\leq j\leq n+m\right\}
\]
Los constructores que usaremos son:

\begin{enumerate}
\item[-] Composicion

\item[-] Recursion primitiva

\item[-] Minimizacion de predicados totales
\end{enumerate}

\bigskip

Estos constructores nos permiten dadas ciertas funciones construir o definir
una nueva funcion y tienen la propiedad de preservar la computabilidad
efectiva en el sentido que si las funciones iniciales son $\Sigma
$-efectivamente computables, entonces la funcion obtenida tambien lo es. Un
concepto fundamental es el de funcion $\Sigma$-recursiva primitiva. Estas
funciones seran aquellas que se obtienen a partir de las del conjunto inicial
usando solo los dos primeros constructores: composicion y recursion primitiva.
Nuestro primer objetivo es definir el concepto de funcion $\Sigma$-recursiva
primitiva para lo cual en las proximas dos secciones definiremos y
estudiaremos los constructores de composicion y recursion primitiva. Luego
definiremos el concepto de funcion $\Sigma$-recursiva primitiva y nos
abocaremos a desarrollar este concepto fundamental. Recien despues
estudiaremos el constructor de minimizacion y definiremos el concepto de
funcion $\Sigma$-recursiva. La ultima parte de la seccion esta destinada a
probar un teorema que nos dice que los conceptos de funcion $\Sigma$-recursiva
y $\Sigma$-recursiva primitiva son independientes del alfabeto $\Sigma$.

\bigskip

\subsubsection{Composicion}

Dadas funciones $\Sigma$-mixtas $f,f_{1},...,f_{r}$, con $r\geq1$, diremos que
la funcion $f\circ\lbrack f_{1},...,f_{r}]$ es \textit{obtenida por
composicion a partir de las funciones }$f,f_{1},...,f_{r}$. Para probar que la
composicion preserva la computabilidad efectiva necesitaremos el siguiente lema.

\begin{lemma}
Supongamos que $f,f_{1},...,f_{r}$ son funciones $\Sigma$-mixtas, con $r\geq
1$. Supongamos ademas que $f\circ\lbrack f_{1},...,f_{r}]\neq\emptyset$.
Entonces hay $n,m,k,l\in\omega$ y $s\in\{\#,\ast\}$ tales que

\begin{enumerate}
\item[-] $r=n+m$

\item[-] $f$ es de tipo $(n,m,s)$

\item[-] $f_{i}$ es de tipo $(k,l,\#)$, para cada $i=1,...,n$

\item[-] $f_{i}$ es de tipo $(k,l,\ast)$, para cada $i=n+1,...,n+m$
\end{enumerate}

Mas aun, en tal caso la funcion $f\circ\lbrack f_{1},...,f_{n+m}]$ es de tipo
$(k,l,s)$ y:%
\begin{align*}
D_{f\circ\lbrack f_{1},...,f_{n+m}]}  & =\left\{  (\vec{x},\vec{\alpha}%
)\in\bigcap_{i=1}^{n+m}D_{f_{i}}:(f_{1}(\vec{x},\vec{\alpha}),...,f_{n+m}%
(\vec{x},\vec{\alpha}))\in D_{f}\right\} \\
f\circ\lbrack f_{1},...,f_{n+m}](\vec{x},\vec{\alpha})  & =f(f_{1}(\vec
{x},\vec{\alpha}),...,f_{n+m}(\vec{x},\vec{\alpha})).
\end{align*}

\end{lemma}

\begin{proof}
Notese que $f\neq\emptyset$ y $[f_{1},...,f_{r}]\neq\emptyset$ (por que?). Ya
que $f\neq\emptyset$ tenemos que hay unicos $n,m\in\omega$ y $s\in\{\#,\ast\}$
tales que $f$ es de tipo $(n,m,s)$. Ya que $f\circ\lbrack f_{1},...,f_{r}%
]\neq\emptyset$ y $I_{[f_{1},...,f_{r}]}\subseteq I_{f_{1}}\times...\times
I_{f_{r}}$, tenemos que

\begin{enumerate}
\item[-] $r=n+m$

\item[-] $I_{f_{i}}\subseteq\omega$, para cada $i=1,...,n$

\item[-] $I_{f_{i}}\subseteq\Sigma^{\ast}$, para cada $i=n+1,...,n+m$
\end{enumerate}

Ya que $[f_{1},...,f_{r}]\neq\emptyset$ tenemos que $D_{[f_{1},...,f_{r}%
]}=\bigcap_{i=1}^{r}D_{f_{i}}\neq\emptyset$, por lo cual los conjuntos
$D_{f_{1}},...,D_{f_{n+m}}$ deberan ser todos de un mismo tipo, digamos de
tipo $(k,l)$. Es decir que $f_{i}$ es de tipo $(k,l,\#)$, para cada
$i=1,...,n$ y $f_{i}$ es de tipo $(k,l,\ast)$, para cada $i=n+1,...,n+m$.

Las ultimas observaciones del lema son directas de las definiciones de
$[f_{1},...,f_{n+m}]$ y de composicion de funciones
\end{proof}

\bigskip

Ahora si podemos probar facilmente que el contructor composicion preserva la
computabilidad efectiva

\begin{lemma}
Si $f,f_{1},...,f_{r}$, con $r\geq1$, son $\Sigma$-efectivamente computables,
entonces $f\circ\lbrack f_{1},...,f_{r}]$ lo es.
\end{lemma}

\begin{proof}
Si $f\circ\lbrack f_{1},...,f_{r}]=\emptyset$, entonces claramente es $\Sigma
$-efectivamente computable. Supongamos entonces que $f\circ\lbrack
f_{1},...,f_{r}]\neq\emptyset$. Por el lema anterior hay $n,m,k,l\in\omega$ y
$s\in\{\#,\ast\}$ tales que

\begin{enumerate}
\item[-] $r=n+m$

\item[-] $f$ es de tipo $(n,m,s)$

\item[-] $f_{i}$ es de tipo $(k,l,\#)$, para cada $i=1,...,n$

\item[-] $f_{i}$ es de tipo $(k,l,\ast)$, para cada $i=n+1,...,n+m$
\end{enumerate}

Sean $\mathbb{P},\mathbb{P}_{1},...,\mathbb{P}_{n+m}$ procedimientos efectivos
los cuales computen las funciones $f,f_{1},...,f_{n+m}$, respectivamente.
Usando estos procedimientos es facil definir un procedimiento efectivo el cual
compute a $f\circ\lbrack f_{1},...,f_{n+m}]$.
\end{proof}

\bigskip

\subsubsection{Recursion primitiva}

La recursion primitiva es un tipo muy particular de recursion. Consideremos
por ejemplo las siguientes ecuaciones:

\begin{enumerate}
\item[(1)] $R(0)=1$

\item[(2)] $R(t+1)=1+R(t)+R(t)^{2}$
\end{enumerate}

Notese que hay una unica funcion $R:\omega\rightarrow\omega$ la cual cumple
(1) y (2). Esto es ya que el valor de $R$ en $t$ esta determinado por
sucesivas aplicaciones de las ecuaciones (1) y (2). Por ejemplo la ecuacion
(1) nos dice que $R(0)=1$ pero entonces la ecuacion (2) nos dice que
$R(1)=1+1+1^{2}=3$ por lo cual nuevamente la ecuacion (2) nos dice que
$R(2)=1+3+3^{2}=13$ y asi podemos notar facilmente que $R$ esta determinada
por dichas ecuaciones.

Se suele decir que las ecuaciones (1) y (2) definen recursivamente a la
funcion $R$ pero hay que tener cuidado porque esto es una manera de hablar ya
que la funcion $R$ podria en nuestro discurso ya haber sido definida de otra
manera. Mas propio es pensar que dichas ecuaciones determinan a $R$ en el
sentido que $R$ es la unica que las cumple. Por ejemplo las ecuaciones:

\begin{enumerate}
\item[(a)] $R(0)=50$

\item[(b)] $R(t+1)=R(t)$
\end{enumerate}

definen recursivamente a la funcion $C_{50}^{1,0}$ pero esta claro que la
definicion de $C_{50}^{1,0}$ en esta materia no fue dada de esta forma.

Hay casos de recursiones en las cuales el valor de $R(t+1)$ no solo depende de
$R(t)$ sino que tambien depende de $t$. Por ejemplo

\begin{enumerate}
\item[(i)] $R(0)=1$

\item[(ii)] $R(t+1)=t.R(t)+1$
\end{enumerate}

De todas maneras deberia quedar claro que las ecuaciones (i) y (ii) determinan
una unica funcion $R:\omega\rightarrow\omega$ que las satisface.

Tambien podemos generalizar pensando que la funcion $R$ depende no solo de un
parametro $t$ sino que su dominio es $\omega^{4}$, es decir depende de $t $ y
$x_{1},x_{2},x_{3}$. Por ejemplo

\begin{enumerate}
\item[(p)] $R(0,x_{1},x_{2},x_{3})=x_{1}+2x_{3}$

\item[(q)] $R(t+1,x_{1},x_{2},x_{3})=t+x_{1}+x_{2}+x_{3}+R(t,x_{1},x_{2}%
,x_{3})$
\end{enumerate}

\bigskip

Dejamos al lector convencerse de que (p) y (q) son cumplidas por una unica
funcion $R:\omega^{4}\rightarrow\omega$. Tambien podriamos tener variables
alfabeticas. Por ejemplo consideremos

\begin{enumerate}
\item[(r)] $R(0,x_{1},x_{2},\alpha_{1},\alpha_{2})=x_{1}+\left\vert \alpha
_{1}\right\vert ^{x_{2}}$

\item[(s)] $R(t+1,x_{1},x_{2},\alpha_{1},\alpha_{2})=t+x_{1}+x_{2}+\left\vert
\alpha_{1}\right\vert +\left\vert \alpha_{2}\right\vert +R(t,x_{1}%
,x_{2},\alpha_{1},\alpha_{2})$
\end{enumerate}

\bigskip

Es claro aqui que las ecuaciones (r) y (s) determinan una unica funcion
$R:\omega^{3}\times\Sigma^{\ast2}\rightarrow\omega$ que las cumple. Esto se
puede explicar de la siguiente manera:

\begin{enumerate}
\item[-] La ecuacion (r) determina los valores de $R$ sobre el conjunto
$\{0\}\times\omega\times\omega\times\Sigma^{\ast}\times\Sigma^{\ast} $. Pero
una ves determinados estos valores, la ecuacion (s) tomada con $t=0 $,
determina los valores de $R$ sobre el conjunto $\{1\}\times\omega\times
\omega\times\Sigma^{\ast}\times\Sigma^{\ast}$. Pero una ves determinados estos
valores, la ecuacion (s) tomada con $t=1$, determina los valores de $R$ sobre
el conjunto $\{2\}\times\omega\times\omega\times\Sigma^{\ast}\times
\Sigma^{\ast}$, etc
\end{enumerate}

\bigskip

El caso anterior podria generalizarse de la siguiente manera: Si tenemos dadas
dos funciones%
\begin{align*}
f  & :\omega^{n}\times\Sigma^{\ast m}\rightarrow\omega\\
g  & :\omega^{n+2}\times\Sigma^{\ast m}\rightarrow\omega
\end{align*}
entonces las ecuaciones:

\begin{enumerate}
\item[(a)] $R(0,\vec{x},\vec{\alpha})=f(\vec{x},\vec{\alpha})$

\item[(b)] $R(t+1,\vec{x},\vec{\alpha})=g(R(t,\vec{x},\vec{\alpha}),t,\vec
{x},\vec{\alpha})$
\end{enumerate}

determinan una unica funcion $R:\omega^{n+1}\times\Sigma^{\ast m}%
\rightarrow\omega$ que las cumple. Notese que para el caso%
\begin{align*}
n  & =m=2\\
f  & =\lambda x_{1}x_{2}\alpha_{1}\alpha_{2}[x_{1}+\left\vert \alpha
_{1}\right\vert ^{x_{2}}]\\
g  & =\lambda xtx_{1}x_{2}\alpha_{1}\alpha_{2}[t+x_{1}+x_{2}+\left\vert
\alpha_{1}\right\vert +\left\vert \alpha_{2}\right\vert +x]
\end{align*}
las ecuaciones (a) y (b) se transforman en las ecuaciones (r) y (s).

El primer caso de recursion primitiva que definiremos a continuacion engloba
los ejemplos vistos recien dentro de un marco general.

\bigskip

\paragraph{Recursion primitiva sobre variable numerica con valores numericos}

Supongamos tenemos dadas funciones%
\begin{align*}
f  & :S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m}%
\rightarrow\omega\\
g  & :\omega\times\omega\times S_{1}\times...\times S_{n}\times L_{1}%
\times...\times L_{m}\rightarrow\omega
\end{align*}
con $S_{1},...,S_{n}\subseteq\omega$ y $L_{1},...,L_{m}\subseteq\Sigma^{\ast}$
conjuntos no vacios. Usando el razonamiento inductivo usado en los ejemplos
anteriores, se puede probar que hay una unica funcion%

\[
R:\omega\times S_{1}\times...\times S_{n}\times L_{1}\times...\times
L_{m}\rightarrow\omega
\]
la cual cumple las ecuaciones

\begin{enumerate}
\item[-] $R(0,\vec{x},\vec{\alpha})=f(\vec{x},\vec{\alpha})$

\item[-] $R(t+1,\vec{x},\vec{\alpha})=g(R(t,\vec{x},\vec{\alpha}),t,\vec
{x},\vec{\alpha})$
\end{enumerate}

LLamaremos $R(f,g)$ a esta unica funcion que cumple las ecuaciones anteriores.
Resumiendo, diremos que las ecuaciones

\begin{enumerate}
\item[(1)] $R(f,g)(0,\vec{x},\vec{\alpha})=f(\vec{x},\vec{\alpha})$

\item[(2)] $R(f,g)(t+1,\vec{x},\vec{\alpha})=g(R(f,g)(t,\vec{x},\vec{\alpha
}),t,\vec{x},\vec{\alpha})$
\end{enumerate}

definen recursivamente a la funcion $R(f,g)$. Tambien diremos que $R(f,g)$ es
obtenida por \textit{recursion primitiva} a partir de $f$ y $g$.

\bigskip

NOTA\ IMPOTANTE: No confundirse y pensar que $R(f,g)$ es el resultado de
aplicar una funcion $R$ al par $(f,g)$, de hecho hasta el momento no hemos
definido ninguna funcion $R$ cuyo dominio sea cierto conjunto de pares
ordenados de funciones!

\bigskip

Notese que cuando $m=n=0$, se tiene que $D_{f}=\{\Diamond\}$ y (1) y (2) se
transforman en

\begin{enumerate}
\item[(1)] $R(f,g)(0)=f(\Diamond)$

\item[(2)] $R(f,g)(t+1)=g(R(f,g)(t),t)$
\end{enumerate}

\bigskip

Veamos algunos ejemplos

\begin{enumerate}
\item[E$_{1}$] Tomemos $f=p_{1}^{1,0}$ y $g=Suc\circ p_{1}^{3,0}$. De la
definicion de $R(f,g)$, obtenemos que su dominio es $\omega^{2}$ y%
\begin{align*}
R(f,g)(0,x_{1})  & =p_{1}^{1,0}(x_{1})=x_{1}\\
R(f,g)(t+1,x_{1})  & =\left(  Suc\circ p_{1}^{3,0}\right)  (R(f,g)(t,x_{1}%
),t,x_{1})=R(f,g)(t,x_{1})+1
\end{align*}
Es facil notar que la unica funcion que cumple estas dos ecuaciones es
$\lambda tx_{1}\left[  t+x_{1}\right]  $, lo cual implica que $\lambda
tx_{1}\left[  t+x_{1}\right]  =R\left(  p_{1}^{1,0},Suc\circ p_{1}%
^{3,0}\right)  $

\item[E$_{2}$] Sean $f=C_{0}^{0,0}$ y $g=p_{1}^{2,0}$. De la definicion de
$R(f,g)$, obtenemos que su dominio es $\omega$ y%
\begin{align*}
R(f,g)(0)  & =C_{0}^{0,0}(\Diamond)=0\\
R(f,g)(t+1)  & =p_{1}^{2,0}(R(f,g)(t),t)=R(f,g)(t)
\end{align*}
Es facil notar que la unica funcion que cumple estas dos ecuaciones es
$C_{0}^{1,0}$ lo cual implica que $C_{0}^{1,0}=R\left(  C_{0}^{0,0}%
,p_{1}^{2,0}\right)  $
\end{enumerate}

\bigskip

Como era de esperar, este caso del constructor de recursion primitiva preserva
la computabilidad efectiva

\begin{lemma}
Si $f$ y $g$ son $\Sigma$-efectivamente computables, entonces $R(f,g)$ lo es.
\end{lemma}

\begin{proof}
Es dejada al lector
\end{proof}

\bigskip

\textbf{Nota importante:} En los ejemplos anteriores y en todos los casos que
manejaremos en esta primera etapa, en las aplicaciones del constructor de
recursion primitiva (en sus cuatro formas) las funciones iniciales seran
$\Sigma$-totales (es decir $S_{1}=...=S_{n}=\omega$ y $L_{1}=...=L_{m}%
=\Sigma^{\ast}$). Mas adelante veremos aplicaciones con funciones no $\Sigma$-totales.

\bigskip

\paragraph{Recursion primitiva sobre variable numerica con valores
alfabeticos}

Ahora haremos el caso en el que la funcion definida recursivamente tiene
imagen contenida en $\Sigma^{\ast}$. Es claro que entonces $f$ y $g$ tambien
deberan tener imagen contenida en $\Sigma^{\ast}$. El unico detalle a tener en
cuenta en la definicion de este caso es que si solo hicieramos estos cambios y
pusieramos las mismas ecuaciones la funcion $g$ no resultaria $\Sigma$-mixta
en general. Para que la $g$ de la recursion siga siendo $\Sigma$-mixta
deberemos modificar levemente su dominio en relacion al caso ya hecho

Supongamos $\Sigma$ es un alfabeto finito. Sean%
\begin{align*}
f  & :S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m}%
\rightarrow\Sigma^{\ast}\\
g  & :\omega\times S_{1}\times...\times S_{n}\times L_{1}\times...\times
L_{m}\times\Sigma^{\ast}\rightarrow\Sigma^{\ast}%
\end{align*}
con $S_{1},...,S_{n}\subseteq\omega$ y $L_{1},...,L_{m}\subseteq\Sigma^{\ast}$
conjuntos no vacios. Definamos%
\[
R(f,g):\omega\times S_{1}\times...\times S_{n}\times L_{1}\times...\times
L_{m}\rightarrow\Sigma^{\ast}%
\]
de la siguiente manera

\begin{enumerate}
\item[(1)] $R(f,g)(0,\vec{x},\vec{\alpha})=f(\vec{x},\vec{\alpha})$

\item[(2)] $R(f,g)(t+1,\vec{x},\vec{\alpha})=g(t,\vec{x},\vec{\alpha
},R(f,g)(t,\vec{x},\vec{\alpha}))$
\end{enumerate}

\noindent Diremos que $R(f,g)$ es obtenida por \textit{recursion primitiva} a
partir de $f$ y $g$. Notese que cuando $m=n=0$, se tiene que $D_{f}%
=\{\Diamond\}$ y (1) y (2) se transforman en

\begin{enumerate}
\item[(1)] $R(f,g)(0)=f(\Diamond)$

\item[(2)] $R(f,g)(t+1)=g(t,R(f,g)(t))$
\end{enumerate}

\bigskip

Veamos algunos ejemplos

\begin{enumerate}
\item[E$_{1}$] Tomemos $f=C_{\varepsilon}^{0,1}$ y $g=\lambda\alpha
\beta\left[  \alpha\beta\right]  \circ\left[  p_{3}^{1,2},p_{2}^{1,2}\right]
$. De la definicion de $R(f,g)$, obtenemos que%
\begin{align*}
R(f,g)(0,\alpha_{1})  & =C_{\varepsilon}^{0,1}(\alpha_{1})=\varepsilon\\
R(f,g)(t+1,\alpha_{1})  & =\lambda\alpha\beta\left[  \alpha\beta\right]
\circ\left[  p_{3}^{1,2},p_{2}^{1,2}\right]  (t,\alpha_{1},R(f,g)(t,\alpha
_{1}))=R(f,g)(t,\alpha_{1})\alpha_{1}%
\end{align*}
Es facil notar que la unica funcion que cumple estas dos ecuaciones es
$\lambda t\alpha_{1}\left[  \alpha_{1}{}^{t}\right]  $, lo cual implica que
$\lambda t\alpha_{1}\left[  \alpha_{1}{}^{t}\right]  =R\left(  C_{\varepsilon
}^{0,1},\lambda\alpha\beta\left[  \alpha\beta\right]  \circ\left[  p_{3}%
^{1,2},p_{2}^{1,2}\right]  \right)  $

\item[E$_{2}$] Sean $f=C_{\varepsilon}^{0,0}$ y $g=p_{2}^{2,0}$. De la
definicion de $R(f,g)$, obtenemos que%
\begin{align*}
R(f,g)(0)  & =C_{\varepsilon}^{0,0}(\Diamond)=\varepsilon\\
R(f,g)(t+1)  & =p_{2}^{2,0}(t,R(f,g)(t))=R(f,g)(t)
\end{align*}
Es facil notar que la unica funcion que cumple estas dos ecuaciones es
$C_{\varepsilon}^{1,0}$ lo cual implica que $C_{\varepsilon}^{1,0}=R\left(
C_{\varepsilon}^{0,0},p_{2}^{2,0}\right)  $
\end{enumerate}

\bigskip

La prueba del siguiente lema es completamente analoga a la del lema anterior
que fue dejada como ejercicio.

\begin{lemma}
Si $f$ y $g$ son $\Sigma$-efectivamente computables, entonces $R(f,g)$ lo es.
\end{lemma}

\bigskip

\paragraph{Recursion primitiva sobre variable alfabetica con valores
numericos}

Ya vimos dos casos de recursion donde el parametro que comanda la recursion es
numerico. Daremos a continuacion un ejemplo de recursion en el cual el
parametro principal es alfabetico. Sea $\Sigma=\{\%,@,?\}$ y consideremos las
siguientes ecuaciones:

\begin{enumerate}
\item[(1)] $R(\varepsilon)=15$

\item[(2)] $R(\alpha\%)=R(\alpha)+1$

\item[(3)] $R(\alpha@)=R(\alpha).5$

\item[(4)] $R(\alpha?)=R(\alpha)^{20}$
\end{enumerate}

\bigskip

Notese que las ecuaciones anteriores determinan una funcion $R:\Sigma^{\ast
}\rightarrow\omega$. Esto es ya que $R$ en $\varepsilon$ debe valer $15$ y
sabiendo esto las ecuaciones (2), (3) y (4) (con $\alpha=\varepsilon$) nos
dicen que%
\begin{align*}
R(\%)  & =16\\
R(@)  & =75\\
R(?)  & =15^{20}%
\end{align*}
por lo cual podemos aplicarlas nuevamente a dichas ecuaciones (con $\alpha
\in\{\%,@,?\}$) para calcular $R$ en todas las palabras de longitud $2$; y asi sucesivamente.

Daremos otro ejemplo un poco mas complicado para seguir aproximandonos al caso
general. Nuevamente supongamos que $\Sigma=\{\%,@,?\}$ y supongamos tenemos
una funcion
\[
f:\omega\times\Sigma^{\ast}\rightarrow\omega
\]
y tres funciones%
\begin{align*}
\mathcal{G}_{\%}  & :\omega\times\omega\times\Sigma^{\ast}\times\Sigma^{\ast
}\rightarrow\omega\\
\mathcal{G}_{@}  & :\omega\times\omega\times\Sigma^{\ast}\times\Sigma^{\ast
}\rightarrow\omega\\
\mathcal{G}_{?}  & :\omega\times\omega\times\Sigma^{\ast}\times\Sigma^{\ast
}\rightarrow\omega
\end{align*}
Entonces hay una unica funcion $R::\omega\times\Sigma^{\ast}\times\Sigma
^{\ast}\rightarrow\omega$ la cual cumple las siguientes ecuaciones

\begin{enumerate}
\item[(1)] $R(x_{1},\alpha_{1},\varepsilon)=f(x_{1},\alpha_{1})$

\item[(2)] $R(x_{1},\alpha_{1},\alpha\%)=\mathcal{G}_{\%}(R(x_{1},\alpha
_{1},\alpha),x_{1},\alpha_{1},\alpha)$

\item[(3)] $R(x_{1},\alpha_{1},\alpha@)=\mathcal{G}_{@}(R(x_{1},\alpha
_{1},\alpha),x_{1},\alpha_{1},\alpha)$

\item[(4)] $R(x_{1},\alpha_{1},\alpha?)=\mathcal{G}_{?}(R(x_{1},\alpha
_{1},\alpha),x_{1},\alpha_{1},\alpha)$
\end{enumerate}

\noindent(Justifique que las ecuaciones anteriores determinan a la funcion $R$.)

El ejemplo anterior nos muestra que para hacer recursion sobre parametro
alfabetico nos hace falta "una funcion $g$ por cada simbolo de $\Sigma$". Esto
motiva la siguiente definicion. Dado un alfabeto $\Sigma$, una \textit{familia
}$\Sigma$-\textit{indexada de funciones} sera una funcion $\mathcal{G}$ tal
que $D_{\mathcal{G}}=\Sigma$ y para cada $a\in D_{\mathcal{G}}$ se tiene que
$\mathcal{G}(a)$ es una funcion. Algunos ejemplos:

\begin{enumerate}
\item[E$_{1}$] Sea $\mathcal{G}$ dada por%
\[%
\begin{array}
[c]{rcl}%
\mathcal{G}:\{\square,\%,\blacktriangle\} & \rightarrow & \{Suc,Pred\}\\
\square & \rightarrow & Suc\\
\% & \rightarrow & Suc\\
\blacktriangle & \rightarrow & Pred
\end{array}
\]
Claramente $\mathcal{G}$ es una familia $\{\square,\%,\blacktriangle
\}$-indexada de funciones. Notar que%
\[
\mathcal{G}=\{(\square,Suc),(\%,Suc),(\blacktriangle,Pred)\}
\]
Se tiene tambien por ejemplo que $\mathcal{G}(\%)=Suc$ por lo cual tambien es
cierto que $\mathcal{G}(\%)(22)=23$, etc.

\item[E$_{2}$] Si $\Sigma$ es un alfabeto no vacio, la funcion%
\[%
\begin{array}
[c]{rcl}%
\mathcal{G}:\Sigma & \rightarrow & \{f:f\text{ es una funcion de }\Sigma
^{\ast}\text{ en }\Sigma^{\ast}\}\\
a & \rightarrow & d_{a}%
\end{array}
\]
es una familia $\Sigma$-indexada de funciones. Notar que%
\[
\mathcal{G}=\{(a,d_{a}):a\in\Sigma\}
\]


\item[E$_{3}$] $\emptyset$ es una flia $\emptyset$-indexada de funciones
\end{enumerate}

\bigskip

Si $\mathcal{G}$ es una familia $\Sigma$-indexada de funciones, entonces para
$a\in\Sigma$, escribiremos $\mathcal{G}_{a}$ en lugar de $\mathcal{G}(a)$.
Ahora s\'{\i}, nuestro caso de recursion primitiva. Sea%
\[
f:S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m}\rightarrow\omega
\]
con $S_{1},...,S_{n}\subseteq\omega$ y $L_{1},...,L_{m}\subseteq\Sigma^{\ast}$
conjuntos no vacios y sea $\mathcal{G}$ una familia $\Sigma$-indexada de
funciones tal que%
\[
\mathcal{G}_{a}:\omega\times S_{1}\times...\times S_{n}\times L_{1}%
\times...\times L_{m}\times\Sigma^{\ast}\rightarrow\omega
\]
para cada $a\in\Sigma.$ Definamos%
\[
R(f,\mathcal{G}):S_{1}\times...\times S_{n}\times L_{1}\times...\times
L_{m}\times\Sigma^{\ast}\rightarrow\omega
\]
de la siguiente manera

\begin{enumerate}
\item[(1)] $R(f,\mathcal{G})(\vec{x},\vec{\alpha},\varepsilon)=f(\vec{x}%
,\vec{\alpha})$

\item[(2)] $R(f,\mathcal{G})(\vec{x},\vec{\alpha},\alpha a)=\mathcal{G}%
_{a}(R(f,\mathcal{G})(\vec{x},\vec{\alpha},\alpha),\vec{x},\vec{\alpha}%
,\alpha)$
\end{enumerate}

\noindent Diremos que $R(f,\mathcal{G})$ es obtenida por \textit{recursion
primitiva }a partir de $f$ y $\mathcal{G}$. Notese que cuando $m=n=0$, se
tiene que $D_{f}=\{\Diamond\}$ y (1) y (2) se transforman en

\begin{enumerate}
\item[(1)] $R(f,\mathcal{G})(\varepsilon)=f(\Diamond)$

\item[(2)] $R(f,\mathcal{G})(\alpha a)=\mathcal{G}_{a}(R(f,\mathcal{G}%
)(\alpha),\alpha)$
\end{enumerate}

\bigskip

\begin{lemma}
Si $f$ y cada $\mathcal{G}_{a}$ son $\Sigma$-efectivamente computables,
entonces $R(f,\mathcal{G})$ lo es.
\end{lemma}

\begin{proof}
Es dejada al lector
\end{proof}

\bigskip

\paragraph{Recursion primitiva sobre variable alfabetica con valores
alfabeticos}

Supongamos $\Sigma$ es un alfabeto finito. Sea%
\[
f:S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m}\rightarrow
\Sigma^{\ast}%
\]
con $S_{1},...,S_{n}\subseteq\omega$ y $L_{1},...,L_{m}\subseteq\Sigma^{\ast}$
conjuntos no vacios y sea $\mathcal{G}$ una familia $\Sigma$-indexada de
funciones tal que%
\[
\mathcal{G}_{a}:S_{1}\times...\times S_{n}\times L_{1}\times...\times
L_{m}\times\Sigma^{\ast}\times\Sigma^{\ast}\rightarrow\Sigma^{\ast}%
\]
para cada $a\in\Sigma$. Definamos%
\[
R(f,\mathcal{G}):S_{1}\times...\times S_{n}\times L_{1}\times...\times
L_{m}\times\Sigma^{\ast}\rightarrow\Sigma^{\ast}%
\]
de la siguiente manera

\begin{enumerate}
\item[(1)] $R(f,\mathcal{G})(\vec{x},\vec{\alpha},\varepsilon)=f(\vec{x}%
,\vec{\alpha})$

\item[(2)] $R(f,\mathcal{G})(\vec{x},\vec{\alpha},\alpha a)=\mathcal{G}%
_{a}(\vec{x},\vec{\alpha},\alpha,R(f,\mathcal{G})(\vec{x},\vec{\alpha}%
,\alpha)). $
\end{enumerate}

\noindent Diremos que $R(f,\mathcal{G})$ es obtenida por \textit{recursion
primitiva }a partir de $f$ y $\mathcal{G}$. Notese que cuando $m=n=0$, se
tiene que $D_{f}=\{\Diamond\}$ y (1) y (2) se transforman en

\begin{enumerate}
\item[(1)] $R(f,\mathcal{G})(\varepsilon)=f(\Diamond)$

\item[(2)] $R(f,\mathcal{G})(\alpha a)=\mathcal{G}_{a}(\alpha,R(f,\mathcal{G}%
)(\alpha))$
\end{enumerate}

\bigskip

La prueba del siguiente lema es completamente analoga a la del lema anterior
que fue dejada como ejercicio.

\begin{lemma}
Si $f$ y cada $\mathcal{G}_{a}$ son $\Sigma$-efectivamente computables,
entonces $R(f,\mathcal{G})$ lo es.
\end{lemma}

\bigskip

\subsubsection{Funciones $\Sigma$-recursivas primitivas}

Intuitivamente hablando una funcion es $\Sigma$-recursiva primitiva si se
puede obtener de las iniciales usando los constructores de composicion y
recursion primitiva. Daremos ahora una definicion matematica de este concepto.
Definamos los conjuntos $\mathrm{PR}_{0}^{\Sigma}\subseteq\mathrm{PR}%
_{1}^{\Sigma}\subseteq\mathrm{PR}_{2}^{\Sigma}\subseteq...\subseteq
\mathrm{PR}^{\Sigma}$ de la siguiente manera%
\[%
\begin{array}
[c]{lll}%
\mathrm{PR}_{0}^{\Sigma} & = & \left\{  Suc,Pred,C_{0}^{0,0},C_{\varepsilon
}^{0,0}\right\}  \cup\left\{  d_{a}:a\in\Sigma\right\}  \cup\left\{
p_{j}^{n,m}:1\leq j\leq n+m\right\} \\
\mathrm{PR}_{k+1}^{\Sigma} & = & \mathrm{PR}_{k}^{\Sigma}\cup\left\{
f\circ\lbrack f_{1},...,f_{r}]:f,f_{1},...,f_{r}\in\mathrm{PR}_{k}^{\Sigma
}\text{, }r\geq1\right\}  \cup\\
&  & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\left\{  R(f,\mathcal{G}):f\text{ y cada
}\mathcal{G}_{a}\text{ pertenecen a }\mathrm{PR}_{k}^{\Sigma}\right\}  \cup\\
&  &
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\left\{
R(f,g):f,g\in\mathrm{PR}_{k}^{\Sigma}\right\}  \medskip\\
\mathrm{PR}^{\Sigma} & = & \bigcup_{k\geq0}\mathrm{PR}_{k}^{\Sigma}%
\end{array}
\]
Una funcion es llamada $\Sigma$-\textit{recursiva primitiva} ($\Sigma
$\textit{-p.r.}) si pertenece a $\mathrm{PR}^{\Sigma}$.

\bigskip

\begin{proposition}
Si $f\in\mathrm{PR}^{\Sigma}$, entonces $f$ es $\Sigma$-efectivamente computable.
\end{proposition}

\begin{proof}
Dejamos al lector la prueba por induccion en $k$ de que si $f\in
\mathrm{PR}_{k}^{\Sigma}$, entonces $f$ es $\Sigma$-efectivamente computable,
la cual sale en forma directa usando los lemas anteriores que garantizan que
los constructores de composicion y recursion primitiva preservan la
computabilidad efectiva
\end{proof}

\bigskip

\paragraph{Algunas funciones $\Sigma$-recursivas primitivas}

En los siguientes cuatro lemas se prueba bien formalmente que varias funciones
bien conocidas son $\Sigma$-primitivas recursivas.

\begin{lemma}
Sea $\Sigma$ un alfabeto finito.

\begin{enumerate}
\item[(1)] $\emptyset\in\mathrm{PR}^{\Sigma}$.

\item[(2)] $\lambda xy\left[  x+y\right]  \in\mathrm{PR}^{\Sigma}$.

\item[(3)] $\lambda xy\left[  x.y\right]  \in\mathrm{PR}^{\Sigma}$.

\item[(4)] $\lambda x\left[  x!\right]  \in\mathrm{PR}^{\Sigma}$.
\end{enumerate}
\end{lemma}

\begin{proof}
(1) Notese que $\emptyset=Pred\circ C_{0}^{0,0}\in\mathrm{PR}_{1}^{\Sigma} $

(2) Notar que%
\begin{align*}
\lambda xy\left[  x+y\right]  (0,x_{1})  & =x_{1}=p_{1}^{1,0}(x_{1})\\
\lambda xy\left[  x+y\right]  (t+1,x_{1})  & =\lambda xy\left[  x+y\right]
(t,x_{1})+1\\
& =\left(  Suc\circ p_{1}^{3,0}\right)  \left(  \lambda xy\left[  x+y\right]
(t,x_{1}),t,x_{1}\right)
\end{align*}
lo cual implica que $\lambda xy\left[  x+y\right]  =R\left(  p_{1}%
^{1,0},Suc\circ p_{1}^{3,0}\right)  \in\mathrm{PR}_{2}^{\Sigma}.$

(3) Primero note que%
\begin{align*}
C_{0}^{1,0}(0)  & =C_{0}^{0,0}(\Diamond)\\
C_{0}^{1,0}(t+1)  & =C_{0}^{1,0}(t)
\end{align*}
lo cual implica que $C_{0}^{1,0}=R\left(  C_{0}^{0,0},p_{1}^{2,0}\right)
\in\mathrm{PR}_{1}^{\Sigma}.$ Tambien note que%
\[
\lambda tx\left[  t.x\right]  =R\left(  C_{0}^{1,0},\lambda xy\left[
x+y\right]  \circ\left[  p_{1}^{3,0},p_{3}^{3,0}\right]  \right)  ,
\]
lo cual por (2) implica que $\lambda tx\left[  t.x\right]  \in\mathrm{PR}%
_{4}^{\Sigma}$.

(4) Note que%
\begin{align*}
\lambda x\left[  x!\right]  (0)  & =1=C_{1}^{0,0}(\Diamond)\\
\lambda x\left[  x!\right]  (t+1)  & =\lambda x\left[  x!\right]  (t).(t+1),
\end{align*}
lo cual implica que%
\[
\lambda x\left[  x!\right]  =R\left(  C_{1}^{0,0},\lambda xy\left[
x.y\right]  \circ\left[  p_{1}^{2,0},Suc\circ p_{2}^{2,0}\right]  \right)  .
\]
Ya que $C_{1}^{0,0}=$ $Suc\circ C_{0}^{0,0}$, tenemos que $C_{1}^{0,0}%
\in\mathrm{PR}_{1}^{\Sigma}$. Por (3), tenemos que%
\[
\lambda xy\left[  x.y\right]  \circ\left[  p_{1}^{2,0},Suc\circ p_{2}%
^{2,0}\right]  \in\mathrm{PR}_{5}^{\Sigma},
\]
obteniendo que $\lambda x\left[  x!\right]  \in\mathrm{PR}_{6}^{\Sigma}$.
\end{proof}

\bigskip

Ahora consideraremos dos funciones las cuales son obtenidas naturalmente por
recursion primitiva sobre variable alfabetica.

\begin{lemma}
Supongamos $\Sigma$ es un alfabeto finito.

\begin{enumerate}
\item[(a)] $\lambda\alpha\beta\left[  \alpha\beta\right]  \in\mathrm{PR}%
^{\Sigma}$

\item[(b)] $\lambda\alpha\left[  \left\vert \alpha\right\vert \right]
\in\mathrm{PR}^{\Sigma}$
\end{enumerate}
\end{lemma}

\begin{proof}
(a) Ya que%
\begin{align*}
\lambda\alpha\beta\left[  \alpha\beta\right]  (\alpha_{1},\varepsilon)  &
=\alpha_{1}=p_{1}^{0,1}(\alpha_{1})\\
\lambda\alpha\beta\left[  \alpha\beta\right]  (\alpha_{1},\alpha a)  &
=d_{a}(\lambda\alpha\beta\left[  \alpha\beta\right]  (\alpha_{1}%
,\alpha)),\ a\in\Sigma
\end{align*}
tenemos que $\lambda\alpha\beta\left[  \alpha\beta\right]  =R\left(
p_{1}^{0,1},\mathcal{G}\right)  $, donde $\mathcal{G}_{a}=d_{a}\circ
p_{3}^{0,3}$, para cada $a\in\Sigma$.

(b) Ya que%
\begin{align*}
\lambda\alpha\left[  \left\vert \alpha\right\vert \right]  (\varepsilon)  &
=0=C_{0}^{0,0}(\Diamond)\\
\lambda\alpha\left[  \left\vert \alpha\right\vert \right]  (\alpha a)  &
=\lambda\alpha\left[  \left\vert \alpha\right\vert \right]  (\alpha)+1
\end{align*}
tenemos que $\lambda\alpha\left[  \left\vert \alpha\right\vert \right]
=R\left(  C_{0}^{0,0},\mathcal{G}\right)  $, donde $\mathcal{G}_{a}=$
$Suc\circ p_{1}^{1,1}$, para cada $a\in\Sigma.$.
\end{proof}

\bigskip

\begin{lemma}
Sea $\Sigma$ un alfabeto finito. Entonces $C_{k}^{n,m},C_{\alpha}^{n,m}%
\in\mathrm{PR}^{\Sigma}$, para cada $n,m,k\geq0$ y $\alpha\in\Sigma^{\ast} $.
\end{lemma}

\begin{proof}
(a) Note que $C_{k+1}^{0,0}=$ $Suc\circ C_{k}^{0,0}$, lo cual implica
$C_{k}^{0,0}\in\mathrm{PR}_{k}^{\Sigma}$, para $k\geq0$. Tambien note que
$C_{\alpha a}^{0,0}=d_{a}\circ C_{\alpha}^{0,0}$, lo cual dice que $C_{\alpha
}^{0,0}\in\mathrm{PR}^{\Sigma}$, para $\alpha\in\Sigma^{\ast} $. Para ver que
$C_{k}^{0,1}\in\mathrm{PR}^{\Sigma}$ notar que%
\begin{align*}
C_{k}^{0,1}(\varepsilon)  & =k=C_{k}^{0,0}(\Diamond)\\
C_{k}^{0,1}(\alpha a)  & =C_{k}^{0,1}(\alpha)=p_{1}^{1,1}\left(  C_{k}%
^{0,1}(\alpha),\alpha\right)
\end{align*}
lo cual implica que $C_{k}^{0,1}=R\left(  C_{k}^{0,0},\mathcal{G}\right)  $,
con $\mathcal{G}_{a}=p_{1}^{1,1}$, $a\in\Sigma$. En forma similar podemos ver
que $C_{k}^{1,0},C_{\alpha}^{1,0},C_{\alpha}^{0,1}\in\mathrm{PR}^{\Sigma}$.
Supongamos ahora que $m>0$. Entonces%
\begin{align*}
C_{k}^{n,m}  & =C_{k}^{0,1}\circ p_{n+1}^{n,m}\\
C_{\alpha}^{n,m}  & =C_{\alpha}^{0,1}\circ p_{n+1}^{n,m}%
\end{align*}
de lo cual obtenemos que $C_{k}^{n,m},C_{\alpha}^{n,m}\in\mathrm{PR}^{\Sigma}%
$. El caso $n>0$ es similar.
\end{proof}

\bigskip

\begin{lemma}
Sea $\Sigma$ un alfabeto finito.

\begin{enumerate}
\item[(a)] $\lambda xy\left[  x^{y}\right]  \in\mathrm{PR}^{\Sigma}$.

\item[(b)] $\lambda t\alpha\left[  \alpha^{t}\right]  \in\mathrm{PR}^{\Sigma}$.
\end{enumerate}
\end{lemma}

\begin{proof}
(a) Note que%
\[
\lambda tx\left[  x^{t}\right]  =R\left(  C_{1}^{1,0},\lambda xy\left[
x.y\right]  \circ\left[  p_{1}^{3,0},p_{3}^{3,0}\right]  \right)
\in\mathrm{PR}^{\Sigma}.
\]
O sea que $\lambda xy\left[  x^{y}\right]  =\lambda tx\left[  x^{t}\right]
\circ\left[  p_{2}^{2,0},p_{1}^{2,0}\right]  \in\mathrm{PR}^{\Sigma}$.

(b) Note que%
\[
\lambda t\alpha\left[  \alpha^{t}\right]  =R\left(  C_{\varepsilon}%
^{0,1},\lambda\alpha\beta\left[  \alpha\beta\right]  \circ\left[  p_{3}%
^{1,2},p_{2}^{1,2}\right]  \right)  \in\mathrm{PR}^{\Sigma}.
\]

\end{proof}

\bigskip

Ahora probaremos que si $\Sigma$ es no vacio, entonces las biyeciones
naturales entre $\Sigma^{\ast}$ y $\omega$, dadas en el Lema \ref{biyeccion0},
son $\Sigma$-p.r..

\begin{lemma}
\label{sonpr}Si $\leq$ es un orden total sobre un alfabeto no vacio $\Sigma$,
entonces $s^{\leq}$, $\#^{\leq}$ y $\ast^{\leq}$ pertenecen a $\mathrm{PR}%
^{\Sigma}$
\end{lemma}

\begin{proof}
Supongamos $\Sigma=\{a_{1},...,a_{k}\}$ y $\leq$ es dado por $a_{1}<...<a_{k}%
$. Ya que%
\begin{align*}
s^{\leq}(\varepsilon)  & =a_{1}\\
s^{\leq}(\alpha a_{i})  & =\alpha a_{i+1}\text{, para }i<k\\
s^{\leq}(\alpha a_{k})  & =s^{\leq}(\alpha)a_{1}%
\end{align*}
tenemos que $s^{\leq}=R\left(  C_{a_{1}}^{0,0},\mathcal{G}\right)  $, donde
$\mathcal{G}_{a_{i}}=d_{a_{i+1}}\circ p_{1}^{0,2}$, para $i=1,...,k-1$ y
$\mathcal{G}_{a_{k}}=d_{a_{1}}\circ p_{2}^{0,2}.$ O sea que $s^{\leq}%
\in\mathrm{PR}^{\Sigma}.$ Ya que%
\begin{align*}
\ast^{\leq}(0)  & =\varepsilon\\
\ast^{\leq}(t+1)  & =s^{\leq}(\ast^{\leq}(t))
\end{align*}
podemos ver que $\ast^{\leq}\in\mathrm{PR}^{\Sigma}$. Ya que%
\begin{align*}
\#^{\leq}(\varepsilon)  & =0\\
\#^{\leq}(\alpha a_{i})  & =\#^{\leq}(\alpha).k+i\text{, para }i=1,...,k,
\end{align*}
tenemos que $\#^{\leq}=R\left(  C_{0}^{0,0},\mathcal{G}\right)  $, donde%
\[
\mathcal{G}_{a_{i}}=\lambda xy\left[  x+y\right]  \circ\left[  \lambda
xy\left[  x.y\right]  \circ\left[  p_{1}^{1,1},C_{k}^{1,1}\right]
,C_{i}^{1,1}\right]  \text{, para }i=1,...,k\text{.}%
\]
O sea que $\#^{\leq}\in\mathrm{PR}^{\Sigma}$.
\end{proof}

\bigskip

Dados $x,y\in\omega$, definamos%
\[
x\dot{-}y=\max(x-y,0).
\]


\begin{lemma}


\begin{enumerate}
\item[(a)] $\lambda xy\left[  x\dot{-}y\right]  \in\mathrm{PR}^{\Sigma}.$

\item[(b)] $\lambda xy\left[  \max(x,y)\right]  \in\mathrm{PR}^{\Sigma}.$

\item[(c)] $\lambda xy\left[  x=y\right]  \in\mathrm{PR}^{\Sigma}.$

\item[(d)] $\lambda xy\left[  x\leq y\right]  \in\mathrm{PR}^{\Sigma}.$

\item[(e)] $\lambda\alpha\beta\left[  \alpha=\beta\right]  \in\mathrm{PR}%
^{\Sigma}$
\end{enumerate}
\end{lemma}

\begin{proof}
(a) Primero notar que $\lambda x\left[  x\dot{-}1\right]  =R\left(
C_{0}^{0,0},p_{2}^{2,0}\right)  \in\mathrm{PR}^{\Sigma}.$ Tambien note que%
\[
\lambda tx\left[  x\dot{-}t\right]  =R\left(  p_{1}^{1,0},\lambda x\left[
x\dot{-}1\right]  \circ p_{1}^{3,0}\right)  \in\mathrm{PR}^{\Sigma}.
\]
O sea que $\lambda xy\left[  x\dot{-}y\right]  =\lambda tx\left[  x\dot
{-}t\right]  \circ\left[  p_{2}^{2,0},p_{1}^{2,0}\right]  \in\mathrm{PR}%
^{\Sigma} $.

(b) Note que $\lambda xy\left[  \max(x,y)\right]  =\lambda xy\left[
x+(y\dot{-}x)\right]  $.

(c) Note que $\lambda xy\left[  x=y\right]  =\lambda xy\left[  1\dot{-}%
((x\dot{-}y)+(y\dot{-}x))\right]  $.

(d) Note que $\lambda xy\left[  x\leq y\right]  =\lambda xy\left[  1\dot
{-}(x\dot{-}y)\right]  $.

(e) Sea $\leq$ un orden total sobre $\Sigma.$ Ya que%
\[
\alpha=\beta\text{ sii }\#^{\leq}(\alpha)=\#^{\leq}(\beta)
\]
tenemos que%
\[
\lambda\alpha\beta\left[  \alpha=\beta\right]  =\lambda xy\left[  x=y\right]
\circ\left[  \#^{\leq}\circ p_{1}^{0,2},\#^{\leq}\circ p_{2}^{0,2}\right]
\]
lo cual nos dice que $\lambda\alpha\beta\left[  \alpha=\beta\right]  $ es
$\Sigma$-p.r.
\end{proof}

\bigskip

\paragraph{Operaciones logicas entre predicados}

Dados predicados $P:S\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow
\omega$ y $Q:S\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow\omega$, con
el mismo dominio, definamos nuevos predicados $(P\vee Q)$, $(P\wedge Q)$ y
$\lnot P$ de la siguiente manera%

\begin{align*}
&
\begin{array}
[c]{rll}%
(P\vee Q):S & \rightarrow & \omega\\
(\vec{x},\vec{\alpha}) & \rightarrow & \left\{
\begin{array}
[c]{lll}%
1 &  & \text{si }P(\vec{x},\vec{\alpha})=1\text{ o }Q(\vec{x},\vec{\alpha
})=1\\
0 &  & \text{caso contrario}%
\end{array}
\right.
\end{array}
\\
&
\begin{array}
[c]{rll}%
(P\wedge Q):S & \rightarrow & \omega\\
(\vec{x},\vec{\alpha}) & \rightarrow & \left\{
\begin{array}
[c]{lll}%
1 &  & \text{si }P(\vec{x},\vec{\alpha})=1\text{ y }Q(\vec{x},\vec{\alpha
})=1\\
0 &  & \text{caso contrario}%
\end{array}
\right.
\end{array}
\\
&
\begin{array}
[c]{rll}%
\lnot P:S & \rightarrow & \omega\\
(\vec{x},\vec{\alpha}) & \rightarrow & \left\{
\begin{array}
[c]{lll}%
1 &  & \text{si }P(\vec{x},\vec{\alpha})=0\\
0 &  & \text{si }P(\vec{x},\vec{\alpha})=1
\end{array}
\right.
\end{array}
\end{align*}


\begin{lemma}
\label{boolean op}Si $P:S\subseteq\omega^{n}\times\Sigma^{\ast m}%
\rightarrow\omega$ y $Q:S\subseteq\omega^{n}\times\Sigma^{\ast m}%
\rightarrow\omega$ son predicados $\Sigma$-p.r., entonces $(P\vee Q)$,
$(P\wedge Q)$ y $\lnot P$ lo son tambien.
\end{lemma}

\begin{proof}
Note que%
\begin{align*}
\lnot P  & =\lambda xy\left[  x\dot{-}y\right]  \circ\left[  C_{1}%
^{n,m},P\right] \\
(P\wedge Q)  & =\lambda xy\left[  x.y\right]  \circ\lbrack P,Q]\\
(P\vee Q)  & =\lnot(\lnot P\wedge\lnot Q).
\end{align*}

\end{proof}

\bigskip

\paragraph{Conjuntos $\Sigma$-recursivos primitivos}

Un conjunto $\Sigma$-mixto $S\subseteq\omega^{n}\times\Sigma^{\ast m}$ es
llamado $\Sigma$\textit{-recursivo primitivo} si su funcion caracteristica
$\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}}$ es $\Sigma$-p.r.. (Notese que
$\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}}$ es el predicado $\lambda\vec
{x}\vec{\alpha}\left[  (\vec{x},\vec{\alpha})\in S\right]  $.)

\begin{lemma}
\label{union}Si $S_{1},S_{2}\subseteq\omega^{n}\times\Sigma^{\ast m}$ son
$\Sigma$-p.r., entonces $S_{1}\cup S_{2}$, $S_{1}\cap S_{2}$ y $S_{1}-S_{2}$
lo son.
\end{lemma}

\begin{proof}
Note que%
\begin{align*}
\chi_{S_{1}\cup S_{2}}^{\omega^{n}\times\Sigma^{\ast m}}  & =(\chi_{S_{1}%
}^{\omega^{n}\times\Sigma^{\ast m}}\vee\chi_{S_{2}}^{\omega^{n}\times
\Sigma^{\ast m}})\\
\chi_{S_{1}\cap S_{2}}^{\omega^{n}\times\Sigma^{\ast m}}  & =(\chi_{S_{1}%
}^{\omega^{n}\times\Sigma^{\ast m}}\wedge\chi_{S_{2}}^{\omega^{n}\times
\Sigma^{\ast m}})\\
\chi_{S_{1}-S_{2}}^{\omega^{n}\times\Sigma^{\ast m}}  & =\lambda xy\left[
x\dot{-}y\right]  \circ\left[  \chi_{S_{1}}^{\omega^{n}\times\Sigma^{\ast m}%
},\chi_{S_{2}}^{\omega^{n}\times\Sigma^{\ast m}}\right]
\end{align*}

\end{proof}

\begin{corollary}
\label{finito}Si $S\subseteq\omega^{n}\times\Sigma^{\ast m}$ es finito,
entonces $S$ es $\Sigma$-p.r..
\end{corollary}

\begin{proof}
Si $S=\emptyset$, entonces es claro que $S$ es $\Sigma$-p.r.. Probaremos ahora
el lema para el caso en que $S$ tiene un solo elemento. Supongamos entonces%
\[
S=\{(z_{1},...,z_{n},\gamma_{1},...,\gamma_{m})\}.
\]
Note que $\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}}$ es el siguiente
predicado%
\[
\left(  \chi_{\{z_{1}\}}^{\omega}\circ p_{1}^{n,m}\wedge...\wedge\chi
_{\{z_{n}\}}^{\omega}\circ p_{n}^{n,m}\wedge\chi_{\{\gamma_{1}\}}%
^{\Sigma^{\ast}}\circ p_{n+1}^{n,m}\wedge...\wedge\chi_{\{\gamma_{m}%
\}}^{\Sigma^{\ast}}\circ p_{n+m}^{n,m}\right)  .
\]
Ya que los predicados%
\begin{align*}
\chi_{\{z_{i}\}}^{\omega}  & =\lambda xy\left[  x=y\right]  \circ\left[
p_{1}^{1,0},C_{z_{i}}^{1,0}\right] \\
\chi_{\{\gamma_{i}\}}^{\Sigma^{\ast}}  & =\lambda\alpha\beta\left[
\alpha=\beta\right]  \circ\left[  p_{1}^{0,1},C_{\gamma_{i}}^{0,1}\right]
\end{align*}
son $\Sigma$-p.r., el Lema \ref{boolean op} (aplicado $(n+m)-1$ veces),
implica que $\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}}$ es $\Sigma$-p.r..
Cuando $S$ tiene mas de un elemento, ya que entonces es la union de una
cantidad finita de conjuntos de un solo elemento, se puede aplicar el Lema
\ref{union} ($\left\vert S\right\vert -1$ veces) para obtener que $S$ es
$\Sigma$-p.r..
\end{proof}

\bigskip

El siguiente lema caracteriza cuando un conjunto rectangular es $\Sigma$-p.r..

\begin{lemma}
\label{rectangulos pr}Supongamos $S_{1},...,S_{n}\subseteq\omega$,
$L_{1},...,L_{m}\subseteq\Sigma^{\ast}$ son conjuntos no vacios. Entonces
$S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m}$ es $\Sigma$-p.r.
sii $S_{1},...,S_{n},L_{1},...,L_{m}$ son $\Sigma$-p.r.
\end{lemma}

\begin{proof}
($\Rightarrow$) Veremos por ejemplo que $L_{1}$ es $\Sigma$-p.r.. Sea
$(z_{1},...,z_{n},\zeta_{1},...,\zeta_{m})$ un elemento fijo de $S_{1}%
\times...\times S_{n}\times L_{1}\times...\times L_{m}.$ Note que%
\[
\alpha\in L_{1}\text{ sii }(z_{1},...,z_{n},\alpha,\zeta_{2},...,\zeta_{m})\in
S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m},
\]
lo cual implica que%
\[
\chi_{L_{1}}^{\Sigma^{\ast}}=\chi_{S_{1}\times...\times S_{n}\times
L_{1}\times...\times L_{m}}^{\omega^{n}\times\Sigma^{\ast m}}\circ\left[
C_{z_{1}}^{0,1},...,C_{z_{n}}^{0,1},p_{1}^{0,1},C_{\zeta_{2}}^{0,1}%
,...,C_{\zeta_{m}}^{0,1}\right]
\]
($\Leftarrow$) Note que $\chi_{S_{1}\times...\times S_{n}\times L_{1}%
\times...\times L_{m}}^{\omega^{n}\times\Sigma^{\ast m}}$ es el predicado%
\[
\left(  \chi_{S_{1}}^{\omega}\circ p_{1}^{n,m}\wedge...\wedge\chi_{S_{n}%
}^{\omega}\circ p_{n}^{n,m}\wedge\chi_{L_{1}}^{\Sigma^{\ast}}\circ
p_{n+1}^{n,m}\wedge...\wedge\chi_{L_{m}}^{\Sigma^{\ast}}\circ p_{n+m}%
^{n,m}\right)  .
\]

\end{proof}

\bigskip

Dada una funcion $f$ y un conjunto $S\subseteq D_{f}$, usaremos $f|_{S}$ para
denotar la \textit{restriccion} de $f$ al conjunto $S$, i.e. $f|_{S}%
=f\cap(S\times I_{f})$. Notese que $f|_{S}$ es la funcion dada por%
\[
D_{f|_{S}}=S\text{ \ \ \ y \ \ }f|_{S}(e)=f(e)\text{, para cada }e\in S
\]


\begin{lemma}
\label{restriccion}Sean $n,m\in\omega$ y $O\in\{\omega,\Sigma^{\ast}\}$.
Supongamos $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow O$ es
$\Sigma$-p.r.. Si $S\subseteq D_{f}$ es $\Sigma$-p.r., entonces $f|_{S}$ es
$\Sigma$-p.r..
\end{lemma}

\begin{proof}
Supongamos $O=\Sigma^{\ast}$. Entonces%
\[
f|_{S}=\lambda x\alpha\left[  \alpha^{x}\right]  \circ\left[  Suc\circ
Pred\circ\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}},f\right]
\]
lo cual nos dice que $f|_{S}$ es $\Sigma$-p.r.. El caso $O=\omega$ es similar
usando $\lambda xy\left[  x^{y}\right]  $ en lugar de $\lambda x\alpha\left[
\alpha^{x}\right]  $.
\end{proof}

\bigskip

Usando el lema anterior en combinacion con el Lema \ref{boolean op} podemos
ver que muchos predicados usuales son $\Sigma$-p.r.. Por ejemplo sea%
\[
P=\lambda x\alpha\beta\gamma\left[  x=\left\vert \gamma\right\vert
\wedge\alpha=\gamma^{Pred(\left\vert \beta\right\vert )}\right]  .
\]
Notese que%
\[
D_{P}=\omega\times\Sigma^{\ast}\times(\Sigma^{\ast}-\{\varepsilon
\})\times\Sigma^{\ast}%
\]
es $\Sigma$-p.r. ya que%
\[
\chi_{D_{P}}^{\omega\times\Sigma^{\ast3}}=\lnot\lambda\alpha\beta\left[
\alpha=\beta\right]  \circ\left[  p_{3}^{1,3},C_{\varepsilon}^{1,3}\right]
\]
Tambien note que los predicados%
\begin{align*}
& \lambda x\alpha\beta\gamma\left[  x=\left\vert \gamma\right\vert \right] \\
& \lambda x\alpha\beta\gamma\left[  \alpha=\gamma^{Pred(\left\vert
\beta\right\vert )}\right]
\end{align*}
son $\Sigma$-p.r. ya que pueden obtenerse componiendo funciones $\Sigma$-p.r..
O sea que $P$ es $\Sigma$-p.r. ya que%
\[
P=\left(  \lambda x\alpha\beta\gamma\left[  x=\left\vert \gamma\right\vert
\right]  |_{D_{P}}\wedge\lambda x\alpha\beta\gamma\left[  \alpha
=\gamma^{Pred(\left\vert \beta\right\vert )}\right]  \right)  .
\]


\bigskip

\begin{lemma}
\label{extension}Sean $n,m\in\omega$ y $O\in\{\omega,\Sigma^{\ast}\}$. Si
$f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow O$ es $\Sigma
$-p.r., entonces existe una funcion $\Sigma$-p.r. $\bar{f}:\omega^{n}%
\times\Sigma^{\ast m}\rightarrow O$, tal que $f=\bar{f}|_{D_{f}}$.
\end{lemma}

\begin{proof}
Es facil ver por induccion en $k$ que el enunciado se cumple para cada
$f\in\mathrm{PR}_{k}^{\Sigma}$
\end{proof}

\bigskip

Ahora podemos probar el siguiente importante resultado

\begin{proposition}
\label{caract-dominios}Un conjunto $S$ es $\Sigma$-p.r. sii $S$ es el dominio
de alguna funcion $\Sigma$-p.r.$.$
\end{proposition}

\begin{proof}
Supongamos que $S\subseteq\omega^{n}\times\Sigma^{\ast m}$.

($\Rightarrow$) Note que $S=D_{Pred\circ\chi_{S}^{\omega^{n}\times\Sigma^{\ast
m}}}$.

($\Leftarrow$) Probaremos por induccion en $k$ que $D_{F}$ es $\Sigma$-p.r.,
para cada $F\in\mathrm{PR}_{k}^{\Sigma}.$ El caso $k=0$ es facil$.$ Supongamos
el resultado vale para un $k$ fijo y supongamos $F\in\mathrm{PR}_{k+1}%
^{\Sigma}.$ Veremos entonces que $D_{F}$ es $\Sigma$-p.r.. Hay varios casos.
Consideremos primero el caso en que $F=R(f,g)$, donde%
\begin{align*}
f  & :S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m}%
\rightarrow\Sigma^{\ast}\\
g  & :\omega\times S_{1}\times...\times S_{n}\times L_{1}\times...\times
L_{m}\times\Sigma^{\ast}\rightarrow\Sigma^{\ast},
\end{align*}
con $S_{1},...,S_{n}\subseteq\omega$ y $L_{1},...,L_{m}\subseteq\Sigma^{\ast}$
conjuntos no vacios y $f,g\in\mathrm{PR}_{k}^{\Sigma}$. Notese que por
definicion de $R(f,g)$, tenemos que%
\[
D_{F}=\omega\times S_{1}\times...\times S_{n}\times L_{1}\times...\times
L_{m}.
\]
Por hipotesis inductiva tenemos que $D_{f}=S_{1}\times...\times S_{n}\times
L_{1}\times...\times L_{m}$ es $\Sigma$-p.r., lo cual por el Lema
\ref{rectangulos pr} nos dice que los conjuntos $S_{1},...,S_{n}$,
$L_{1},...,L_{m}$ son $\Sigma$-p.r.. Ya que $\omega$ es $\Sigma$-p.r., el Lema
\ref{rectangulos pr} nos dice que $D_{F}$ es $\Sigma$-p.r..

Los otros casos de recursion primitiva son dejados al lector.

Supongamos ahora que $F=g\circ\lbrack g_{1},...,g_{r}]$ con $g,g_{1}%
,...,g_{r}\in\mathrm{PR}_{k}^{\Sigma}$. Si $F=\emptyset$, entonces es claro
que $D_{F}=\emptyset$ es $\Sigma$-p.r.. Supongamos entonces que $F $ no es la
funcion $\emptyset$. Tenemos entonces que $r$ es de la forma $n+m $ y%
\begin{align*}
g  & :D_{g}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow O\\
g_{i}  & :D_{g_{i}}\subseteq\omega^{k}\times\Sigma^{\ast l}\rightarrow
\omega\text{, }i=1,...,n\\
g_{i}  & :D_{g_{i}}\subseteq\omega^{k}\times\Sigma^{\ast l}\rightarrow
\Sigma^{\ast},i=n+1,...,n+m
\end{align*}
con $O\in\{\omega,\Sigma^{\ast}\}$ y $k,l\in\omega$. Por Lema \ref{extension},
hay funciones $\Sigma$-p.r. $\bar{g}_{1},...,\bar{g}_{n+m}$ las cuales son
$\Sigma$-totales y cumplen%
\[
g_{i}=\bar{g}_{i}|_{D_{g_{i}}}\text{, para }i=1,...,n+m.
\]
Por hipotesis inductiva los conjuntos $D_{g}$, $D_{g_{i}}$, $i=1,...,n+m$, son
$\Sigma$-p.r. y por lo tanto%
\[
S=\bigcap_{i=1}^{n+m}D_{g_{i}}%
\]
lo es. Notese que%
\[
\chi_{D_{F}}^{\omega^{k}\times\Sigma^{\ast l}}=(\chi_{D_{g}}^{\omega^{n}%
\times\Sigma^{\ast m}}\circ\left[  \bar{g}_{1},...,\bar{g}_{n+m}\right]
\wedge\chi_{S}^{\omega^{k}\times\Sigma^{\ast l}})
\]
lo cual nos dice que $D_{F}$ es $\Sigma$-p.r..
\end{proof}

\bigskip

\paragraph{Lema de division por casos para funciones $\Sigma$-p.r.}

Una observacion interesante es que si $f_{i}:D_{f_{i}}\rightarrow O$,
$i=1,...,k$, son funciones tales que $D_{f_{i}}\cap D_{f_{j}}=\emptyset$ para
$i\neq j$, entonces $f_{1}\cup...\cup f_{k}$ es la funcion%
\[%
\begin{array}
[c]{rll}%
D_{f_{1}}\cup...\cup D_{f_{k}} & \rightarrow & O\\
e & \rightarrow & \left\{
\begin{array}
[c]{clc}%
f_{1}(e) &  & \text{si }e\in D_{f_{1}}\\
\vdots &  & \vdots\\
f_{k}(e) &  & \text{si }e\in D_{f_{k}}%
\end{array}
\right.
\end{array}
\]


\begin{lemma}
\label{dpc}Sean $n,m\in\omega$ y $O\in\{\omega,\Sigma^{\ast}\}$. Supongamos
$f_{i}:D_{f_{i}}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow O$,
$i=1,...,k$, son funciones $\Sigma$-p.r. tales que $D_{f_{i}}\cap D_{f_{j}%
}=\emptyset$ para $i\neq j.$ Entonces $f_{1}\cup...\cup f_{k}$ es $\Sigma$-p.r..
\end{lemma}

\begin{proof}
Supongamos $O=\Sigma^{\ast}$ y $k=2.$ Sean%
\[
\bar{f}_{i}:\omega^{n}\times\Sigma^{\ast m}\rightarrow\Sigma^{\ast},i=1,2,
\]
funciones $\Sigma$-p.r. tales que $\bar{f}_{i}|_{D_{f_{i}}}=f_{i}$, $i=1,2$
(Lema \ref{extension})$.$ Por Lema \ref{caract-dominios} los conjuntos
$D_{f_{1}}$ y $D_{f_{2}}$ son $\Sigma$-p.r. y por lo tanto lo es $D_{f_{1}%
}\cup D_{f_{2}}$. Ya que%
\[
f_{1}\cup f_{2}=\left(  \lambda\alpha\beta\left[  \alpha\beta\right]
\circ\left[  \lambda x\alpha\left[  \alpha^{x}\right]  \circ\left[
\chi_{D_{f_{1}}}^{\omega^{n}\times\Sigma^{\ast m}},\bar{f}_{1}\right]
,\lambda x\alpha\left[  \alpha^{x}\right]  \circ\left[  \chi_{D_{f_{2}}%
}^{\omega^{n}\times\Sigma^{\ast m}},\bar{f}_{2}\right]  \right]  \right)
|_{D_{f_{1}}\cup D_{f_{2}}}%
\]
tenemos que $f_{1}\cup f_{2}$ es $\Sigma$-p.r..

El caso $k>2$ puede probarse por induccion ya que%
\[
f_{1}\cup...\cup f_{k}=(f_{1}\cup...\cup f_{k-1})\cup f_{k}.
\]

\end{proof}

\begin{corollary}
\label{dom-finito}Supongamos $f$ es una funcion $\Sigma$-mixta cuyo dominio es
finito. Entonces $f$ es $\Sigma$-p.r..
\end{corollary}

\begin{proof}
Supongamos $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow O$, con
$D_{f}=\{e_{1},...,e_{k}\}$. Por el Corolario \ref{finito}, cada $\{e_{i}\}$
es $\Sigma$-p.r. por lo cual el Lema \ref{restriccion} nos dice que
$C_{f(e_{i})}^{n,m}|_{\{e_{i}\}}$ es $\Sigma$-p.r.. O sea que%
\[
f=C_{f(e_{1})}^{n,m}|_{\{e_{1}\}}\cup...\cup C_{f(e_{k})}^{n,m}|_{\{e_{k}\}}%
\]
es $\Sigma$-p.r..
\end{proof}

\bigskip

Recordemos que dados $i\in\omega$ y $\alpha\in\Sigma^{\ast}$, definimos%
\[
\left[  \alpha\right]  _{i}=\left\{
\begin{array}
[c]{lll}%
i\text{-esimo elemento de }\alpha &  & \text{si }1\leq i\leq\left\vert
\alpha\right\vert \\
\varepsilon &  & \text{caso contrario}%
\end{array}
\right.
\]


\begin{lemma}
$\lambda i\alpha\left[  \lbrack\alpha]_{i}\right]  $ es $\Sigma$-p.r..
\end{lemma}

\begin{proof}
Note que%
\begin{align*}
\lbrack\varepsilon]_{i}  & =\varepsilon\\
\lbrack\alpha a]_{i}  & =\left\{
\begin{array}
[c]{lll}%
\lbrack\alpha]_{i} &  & \text{si }i\neq\left\vert \alpha\right\vert +1\\
a &  & \text{si }i=\left\vert \alpha\right\vert +1
\end{array}
\right.
\end{align*}
lo cual dice que $\lambda i\alpha\left[  \lbrack\alpha]_{i}\right]  =R\left(
C_{\varepsilon}^{1,0},\mathcal{G}\right)  $, donde $\mathcal{G}_{a}%
:\omega\times\Sigma^{\ast}\times\Sigma^{\ast}\rightarrow\Sigma^{\ast}$ es dada
por%
\[
\mathcal{G}_{a}(i,\alpha,\zeta)=\left\{
\begin{array}
[c]{lll}%
\zeta &  & \text{si }i\neq\left\vert \alpha\right\vert +1\\
a &  & \text{si }i=\left\vert \alpha\right\vert +1
\end{array}
\right.
\]
O sea que solo resta probar que cada $\mathcal{G}_{a}$ es $\Sigma$-p.r..
Primero note que los conjuntos%
\begin{align*}
S_{1}  & =\left\{  (i,\alpha,\zeta)\in\omega\times\Sigma^{\ast}\times
\Sigma^{\ast}:i\neq\left\vert \alpha\right\vert +1\right\} \\
S_{2}  & =\left\{  (i,\alpha,\zeta)\in\omega\times\Sigma^{\ast}\times
\Sigma^{\ast}:i=\left\vert \alpha\right\vert +1\right\}
\end{align*}
son $\Sigma$-p.r. ya que%
\begin{align*}
\chi_{S_{1}}^{\omega\times\Sigma^{\ast}\times\Sigma^{\ast}}  & =\lambda
xy\left[  x\neq y\right]  \circ\left[  p_{1}^{1,2},Suc\circ\lambda
\alpha\left[  \left\vert \alpha\right\vert \right]  \circ p_{2}^{1,2}\right]
\\
\chi_{S_{2}}^{\omega\times\Sigma^{\ast}\times\Sigma^{\ast}}  & =\lambda
xy\left[  x=y\right]  \circ\left[  p_{1}^{1,2},Suc\circ\lambda\alpha\left[
\left\vert \alpha\right\vert \right]  \circ p_{2}^{1,2}\right]
\end{align*}
Ya que%
\[
\mathcal{G}_{a}=p_{3}^{1,2}|_{S_{1}}\cup C_{a}^{1,2}|_{S_{2}}%
\]
el Lema \ref{dpc} nos dice que $\mathcal{G}_{a}$ es $\Sigma$-p.r., para cada
$a\in\Sigma$.
\end{proof}

\bigskip

\paragraph{Sumatoria, productoria y concatenatoria de funciones $\Sigma$-p.r.}

Sea $\Sigma$ un alfabeto finito. Sea $f:\omega\times S_{1}\times...\times
S_{n}\times L_{1}\times...\times L_{m}\rightarrow\omega$, con $S_{1}%
,...,S_{n}\subseteq\omega$ y $L_{1},...,L_{m}\subseteq\Sigma^{\ast} $ no
vacios. Para $x,y\in\omega$ y $(\vec{x},\vec{\alpha})\in S_{1}\times...\times
S_{n}\times L_{1}\times...\times L_{m}$, definamos%
\begin{align*}
\sum\limits_{t=x}^{t=y}f(t,\vec{x},\vec{\alpha})  & =\left\{
\begin{array}
[c]{lll}%
0 &  & \text{si }x>y\\
f(x,\vec{x},\vec{\alpha})+f(x+1,\vec{x},\vec{\alpha})+...+f(y,\vec{x}%
,\vec{\alpha}) &  & \text{si }x\leq y
\end{array}
\right. \\
\prod\limits_{t=x}^{t=y}f(t,\vec{x},\vec{\alpha})  & =\left\{
\begin{array}
[c]{lll}%
1 &  & \text{si }x>y\\
f(x,\vec{x},\vec{\alpha}).f(x+1,\vec{x},\vec{\alpha})....f(y,\vec{x}%
,\vec{\alpha}) &  & \text{si }x\leq y
\end{array}
\right.
\end{align*}
En forma similar, cuando $I_{f}\subseteq\Sigma^{\ast}$, definamos%
\[
\overset{t=y}{\underset{t=x}{\subset}}f(t,\vec{x},\vec{\alpha})=\left\{
\begin{array}
[c]{lll}%
\varepsilon &  & \text{si }x>y\\
f(x,\vec{x},\vec{\alpha})f(x+1,\vec{x},\vec{\alpha})....f(y,\vec{x}%
,\vec{\alpha}) &  & \text{si }x\leq y
\end{array}
\right.
\]
Note que, en virtud de la definicion anterior, el dominio de las funciones%

\[
\lambda xy\vec{x}\vec{\alpha}\left[  \sum_{t=x}^{t=y}f(t,\vec{x},\vec{\alpha
})\right]  \ \ \ \ \ \ \ \ \ \ \ \ \lambda xy\vec{x}\vec{\alpha}\left[
\prod_{t=x}^{t=y}f(t,\vec{x},\vec{\alpha})\right]
\ \ \ \ \ \ \ \ \ \ \ \ \lambda xy\vec{x}\vec{\alpha}\left[  \subset
_{t=x}^{t=y}f(t,\vec{x},\vec{\alpha})\right]
\]
es $\omega\times\omega\times S_{1}\times...\times S_{n}\times L_{1}%
\times...\times L_{m}$.

\begin{lemma}
\label{iteracion}Sea $\Sigma$ un alfabeto finito.

\begin{enumerate}
\item[(a)] Si $f:\omega\times S_{1}\times...\times S_{n}\times L_{1}%
\times...\times L_{m}\rightarrow\omega$ es $\Sigma$-p.r., con $S_{1}%
,...,S_{n}\subseteq\omega$ y $L_{1},...,L_{m}\subseteq\Sigma^{\ast} $ no
vacios, entonces las funciones $\lambda xy\vec{x}\vec{\alpha}\left[
\sum_{t=x}^{t=y}f(t,\vec{x},\vec{\alpha})\right]  $ y $\lambda xy\vec{x}%
\vec{\alpha}\left[  \prod_{t=x}^{t=y}f(t,\vec{x},\vec{\alpha})\right]  $ son
$\Sigma$-p.r.

\item[(b)] Si $f:\omega\times S_{1}\times...\times S_{n}\times L_{1}%
\times...\times L_{m}\rightarrow\Sigma^{\ast}$ es $\Sigma$-p.r., con
$S_{1},...,S_{n}\subseteq\omega$ y $L_{1},...,L_{m}\subseteq\Sigma^{\ast} $ no
vacios, entonces la funcion $\lambda xy\vec{x}\vec{\alpha}\left[
\subset_{t=x}^{t=y}f(t,\vec{x},\vec{\alpha})\right]  $ es $\Sigma$-p.r.
\end{enumerate}
\end{lemma}

\begin{proof}
(a) Sea $G=\lambda tx\vec{x}\vec{\alpha}\left[  \sum_{i=x}^{i=t}f(i,\vec
{x},\vec{\alpha})\right]  $. Ya que%
\[
\lambda xy\vec{x}\vec{\alpha}\left[  \sum_{i=x}^{i=y}f(i,\vec{x},\vec{\alpha
})\right]  =G\circ\left[  p_{2}^{n+2,m},p_{1}^{n+2,m},p_{3}^{n+2,m}%
,...,p_{n+m+2}^{n+2,m}\right]
\]
solo tenemos que probar que $G$ es $\Sigma$-p.r.. Primero note que%
\begin{align*}
G(0,x,\vec{x},\vec{\alpha})  & =\left\{
\begin{array}
[c]{lll}%
0 &  & \text{si }x>0\\
f(0,\vec{x},\vec{\alpha}) &  & \text{si }x=0
\end{array}
\right. \\
G(t+1,x,\vec{x},\vec{\alpha})  & =\left\{
\begin{array}
[c]{lll}%
0 &  & \text{si }x>t+1\\
G(t,x,\vec{x},\vec{\alpha})+f(t+1,\vec{x},\vec{\alpha}) &  & \text{si }x\leq
t+1
\end{array}
\right.
\end{align*}
O sea que si definimos%
\[%
\begin{array}
[c]{rll}%
h:\omega\times S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m} &
\rightarrow & \omega\\
(x,\vec{x},\vec{\alpha}) & \rightarrow & \left\{
\begin{array}
[c]{lll}%
0 &  & \text{si }x>0\\
f(0,\vec{x},\vec{\alpha}) &  & \text{si }x=0
\end{array}
\right.
\end{array}
\]%
\[%
\begin{array}
[c]{rll}%
g:\omega^{3}\times S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m}
& \rightarrow & \omega\\
(A,t,x,\vec{x},\vec{\alpha}) & \rightarrow & \left\{
\begin{array}
[c]{lll}%
0 &  & \text{si }x>t+1\\
A+f(t+1,\vec{x},\vec{\alpha}) &  & \text{si }x\leq t+1
\end{array}
\right.
\end{array}
\]
tenemos que $G=R(h,g)$. Es decir que solo nos falta probar que $h$ y $g$ son
$\Sigma$-p.r.. Sean%
\begin{align*}
D_{1}  & =\left\{  (x,\vec{x},\vec{\alpha})\in\omega\times S_{1}%
\times...\times S_{n}\times L_{1}\times...\times L_{m}:x>0\right\} \\
D_{2}  & =\left\{  (x,\vec{x},\vec{\alpha})\in\omega\times S_{1}%
\times...\times S_{n}\times L_{1}\times...\times L_{m}:x=0\right\} \\
H_{1}  & =\left\{  (z,t,x,\vec{x},\vec{\alpha})\in\omega^{3}\times S_{1}%
\times...\times S_{n}\times L_{1}\times...\times L_{m}:x>t+1\right\} \\
H_{2}  & =\left\{  (z,t,x,\vec{x},\vec{\alpha})\in\omega^{3}\times S_{1}%
\times...\times S_{n}\times L_{1}\times...\times L_{m}:x\leq t+1\right\}  .
\end{align*}
Notese que%
\begin{align*}
h  & =C_{0}^{n+1,m}|_{D_{1}}\cup\lambda x\vec{x}\vec{\alpha}\left[
f(0,\vec{x},\vec{\alpha})\right]  |_{D_{2}}\\
g  & =C_{0}^{n+3,m}|_{H_{1}}\cup\lambda Atx\vec{x}\vec{\alpha}\left[
A+f(t+1,\vec{x},\vec{\alpha})\right]  )|_{H_{2}}%
\end{align*}
Ya que $f$ es $\Sigma$-p.r. y%
\begin{align*}
\lambda x\vec{x}\vec{\alpha}\left[  f(0,\vec{x},\vec{\alpha})\right]   &
=f\circ\left[  C_{0}^{n+1,m},p_{2}^{n+1,m},p_{3}^{n+1,m},...,p_{n+1+m}%
^{n+1,m}\right] \\
\lambda Atx\vec{x}\vec{\alpha}\left[  A+f(t+1,\vec{x},\vec{\alpha})\right]  )
& =\lambda xy[x+y]\circ\left[  p_{1}^{n+3,m},f\circ\left[  Suc\circ
p_{2}^{n+3,m},p_{4}^{n+3,m},...,p_{n+3+m}^{n+3,m}\right]  \right]
\end{align*}
tenemos que $\lambda x\vec{x}\vec{\alpha}\left[  f(0,\vec{x},\vec{\alpha
})\right]  $ y $\lambda Atx\vec{x}\vec{\alpha}\left[  A+f(t+1,\vec{x}%
,\vec{\alpha})\right]  )$ son $\Sigma$-p.r..O sea que para probar que $h$ y
$g$ son $\Sigma$-p.r.solo nos falta ver que los conjuntos $D_{1},D_{2}%
,H_{1},H_{2}$ son $\Sigma$-p.r.. y aplicar luego el Lema \ref{restriccion}.
Veamos que por ejemplo $H_{1}$ lo es. Es decir debemos ver que $\chi_{H_{1}%
}^{\omega^{3+n}\times\Sigma^{\ast m}}$ es $\Sigma$-p.r.. Ya que $f$ es
$\Sigma$-p.r. tenemos que $D_{f}=\omega\times S_{1}\times...\times S_{n}\times
L_{1}\times...\times L_{m}$ es $\Sigma$-p.r., lo cual por el Lema
\ref{rectangulos pr} nos dice que los conjuntos $S_{1},...,S_{n}$,
$L_{1},...,L_{m}$ son $\Sigma$-p.r.. Ya que $\omega$ es $\Sigma$-p.r., el Lema
\ref{rectangulos pr} nos dice que $R=\omega^{3}\times S_{1}\times...\times
S_{n}\times L_{1}\times...\times L_{m}$ es $\Sigma$-p.r.. Notese que
$\chi_{H_{1}}^{\omega^{3+n}\times\Sigma^{\ast m}}=(\chi_{R}^{\omega
^{3+n}\times\Sigma^{\ast m}}\wedge\lambda ztx\vec{x}\vec{\alpha}\left[
x>t+1\right]  )$ por lo cual $\chi_{H_{1}}^{\omega^{3+n}\times\Sigma^{\ast m}%
}$ es $\Sigma$-p.r. ya que es la conjuncion de dos predicados $\Sigma$-p.r.
\end{proof}

\bigskip

Veamos un ejemplo de como se puede aplicar el lema anterior. Sea $F=\lambda
yx_{1}\left[  \sum_{t=0}^{t=y}(x_{1})^{t}\right]  $. Es claro que
$D_{F}=\omega^{2}$. Para ver que $F$ es $\Sigma$-p.r. aplicaremos el lema
anterior por lo cual es importante encontrar la $f$ adecuada a la cual se le
aplicara el lema. Tomemos $f=\lambda tx_{1}[(x_{1})^{t}]$. Claramente $f$ es
$\Sigma$-p.r. por lo cual el lema anterior nos dice que%
\[
G=\lambda xyx_{1}\left[  \sum_{t=x}^{t=y}f(t,x_{1})\right]  =\lambda
xyx_{1}\left[  \sum_{t=x}^{t=y}(x_{1})^{t}\right]
\]
es $\Sigma$-p.r.. Claramente $G$ no es la funcion $F$ pero es en algun sentido
"mas amplia" que $F$ ya que tiene una variable mas y se tiene que
$F(y,x_{1})=G(0,y,x_{1})$, para cada $y,x_{1}\in\omega$. Es facil ver que%
\[
F=G\circ\left[  C_{0}^{2,0},p_{1}^{2,0},p_{2}^{2,0}\right]
\]
por lo cual $F$ es $\Sigma$-p.r..

\bigskip

\paragraph{Cuantificacion acotada de predicados $\Sigma$-p.r. con dominio
rectangular}

Ses $P:S\times S_{1}\times...\times S_{n}\times L_{1}\times...\times
L_{m}\rightarrow\omega$ un predicado, con $S,S_{1},...,S_{n}\subseteq\omega$ y
$L_{1},...,L_{m}\subseteq\Sigma^{\ast}$ no vacios. Supongamos $\bar
{S}\subseteq S$. Entonces la expresion Booleana%
\[
(\forall t\in\bar{S})_{t\leq x}\;P(t,\vec{x},\vec{\alpha})
\]
depende de las variables $x,\vec{x},\vec{\alpha}$ y valdra $1$ en una
$(1+n+m)$-upla $(x,\vec{x},\vec{\alpha})$ cuando $P(t,\vec{x},\vec{\alpha})$
sea igual a $1$ para cada $t\in\{u\in\bar{S}:u\leq x\}$; y $0$ en caso
contrario. Tenemos entonces que el dominio del predicado%
\[
\lambda x\vec{x}\vec{\alpha}\left[  (\forall t\in\bar{S})_{t\leq x}%
\;P(t,\vec{x},\vec{\alpha})\right]
\]
es $\omega\times S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m}$.
En forma analoga se define la forma de interpretar la expresion Booleana%
\[
(\exists t\in\bar{S})_{t\leq x}\;P(t,\vec{x},\vec{\alpha})
\]
Cabe destacar que%
\[
\lambda x\vec{x}\vec{\alpha}\left[  (\exists t\in\bar{S})_{t\leq x}%
\;P(t,\vec{x},\vec{\alpha})\right]  =\lnot\lambda x\vec{x}\vec{\alpha}\left[
(\forall t\in\bar{S})_{t\leq x}\;\lnot P(t,\vec{x},\vec{\alpha})\right]
\]


Tambien podemos cuantificar sobre variable alfabetica. Sea $P:S_{1}%
\times...\times S_{n}\times L_{1}\times...\times L_{m}\times L\rightarrow
\omega$ un predicado, con $S_{1},...,S_{n}\subseteq\omega$ y $L,L_{1}%
,...,L_{m}\subseteq\Sigma^{\ast}$ no vacios. Supongamos $\bar{L}\subseteq L$.
Entonces la expresion Booleana%
\[
(\forall\alpha\in\bar{L})_{\left\vert \alpha\right\vert \leq x}\;P(\vec
{x},\vec{\alpha},\alpha)
\]
depende de las variables $x,\vec{x},\vec{\alpha}$ y valdra $1$ en una
$(1+n+m)$-upla $(x,\vec{x},\vec{\alpha})$ cuando $P(\vec{x},\vec{\alpha
},\alpha)$ sea igual a $1$ para cada $\alpha\in\{\beta\in\bar{L}:\left\vert
\beta\right\vert \leq x\}$; y $0$ en caso contrario. Tenemos entonces que el
dominio del predicado%
\[
\lambda x\vec{x}\vec{\alpha}\left[  (\forall\alpha\in\bar{L})_{\left\vert
\alpha\right\vert \leq x}\;P(\vec{x},\vec{\alpha},\alpha)\right]
\]
es $\omega\times S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m}$.
En forma analoga se define la forma de interpretar la expresion Booleana%
\[
(\exists\alpha\in\bar{L})_{\left\vert \alpha\right\vert \leq x}\;P(\vec
{x},\vec{\alpha},\alpha)
\]
Cabe destacar que%
\[
\lambda x\vec{x}\vec{\alpha}\left[  (\exists\alpha\in\bar{L})_{\left\vert
\alpha\right\vert \leq x}P(\vec{x},\vec{\alpha},\alpha)\right]  =\lnot\lambda
x\vec{x}\vec{\alpha}\left[  (\forall\alpha\in\bar{L})_{\left\vert
\alpha\right\vert \leq x}\lnot P(\vec{x},\vec{\alpha},\alpha)\right]
\]


\bigskip

\begin{lemma}
\label{cuantificacion}Sea $\Sigma$ un alfabeto finito.

\begin{enumerate}
\item[(a)] Sea $P:S\times S_{1}\times...\times S_{n}\times L_{1}%
\times...\times L_{m}\rightarrow\omega$ un predicado $\Sigma$-p.r., con
$S,S_{1},...,S_{n}\subseteq\omega$ y $L_{1},...,L_{m}\subseteq\Sigma^{\ast}$
no vacios. Supongamos $\bar{S}\subseteq S$ es $\Sigma$-p.r.. Entonces $\lambda
x\vec{x}\vec{\alpha}\left[  (\forall t\in\bar{S})_{t\leq x}\;P(t,\vec{x}%
,\vec{\alpha})\right]  $ y $\lambda x\vec{x}\vec{\alpha}\left[  (\exists
t\in\bar{S})_{t\leq x}\;P(t,\vec{x},\vec{\alpha})\right]  $ son predicados
$\Sigma$-p.r..

\item[(b)] Sea $P:S_{1}\times...\times S_{n}\times L_{1}\times...\times
L_{m}\times L\rightarrow\omega$ un predicado $\Sigma$-p.r., con $S_{1}%
,...,S_{n}\subseteq\omega$ y $L,L_{1},...,L_{m}\subseteq\Sigma^{\ast}$ no
vacios. Supongamos $\bar{L}\subseteq L$ es $\Sigma$-p.r.. Entonces $\lambda
x\vec{x}\vec{\alpha}\left[  (\forall\alpha\in\bar{L})_{\left\vert
\alpha\right\vert \leq x}\;P(\vec{x},\vec{\alpha},\alpha)\right]  $ y $\lambda
x\vec{x}\vec{\alpha}\left[  (\exists\alpha\in\bar{L})_{\left\vert
\alpha\right\vert \leq x}\;P(\vec{x},\vec{\alpha},\alpha)\right]  $ son
predicados $\Sigma$-p.r..
\end{enumerate}
\end{lemma}

\begin{proof}
(a) Sea%
\[
\bar{P}=P|_{\bar{S}\times S_{1}\times...\times S_{n}\times L_{1}%
\times...\times L_{m}}\cup C_{1}^{1+n,m}|_{(\omega-\bar{S})\times S_{1}%
\times...\times S_{n}\times L_{1}\times...\times L_{m}}%
\]
Notese que $\bar{P}$ tiene dominio $\omega\times S_{1}\times...\times
S_{n}\times L_{1}\times...\times L_{m}$ y es $\Sigma$-p.r.. Ya que%
\begin{align*}
\lambda x\vec{x}\vec{\alpha}\left[  (\forall t\in\bar{S})_{t\leq x}P(t,\vec
{x},\vec{\alpha})\right]   & =\lambda x\vec{x}\vec{\alpha}\left[
\prod\limits_{t=0}^{t=x}\bar{P}(t,\vec{x},\vec{\alpha})\right] \\
& =\lambda xy\vec{x}\vec{\alpha}\left[  \prod\limits_{t=x}^{t=y}\bar{P}%
(t,\vec{x},\vec{\alpha})\right]  \circ\left[  C_{0}^{1+n,m},p_{1}%
^{1+n,m},...,p_{1+n+m}^{1+n,m}\right]
\end{align*}
el Lema \ref{iteracion} implica que $\lambda x\vec{x}\vec{\alpha}\left[
(\forall t\in\bar{S})_{t\leq x}\;P(t,\vec{x},\vec{\alpha})\right]  $ es
$\Sigma$-p.r..

Ya que%
\[
\lambda x\vec{x}\vec{\alpha}\left[  (\exists t\in\bar{S})_{t\leq x}%
\;P(t,\vec{x},\vec{\alpha})\right]  =\lnot\lambda x\vec{x}\vec{\alpha}\left[
(\forall t\in\bar{S})_{t\leq x}\;\lnot P(t,\vec{x},\vec{\alpha})\right]
\]
tenemos que $\lambda x\vec{x}\vec{\alpha}\left[  (\exists t\in\bar{S})_{t\leq
x}\;P(t,\vec{x},\vec{\alpha})\right]  $ es $\Sigma$-p.r.

(b) Haremos solo el caso del cuantificador $\forall$. Primero supongamos que
$\Sigma=\emptyset$. Ya que $L,L_{1},...,L_{m}$ son no vacios, debera suceder
que $L=L_{1}=...=L_{m}=\{\varepsilon\}$. Ya que $\bar{L}\subseteq L$, tenemos
que $\bar{L}=\emptyset$ o $\bar{L}=\{\varepsilon\}$. Si $\bar{L}=\emptyset$,
entonces%
\begin{align*}
\lambda x\vec{x}\vec{\alpha}\left[  (\forall\alpha\in\bar{L})_{\left\vert
\alpha\right\vert \leq x}\;P(\vec{x},\vec{\alpha},\alpha)\right]   & =\lambda
x\vec{x}\vec{\alpha}\left[  1\right] \\
& =C_{1}^{1+n,m}%
\end{align*}
por lo cual es $\Sigma$-p.r.

Si $\bar{L}=\{\varepsilon\}$, entonces%
\begin{align*}
\lambda x\vec{x}\vec{\alpha}\left[  (\forall\alpha\in\bar{L})_{\left\vert
\alpha\right\vert \leq x}\;P(\vec{x},\vec{\alpha},\alpha)\right]   & =\lambda
x\vec{x}\vec{\alpha}\left[  (P(\vec{x},\vec{\alpha},\varepsilon)\right] \\
& =P\circ\left[  p_{2}^{1+n,m},...,p_{1+n+m}^{1+n,m},C_{\varepsilon}%
^{1+n,m},\right]
\end{align*}
por lo cual es $\Sigma$-p.r.

Ahora supongamos $\Sigma$ es no vacio. Sea $\leq$ un orden total sobre
$\Sigma.$ Sea $k$ el cardinal de $\Sigma$. Primero notese que

\begin{enumerate}
\item[(*)] $\left\vert \alpha\right\vert \leq x$ sii $\#^{\leq}(\alpha
)\leq\sum_{\iota=1}^{i=x}k^{i}$, cualesquiera sean $x\in\omega$ y $\alpha
\in\Sigma^{\ast}$
\end{enumerate}

(queda como ejercicio probar (*). Sean%
\begin{align*}
\#^{\leq}(L)  & =\{\#^{\leq}(\alpha):\alpha\in L\}\\
\#^{\leq}(\bar{L})  & =\{\#^{\leq}(\alpha):\alpha\in\bar{L}\}
\end{align*}
Notese que%
\begin{align*}
\chi_{\#^{\leq}(L)}^{\omega}  & =\chi_{L}^{\Sigma^{\ast}}\circ\ast^{\leq}\\
\chi_{\#^{\leq}(\bar{L})}^{\omega}  & =\chi_{\bar{L}}^{\Sigma^{\ast}}\circ
\ast^{\leq}%
\end{align*}
por lo cual $\#^{\leq}(L)$ y $\#^{\leq}(\bar{L})$ son $\Sigma$-p.r.. Sea
$H=\lambda t\vec{x}\vec{\alpha}\left[  P(\vec{x},\vec{\alpha},\ast^{\leq
}(t))\right]  .$ Notese que%
\[
D_{H}=\#^{\leq}(L)\times S_{1}\times...\times S_{n}\times L_{1}\times...\times
L_{m}%
\]
y $H$ es $\Sigma$-p.r.. O sea que por (a) tenemos que%
\[
\lambda x\vec{x}\vec{\alpha}\left[  (\forall t\in\#^{\leq}(\bar{L}))_{t\leq
x}H(t,\vec{x},\vec{\alpha})\right]  =\lambda x\vec{x}\vec{\alpha}\left[
(\forall t\in\#^{\leq}(\bar{L}))_{t\leq x}P(\vec{x},\vec{\alpha},\ast^{\leq
}(t))\right]
\]
es $\Sigma$-p.r.. Llamemos $Q$ al predicado $\lambda x\vec{x}\vec{\alpha
}\left[  (\forall t\in\#^{\leq}(\bar{L}))_{t\leq x}P(\vec{x},\vec{\alpha}%
,\ast^{\leq}(t))\right]  $. Tenemos que%
\begin{align*}
\lambda x\vec{x}\vec{\alpha}\left[  (\forall\alpha\in\bar{L})_{\left\vert
\alpha\right\vert \leq x}P(\vec{x},\vec{\alpha},\alpha)\right]   & =\lambda
x\vec{x}\vec{\alpha}\left[  (\forall t\in\#^{\leq}(\bar{L}))_{t\leq\sum
_{\iota=1}^{i=x}k^{i}}P(\vec{x},\vec{\alpha},\ast^{\leq}(t))\right]  \text{
(por (*))}\\
& =Q\circ\left[  \lambda x\vec{x}\vec{\alpha}\left[  \sum\limits_{\iota
=1}^{i=x}k^{i}\right]  ,p_{1}^{1+n,m},...,p_{1+n+m}^{1+n,m}\right]
\end{align*}
Pero $\lambda x\vec{x}\vec{\alpha}\left[  \sum\limits_{\iota=1}^{i=x}%
k^{i}\right]  $ es $\Sigma$-p.r. (ejercicio), lo cual nos dice que $\lambda
x\vec{x}\vec{\alpha}\left[  (\forall\alpha\in\bar{L})_{\left\vert
\alpha\right\vert \leq x}\;P(\vec{x},\vec{\alpha},\alpha)\right]  $ lo es
\end{proof}

\bigskip

OBSERVACION: La cuantificacion no acotada no preserva la propiedad de ser
$\Sigma$-p.r.. Como veremos mas adelante si elejimos bien al predicado
$\Sigma$-p.r. $P$, obtenemos que el predicado $\lambda\vec{x}\vec{\alpha
}\left[  (\exists t\in\bar{S})\;P(t,\vec{x},\vec{\alpha})\right]  $ no solo no
es $\Sigma$-p.r. sino que tampoco es $\Sigma$-efectivamente computable
(Teorema \ref{autohalt es no EC}).

\bigskip

Algunos ejemplos en los cuales cuantificacion acotada se aplica naturalmente
son dados a continuacion.

\begin{lemma}
Sea $\Sigma$ un alfabeto finito.

\begin{enumerate}
\item[(a)] El predicado $\lambda xy\left[  x\text{ divide }y\right]  $ es
$\Sigma$-p.r..

\item[(b)] El predicado $\lambda x\left[  x\text{ es primo}\right]  $ es
$\Sigma$-p.r..

\item[(c)] El predicado $\lambda\alpha\beta\left[  \alpha\text{\ }%
\mathrm{inicial}\ \beta\right]  $ es $\Sigma$-p.r..
\end{enumerate}
\end{lemma}

\begin{proof}
(a) Sea $P=\lambda tx_{1}x_{2}\left[  x_{2}=t.x_{1}\right]  $. Es claro que
$P$ es $\Sigma$-p.r.. El lema anterior nos dice que $\lambda xx_{1}%
x_{2}\left[  (\exists t\in\omega)_{t\leq x}\;P(t,x_{1},x_{2})\right]  $ es
$\Sigma$-p.r.. Notese que $x_{1}$ divide $x_{2}$ si y solo si hay un $t\leq
x_{2}$ tal que $x_{2}=t.x_{1}$. Esto nos dice que%
\[
\lambda x_{1}x_{2}\left[  x_{1}\text{ divide }x_{2}\right]  =\lambda
x_{1}x_{2}\left[  (\exists t\in\omega)_{t\leq x_{2}}\;P(t,x_{1},x_{2})\right]
\]
Pero%
\[
\lambda x_{1}x_{2}\left[  (\exists t\in\omega)_{t\leq x_{2}}\;P(t,x_{1}%
,x_{2})\right]  =\lambda xx_{1}x_{2}\left[  (\exists t\in\omega)_{t\leq
x}\;P(t,x_{1},x_{2})\right]  \circ\left[  p_{2}^{2,0},p_{1}^{2,0},p_{2}%
^{2,0}\right]
\]
por lo cual $\lambda x_{1}x_{2}\left[  x_{1}\text{ divide }x_{2}\right]  $ es
$\Sigma$-p.r.

(b) Ya que%
\[
x\text{ es primo sii }x>1\wedge\left(  (\forall t\in\omega)_{t\leq x}\;t=1\vee
t=x\vee\lnot(t\text{ divide }x)\right)
\]
podemos usar un argumento similar al de la prueba de (a).

(c) es dejado al lector.
\end{proof}

\bigskip

La idea fundamental subyacente en las aplicaciones anteriores es que en muchos
casos de predicados obtenidos por cuantificacion a partir de otros predicados,
la variable cuantificada tiene una cota natural en terminos de las otras
variables y ntonces componiendo adecuadamente se lo puede presentar como un
caso de cuantificacion acotada

\bigskip

\subsubsection{Minimizacion y funciones $\Sigma$-recursivas}

Tal como fue explicado anteriormente, para obtener la clase de las funciones
$\Sigma$-recursivas debemos agregar un nuevo constructor a los ya definidos de
composicion y recursion primitiva, a saber el constructor de
\textit{minimizacion}. Tiene dos casos.

\paragraph{Minimizacion de variable numerica}

\noindent Sea $\Sigma$ un alfabeto finito y sea $P:D_{P}\subseteq\omega
\times\omega^{n}\times\Sigma^{\ast m}\rightarrow\omega$ un predicado. Dado
$(\vec{x},\vec{\alpha})\in\omega^{n}\times\Sigma^{\ast m}$, cuando exista al
menos un $t\in\omega$ tal que $P(t,\vec{x},\vec{\alpha})=1$, usaremos
$\min_{t}P(t,\vec{x},\vec{\alpha})$ para denotar al menor de tales $t^{\prime
}s$. Notese que la expresion $\min_{t}P(t,\vec{x},\vec{\alpha})$ esta definida
solo para aquellas $(n+m)$-uplas $(\vec{x},\vec{\alpha})$ para las cuales hay
al menos un $t$ tal que se da $P(t,\vec{x},\vec{\alpha})=1$. Dicho de otra
forma, $\min_{t}P(t,\vec{x},\vec{\alpha})$ no estara definida cuando para cada
$t\in\omega$ se de que $(t,\vec{x},\vec{\alpha})$ no pertenece a $D_{P}$ o
$P(t,\vec{x},\vec{\alpha})=0$. Otro detalle importante a tener en cuenta es
que la expresion $\min_{t}P(t,\vec{x},\vec{\alpha})$ no depende de la variable
$t$. Por ejemplo, las expresiones $\min_{t}P(t,\vec{x},\vec{\alpha})$ y
$\min_{i}P(i,\vec{x},\vec{\alpha})$ son equivalentes en el sentido que estan
definidas en las mismas $(n+m)$-uplas y cuando estan definidas asumen el mismo valor.

Definamos%
\[
M(P)=\lambda\vec{x}\vec{\alpha}\left[  \min\nolimits_{t}P(t,\vec{x}%
,\vec{\alpha})\right]
\]
Notese que%
\begin{align*}
D_{M(P)}  & =\left\{  (\vec{x},\vec{\alpha})\in\omega^{n}\times\Sigma^{\ast
m}:(\exists t\in\omega)\ P(t,\vec{x},\vec{\alpha})\right\} \\
M(P)(\vec{x},\vec{\alpha})  & =\min\nolimits_{t}P(t,\vec{x},\vec{\alpha
})\text{, para cada }(\vec{x},\vec{\alpha})\in D_{M(P)}%
\end{align*}
Diremos que $M(P)$ se obtiene por \textit{minimizacion de variable numerica }a
partir de $P$.

\bigskip

Veamos un par de ejemplos:

\begin{enumerate}
\item[(E1)] Tomemos $P=\lambda tx_{1}[t^{2}=x_{1}]$. Tenemos que:%
\begin{align*}
D_{M(P)}  & =\left\{  x_{1}\in\omega:(\exists t\in\omega)\ P(t,x_{1})\right\}
\\
& =\left\{  x_{1}\in\omega:(\exists t\in\omega)\ t^{2}=x_{1}\right\}
\end{align*}
Es decir el dominio de $M(P)$ es el conjunto de los cuadrados. Ademas para
cada $x_{1}\in D_{M(P)}$ tenemos que%
\[
M(P)(x_{1})=\min\nolimits_{t}P(t,x_{1})=\min\nolimits_{t}(t^{2}=x_{1})
\]
por lo cual $M(P)(x)=\sqrt{x}$, para cada $x\in D_{M(P)}$.

\item[(E2)] Recordemos que dados $x_{1},x_{2}\in\omega$, con $x_{2}$ no nulo,
el \textit{cociente de dividir }$x_{1}$\textit{\ por }$x_{2}$ se define como
el maximo elemento del conjunto $\{t\in\omega:t.x_{2}\leq x_{1}\}$. Sea%
\[%
\begin{array}
[t]{rll}%
Q:\omega\times\mathbf{N} & \rightarrow & \omega\\
(x_{1},x_{2}) & \rightarrow & \text{cociente de dividir }x_{1}\text{ por
}x_{2}%
\end{array}
\]
Sea $P=\lambda tx_{1}x_{2}\left[  x_{1}<t.x_{2}\right]  $. Notar que%
\begin{align*}
D_{M(P)}  & =\{(x_{1},x_{2})\in\omega^{2}:(\exists t\in\omega)\;P(t,x_{1}%
,x_{2})=1\}\\
& =\{(x_{1},x_{2}):(\exists t\in\omega)\;x_{1}<t.x_{2}\}\\
& =\omega\times\mathbf{N}%
\end{align*}
Ademas si $(x_{1},x_{2})\in\omega\times\mathbf{N}$, es facil de probar que%
\[
\min\nolimits_{t}\ x_{1}<t.x_{2}=Q(x_{1},x_{2})+1
\]
por lo que $M(P)=Suc\circ Q$. Si quisieramos encontrar un predicado
$P^{\prime}$ tal que $M(P^{\prime})=Q$, entonces podemos tomar $P^{\prime
}=\lambda tx_{1}x_{2}\left[  x_{1}<(t+1).x_{2}\right]  $ y con un poco de
concentracion nos daremos cuenta que $M(P^{\prime})=Q$. De todas maneras hay
una forma mas facil de hacerlo y es tomando $P^{\prime}$ de tal forma que para
cada $(x_{1},x_{2})\in D_{Q}$ se de que%
\[
Q(x_{1},x_{2})=\mathrm{\ unico\ }t\in\omega\mathrm{\ tal\ que\ }P^{\prime
}(t,x_{1},x_{2})
\]
Por ejemplo se puede tomar $P^{\prime}=\lambda tx_{1}x_{2}\left[  x_{1}\geq
t.x_{2}\text{ y }x_{1}<(t+1).x_{2}\right]  $ que dicho sea de paso es justo la
definicion de cociente dada en la escuela primaria. Dejamos al lector
corroborar que $M(P^{\prime})=Q$, para este ultimo $P^{\prime}$.
\end{enumerate}

\bigskip

Tal como lo vimos recien muchas veces que querramos encontrar un predicado $P
$ tal que $M(P)$ sea igual a una funcion dada $f$, sera mas facil encontrar un
$P$ el cual cumpla%
\[
f(\vec{x},\vec{\alpha})=\mathrm{\ unico\ }t\in\omega\mathrm{\ tal\ que\ }%
P(t,\vec{x},\vec{\alpha})
\]
es decir un predicado $P$ que caracterice al valor que toma $f$. Enunciamos
esto en forma de regla.

\bigskip

\textbf{REGLA U:} Si tenemos una funcion $f:D_{f}\subseteq\omega^{n}%
\times\Sigma^{\ast m}\rightarrow\omega$ y buscamos un predicado $P$ tal que
$f=M(P)$ muchas veces es util tratar de dise\~{n}ar $P$ de manera que para
cada $(\vec{x},\vec{\alpha})\in D_{f}$ se de que%
\[
f(\vec{x},\vec{\alpha})=\mathrm{\ unico\ }t\in\omega\mathrm{\ tal\ que\ }%
P(t,\vec{x},\vec{\alpha})
\]


\bigskip

\begin{lemma}
Si $P:D_{P}\subseteq\omega\times\omega^{n}\times\Sigma^{\ast m}\rightarrow
\omega$ es un predicado $\Sigma$-efectivamente computable y $D_{P}$ es
$\Sigma$-efectivamente computable, entonces la funcion $M(P)$ es $\Sigma
$-efectivamente computable.
\end{lemma}

\begin{proof}
Ejercicio
\end{proof}

\bigskip

Lamentablemente si quitamos la hipotesis en el lema anterior de que $D_{P}$
sea $\Sigma$-efectivamente computable, el lema resulta falso. Mas adelante
veremos un contraejemplo basado en la tesis de Church (Proposicion
\ref{P recursivo no implica M(P) recursiva}). Por el momento el lector puede
ejercitar su comprencion del tema convenciendose de que aun teniendo un
procedimiento efectivo que compute a un predicado $P:D_{P}\subseteq
\omega\times\omega^{n}\times\Sigma^{\ast m}\rightarrow\omega$, no es claro
como construir un procedimiento efectivo que compute a $M(P)$.

\bigskip

\paragraph{Definicion de funcion $\Sigma$-recursiva}

Con este nuevo constructor de funciones estamos en condiciones de definir la
clase de las funciones $\Sigma$-recursivas. Definamos los conjuntos
$\mathrm{R}_{0}^{\Sigma}\subseteq\mathrm{R}_{1}^{\Sigma}\subseteq
\mathrm{R}_{2}^{\Sigma}\subseteq...\subseteq\mathrm{R}^{\Sigma}$ de la
siguiente manera%
\[%
\begin{array}
[c]{lll}%
\mathrm{R}_{0}^{\Sigma} & = & \mathrm{PR}_{0}^{\Sigma}\\
\mathrm{R}_{k+1}^{\Sigma} & = & \mathrm{R}_{k}^{\Sigma}\cup\left\{
f\circ\lbrack f_{1},...,f_{n}]:f,f_{1},...,f_{r}\in\mathrm{R}_{k}^{\Sigma
}\text{, }r\geq1\right\}  \cup\\
&  & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\left\{  R(f,\mathcal{G}):f\text{ y cada
}\mathcal{G}_{a}\text{ pertenecen a }\mathrm{R}_{k}^{\Sigma}\right\}  \cup\\
&  &
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\left\{
R(f,g):f,g\in\mathrm{R}_{k}^{\Sigma}\right\}  \cup\\
&  &
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\left\{
M(P):P\text{ es }\Sigma\text{-total y }P\in\mathrm{R}_{k}^{\Sigma}\right\} \\
\mathrm{R}^{\Sigma} & = & \bigcup_{k\geq0}\mathrm{R}_{k}^{\Sigma}%
\end{array}
\]
Una funcion $f$ es llamada $\Sigma$-\textit{recursiva} si pertenece a
$\mathrm{R}^{\Sigma}$. Cabe destacar que aunque $M(P)$ fue definido para
predicados no necesariamente $\Sigma$-totales, en la definicion de los
conjuntos $\mathrm{R}_{k}^{\Sigma}$, nos restringimos al caso en que $P$ es
$\Sigma$-total.

Notese que $\mathrm{PR}_{k}^{\Sigma}\subseteq\mathrm{R}_{k}^{\Sigma}$, para
cada $k\in\omega$, por lo cual $\mathrm{PR}^{\Sigma}\subseteq\mathrm{R}%
^{\Sigma}$.

\begin{proposition}
\label{f recursiva implica f efectivamente computable}Si $f\in\mathrm{R}%
^{\Sigma}$, entonces $f$ es $\Sigma$-efectivamente computable.
\end{proposition}

\begin{proof}
Dejamos al lector la prueba por induccion en $k$ de que si $f\in\mathrm{R}%
_{k}^{\Sigma}$, entonces $f$ es $\Sigma$-efectivamente computable.
\end{proof}

\bigskip

Daremos sin prueba el siguiente conceptualmente importante resultado.

\begin{proposition}
\label{recursivo no implica PR}Sea $\Sigma$ un alfabeto finito. Entonces no
toda funcion $\Sigma$-recursiva es $\Sigma$-p.r.
\end{proposition}

\bigskip

Este resultado no es facil de probar. Mas adelante (Proposicion
\ref{T y FI no son PR}) veremos ejemplos naturales de funciones $\Sigma
$-recursivas que no son $\Sigma$-p.r.. Otro ejemplo natural es la famosa
funcion de Ackermann.

\bigskip

\paragraph{Lema de minimizacion acotada de variable numerica de predicados
$\Sigma$-p.r.}

Aunque no siempre que $P\in\mathrm{R}^{\Sigma}$, tendremos que $M(P)\in
\mathrm{R}^{\Sigma}$ (Proposicion \ref{P recursivo no implica M(P) recursiva}%
), el siguiente lema nos garantiza que este es el caso cuando $P\in
\mathrm{PR}^{\Sigma}$ y ademas da condiciones para que $M(P)$ sea $\Sigma$-p.r..

\begin{lemma}
\label{minimizacion}Sean $n,m\geq0$. Sea $P:D_{P}\subseteq\omega\times
\omega^{n}\times\Sigma^{\ast m}\rightarrow\omega$ un predicado $\Sigma$-p.r.. Entonces

\begin{enumerate}
\item[(a)] $M(P)$ es $\Sigma$-recursiva.

\item[(b)] Si hay una funcion $\Sigma$-p.r. $f:\omega^{n}\times\Sigma^{\ast
m}\rightarrow\omega$ tal que%
\[
M(P)(\vec{x},\vec{\alpha})=\min\nolimits_{t}P(t,\vec{x},\vec{\alpha})\leq
f(\vec{x},\vec{\alpha})\text{, para cada }(\vec{x},\vec{\alpha})\in
D_{M(P)}\text{,}%
\]
entonces $M(P)$ es $\Sigma$-p.r..
\end{enumerate}
\end{lemma}

\begin{proof}
(a) Sea $\bar{P}=P\cup C_{0}^{n+1,m}|_{(\omega^{n+1}\times\Sigma^{\ast
m})-D_{P}}$. Note que $\bar{P}$ es $\Sigma$-p.r. (por que?). Veremos a
continuacion que $M(P)=M(\bar{P})$. Notese que%
\[
\{t\in\omega:P(t,\vec{x},\vec{\alpha})=1\}=\{t\in\omega:\bar{P}(t,\vec{x}%
,\vec{\alpha})=1\}
\]
Esto claramente dice que $D_{M(P)}=D_{M(\bar{P})}$ y que $M(P)(\vec{x}%
,\vec{\alpha})=M(\bar{P})(\vec{x},\vec{\alpha})$, para cada $(\vec{x}%
,\vec{\alpha})\in D_{M(P)}$., por lo cual $M(P)=M(\bar{P})$.

Veremos entonces que $M(\bar{P})$ es $\Sigma$-recursiva. Sea $k$ tal que
$\bar{P}\in\mathrm{PR}_{k}^{\Sigma}$. Ya que $\bar{P}$ es $\Sigma$-total y
$\bar{P}\in\mathrm{PR}_{k}^{\Sigma}\subseteq\mathrm{R}_{k}^{\Sigma}$, tenemos
que $M(\bar{P})\in\mathrm{R}_{k+1}^{\Sigma}$ y por lo tanto $M(\bar{P}%
)\in\mathrm{R}^{\Sigma}$.

(b) Ya que $M(P)=M(\bar{P})$, basta con probar que $M(\bar{P})$ es $\Sigma
$-p.r. Primero veremos que $D_{M(\bar{P})}$ es un conjunto $\Sigma$-p.r..
Notese que%
\[
\chi_{D_{M(\bar{P})}}^{\omega^{n}\times\Sigma^{\ast m}}=\lambda\vec{x}%
\vec{\alpha}\left[  (\exists t\in\omega)_{t\leq f(\vec{x},\vec{\alpha})}%
\;\bar{P}(t,\vec{x},\vec{\alpha})\right]
\]
lo cual nos dice que%
\[
\chi_{D_{M(\bar{P})}}^{\omega^{n}\times\Sigma^{\ast m}}=\lambda x\vec{x}%
\vec{\alpha}\left[  (\exists t\in\omega)_{t\leq x}\;\bar{P}(t,\vec{x}%
,\vec{\alpha})\right]  \circ\left[  f,p_{1}^{n,m},...,p_{n+m}^{n,m}\right]
\]
Pero el Lema \ref{cuantificacion} nos dice que $\lambda x\vec{x}\vec{\alpha
}\left[  (\exists t\in\omega)_{t\leq x}\;\bar{P}(t,\vec{x},\vec{\alpha
})\right]  $ es $\Sigma$-p.r. por lo cual tenemos que $\chi_{D_{M(\bar{P})}%
}^{\omega^{n}\times\Sigma^{\ast m}}$ lo es.

Sea%
\[
P_{1}=\lambda t\vec{x}\vec{\alpha}\left[  \bar{P}(t,\vec{x},\vec{\alpha
})\wedge(\forall j\in\omega)_{j\leq t}\;j=t\vee\lnot\bar{P}(j,\vec{x}%
,\vec{\alpha})\right]
\]
Note que $P_{1}$ es $\Sigma$-total. Dejamos al lector usando lemas anteriores
probar que $P_{1}$ es $\Sigma$-p.r. Ademas notese que para $(\vec{x}%
,\vec{\alpha})\in\omega^{n}\times\Sigma^{\ast m}$ se tiene que%
\[
P_{1}(t,\vec{x},\vec{\alpha})=1\text{ si y solo si }(\vec{x},\vec{\alpha})\in
D_{M(\bar{P})}\text{ y }t=M(\bar{P})(\vec{x},\vec{\alpha})
\]
Esto nos dice que%
\[
M(\bar{P})=\left(  \lambda\vec{x}\vec{\alpha}\left[  \prod_{t=0}^{f(\vec
{x},\vec{\alpha})}t^{P_{1}(t,\vec{x},\vec{\alpha})}\right]  \right)
|_{D_{M(\bar{P})}}%
\]
por lo cual para probar que $M(\bar{P})$ es $\Sigma$-p.r. solo nos resta
probar que%
\[
F=\lambda\vec{x}\vec{\alpha}\left[  \prod_{t=0}^{f(\vec{x},\vec{\alpha}%
)}t^{P_{1}(t,\vec{x},\vec{\alpha})}\right]
\]
lo es. Pero%
\[
F=\lambda xy\vec{x}\vec{\alpha}\left[  \prod_{t=x}^{y}t^{P_{1}(t,\vec{x}%
,\vec{\alpha})}\right]  \circ\left[  C_{0}^{n,m},f,p_{1}^{n,m},...,p_{n+m}%
^{n,m}\right]
\]
y por lo tanto el Lema \ref{iteracion} nos dice que $F$ es $\Sigma$-p.r..
\end{proof}

\bigskip

OBSERVACION: No siempre que $P$ sea $\Sigma$-p.r. tendremos que $M(P)$ lo
sera. Notese que si $M(P)$ fuera $\Sigma$-p.r., cada ves que $P$ lo sea,
entonces tendriamos que $\mathrm{PR}^{\Sigma}=\mathrm{R}^{\Sigma}$
(justifique) lo cual contradiria la Proposicion \ref{recursivo no implica PR}.
Mas adelante (Corolario \ref{minimizacion de PR que no es PR}) veremos un
ejemplo de un predicado $P$ el cual es $\Sigma$-p.r. pero $M(P)$ no es
$\Sigma$-p.r.

\bigskip

El lema de minimizacion recien probado es muy util como veremos en los
siguientes dos lemas.

\begin{lemma}
\label{cociente y resto}Sea $\Sigma$ un alfabeto finito. Las siguientes
funciones son $\Sigma$-p.r.:

\begin{enumerate}
\item[(a)] $%
\begin{array}
[t]{rll}%
Q:\omega\times\mathbf{N} & \rightarrow & \omega\\
(x,y) & \rightarrow & \text{cociente de la division de }x\text{ por }y
\end{array}
$

\item[(b)] $%
\begin{array}
[t]{rll}%
R:\omega\times\mathbf{N} & \rightarrow & \omega\\
(x,y) & \rightarrow & \text{resto de la division de }x\text{ por }y
\end{array}
$

\item[(c)] $%
\begin{array}
[t]{rll}%
pr:\mathbf{N} & \rightarrow & \omega\\
n & \rightarrow & n\text{-esimo numero primo}%
\end{array}
$
\end{enumerate}
\end{lemma}

\begin{proof}
(a) Ya vimos anteriormente que $Q=M(P^{\prime})$, donde $P^{\prime}=\lambda
tx_{1}x_{2}\left[  x_{1}\geq t.x_{2}\text{ y }x_{1}<(t+1).x_{2}\right]  $. Ya
que $P^{\prime}$ es $\Sigma$-p.r. y%
\[
Q(x_{1},x_{2})\leq p_{1}^{2,0}(x_{1},x_{2}),\text{ para cada }(x_{1},x_{2}%
)\in\omega\times\mathbf{N}%
\]
(b) del Lema \ref{minimizacion} implica que $Q\in\mathrm{PR}^{\Sigma}$.

(b) Notese que%
\[
R=\lambda xy\left[  x\dot{-}Q(x,y).y\right]
\]
y por lo tanto $R\in\mathrm{PR}^{\Sigma}$.

(c) Para ver que $pr$ es $\Sigma$-p.r., veremos que la extension
$h:\omega\rightarrow\omega$, dada por $h(0)=0$ y $h(n)=pr(n)$, $n\geq1$, es
$\Sigma$-p.r.. Luego $pr=h|_{\mathbf{N}}$ resultara $\Sigma$-p.r. por ser la
restriccion de una funcion $\Sigma$-p.r. a un conjunto $\Sigma$-p.r.. Primero
note que%
\begin{align*}
h(0)  & =0\\
h(t+1)  & =\min\nolimits_{i}\left(  i\text{ es primo}\wedge i>h(t)\right)
\end{align*}
O sea que $h=R\left(  C_{0}^{0,0},g\right)  $, donde%
\[%
\begin{array}
[t]{rll}%
g:\omega\times\omega & \rightarrow & \omega\\
(A,t) & \rightarrow & \min\nolimits_{i}\left(  i\text{ es primo}\wedge
i>A\right)
\end{array}
\]
Es decir que solo nos resta ver que $g$ es $\Sigma$-p.r.. Pero notese que
$g=M(P)$, donde $P=\lambda iAt\left[  i\text{ es primo}\wedge i>A\right]  $.
Claramente $P$ es $\Sigma$-p.r. por lo cual para poder aplicar (b) del lema
anterior debemos encontrar una funcion $f:\omega\times\omega\rightarrow\omega$
tal que%
\[
M(P)(A,t)\leq f(A,t)\text{, para cada }(A,t)\in\omega^{2}%
\]
Es decir $f$ debera cumplir%
\[
\min\nolimits_{i}\left(  i\text{ es primo}\wedge i>A\right)  \leq
f(A,t)\text{, para cada }(A,t)\in\omega^{2}%
\]
Definamos $f=\lambda At[A!+1]$. Debemos probar entonces que%
\[
\min\nolimits_{i}\left(  i\text{ es primo}\wedge i>A\right)  \leq A!+1\text{,
para cada }A\in\omega
\]
Sea $p$ un primo tal que $p$ divide a $A!+1$. Es facil ver que entonces $p>A$
ya que de lo contrario $p$ dividiria a $A!$ lo cual nos diria que $p$ divide a
$1=A!+1-A!$, lo cual es absurdo. Pero esto claramente nos dice que%
\[
\min\nolimits_{i}\left(  i\text{ es primo}\wedge i>A\right)  \leq p\leq A!+1
\]
O sea que (b) del Lema \ref{minimizacion} implica que $g=M(P)$ es $\Sigma$-p.r.
\end{proof}

\bigskip

\begin{lemma}
Las funciones $\lambda xi\left[  (x)_{i}\right]  $ y $\lambda x\left[
Lt(x)\right]  $ son $\Sigma$-p.r.
\end{lemma}

\begin{proof}
Note que $D_{\lambda xi\left[  (x)_{i}\right]  }=\mathbf{N}\times\mathbf{N}$.
Sea%
\[
P=\lambda txi\left[  \lnot(pr(i)^{t+1}\ \text{divide }x)\right]
\]
Note que $P$ es $\Sigma$-p.r. y que $D_{P}=\omega\times\omega\times\mathbf{N}%
$. Dejamos al lector la prueba de que $\lambda xi\left[  (x)_{i}\right]
=M(P)$. Ya que $(x)_{i}\leq x$, para todo $(x,i)\in\mathbf{N}\times\mathbf{N}%
$, (b) del Lema \ref{minimizacion} implica que $\lambda xi\left[
(x)_{i}\right]  $ es $\Sigma$-p.r..

Veamos que $\lambda x\left[  Lt(x)\right]  $ es $\Sigma$-p.r.. Sea%
\[
Q=\lambda tx\left[  (\forall i\in\mathbf{N})_{i\leq x}\;(i\leq t\vee
(x)_{i}=0)\right]
\]
Notese que $D_{Q}=\omega\times\mathbf{N}$ y que ademas por el Lema
\ref{cuantificacion} tenemos que $Q$ es $\Sigma$-p.r. (dejamos al lector
explicar como se aplica tal lema en este caso). Ademas notese que $\lambda
x\left[  Lt(x)\right]  =M(Q)$ y que%
\[
Lt(x)\leq x,\text{para todo }x\in\mathbf{N}%
\]
lo cual por (b) del Lema \ref{minimizacion} nos dice que $\lambda x\left[
Lt(x)\right]  $ es $\Sigma$-p.r..
\end{proof}

\bigskip

Para $x_{1},...,x_{n}\in\omega$, con $n\geq1$, escribiremos $\left\langle
x_{1},...,x_{n}\right\rangle $ en lugar de $\left\langle x_{1},...,x_{n}%
,0,...\right\rangle $.

\begin{lemma}
\label{CodificadorasSonPR}Sea $n\geq1$. La funcion $\lambda x_{1}%
...x_{n}\left[  \left\langle x_{1},...,x_{n}\right\rangle \right]  $ es
$\Sigma$-p.r.
\end{lemma}

\begin{proof}
Sea $f_{n}=\lambda x_{1}...x_{n}\left[  \left\langle x_{1},...,x_{n}%
\right\rangle \right]  $. Claramente $f_{1}$ es $\Sigma$-p.r.. Ademas note que
para cada $n\geq1$, tenemos%
\[
f_{n+1}=\lambda x_{1}...x_{n+1}\left[  \left(  f_{n}(x_{1},...,x_{n}%
)pr(n+1)^{x_{n+1}}\right)  \right]  \text{.}%
\]
O sea que podemos aplicar un argumento inductivo.
\end{proof}

\bigskip

\paragraph{Minimizacion de variable alfabetica}

Supongamos que $\Sigma\neq\emptyset$. Sea $\leq$ un orden total sobre $\Sigma
$. Recordemos que $\leq$ puede ser naturalmente extendido a un orden total
sobre $\Sigma^{\ast}$. Sea $P:D_{P}\subseteq\omega^{n}\times\Sigma^{\ast
m}\times\Sigma^{\ast}\rightarrow\omega$ un predicado. Cuando $(\vec{x}%
,\vec{\alpha})\in\omega^{n}\times\Sigma^{\ast m}$ es tal que existe al menos
un $\alpha\in\Sigma^{\ast}$ tal que $P(\vec{x},\vec{\alpha},\alpha)=1$,
usaremos $\min_{\alpha}^{\leq}P(\vec{x},\vec{\alpha},\alpha)$ para denotar al
menor $\alpha\in\Sigma^{\ast}$ tal que $P(\vec{x},\vec{\alpha},\alpha)=1$.
Notese que la expresion $\min_{\alpha}^{\leq}P(\vec{x},\vec{\alpha},\alpha)$
esta definida solo para aquellas $(n+m)$-uplas $(\vec{x},\vec{\alpha})$ para
las cuales hay al menos un $\alpha$ tal que se da $P(\vec{x},\vec{\alpha
},\alpha)=1$. Dicho de otra forma, $\min_{\alpha}^{\leq}P(\vec{x},\vec{\alpha
},\alpha)$ no estara definida cuando para cada $\alpha\in\Sigma^{\ast}$ se de
que $(\vec{x},\vec{\alpha},\alpha)$ no pertenece a $D_{P}$ o $P(\vec{x}%
,\vec{\alpha},\alpha)=0$. Otro detalle importante a tener en cuenta es que la
expresion $\min_{\alpha}^{\leq}P(\vec{x},\vec{\alpha},\alpha)$ no depende de
la variable $\alpha$. Por ejemplo, las expresiones $\min_{\alpha}^{\leq}%
P(\vec{x},\vec{\alpha},\alpha)$ y $\min_{\beta}^{\leq}P(\vec{x},\vec{\alpha
},\beta)$ son equivalentes en el sentido que estan definidas en las mismas
$(n+m)$-uplas y cuando estan definidas asumen el mismo valor.

Definamos%
\[%
\begin{array}
[c]{c}%
M^{\leq}(P)=\lambda\vec{x}\vec{\alpha}\left[  \min_{\alpha}^{\leq}P(\vec
{x},\vec{\alpha},\alpha)\right]
\end{array}
\]
Notese que%
\begin{align*}
D_{M^{\leq}(P)}  & =\left\{  (\vec{x},\vec{\alpha})\in\omega^{n}\times
\Sigma^{\ast m}:(\exists\alpha\in\Sigma^{\ast})\ P(\vec{x},\vec{\alpha}%
,\alpha)\right\} \\
M^{\leq}(P)(\vec{x},\vec{\alpha})  & =\min\nolimits_{\alpha}^{\leq}P(\vec
{x},\vec{\alpha},\alpha)\text{, para cada }(\vec{x},\vec{\alpha})\in
D_{M^{\leq}(P)}%
\end{align*}
Diremos que $M^{\leq}(P)$ es obtenida por \textit{minimizacion de variable
alfabetica }a partir de $P$.

Vemos un ejemplo. Sea $\Sigma=\{@,a,b,c,d,e\}$ y sea $\leq$ un orden total
sobre $\Sigma$. Sea $Dir=\{\alpha_{1}\in\Sigma^{\ast}:\left\vert \alpha
_{1}\right\vert _{@}=1\}$ y definamos $U:Dir\rightarrow\Sigma^{\ast}$ de la
siguiente manera%
\[
U(\alpha_{1})=\text{unico }\alpha\text{ tal que }\alpha@\text{ es tramo
inicial de }\alpha_{1}%
\]
Sea%
\[
P=\lambda\alpha_{1}\alpha\lbrack\alpha_{1}\in Dir\text{ y }\alpha@\text{ es
tramo inicial de }\alpha_{1}]
\]
Tenemos que%
\begin{align*}
D_{M^{\leq}(P)}  & =\left\{  \alpha_{1}\in\Sigma^{\ast}:(\exists\alpha
\in\Sigma^{\ast})\ P(\alpha_{1},\alpha)\right\} \\
& =\left\{  \alpha_{1}\in\Sigma^{\ast}:\alpha_{1}\in Dir\text{ y }%
(\exists\alpha\in\Sigma^{\ast})\ \alpha@\text{ es tramo inicial de }\alpha
_{1}\right\} \\
& =Dir
\end{align*}
y ademas es claro que $M^{\leq}(P)(\alpha_{1})=U(\alpha_{1})$, para cada
$\alpha_{1}\in Dir$, por lo cual $M^{\leq}(P)=U$. Intente explicar por que se
utiizaron los nombres $Dir$ y $U$.

\bigskip

\paragraph{Lema de minimizacion acotada de variable alfabetica de predicados
$\Sigma$-p.r.}

\begin{lemma}
\label{minimizacion1}Supongamos que $\Sigma\neq\emptyset$. Sea $\leq$ un orden
total sobre $\Sigma$, sean $n,m\geq0$ y sea $P:D_{P}\subseteq\omega^{n}%
\times\Sigma^{\ast m}\times\Sigma^{\ast}\rightarrow\omega$ un predicado
$\Sigma$-p.r.. Entonces

\begin{enumerate}
\item[(a)] $M^{\leq}(P)$ es $\Sigma$-recursiva.

\item[(b)] Si existe una funcion $\Sigma$-p.r. $f:\omega^{n}\times\Sigma^{\ast
m}\rightarrow\omega$ tal que%
\[
\left\vert M^{\leq}(P)(\vec{x},\vec{\alpha})\right\vert =\left\vert
\min\nolimits_{\alpha}^{\leq}P(\vec{x},\vec{\alpha},\alpha)\right\vert \leq
f(\vec{x},\vec{\alpha})\text{, para cada }(\vec{x},\vec{\alpha})\in
D_{M^{\leq}(P)}\text{,}%
\]
entonces $M^{\leq}(P)$ es $\Sigma$-p.r..
\end{enumerate}
\end{lemma}

\begin{proof}
Sea $Q=P\circ\left[  p_{2}^{1+n,m},...,p_{1+n+m}^{1+n,m},\ast^{\leq}\circ
p_{1}^{1+n,m}\right]  $. Note que%
\[
M^{\leq}(P)=\ast^{\leq}\circ M(Q)
\]
lo cual por (a) del Lema \ref{minimizacion} implica que $M^{\leq}(P)$ es
$\Sigma$-recursiva.

Sea $k$ el cardinal de $\Sigma$. Ya que%
\[
\left\vert \ast^{\leq}(M(Q)(\vec{x},\vec{\alpha}))\right\vert =\left\vert
M^{\leq}(P)(\vec{x},\vec{\alpha})\right\vert \leq f(\vec{x},\vec{\alpha
})\text{,}%
\]
para todo $(\vec{x},\vec{\alpha})\in D_{M^{\leq}(P)}=D_{M(Q)}$, tenemos que%
\[
M(Q)(\vec{x},\vec{\alpha})\leq\sum_{\iota=1}^{i=f(\vec{x},\vec{\alpha})}%
k^{i}\text{, para cada }(\vec{x},\vec{\alpha})\in D_{M(Q)}\text{.}%
\]
O sea que por (b) del Lema \ref{minimizacion}, $M(Q)$ es $\Sigma$-p.r. y por
lo tanto $M^{\leq}(P)$ lo es.
\end{proof}

\bigskip

En el ejemplo de recien vimos que $U=M(P)$, con $P=\lambda\alpha_{1}%
\alpha\lbrack\alpha@$ es tramo inicial de $\alpha_{1}]$ por lo cual, dado que
$P $ es $\Sigma$-p.r. y ademas%
\[
\left\vert U(\alpha_{1})\right\vert \leq\left\vert \alpha_{1}\right\vert
\text{, para cada }\alpha_{1}\in Dir
\]
el lema anterior nos dice que $U$ es $\Sigma$-p.r.

\bigskip

\subsubsection{Conjuntos $\Sigma$-recursivamente enumerables}

Ya que la nocion de funcion $\Sigma$-recursiva es el modelo matematico
Godeliano del concepto de funcion $\Sigma$-efectivamente computable, nos
podriamos preguntar entonces cual es el modelo matematico Godeliano del
concepto de conjunto $\Sigma$-efectivamente enumerable. Si prestamos atencion
a la definicion de conjunto $\Sigma$-efectivamente enumerable, notaremos que
depende de la existencia de ciertas funciones $\Sigma$-efectivamente
computables por lo cual la siguiente definicion cae de maduro:

Diremos que un conjunto $S\subseteq\omega^{n}\times\Sigma^{\ast m}$ sera
llamado $\Sigma$\textit{-recursivamente enumerable} cuando sea vacio o haya
una funcion $F:\omega\rightarrow\omega^{n}\times\Sigma^{\ast m}$ tal que
$I_{F}=S$ y $F_{(i)}$ sea $\Sigma$-recursiva, para cada $i\in\{1,...,n+m\}$.

Deberia entonces quedar claro que si el concepto de funcion $\Sigma$-recursiva
modeliza correctamente al concepto de funcion $\Sigma$-efectivamente
computable, entonces el concepto de conjunto $\Sigma$-recursivamente
enumerable recien definido modeliza correctamente al concepto de conjunto
$\Sigma$-efectivamente enumerable.

\bigskip

\subsubsection{Conjuntos $\Sigma$-recursivos}

La version Godeliana del concepto de conjunto $\Sigma$-efectivamente
computable es facil de dar: un conjunto $S\subseteq\omega^{n}\times
\Sigma^{\ast m}$ sera llamado $\Sigma$\textit{-recursivo} cuando la funcion
$\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}}$ sea $\Sigma$-recursiva.

\bigskip

\subsubsection{Algunos resultados basicos}

Muchos resultados ya probados para el caso primitivo recursivo pueden ser
probados usando basicamente las mismas pruebas e ideas para el caso recursivo.
Por ejemplo las pruebas de los siguientes cuatro lemas son identicas a las del
caso primitivo recursivo

\bigskip

\begin{lemma}
\label{boolean op para recursivos}Si $P:S\subseteq\omega^{n}\times\Sigma^{\ast
m}\rightarrow\omega$ y $Q:S\subseteq\omega^{n}\times\Sigma^{\ast m}%
\rightarrow\omega$ son predicados $\Sigma$-r., entonces $(P\vee Q)$, $(P\wedge
Q)$ y $\lnot P$ lo son tambien.\bigskip
\end{lemma}

\bigskip

\begin{lemma}
\label{union recursivos}Si $S_{1},S_{2}\subseteq\omega^{n}\times\Sigma^{\ast
m}$ son $\Sigma$-r., entonces $S_{1}\cup S_{2}$, $S_{1}\cap S_{2}$ y
$S_{1}-S_{2}$ lo son.
\end{lemma}

\bigskip

\begin{lemma}
\label{rectangulos recursivos}Supongamos $S_{1},...,S_{n}\subseteq\omega$,
$L_{1},...,L_{m}\subseteq\Sigma^{\ast}$ son conjuntos no vacios. Entonces
$S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m}$ es $\Sigma$-r.
sii $S_{1},...,S_{n},L_{1},...,L_{m}$ son $\Sigma$-r.
\end{lemma}

\bigskip

\begin{lemma}
\label{restriccion de recursivas a conj recursivos}Si $f:D_{f}\subseteq
\omega^{n}\times\Sigma^{\ast m}\rightarrow O$ es $\Sigma$-r. y $S\subseteq
D_{f}$ es $\Sigma$-r., entonces $f|_{S}$ es $\Sigma$-r
\end{lemma}

\bigskip

Tambien se puede probar una version del lema de division por casos para
funciones $\Sigma$-recursivas con dominio $\Sigma$-recursivo, la cual
generaliza el caso $\Sigma$-p.r.. La prueba es la misma que la del caso
primitivo recursivo aunque al lema previo de existencia de extensiones lo
probaremos en forma mas directa que para el caso primitivo recursivo. A saber:

\begin{lemma}
\label{extension para recursivas con dominio recursivo}Si $f:D_{f}%
\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow O$ es $\Sigma$-r. y
$D_{f}$ es $\Sigma$-r., entonces existe una funcion $\Sigma$-r. $\bar
{f}:\omega^{n}\times\Sigma^{\ast m}\rightarrow O$, tal que $f=\bar{f}|_{D_{f}%
}$
\end{lemma}

\begin{proof}
Si $f=\emptyset$, es facil de probar y dejado al lector. Supongamos entonces
$f$ es no vacia. Sin perdida de generalidad podemos suponer que
$(0,...,0,\varepsilon,...,\varepsilon)\in D_{f}$. Sea%
\[%
\begin{array}
[t]{rll}%
F:\omega^{n}\times\Sigma^{\ast m} & \rightarrow & \omega^{n}\times\Sigma^{\ast
m}\\
(\vec{x},\vec{\alpha}) & \rightarrow & \left\{
\begin{array}
[c]{lll}%
(\vec{x},\vec{\alpha}) &  & \text{si }(\vec{x},\vec{\alpha})\in D_{f}\\
(0,...,0,\varepsilon,...,\varepsilon) &  & \text{caso contrario}%
\end{array}
\right.
\end{array}
\]
Ya que%
\begin{align*}
F_{(i)}  & =\lambda\vec{x}\vec{\alpha}\left[  x_{i}.\chi_{D_{f}}^{\omega
^{n}\times\Sigma^{\ast m}}(\vec{x},\vec{\alpha})\right]  \text{, para
}i=1,...,n\\
F_{(i)}  & =\lambda\vec{x}\vec{\alpha}\left[  \alpha_{i}^{\chi_{D_{f}}%
^{\omega^{n}\times\Sigma^{\ast m}}(\vec{x},\vec{\alpha})}\right]  \text{, para
}i=n+1,...,n+m
\end{align*}
tenemos que cada $F_{(i)}$ es $\Sigma$-recursiva. Es claro que $\bar{f}=f\circ
F$ cumple que $f=\bar{f}|_{D_{f}}$ por lo cual solo falta ver que $\bar{f}$ es
$\Sigma$-recursiva. Pero esto es obvio ya que $F=\left[  F_{(1)}%
,...,F_{(n+m)}\right]  $
\end{proof}

\bigskip

\begin{lemma}
\label{dpc para recursivas con dominio recursivo}Supongamos $f_{i}:D_{f_{i}%
}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow O$, $i=1,...,k$, son
funciones $\Sigma$-recursivas tales que cada $D_{f_{i}}$ es $\Sigma$-recursivo
y $D_{f_{i}}\cap D_{f_{j}}=\emptyset$ para $i\neq j$. Entonces la funcion
$f_{1}\cup...\cup f_{k}$ es $\Sigma$-recursiva.
\end{lemma}

\begin{proof}
Completamente analoga a la del caso primitivo recursivo.
\end{proof}

\bigskip

\begin{lemma}
Si $S$ es $\Sigma$-recursivo, entonces $S$ es $\Sigma$-r.e.
\end{lemma}

\begin{proof}
Supongamos $\emptyset\neq S\subseteq\omega^{n}\times\Sigma^{\ast m}$. Sea
$(z_{1},...,z_{n},\gamma_{1},...,\gamma_{m})\in S$ fijo. Sea $\leq$ un orden
total sobre $\Sigma$. Sea $G:\omega\rightarrow\omega^{n}\times\Sigma^{\ast m}$
dada por%
\[
G(x)=\left(  (x+1)_{1},...,(x+1)_{n},\ast^{\leq}((x+1)_{n+1}),...,\ast^{\leq
}((x+1)_{n+m})\right)
\]
Es claro que cada $G_{(i)}$ es $\Sigma$-recursiva y que $\operatorname{Im}%
G=\omega^{n}\times\Sigma^{\ast m}$.

Para $i=1,...,n$, definamos $F_{i}:\omega\rightarrow\omega$ de la siguiente
manera%
\[
F_{i}(x)=\left\{
\begin{array}
[c]{ccl}%
G_{(i)}(x) &  & \text{si }G(x)\in S\\
z_{i} &  & \text{caso contrario}%
\end{array}
\right.
\]
Para $i=n+1,...,n+m$, definamos $F_{i}:\omega\rightarrow\Sigma^{\ast}$ de la
siguiente manera%
\[
F_{i}(x)=\left\{
\begin{array}
[c]{ccl}%
G_{(i)}(x) &  & \text{si }G(x)\in S\\
\gamma_{i} &  & \text{caso contrario}%
\end{array}
\right.
\]
Usando que $S$ es $\Sigma$-recursivo podemos aplicar el lema anterior y ver
que cada $F_{i}$ es $\Sigma$-recursiva. Sea $F=[F_{1},...,F_{n+m}]$. Notese
que $F_{(i)}=F_{i}$ para cada $i=1,...,n+m$. Esto nos dice que $S$ es $\Sigma
$-r.e. ya que $\operatorname{Im}F=S$.
\end{proof}

\bigskip

Mas adelante (Lema \ref{A es RE y no R}) daremos un ejemplo natural de un
conjunto que es $\Sigma$-r.e. pero el cual no es $\Sigma$-recursivo.

Deberia quedar claro que si el modelo de Godel es correcto, entonces todos los
resultados probados dentro del paradigma filosofico de la computabilidad
efectiva son ciertos una ves reenunciados de acuerdo al paradigma Godeliano.
Tal como vimos arriba muchos de estos resultados se prueban en forma facil en
su version recursiva. Sin envargo muchos otros requieren mas trabajo y es
necesario utilizar algun paradigma mas constructivo (como el imperativo o el
de Turing) para poder probarlos en su version recursiva. Por ejemplo
consideremos el teorema siguiente dado en el contexto del paradigma filosofico:

\begin{theorem}
Sea $S\subseteq\omega^{n}\times\Sigma^{\ast m}$. Son equivalentes

\begin{enumerate}
\item[(a)] $S$ es $\Sigma$-efectivamente computable

\item[(b)] $S$ y $(\omega^{n}\times\Sigma^{\ast m})-S$ son $\Sigma
$-efectivamente enumerables
\end{enumerate}
\end{theorem}

\bigskip

\noindent Se tiene que la version recursiva de (a)$\Rightarrow$(b) es probada
sin problemas en el lema anterior pero para probar la version recursiva de
(b)$\Rightarrow$(a), nos sera necesario utilizar el paradigma imperativo
(Teorema \ref{carac recursivos}). Lo mismo sucede con el lema de division por
casos en su forma mas general (Lema \ref{dpc para efectivamente computables})
y con el teorema de caracterizacion de conjuntos $\Sigma$-efectivamente
enumerables (Teorema \ref{equivalencias de efectivamente enumerable}), ambos
cuando son enunciados en su version recursiva no son faciles de probar con las
herramientas desarrolladas hasta ahora y nos sera necesario usar el paradigma
imperativo para representar a los objetos recursivos involucrados. Estas
pruebas estan en la Seccion \ref{survey recursivo} donde se compilan todos los
resultados basicos (expresados en paradigma recursivo) y se obtienen algunos
resultados los cuales en esta instancia todavia no se pueden probar ya que
para obtenerlos es necesario hacer uso de la formalizacion matematica de ambos
paradigmas el funcional y el imperativo (por ejemplo la existencia de un
conjunto que es $\Sigma$-r.e. pero el cual no es $\Sigma$-recursivo).

\bigskip

@@finpagina@@

\subsubsection{Recursion primitiva sobre valores anteriores}

Dada una funcion $h:\omega\times S_{1}\times...\times S_{n}\times L_{1}%
\times...\times L_{m}\rightarrow\omega$, con $S_{1},...,S_{n}\subseteq\omega$
y $L_{1},...,L_{m}\subseteq\Sigma^{\ast}$, no vacios, definamos $h^{\downarrow
}:\omega\times S_{1}\times...\times S_{n}\times L_{1}\times...\times
L_{m}\rightarrow\omega$ de la siguiente manera%
\begin{align*}
h^{\downarrow}(x,\vec{x},\vec{\alpha})  & =\left\langle h(0,\vec{x}%
,\vec{\alpha}),h(1,\vec{x},\vec{\alpha}),...,h(x,\vec{x},\vec{\alpha
})\right\rangle \\
& =\Pi_{i=0}^{x}pr(i+1)^{h(i,\vec{x},\vec{\alpha})}%
\end{align*}


\begin{lemma}
\label{f-flecha}Supongamos%
\begin{align*}
f  & :S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m}%
\rightarrow\omega\\
g  & :\omega\times\omega\times S_{1}\times...\times S_{n}\times L_{1}%
\times...\times L_{m}\rightarrow\omega\\
h  & :\omega\times S_{1}\times...\times S_{n}\times L_{1}\times...\times
L_{m}\rightarrow\omega
\end{align*}
son funciones tales que%
\begin{align*}
h(0,\vec{x},\vec{\alpha})  & =f(\vec{x},\vec{\alpha})\\
h(x+1,\vec{x},\vec{\alpha})  & =g(h^{\downarrow}(x,\vec{x},\vec{\alpha
}),x,\vec{x},\vec{\alpha})\text{,}%
\end{align*}
para cada $x\in\omega$ y $(\vec{x},\vec{\alpha})\in S_{1}\times...\times
S_{n}\times L_{1}\times...\times L_{m}$. Entonces $h$ es $\Sigma$-r. (resp.
$\Sigma$-p.r.) si $f$ y $g$ lo son.
\end{lemma}

\begin{proof}
Supongamos $f,g$ son $\Sigma$-p.r.. Primero veremos que $h^{\downarrow}$ es
$\Sigma$-r. (resp. $\Sigma$-p.r.). Notese que para cada $(\vec{x},\vec{\alpha
})\in S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m}$ tenemos que%
\begin{align*}
h^{\downarrow}(0,\vec{x},\vec{\alpha})  & =\left\langle h(0,\vec{x}%
,\vec{\alpha})\right\rangle \\
& =\left\langle f(\vec{x},\vec{\alpha})\right\rangle \\
& =2^{f(\vec{x},\vec{\alpha})}\\
h^{\downarrow}(x+1,\vec{x},\vec{\alpha})  & =h^{\downarrow}(x,\vec{x}%
,\vec{\alpha})pr(x+2)^{h(x+1,\vec{x},\vec{\alpha})}\\
& =h^{\downarrow}(x,\vec{x},\vec{\alpha})pr(x+2)^{g(h^{\downarrow}(x,\vec
{x},\vec{\alpha}),x,\vec{x},\vec{\alpha})}%
\end{align*}
lo cual nos dice que $h^{\downarrow}=R(f_{1},g_{1})$ donde%
\begin{align*}
f_{1}  & =\lambda\vec{x}\vec{\alpha}\left[  2^{f(\vec{x},\vec{\alpha})}\right]
\\
g_{1}  & =\lambda Ax\vec{x}\vec{\alpha}\left[  Apr(x+2)^{g(A,x,\vec{x}%
,\vec{\alpha})}\right]
\end{align*}
O sea que $h^{\downarrow}$ es $\Sigma$-r. (resp. $\Sigma$-p.r.) ya que $f_{1}$
y $g_{1}$ lo son. Finalmente notese que%
\[
h=\lambda ix[(x)_{i}]\circ\left[  Suc\circ p_{1}^{1+n,m},h^{\downarrow
}\right]
\]
lo cual nos dice que $h$ es $\Sigma$-r. (resp. $\Sigma$-p.r.).
\end{proof}

\bigskip

\subsubsection{Independencia del alfabeto}

Probaremos que los conceptos de $\Sigma$-recursividad y $\Sigma$-recursividad
primitiva son en realidad independientes del alfabeto $\Sigma$, es decir que
si $f$ es una funcion la cual es $\Sigma$-mixta y $\Gamma$-mixta, entonces $f$
es $\Sigma$-recursiva (resp. $\Sigma$-p.r.) sii $f$ es $\Gamma$-recursiva
(resp. $\Gamma$-p.r.).

Ya definimos para el caso de un alfabeto $\Sigma\neq\emptyset$ y $\leq$ un
orden total sobre $\Sigma$, las funciones $\#^{\leq}$ y $\ast^{\leq}$. Sea
$\Sigma=\emptyset$. Notese que el conjunto $\emptyset$ es un orden total sobre
$\Sigma$ (de hecho es el unico orden total sobre $\Sigma$). Definamos%
\[%
\begin{array}
[c]{rll}%
\#^{\emptyset}:\{0\} & \rightarrow & \{\varepsilon\}\\
0 & \rightarrow & \varepsilon
\end{array}
\ \ \ \ \ \ \ \ \ \ \ \ \ \
\begin{array}
[c]{rll}%
\ast^{\emptyset}:\{\varepsilon\} & \rightarrow & \{0\}\\
\varepsilon & \rightarrow & 0
\end{array}
\]
Ya que $\Sigma^{\ast}=\{\varepsilon\}$, las funciones $\#^{\emptyset}$ y
$\ast^{\emptyset}$ son biyecciones mutuamente inversas entre $\{0\}$ y
$\Sigma^{\ast}$. Ademas notese que estas funciones son $\Sigma$-p.r..

\begin{lemma}
\label{aux}Supongamos $\Sigma\subseteq\Gamma$.

\begin{enumerate}
\item[(a)] Si $\leq$ es un orden total sobre $\Sigma$, entonces las funciones
$\Sigma$-mixtas $\ast^{\leq}$ y $\#^{\leq}$ son $\Gamma$-p.r..

\item[(b)] Si $\leq^{\prime}$ es un orden total sobre $\Gamma$, entonces las
funciones $\Sigma$-mixtas $\#^{\leq^{\prime}}|_{\Sigma^{\ast}}$ y $\ast
^{\leq^{\prime}}|_{\#^{\leq^{\prime}}(\Sigma^{\ast})}$ son $\Sigma$-p.r..
\end{enumerate}
\end{lemma}

\begin{proof}
(a) Si $\Sigma=\emptyset$, entonces es facil ver que $\ast^{\leq}$ y
$\#^{\leq}$ son $\Gamma$-p.r., y es dejado como ejercicio. Supongamos
$\Sigma=\{a_{1},...,a_{k}\}$ con $k\geq1$ y $\leq$ es dado por $a_{1}%
<...<a_{k}$. Sea $s_{e}^{\leq}:\Gamma^{\ast}\rightarrow\Gamma^{\ast}$ dada por%
\begin{align*}
s_{e}^{\leq}(\varepsilon)  & =a_{1}\\
s_{e}^{\leq}(\alpha a_{i})  & =\alpha a_{i+1}\text{, si }i<k\\
s_{e}^{\leq}(\alpha a_{k})  & =s_{e}^{\leq}(\alpha)a_{1}\\
s_{e}^{\leq}(\alpha a)  & =\varepsilon\text{, si }a\in\Gamma-\Sigma.
\end{align*}
Note que $s_{e}^{\leq}$ es $\Gamma$-p.r. y que $s_{e}^{\leq}|_{\Sigma^{\ast}%
}=s^{\leq}$. Ya que%
\begin{align*}
\ast^{\leq}(0)  & =\varepsilon\\
\ast^{\leq}(x+1)  & =s^{\leq}(\ast^{\leq}(x))
\end{align*}
para cada $x\in\omega$, tenemos que%
\begin{align*}
\ast^{\leq}(0)  & =\varepsilon\\
\ast^{\leq}(x+1)  & =s_{e}^{\leq}(\ast^{\leq}(x))
\end{align*}
Pero esto nos dice que $\ast^{\leq}=R(C_{\varepsilon}^{0,0},g)$ donde%
\[%
\begin{array}
[c]{lll}%
g:\omega\times\Gamma^{\ast} & \rightarrow & \Gamma^{\ast}\\
\;\;\;\;\;(x,\alpha) & \rightarrow & s_{e}^{\leq}(\alpha)
\end{array}
\]
Pero es claro que $g$ es $\Gamma$-p.r. por lo cual $\ast^{\leq}$ es $\Gamma$-p.r..

Para ver que $\#^{\leq}:\Sigma^{\ast}\rightarrow\omega$ es $\Gamma$-p.r., sea
$\#_{e}^{\leq}:\Gamma^{\ast}\rightarrow\omega$ dada por%
\begin{align*}
\#_{e}^{\leq}(\varepsilon)  & =0\\
\#_{e}^{\leq}(\alpha a_{i})  & =\#_{e}^{\leq}(\alpha).k+i\\
\#_{e}^{\leq}(\alpha a)  & =0\text{, si }a\in\Gamma-\Sigma.
\end{align*}
Ya que $\#_{e}^{\leq}$ es $\Gamma$-p.r., eso es $\#^{\leq}=\#_{e}^{\leq
}|_{\Sigma^{\ast}}$.

(b) El caso $\Sigma=\emptyset$ es facil y queda como ejercicio. Supongamos
entonces $\Sigma$ es no vacio. Sea $n$ el cardinal de $\Gamma.$ Ya que%
\begin{align*}
\#^{\leq^{\prime}}|_{\Sigma^{\ast}}(\varepsilon)  & =0\\
\#^{\leq^{\prime}}|_{\Sigma^{\ast}}(\alpha a)  & =\#^{\leq^{\prime}}%
|_{\Sigma^{\ast}}(\alpha).n+\#^{\leq^{\prime}}(a)\text{, para cada }a\in\Sigma
\end{align*}
la funcion $\#^{\leq^{\prime}}|_{\Sigma^{\ast}}$ es $\Sigma$-p.r.. O sea que
el predicado $P=\lambda x\alpha\left[  \#^{\leq^{\prime}}|_{\Sigma^{\ast}%
}(\alpha)=x\right]  $ es $\Sigma$-p.r.. Sea $\leq$ un orden total sobre
$\Sigma$. Note que $\ast^{\leq^{\prime}}|_{\#^{\leq^{\prime}}(\Sigma^{\ast}%
)}=M^{\leq}(P)$, lo cual ya que%
\[
\left\vert \ast^{\leq^{\prime}}|_{\#^{\leq^{\prime}}(\Sigma^{\ast}%
)}(x)\right\vert \leq x
\]
nos dice que $\ast^{\leq^{\prime}}|_{\#^{\leq^{\prime}}(\Sigma^{\ast})}$ es
$\Sigma$-p.r. (Lema \ref{minimizacion1}).
\end{proof}

\bigskip

\begin{lemma}
$\mathrm{PR}^{\emptyset}\subseteq\mathrm{PR}^{\Sigma}$ y $\mathrm{R}%
^{\emptyset}\subseteq\mathrm{R}^{\Sigma}$
\end{lemma}

\begin{proof}
Veamos que $\mathrm{R}^{\emptyset}\subseteq\mathrm{R}^{\Sigma}$. Probaremos
por induccion en $k$ que $\mathrm{R}_{k}^{\emptyset}\subseteq\mathrm{R}%
^{\Sigma}$. El caso $k=0$ es trivial. Supongamos entonces que vale la
hipotesis inductiva $\mathrm{R}_{k}^{\emptyset}\subseteq\mathrm{R}^{\Sigma}$ y
veamos que $\mathrm{R}_{k+1}^{\emptyset}\subseteq\mathrm{R}^{\Sigma}$. Sea
$F\in\mathrm{R}_{k+1}^{\emptyset}-\mathrm{R}_{k}^{\emptyset}$ veremos que
$F\in\mathrm{R}^{\Sigma}$. Hay varios casos:

Caso $F=R(f,\mathcal{G})$, con%
\begin{align*}
f  & :S_{1}\times...\times S_{n}\times\emptyset^{\ast m}\rightarrow
\emptyset^{\ast}\\
\mathcal{G}_{a}  & :S_{1}\times...\times S_{n}\times\emptyset^{\ast m}%
\times\emptyset^{\ast}\times\emptyset^{\ast}\rightarrow\emptyset^{\ast}\text{,
para cada }a\in\emptyset
\end{align*}
funciones en $\mathrm{R}_{k}^{\emptyset}$ y cada $S_{i}$ no vacio. Por
hipotesis inductiva tenemos que $f\in\mathrm{R}^{\Sigma}$. Notese que
$\mathcal{G}=\emptyset$, lo cual nos dice que por definicion%
\[%
\begin{array}
[c]{rll}%
R(f,\mathcal{G}):S_{1}\times...\times S_{n}\times\emptyset^{\ast m}%
\times\emptyset^{\ast} & \rightarrow & \emptyset^{\ast}\\
(\vec{x},\varepsilon,...,\varepsilon,\varepsilon) & \rightarrow & f(\vec
{x},\varepsilon,...,\varepsilon)
\end{array}
\]
Es claro que $\omega^{n}\times\Sigma^{\ast m}\times\emptyset^{\ast}$ es un
conjunto $\Sigma$-p.r. por lo cual las funciones $p_{i}^{n,m+1}|_{\omega
^{n}\times\Sigma^{\ast m}\times\emptyset^{\ast}}$ son $\Sigma$-p.r. (aqui las
$p_{i}^{n,m+1}$ son respecto de $\Sigma$). Ya que%
\[
R(f,\mathcal{G})=f\circ\left[  p_{1}^{n,m+1}|_{\omega^{n}\times\Sigma^{\ast
m}\times\emptyset^{\ast}},...,p_{n+m}^{n,m+1}|_{\omega^{n}\times\Sigma^{\ast
m}\times\emptyset^{\ast}}\right]
\]
tenemos que $F$ es $\Sigma$-recursiva

Caso $F=R(f,g)$, con%
\begin{align*}
f  & :S_{1}\times...\times S_{n}\times\emptyset^{\ast m}\rightarrow
\emptyset^{\ast}\\
g  & :\omega\times S_{1}\times...\times S_{n}\times\emptyset^{\ast m}%
\times\emptyset^{\ast}\rightarrow\emptyset^{\ast}%
\end{align*}
funciones en $\mathrm{R}_{k}^{\emptyset}$ y cada $S_{i}$ no vacio. Por
hipotesis inductiva tenemos que $f,g\in\mathrm{R}^{\Sigma}$. Notese que
respecto de $\Sigma$, la funcion $R(f,g)$ no esta definida ya que por la forma
de $f$, el dominio de $g$ deberia ser $\omega\times S_{1}\times...\times
S_{n}\times\emptyset^{\ast m}\times\Sigma^{\ast}$. Sea%
\[
\tilde{g}=g\circ\left[  p_{1}^{1+n,m+1},...,p_{1+n+m}^{1+n,m+1},C_{\varepsilon
}^{1+n,m+1}\right]
\]
(aqui las $p_{i}^{1+n,m+1}$ y $C_{\varepsilon}^{1+n,m+1}$ son respecto de
$\Sigma$). Notese que $D_{\tilde{g}}=\omega\times S_{1}\times...\times
S_{n}\times\emptyset^{\ast m}\times\Sigma^{\ast}$ y $\tilde{g}$ es $\Sigma
$-recursiva. Ademas es facil ver que $F=Rf,\tilde{g})$ (respecto del alfabeto
$\Sigma$) por lo cual $F$ es $\Sigma$-recursiva

Caso $F=M(P)$, con $P:\omega\times\omega^{n}\times\emptyset^{\ast
m}\rightarrow\omega$, un predicado en $\mathrm{R}_{k}^{\emptyset}$. Por
hipotesis inductiva tenemos que $P\in\mathrm{R}^{\Sigma}$. Sea%
\[
\bar{P}=P\circ\left[  p_{1}^{1+n,m},...,p_{1+n}^{1+n,m},C_{\varepsilon
}^{1+n,m},...,C_{\varepsilon}^{1+n,m}\right]
\]
Notese que $\bar{P}$ es $\Sigma$-total y $\Sigma$-recursivo y ademas extiende
a $P$. Sea%
\[
\tilde{P}=\lambda xy[x.y]\circ\left[  \bar{P},\chi_{\omega\times\omega
^{n}\times\emptyset^{\ast m}}^{\omega\times\omega^{n}\times\Sigma^{\ast m}%
}\right]
\]
Tambien $\tilde{P}$ es $\Sigma$-total y $\Sigma$-recursivo y extiende a $P$
pero ademas fuera del dominio de $P$ vale $0$. Esto nos dice que $M(\tilde
{P})=M(P)$ por lo cual $F$ es $\Sigma$-recursiva ya que $M(\tilde{P})$ lo es

Los otros casos de recursion primitiva son parecidos a los hechos y el caso de
la composicion es trivial.

La prueba de que $\mathrm{PR}^{\emptyset}\subseteq\mathrm{PR}^{\Sigma}$ es muy
similar. Se dejan los detalles como ejercicio para el lector
\end{proof}

\bigskip

Sea $\Sigma$ un alfabeto finito (puede ser vacio) y sea $\leq$ un orden total
sobre $\Sigma$. Para $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast
m}\rightarrow\omega$, definamos%
\[
f^{\#^{\leq}}=f\circ\left[  p_{1}^{n+m,0},...,p_{n}^{n+m,0},\ast^{\leq}\circ
p_{n+1}^{n+m,0},...,\ast^{\leq}\circ p_{n+m}^{n+m,0}\right]
\]
Similarmente, para $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast m}%
\rightarrow\Sigma^{\ast}$, definamos%
\[
f^{\#^{\leq}}=\#^{\leq}\circ f\circ\left[  p_{1}^{n+m,0},...,p_{n}%
^{n+m,0},\ast^{\leq}\circ p_{n+1}^{n+m,0},...,\ast^{\leq}\circ p_{n+m}%
^{n+m,0}\right]
\]


\begin{lemma}
Sea $\Gamma$ un alfabeto finito y sea $\leq$ un orden total sobre $\Gamma$.
Dada $h$ una funcion $\Gamma$-mixta, son equivalentes

\begin{enumerate}
\item[(1)] $h$ es $\Gamma$-recursiva (resp. $\Gamma$-p.r.)

\item[(2)] $h^{\#^{\leq}}$ es $\emptyset$-recursiva (resp. $\emptyset$-p.r.)
\end{enumerate}
\end{lemma}

\begin{proof}
(2)$\Rightarrow$(1). Supongamos $h:D_{h}\subseteq\omega^{n}\times\Gamma^{\ast
m}\rightarrow\Gamma^{\ast}$ es tal que $h^{\#^{\leq}}$ es $\emptyset
$-recursiva (resp. $\emptyset$-p.r.). Dejamos al lector chequear que%
\[
h=\ast^{\leq}\circ h^{\#^{\leq}}\circ\left[  p_{1}^{n,m},...,p_{n}%
^{n,m},\#^{\leq}\circ p_{n+1}^{n,m},...,\#^{\leq}\circ p_{n+m}^{n,m}\right]
\]
(aqui las $p_{i}^{n,m}$ son respecto de $\Gamma$). Por el lema anterior
tenemos que $h^{\#^{\leq}}$ es $\Gamma$-recursiva (resp. $\Gamma$-p.r.). Ya
que (aun cuando $\Gamma=\emptyset$) tenemos que las funciones $\ast^{\leq}$ y
$\#^{\leq}$ son $\Gamma$-p.r., tenemos que $h$ es $\Gamma$-recursiva (resp.
$\Gamma$-p.r.) ya que es composicion de funciones $\Gamma$-recursivas (resp.
$\Gamma$-p.r.).

(1)$\Rightarrow$(2). El caso $\Gamma=\emptyset$ es trivial ya que
$h^{\#^{\leq}}$ se define como composicion de funciones $\emptyset$-recursivas
(resp. $\emptyset$-p.r.). Supongamos entonces que $\Gamma=\{a_{1},...,a_{r}%
\}$, con $a_{1}<a_{2}<...<a_{r}$ y $r>0$. Probaremos por induccion en $k$ que

\begin{enumerate}
\item[(*)] Si $h\in\mathrm{R}_{k}^{\Gamma}$ (resp. $h\in\mathrm{PR}%
_{k}^{\Gamma}$), entonces $h^{\#^{\leq}}$ es $\emptyset$-recursiva (resp.
$\emptyset$-p.r.).
\end{enumerate}

\noindent El caso $k=0$ es facil y dejado al lector. Supongamos (*) vale para
un $k$ fijo. Veremos que vale para $k+1$. Sea $h\in\mathrm{R}_{k+1}^{\Gamma}$
(resp. $h\in\mathrm{PR}_{k+1}^{\Gamma}$). Hay varios casos

Caso 1. Supongamos $h=f\circ\lbrack f_{1},...,f_{n}]$, con $f,f_{1}%
,...,f_{n}\in\mathrm{R}_{k}^{\Gamma}$ (resp. $f,f_{1},...,f_{n}\in
\mathrm{PR}_{k}^{\Gamma}$). Por hipotesis inductiva tenemos que $f^{\#^{\leq}%
},f_{1}^{\#^{\leq}},...,f_{n}^{\#^{\leq}}$ son $\emptyset$-recursivas (resp.
$\emptyset$-p.r.). Ya que $h^{\#^{\leq}}=f^{\#^{\leq}}\circ\left[
f_{1}^{\#^{\leq}},...,f_{n}^{\#^{\leq}}\right]  $, tenemos que $h^{\#^{\leq}}$
es $\emptyset$-recursiva (resp. $\emptyset$-p.r.).

Caso 2. Supongamos $h=M(P)$, con $P:\omega\times\omega^{n}\times\Gamma^{\ast
m}\rightarrow\omega$, un predicado en $\mathrm{R}_{k}^{\Gamma}$. Ya que
$h^{\#^{\leq}}=M(P^{\#^{\leq}})$, tenemos que $h^{\#^{\leq}}$ es $\emptyset$-recursiva.

Caso 3. Supongamos $h=R(f,\mathcal{G})$, con%
\begin{align*}
f  & :S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m}%
\rightarrow\Gamma^{\ast}\\
\mathcal{G}_{a}  & :S_{1}\times...\times S_{n}\times L_{1}\times...\times
L_{m}\times\Gamma^{\ast}\times\Gamma^{\ast}\rightarrow\Gamma^{\ast}\text{,
}a\in\Gamma
\end{align*}
funciones en $\mathrm{R}_{k}^{\Gamma}$ (resp. $\mathrm{PR}_{k}^{\Gamma}$) y
$S_{1},...,S_{n}\subseteq\omega$ y $L_{1},...,L_{m}\subseteq\Sigma^{\ast}$, no
vacios. Notese que%
\begin{align*}
f^{\#^{\leq}}  & :S_{1}\times...\times S_{n}\times\#^{\leq}(L_{1}%
)\times...\times\#^{\leq}(L_{m})\rightarrow\omega\\
\mathcal{G}_{a}^{\#^{\leq}}  & :S_{1}\times...\times S_{n}\times\#^{\leq
}(L_{1})\times...\times\#^{\leq}(L_{m})\times\omega\times\omega\rightarrow
\omega\text{, }a\in\Gamma\\
h^{\#^{\leq}}  & :S_{1}\times...\times S_{n}\times\#^{\leq}(L_{1}%
)\times...\times\#^{\leq}(L_{m})\times\omega\rightarrow\omega
\end{align*}
Por hipotesis inductiva tenemos que $f^{\#^{\leq}}$ y cada $\mathcal{G}%
_{a}^{\#^{\leq}}$ son $\emptyset$-recursivas (resp. $\emptyset$-p.r.). Sea%
\[%
\begin{array}
[c]{rll}%
i_{0}:\omega & \rightarrow & \omega\\
x & \rightarrow & \left\{
\begin{array}
[c]{lll}%
r &  & \text{si }r\text{ divide }x\\
R(x,r) &  & \text{caso contrario}%
\end{array}
\right.
\end{array}
\]
y sea%
\[
B=\lambda x\left[  Q(x\dot{-}i_{0}(x),r)\right]
\]
($R$ y $Q$ son definidas en el Lema \ref{cociente y resto}). Note que $i_{0}$
y $B$ son $\emptyset$-p.r. y que%
\[
\ast^{\leq}(x)=\ast^{\leq}(B(x))a_{i_{0}(x)}\text{, para }x\geq1
\]
(ejercicio). Tambien tenemos para cada $(\vec{x},\vec{y},t)\in S_{1}%
\times...\times S_{n}\times\#^{\leq}(L_{1})\times...\times\#^{\leq}%
(L_{m})\times\omega$ se da%
\begin{align*}
h^{\#^{\leq}}(\vec{x},\vec{y},t+1)  & =\#^{\leq}(h(\vec{x},\ast^{\leq}(\vec
{y}),\ast^{\leq}(t+1)))\\
& =\#^{\leq}(h(\vec{x},\ast^{\leq}(\vec{y}),\ast^{\leq}(B(t+1))a_{i_{0}%
(t+1)}))\\
& =\#^{\leq}\left(  \mathcal{G}_{a_{i_{0}(t+1)}}(\vec{x},\ast^{\leq}(\vec
{y}),\ast^{\leq}(B(t+1)),h(\vec{x},\ast^{\leq}(\vec{y}),\ast^{\leq
}(B(t+1)))\right) \\
& =\#^{\leq}\left(  \mathcal{G}_{a_{i_{0}(t+1)}}(\vec{x},\ast^{\leq}(\vec
{y}),\ast^{\leq}(B(t+1)),\ast^{\leq}(h^{\#^{\leq}}(\vec{x},\vec{y}%
,B(t+1))))\right) \\
& =\mathcal{G}_{a_{i_{0}(t+1)}}^{\#^{\leq}}(\vec{x},\vec{y},B(t+1),h^{\#^{\leq
}}(\vec{x},\vec{y},B(t+1)))
\end{align*}
y ya que $B(t+1)<t+1$, tenemos que

\begin{enumerate}
\item[(**)] $h^{\#^{\leq}}(\vec{x},\vec{y},t+1)=\mathcal{G}_{a_{i_{0}(t+1)}%
}^{\#^{\leq}}(\vec{x},\vec{y},B(t+1),\left(  \left\langle h^{\#^{\leq}}%
(\vec{x},\vec{y},0),...,h^{\#^{\leq}}(\vec{x},\vec{y},t)\right\rangle \right)
_{B(t+1)+1})$, para cada $(\vec{x},\vec{y},t)\in S_{1}\times...\times
S_{n}\times\#^{\leq}(L_{1})\times...\times\#^{\leq}(L_{m})\times\omega$
\end{enumerate}

A continuacion aplicaremos la idea del Lema \ref{f-flecha}. Sera mas claro asi
ya que para aplicarlo directamente deberiamos cambiar el orden de los
parametros de las funciones $h^{\#^{\leq}}$, $\mathcal{G}_{a_{i}}^{\#^{\leq}}$
componiendolas adecuadamente y seria muy engorroso notacionalmente.

Definamos%
\[
H=\lambda t\vec{x}\vec{y}\left[  \left\langle h^{\#^{\leq}}(\vec{x},\vec
{y},0),...,h^{\#^{\leq}}(\vec{x},\vec{y},t)\right\rangle \right]
\]
Notar que%
\[
D_{H}=\omega\times S_{1}\times...\times S_{n}\times\#^{\leq}(L_{1}%
)\times...\times\#^{\leq}(L_{m})
\]
Tenemos que%
\begin{align*}
H(0,\vec{x},\vec{y})  & =\left\langle h^{\#^{\leq}}(\vec{x},\vec
{y},0)\right\rangle =\left\langle f^{\#^{\leq}}(\vec{x},\vec{y})\right\rangle
=2^{f^{\#^{\leq}}(\vec{x},\vec{y})}\\
H(t+1,\vec{x},\vec{y})  & =\left(  H(t,\vec{x},\vec{y}).pr(t+2)^{h^{\#^{\leq}%
}(\vec{x},\vec{y},t+1)}\right) \\
& =\left(  H(t,\vec{x},\vec{y}).pr(t+2)^{\mathcal{G}_{a_{i_{0}(t+1)}%
}^{\#^{\leq}}(\vec{x},\vec{y},B(t+1),(H(t,\vec{x},\vec{y}))_{B(t+1)+1}%
)}\right)  \text{ (por (**))}%
\end{align*}
para cada $(t,\vec{x},\vec{y})\in\omega\times S_{1}\times...\times S_{n}%
\times\#^{\leq}(L_{1})\times...\times\#^{\leq}(L_{m})$. O sea que si definimos%
\[
g:\omega\times\omega\times S_{1}\times...\times S_{n}\times\#^{\leq}%
(L_{1})\times...\times\#^{\leq}(L_{m})\rightarrow\omega
\]
por%
\[
g(A,t,\vec{x},\vec{y})=\left\{
\begin{array}
[c]{clc}%
\left(  A.pr(t+2)^{\mathcal{G}_{a_{1}}^{\#^{\leq}}(\vec{x},\vec{y}%
,B(t+1),(A)_{B(t+1)+1})}\right)  & \text{si} & i_{0}(t+1)=1\\
\vdots &  & \vdots\\
\left(  A.pr(t+2)^{\mathcal{G}_{a_{r}}^{\#^{\leq}}(\vec{x},\vec{y}%
,B(t+1),(A)_{B(t+1)+1})}\right)  & \text{si} & i_{0}(t+1)=r
\end{array}
\right.
\]
tenemos que $H=R(\lambda x\left[  2^{x}\right]  \circ f^{\#^{\leq}},g)$. Note
que $g$ es $\emptyset$-recursiva (resp. $\emptyset$-p.r.), ya que%
\[
g=\lambda At\vec{x}\vec{y}\left[  f_{1}(A,t,\vec{x},\vec{y})P_{1}(A,t,\vec
{x},\vec{y})+...+f_{r}(A,t,\vec{x},\vec{y})P_{r}(A,t,\vec{x},\vec{y})\right]
\text{,}%
\]
con%
\begin{align*}
f_{i}  & =\lambda At\vec{x}\vec{y}\left[  \left(  A.pr(t+2)^{\mathcal{G}%
_{a_{i}}^{\#^{\leq}}(\vec{x},\vec{y},B(t+1),(A)_{B(t+1)})}\right)  \right] \\
P_{i}  & =\lambda At\vec{x}\vec{y}\left[  i_{0}(t+1)=i\right]
\end{align*}
O sea que $H$ es $\emptyset$-recursiva (resp. $\emptyset$-p.r.) y por lo tanto
lo es%
\[
h^{\#^{\leq}}=\lambda\vec{x}\vec{y}t\left[  (H(t,\vec{x},\vec{y}%
))_{t+1}\right]
\]
Los otros casos en los cuales $h$ es obtenida por recursion primitiva son similares.
\end{proof}

\bigskip

Ahora podemos probar el anunciado resultado de independencia.

\begin{theorem}
\label{independencia}Sean $\Sigma$ y $\Gamma$ alfabetos cualesquiera.

\begin{enumerate}
\item[(a)] Supongamos una funcion $f$ es $\Sigma$-mixta y $\Gamma$-mixta,
entonces $f$ es $\Sigma$-recursiva (resp. $\Sigma$-p.r.) sii $f$ es $\Gamma
$-recursiva (resp. $\Gamma$-p.r.).

\item[(b)] Supongamos un conjunto $S$ es $\Sigma$-mixto y $\Gamma$-mixto,
entonces $S$ es $\Sigma$-recursivo (resp. $\Sigma$-r.e., $\Sigma$-p.r.) sii
$S$ es $\Gamma$-recursivo (resp. $\Gamma$-r.e., $\Gamma$-p.r.).
\end{enumerate}
\end{theorem}

\begin{proof}
(a) Ya que $f$ es $(\Sigma\cap\Gamma)$-mixta, podemos suponer sin perdida de
generalidad que $\Sigma\subseteq\Gamma$ (por que?). Sea $\leq$ un orden total
sobre $\Sigma$ y sea $\leq^{\prime}$ un orden total sobre $\Gamma$. Primero
supongamos que $f$ es $\Sigma$-recursiva (resp. $\Sigma$-p.r.). Probaremos que
$f$ es $\Gamma$-recursiva (resp. $\Gamma$-p.r.). Ya que $f$ es $\Sigma$ mixta,
tenemos que $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow O$,
con $O\in\{\omega,\Sigma^{\ast}\}$. Haremos el caso $O=\Sigma^{\ast}$. Ya que
las funciones $\#^{\leq^{\prime}}|_{\Sigma^{\ast}}$ y $\ast^{\leq^{\prime}%
}|_{\#^{\leq^{\prime}}(\Sigma^{\ast})}$ son $\Sigma$-p.r. (Lema \ref{aux}) y
ademas%
\begin{align*}
f^{\#^{\leq^{\prime}}}  & =\#^{\leq^{\prime}}\circ f\circ\left[  p_{1}%
^{n+m,0},...,p_{n}^{n+m,0},\ast^{\leq^{\prime}}\circ p_{n+1}^{n+m,0}%
,...,\ast^{\leq^{\prime}}\circ p_{n+m}^{n+m,0}\right] \\
& =\#^{\leq^{\prime}}|_{\Sigma^{\ast}}\circ f\circ\left[  p_{1}^{n+m,0}%
,...,p_{n}^{n+m,0},\ast^{\leq^{\prime}}|_{\#^{\leq^{\prime}}(\Sigma^{\ast}%
)}\circ p_{n+1}^{n+m,0},...,\ast^{\leq^{\prime}}|_{\#^{\leq^{\prime}}%
(\Sigma^{\ast})}\circ p_{n+m}^{n+m,0}\right]
\end{align*}
(justifique) tenemos que $f^{\#^{\leq^{\prime}}}$ es $\Sigma$-recursiva (resp.
$\Sigma$-p.r.). Por el lema anterior tenemos que $\left(  f^{\#^{\leq^{\prime
}}}\right)  ^{\#^{\leq}}$ es $\emptyset$-recursiva (resp. $\emptyset$-p.r.),
pero notese que $\left(  f^{\#^{\leq^{\prime}}}\right)  ^{\#^{\leq}%
}=f^{\#^{\leq^{\prime}}}$ ya que $f^{\#^{\leq^{\prime}}}$ es de tipo
$(n+m,0,\#)$, por lo cual tenemos que $f^{\#^{\leq^{\prime}}}$ es $\emptyset
$-recursiva (resp. $\emptyset$-p.r.). Pero esto por el lema anterior nos dice
que $f$ es $\Gamma$-recursiva (resp. $\Gamma$-p.r.).

Supongamos ahora que $f$ es $\Gamma$-recursiva (resp. $\Gamma$-p.r.).
Probaremos que $f$ es $\Sigma$-recursiva (resp. $\Sigma$-p.r.). Ya que
$\#^{\leq}$ y $\ast^{\leq}$ son $\Gamma$-p.r. (Lema \ref{aux}), la funcion%
\[
f^{\#^{\leq}}=\#^{\leq}\circ f\circ\left[  p_{1}^{n+m,0},...,p_{n}%
^{n+m,0},\ast^{\leq}\circ p_{n+1}^{n+m,0},...,\ast^{\leq}\circ p_{n+m}%
^{n+m,0}\right]
\]
es $\Gamma$-recursiva (resp. $\Gamma$-p.r.). Por el lema anterior $\left(
f^{\#^{\leq}}\right)  ^{\#^{\leq^{\prime}}}$ es $\emptyset$-recursiva (resp.
$\emptyset$-p.r.). Pero notese que $\left(  f^{\#^{\leq}}\right)
^{\#^{\leq^{\prime}}}=f^{\#^{\leq}}$ ya que $f^{\#^{\leq}}$ es de tipo
$(n+m,0,\#)$, por lo cual $f^{\#^{\leq}}$ es $\emptyset$-recursiva (resp.
$\emptyset$-p.r.). Esto por el lema anterior nos dice que $f$ es $\Sigma
$-recursiva (resp. $\Sigma$-p.r.).

(b) Supongamos $S$ es $\Sigma$-mixto y $\Gamma$-mixto. Ya que $S$ es
$(\Sigma\cap\Gamma)$-mixto, podemos suponer sin perdida de generalidad que
$\Sigma\subseteq\Gamma$. Que%
\[
S\text{ es }\Sigma\text{-r.e. sii }S\text{ es }\Gamma\text{-r.e.}%
\]
sigue directo de (a). Supongamos ahora que $S$ es $\Sigma$-recursivo. Veremos
que $S$ es $\Gamma$-recursivo. Supongamos $S$ es de tipo $(n,m)$ es decir
$S\subseteq\omega^{n}\times\Sigma^{\ast m}$. Por definicion tenemos que
$\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}}$ es $\Sigma$-recursiva. Pero
$\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}}$ es tambien $\Gamma$-mixta, por lo
cual (a) nos dice que $\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}}$ es $\Gamma
$-recursiva. Ademas es claro que el conjunto $(\omega^{n}\times\Gamma^{\ast
m})-(\omega^{n}\times\Sigma^{\ast m})$ es $\Gamma$-recursivo. Ya que%
\[
\chi_{S}^{\omega^{n}\times\Gamma^{\ast m}}=\chi_{S}^{\omega^{n}\times
\Sigma^{\ast m}}\cup C_{0}^{n,m}|_{(\omega^{n}\times\Gamma^{\ast m}%
)-(\omega^{n}\times\Sigma^{\ast m})}%
\]
los Lemas \ref{restriccion de recursivas a conj recursivos} y
\ref{dpc para recursivas con dominio recursivo} nos dicen que $\chi
_{S}^{\omega^{n}\times\Gamma^{\ast m}}$ es $\Gamma$-recursiva (aqui
$C_{0}^{n,m}$ es respecto del alfabeto $\Gamma$).

Supongamos ahora que $S$ es $\Gamma$-recursivo. Veremos que $S$ es $\Sigma
$-recursivo. Por definicion tenemos que $\chi_{S}^{\omega^{n}\times
\Gamma^{\ast m}}$ es $\Gamma$-recursiva. Ya que $\omega^{n}\times\Sigma^{\ast
m}$ es $\Gamma$-recursivo, tenemos que $\chi_{S}^{\omega^{n}\times\Gamma^{\ast
m}}|_{\omega^{n}\times\Sigma^{\ast m}}$ es $\Gamma$-recursiva. Por (a) tenemos
que $\chi_{S}^{\omega^{n}\times\Gamma^{\ast m}}|_{\omega^{n}\times\Sigma^{\ast
m}}$ es $\Sigma$-recursiva. Pero $\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}%
}=\chi_{S}^{\omega^{n}\times\Gamma^{\ast m}}|_{\omega^{n}\times\Sigma^{\ast
m}}$ por lo cual $\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}}$ es $\Sigma
$-recursiva, obteniendo que $S$ es $\Sigma$-recursivo.

El caso primitivo recursivo es analogo y dejado al lector.
\end{proof}

\bigskip

@@finpagina@@

\bigskip

\subsection{El paradigma imperativo de Neumann: El lenguaje $\mathcal{S}%
^{\Sigma}$}

En esta seccion daremos una modelizacion matematica del concepto de funcion
$\Sigma$-efectivamente computable utilizando un lenguaje de programacion
teorico el cual depende del alfabeto $\Sigma$. Lo llamaremos $\mathcal{S}%
^{\Sigma}$ a dicho lenguaje. Dado que fue el matematico Von Neumann quien
contribuyo al desarrollo de la primera computadora de proposito general (es
decir a la cual se le pueden hacer correr programas tal como a las
computadoras actuales), nos referiremos a este paradigma de computabilidad
efectiva como el paradigma de Von Neumann.

\subsubsection{\label{SintaxisDeSsigma}Sintaxis de $\mathcal{S}^{\Sigma}$}

Necesitaremos algunas funciones basicas para poder describir la sintaxis de
$\mathcal{S}^{\Sigma}$ en forma precisa. Llamaremos \textit{numerales} a los
siguientes simbolos%
\[
0\ 1\ 2\ 3\ 4\ 5\ 6\ 7\ 8\ 9
\]
Usaremos $Num$ para denotar el conjunto de numerales. Notese que
$Num\cap\omega=\emptyset$. Sea $Sig:Num^{\ast}\rightarrow Num^{\ast}$ definida
de la siguiente manera%
\begin{align*}
Sig(\varepsilon)  & =1\\
Sig(\alpha0)  & =\alpha1\\
Sig(\alpha1)  & =\alpha2\\
Sig(\alpha2)  & =\alpha3\\
Sig(\alpha3)  & =\alpha4\\
Sig(\alpha4)  & =\alpha5\\
Sig(\alpha5)  & =\alpha6\\
Sig(\alpha6)  & =\alpha7\\
Sig(\alpha7)  & =\alpha8\\
Sig(\alpha8)  & =\alpha9\\
Sig(\alpha9)  & =Sig(\alpha)0
\end{align*}
Definamos $Dec:\omega\rightarrow Num^{\ast}$ de la siguiente manera%
\begin{align*}
Dec(0)  & =\varepsilon\\
Dec(n+1)  & =Sig(Dec(n))
\end{align*}
Notese que para $n\in\mathbf{N}$, la palabra $Dec(n)$ es la notacion usual
decimal de $n$. Para hacer mas agil la notacion escribiremos $\bar{n}$ en
lugar de $Dec(n)$. Notese que, en virtud de esta convencion notacional se
tiene que $Dec=\lambda n[\bar{n}]$. Recordemos que para $\alpha\in\Sigma
^{\ast}$, definiamos%
\[
^{\curvearrowright}\alpha=\left\{
\begin{array}
[c]{lll}%
\left[  \alpha\right]  _{2}...\left[  \alpha\right]  _{\left\vert
\alpha\right\vert } & \text{si} & \left\vert \alpha\right\vert \geq2\\
\varepsilon & \text{si} & \left\vert \alpha\right\vert \leq1
\end{array}
\right.
\]
La sintaxis de $\mathcal{S}^{\Sigma}$ sera dada utilizando solo simbolos del
alfabeto $\Sigma\cup\Sigma_{p}$, donde%
\[
\Sigma_{p}=Num\cup\left\{  \leftarrow,+,\dot{-},.,\neq,^{\curvearrowright
},\varepsilon,\mathrm{N},\mathrm{K},\mathrm{P},\mathrm{L},\mathrm{I}%
,\mathrm{F},\mathrm{G},\mathrm{O},\mathrm{T},\mathrm{B},\mathrm{E}%
,\mathrm{S}\right\}  .
\]


Cabe aclarar que la palabra de longitud $0$ no es un elemento de $\Sigma_{p} $
sino que la letra griega $\varepsilon$ que usualmente denota esta palabra, lo
es. Tambien notese que en $\Sigma_{p}$ hay simbolos que a veces representan
operaciones como por ejemplo $+$ y $\dot{-}$, pero deberia quedar claro que en
$\Sigma_{p}$ estan los simbolos $+$ y $\dot{-}$ y no las operaciones que ellos denotan.

Las palabras de la forma $\mathrm{N}\bar{k}$ con $k\in\mathbf{N}$, son
llamadas \textit{variables numericas de }$\mathcal{S}^{\Sigma}$. Las palabras
de la forma $\mathrm{P}\bar{k}$ con $k\in\mathbf{N}$, son llamadas
\textit{variables alfabeticas de }$\mathcal{S}^{\Sigma}$. Las palabras de la
forma $\mathrm{L}\bar{k}$ con $k\in\mathbf{N}$, son llamadas \textit{labels de
}$\mathcal{S}^{\Sigma}$.

Una \textit{instruccion basica de }$\mathcal{S}^{\Sigma}$ es una palabra de
$(\Sigma\cup\Sigma_{p})^{\ast}$ la cual es de alguna de las siguientes formas

\begin{enumerate}
\item[ ] $\mathrm{N}\bar{k}\leftarrow\mathrm{N}\bar{k}+1$

\item[ ] $\mathrm{N}\bar{k}\leftarrow\mathrm{N}\bar{k}\dot{-}1$

\item[ ] $\mathrm{N}\bar{k}\leftarrow\mathrm{N}\bar{n}$

\item[ ] $\mathrm{N}\bar{k}\leftarrow0$

\item[ ] $\mathrm{P}\bar{k}\leftarrow\mathrm{P}\bar{k}.a$

\item[ ] $\mathrm{P}\bar{k}\leftarrow$ $^{\curvearrowright}\mathrm{P}\bar{k}$

\item[ ] $\mathrm{P}\bar{k}\leftarrow\mathrm{P}\bar{n}$

\item[ ] $\mathrm{P}\bar{k}\leftarrow\varepsilon$

\item[ ] $\mathrm{IF}\;\mathrm{N}\bar{k}\neq0\;\mathrm{GOTO}\;\mathrm{L}%
\bar{n}$

\item[ ] $\mathrm{IF}\;\mathrm{P}\bar{k}\;\mathrm{BEGINS}\;a\;\mathrm{GOTO}%
\;\mathrm{L}\bar{n}$

\item[ ] $\mathrm{GOTO}\;\mathrm{L}\bar{n}$

\item[ ] $\mathrm{SKIP}$
\end{enumerate}

\noindent donde $a\in\Sigma$ y $k,n\in\mathbf{N}$. Como puede observarse para
que las instrucciones basicas sean mas lejibles usamos espacios entre ciertos
simbolos. Por ejemplo, hemos escrito $\mathrm{N}\bar{k}\leftarrow
\mathrm{N}\bar{k}+1$ pero en realidad nos referimos a la palabra%
\[
\mathrm{N}\bar{k}\mathrm{\leftarrow}\text{\textrm{N}}\bar{k}\mathrm{+1}%
\]
cuya longitud es $2\left\vert \bar{k}\right\vert +5$. Otro ejemplo, hemos
escrito $\mathrm{IF}\;\mathrm{P}\bar{k}\;\mathrm{BEGINS}\;a\;\mathrm{GOTO}%
\;\mathrm{L}\bar{n}$ pero en realidad nos referiamos a la palabra
$\mathrm{IFP}\bar{k}\mathrm{BEGINS}a\mathrm{GOTOL}\bar{n}$ cuya longitud es
$\left\vert \bar{k}\right\vert +\left\vert \bar{n}\right\vert +15$.

Una \textit{instruccion de }$\mathcal{S}^{\Sigma}$ es ya sea una instruccion
basica de $\mathcal{S}^{\Sigma}$ o una palabra de la forma $\alpha I$, donde
$\alpha\in\{\mathrm{L}\bar{n}:n\in\mathbf{N}\}$ y $I$ es una instruccion
basica de $\mathcal{S}^{\Sigma}$. Usaremos $\mathrm{Ins}^{\Sigma}$ para
denotar el conjunto de todas las instrucciones de $\mathcal{S}^{\Sigma}$.
Cuando la instruccion $I$ es de la forma $\mathrm{L}\bar{n}J$ con $J$ una
instruccion basica, diremos que $\mathrm{L}\bar{n}$ es el \textit{label} de
$I$. Damos a continuacion, a modo de ejemplo, la interpretacion intuitiva
asociada a ciertas instrucciones basicas de $\mathcal{S}^{\Sigma}$:%
\begin{align*}
\text{INSTRUCCION}  & :\mathrm{N}\bar{k}\leftarrow\mathrm{N}\bar{k}\dot{-}1\\
\text{INTERPRETACION}  & :%
\begin{array}
[t]{c}%
\text{Si el contenido de }\mathrm{N}\bar{k}\text{ es }0\text{ dejarlo sin
modificar; en}\\
\text{caso contrario disminuya en 1 el contenido de }\mathrm{N}\bar{k}\;
\end{array}
\\
\text{INSTRUCCION}  & :\mathrm{N}\bar{k}\leftarrow\mathrm{N}\bar{n}\\
\text{INTERPRETACION}  & :%
\begin{array}
[t]{l}%
\text{Copiar en }\mathrm{N}\bar{k}\text{ el contenido de }\mathrm{N}\bar
{n}\text{ (sin modificar el}\\
\text{contenido de }\mathrm{N}\bar{n}\text{)}%
\end{array}
\\
\text{INSTRUCCION}  & :\mathrm{P}\bar{k}\leftarrow^{\curvearrowright
}\mathrm{P}\bar{k}\\
\text{INTERPRETACION}  & :%
\begin{array}
[t]{l}%
\text{Si el contenido de }\mathrm{P}\bar{k}\text{ es }\varepsilon\text{
dejarlo sin modificar;}\\
\text{en caso contrario remueva el 1er simbolo del}\\
\text{contenido de }\mathrm{P}\bar{k}%
\end{array}
\end{align*}
%

\begin{align*}
\text{INSTRUCCION}  & :\mathrm{P}\bar{k}\leftarrow\mathrm{P}\bar{k}.a\\
\text{INTERPRETACION}  & :%
\begin{array}
[t]{l}%
\text{Modificar el contenido de }\mathrm{P}\bar{k}\text{ agregandole}\\
\text{el simbolo }a\text{ a la derecha}%
\end{array}
\\
\text{INSTRUCCION}  & :\mathrm{IF}\;\mathrm{P}\bar{k}\;\mathrm{BEGINS}%
\;a\;\mathrm{GOTO}\;\mathrm{L}\bar{m}\\
\text{INTERPRETACION}  & :%
\begin{array}
[t]{l}%
\text{Si el contenido de }\mathrm{P}\bar{k}\text{ comiensa con }%
a,\text{ejecute}\\
\text{la primer instruccion con label }\mathrm{L}\bar{m}\text{; en caso}\\
\text{contrario ejecute la siguiente instruccion}%
\end{array}
\end{align*}


\bigskip

Un \textit{programa de }$\mathcal{S}^{\Sigma}$ es una palabra de la forma%
\[
I_{1}I_{2}...I_{n}%
\]
donde $n\geq1$, $I_{1},...,I_{n}\in\mathrm{Ins}^{\Sigma}$ y ademas se cumple
la siguiente propiedad, llamada \textit{la ley de los GOTO},

\begin{enumerate}
\item[(G)] Para cada $i\in\{1,...,n\}$, si $\mathrm{GOTOL}\bar{m}$ es un tramo
final de $I_{i}$, entonces existe $j\in\{1,...,n\}$ tal que $I_{j}$ tiene
label $\mathrm{L}\bar{m}$
\end{enumerate}

\noindent Usaremos $\mathrm{Pro}^{\Sigma}$ para denotar el conjunto de todos
los programas de $\mathcal{S}^{\Sigma}$. Como es usual cuando escribamos un
programa lo haremos linea por linea, con la finalidad de que sea mas lejible.
Por ejemplo, escribiremos%
\[%
\begin{array}
[c]{ll}%
\mathrm{L}2 & \mathrm{N}12\leftarrow\mathrm{N}12\dot{-}1\\
& \mathrm{P}1\leftarrow\text{ }^{\curvearrowright}\mathrm{P}1\\
& \mathrm{IF\;N}12\neq0\;\mathrm{GOTO}\;\mathrm{L}2
\end{array}
\]
en lugar de%
\[
\mathrm{L}2\mathrm{N}12\mathrm{\leftarrow}\text{N}12\mathrm{\dot{-}%
}1\mathrm{P}1\mathrm{\leftarrow}^{\curvearrowright}\mathrm{P}1\mathrm{IFN}%
12\mathrm{\neq}0\mathrm{GOTOL}2
\]
Un importante resultado es el siguiente lema que garantiza que los programas
pueden ser parseados en forma unica como concatenacion de instrucciones.

\begin{lemma}
Se tiene que:

\begin{enumerate}
\item[(a)] Si $I_{1}...I_{n}=J_{1}...J_{m}$, con $I_{1},...,I_{n}%
,J_{1},...,J_{m}\in\mathrm{Ins}^{\Sigma}$, entonces $n=m$ y $I_{j}=J_{j}$ para
cada $j\geq1$.

\item[(b)] Si $\mathcal{P}\in\mathrm{Pro}^{\Sigma}$, entonces existe una unica
sucesion de instrucciones $I_{1},...,I_{n}$ tal que $\mathcal{P}=I_{1}%
...I_{n}$
\end{enumerate}
\end{lemma}

\begin{proof}
(a) Supongamos $I_{n}$ es un tramo final propio de $J_{m}.$ Notar que entonces
$n>1$. Es facil ver que entonces ya sea $J_{m}=\mathrm{L}\bar{u}I_{n}$ para
algun $u\in\mathbf{N}$, o $I_{n}$ es de la forma $\mathrm{GOTO}\;\mathrm{L}%
\bar{n}$ y $J_{m}$ es de la forma $w\mathrm{IF}\;\mathrm{P}\bar{k}%
\;\mathrm{BEGINS}\;a\;\mathrm{GOTO}\;\mathrm{L}\bar{n}$ donde $w\in
\{\mathrm{L}\bar{n}:n\in\mathbf{N}\}\cup\{\varepsilon\}$. El segundo caso no
puede darse porque entonces el anteultimo simbolo de $I_{n-1}$ deberia ser
$\mathrm{S}$ lo cual no sucede para ninguna instruccion. O sea que%
\[
I_{1}...I_{n}=J_{1}...J_{m-1}\mathrm{L}\bar{u}I_{n}%
\]
lo cual dice que

\begin{enumerate}
\item[(*)] $I_{1}...I_{n-1}=J_{1}...J_{m-1}\mathrm{L}\bar{u}.$
\end{enumerate}

\noindent Es decir que $\mathrm{L}\bar{u}$ es tramo final de $I_{n-1}$ y por
lo tanto $\mathrm{GOTO}\;\mathrm{L}\bar{u}$ es tramo final de $I_{n-1}.$ Por
(*), $\mathrm{GOTO}$ es tramo final de $J_{1}...J_{m-1}$, lo cual es
impossible. Hemos llegado a una contradiccion lo cual nos dice que $I_{n}$ no
es un tramo final propio de $J_{m}.$ Por simetria tenemos que $I_{n}=J_{m} $,
lo cual usando un razonamiento inductivo nos dice que $n=m$ y $I_{j}=J_{j} $
para cada $j\geq1$.

(b) Es consecuencia directa de (a).
\end{proof}

\bigskip

(b) del lema anterior nos dice que dado un programa $\mathcal{P}$, tenemos
univocamente determinados $n(\mathcal{P})\in\mathbf{N}$ y $I_{1}^{\mathcal{P}%
},...,I_{n(\mathcal{P})}^{\mathcal{P}}\in\mathrm{Ins}^{\Sigma}$ tales que
$\mathcal{P}=I_{1}^{\mathcal{P}}...I_{n(\mathcal{P})}^{\mathcal{P}}$.
Definamos tambien%
\[
I_{i}^{\mathcal{P}}=\varepsilon
\]
cuando $i=0$ o $i>n(\mathcal{P})$. Notese que las expresiones $n(\alpha)$ y
$I_{i}^{\alpha}$ estan definidas solo cuando $\alpha$ es un programa (y $i$ es
un elemento de $\omega$), es decir, cierta palabra del alfabeto $\Sigma
\cup\Sigma_{p}$. O sea que cuando usemos notacion lambda que involucre dichas
expresiones, el alfabeto respecto del cual usaremos dicha notacion sera
$\Sigma\cup\Sigma_{p}$. Esto nos dice entonces que $\lambda\alpha\lbrack
n(\alpha)]$ tiene dominio igual a $\mathrm{Pro}^{\Sigma}\subseteq(\Sigma
\cup\Sigma_{p})^{\ast}$ y $\lambda i\alpha\lbrack I_{i}^{\alpha}]$ tiene
dominio igual a $\omega\times\mathrm{Pro}^{\Sigma}$. Para hacer mas sugestiva
la notacion a veces escribiremos $\lambda\mathcal{P}[n(\mathcal{P})]$ y
$\lambda i\mathcal{P}[I_{i}^{\mathcal{P}}]$ en lugar de $\lambda\alpha\lbrack
n(\alpha)]$ y $\lambda i\alpha\lbrack I_{i}^{\alpha}]$.

Sera necesaria la funcion $Bas:\mathrm{Ins}^{\Sigma}\rightarrow(\Sigma
\cup\Sigma_{p})^{\ast}$, dada por%
\[
Bas(I)=\left\{
\begin{array}
[c]{ccl}%
J &  & \text{si }I\text{ es de la forma }\mathrm{L}\bar{k}J\text{, con }%
k\in\mathbf{N}\text{ y }J\in\mathrm{Ins}^{\Sigma}\\
I &  & \text{caso contrario}%
\end{array}
\right.
\]


\bigskip

\subsubsection{Semantica de $\mathcal{S}^{\Sigma}$}

Definamos%

\begin{align*}
\omega^{\left[  \mathbf{N}\right]  }  & =\left\{  (s_{1},s_{2},...)\in
\omega^{\mathbf{N}}:\text{ hay }n\in\mathbf{N}\text{ tal que }s_{i}%
=0,\text{para }i\geq n\right\} \\
\Sigma^{\ast\left[  \mathbf{N}\right]  }  & =\left\{  (\sigma_{1},\sigma
_{2},...)\in\Sigma^{\ast\mathbf{N}}:\text{ hay }n\in\mathbf{N}\text{ tal que
}\sigma_{i}=\varepsilon,\text{para }i\geq n\right\}  .
\end{align*}
Asumiremos siempre que en una computacion via un programa de $\mathcal{S}%
^{\Sigma}$, todas exepto una cantidad finita de las variables numericas tienen
el valor $0$ y todas exepto una cantiad finita de las variables alfabeticas
tienen el valor $\varepsilon$. Esto no quita generalidad a nuestra
modelizacion del funcionamiento de los programas ya que todo programa envuelve
una cantidad finita de variables.

Un \textit{estado} es un par%
\[
(\vec{s},\vec{\sigma})=((s_{1},s_{2},...),(\sigma_{1},\sigma_{2}%
,...))\in\omega^{\left[  \mathbf{N}\right]  }\times\Sigma^{\ast\left[
\mathbf{N}\right]  }.
\]
Si $i\geq1$, entonces diremos que $s_{i}$ es el \textit{contenido
}o\textit{\ valor }de la variable $\mathrm{N}\bar{\imath}$ en el estado
$(\vec{s},\vec{\sigma})$ y $\sigma_{i}$ es el \textit{contenido }%
o\textit{\ valor }de la variable $\mathrm{P}\bar{\imath}$ en el estado
$(\vec{s},\vec{\sigma})$. Intuitivamente hablando, un estado es un par de
infinituplas que contiene la informacion de que valores tienen alojados las
distintas variables.

Imaginemos que corremos un programa $\mathcal{P}$ partiendo de un estado
inicial $(\vec{s},\vec{\sigma})$. Por supuesto la primera instruccion a
realizar sera $I_{1}^{\mathcal{P}}$ pero, dado que $I_{1}^{\mathcal{P}}$ puede
ser de tipo GOTO, la segunda instruccion que realizaremos puede no ser
$I_{2}^{\mathcal{P}}$. Es decir en cada paso iremos decidiendo en funcion de
la instruccion ejecutada cual es la siguiente instruccion a realizar. O sea
que mientras corremos $\mathcal{P}$, en cada paso la informacion importante a
tener en cuenta es, por una parte, cuales son los valores que tienen cada una
de las variables y, por otra parte, cual es la instruccion que nos tocara
realizar a continuacion. Esto da lugar al concepto de descripcion instantanea,
a saber, un objeto matematico que describe en un instante dado de la
computacion cuales son los valores de las variables y cual es la instruccion
que se debe realizar en el instante siguiente. Mas formalmente una
\textit{descripcion instantanea} es una terna $(i,\vec{s},\vec{\sigma})$ tal
que $(\vec{s},\vec{\sigma})$ es un estado e $i\in\omega$. Es decir que
$\omega\times\omega^{\left[  \mathbf{N}\right]  }\times\Sigma^{\ast\left[
\mathbf{N}\right]  }$ es el conjunto formado por todas las descripciones
instantaneas. Intuitivamente hablando, cuando $i\in\{1,...,n(\mathcal{P})\}$,
la descripcion instantanea $(i,\vec{s},\vec{\sigma})$ nos dice que las
variables estan en el estado $(\vec{s},\vec{\sigma})$ y que la instruccion que
\textit{debemos realizar} es $I_{i}^{\mathcal{P}}$. Dado que sera conveniente
para simplificar el tratamiento formal, nos abstraeremos un poco y cuando
$i=0$ o $i>n(\mathcal{P})$ pensaremos tambien que la descripcion instantanea
$(i,\vec{s},\vec{\sigma})$ nos dice que las variables estan en el estado
$(\vec{s},\vec{\sigma})$ y que debemos realizar $I_{i}^{\mathcal{P}%
}=\varepsilon$ (aunque por supuesto no podremos realizarla ya que no es una instruccion).

Dado un programa $\mathcal{P}$ definiremos a continuacion una funcion%
\[
S_{\mathcal{P}}:\omega\times\omega^{\left[  \mathbf{N}\right]  }\times
\Sigma^{\ast\left[  \mathbf{N}\right]  }\rightarrow\omega\times\omega^{\left[
\mathbf{N}\right]  }\times\Sigma^{\ast\left[  \mathbf{N}\right]  }%
\]
la cual le asignara a una descripcion instantanea $(i,\vec{s},\vec{\sigma})$
la \textit{descripcion instantanea sucesora de }$(i,\vec{s},\vec{\sigma})$
\textit{con respecto a }$\mathcal{P}$. Cuando $i\in\{1,...,n(\mathcal{P})\}$,
intuitivamente hablando, $S_{\mathcal{P}}(i,\vec{s},\vec{\sigma})$ sera la
descripcion instantanea que resulta luego de realizar $I_{i}^{\mathcal{P}}$
estando en el estado $(\vec{s},\vec{\sigma})$. Cuando $i=0$ o $i>n(\mathcal{P}%
)$ definiremos $S_{\mathcal{P}}(i,\vec{s},\vec{\sigma})=(i,\vec{s},\vec
{\sigma})$, lo cual es bastante intuitivo ya que si estamos en estado
$(\vec{s},\vec{\sigma})$ y debemos realizar $I_{i}^{\mathcal{P}}=\varepsilon$,
dado que $\varepsilon$ no es una instruccion y por lo tanto no la podremos
realizar, seguiremos en el mismo estado y teniendo que realizar $I_{i}%
^{\mathcal{P}}$.

Para darle una semantica mas unificada al concepto de descripcion instantanea
sucesora debemos crear un nuevo verbo. El verbo "realizarp". Dada una
actividad A, diremos que un individuo P \textit{realizap} la actividad A, si P
realiza A, en caso de que pueda hacerlo. O sea realizarp una actividad es
realizarla si se puede.

Para dar otro ejemplo de este tipo de verbos, consideremos el verbo
"comprarp", es decir "comprar si se puede". Un hijo le pide a su padre que le
compre un determinado juguete y el padre le dice "si, hijo mio, te lo voy a
comprarp". Luego el padre es despedido de su empleo y su cituacion economica
hace que no le sea posible comprar dicho juguete. Sin envargo el padre no
mintio ya que si bien no compro dicho juguete, \'{e}l si lo comprop.

Con este verbo podemos describir intuitivamente $S_{\mathcal{P}}(i,\vec
{s},\vec{\sigma})$:%
\begin{align*}
S_{\mathcal{P}}(i,\vec{s},\vec{\sigma})  &
=\mathrm{desc\ inst\ que\ resulta\ luego\ de}\\
& \mathrm{rea}\text{l}\mathrm{izarp\ }I_{i}^{\mathcal{P}}\text{, estando en
estado }(\vec{s},\vec{\sigma})
\end{align*}
Ahora si, daremos la definicion matematica de $S_{\mathcal{P}}(i,\vec{s}%
,\vec{\sigma})$, segun se den distintos casos posibles.

Caso $i\notin\{1,...,n(\mathcal{P})\}$. Entonces $S_{\mathcal{P}}(i,\vec
{s},\vec{\sigma})=(i,\vec{s},\vec{\sigma})$

Caso $Bas(I_{i}^{\mathcal{P}})=\mathrm{N}\bar{k}\leftarrow\mathrm{N}\bar
{k}\dot{-}1.$ Entonces%
\[
S_{\mathcal{P}}(i,\vec{s},\vec{\sigma})=(i+1,(s_{1},...,s_{k-1},s_{k}\dot
{-}1,s_{k+1},...),\vec{\sigma})
\]


Caso $Bas(I_{i}^{\mathcal{P}})=\mathrm{N}\bar{k}\leftarrow\mathrm{N}\bar
{k}+1.$ Entonces%
\[
S_{\mathcal{P}}(i,\vec{s},\vec{\sigma})=(i+1,(s_{1},...,s_{k-1},s_{k}%
+1,s_{k+1},...),\vec{\sigma})
\]


Caso $Bas(I_{i}^{\mathcal{P}})=\mathrm{N}\bar{k}\leftarrow\mathrm{N}\bar{n}$.
Entonces%
\[
S_{\mathcal{P}}(i,\vec{s},\vec{\sigma})=(i+1,(s_{1},...,s_{k-1},s_{n}%
,s_{k+1},...),\vec{\sigma})
\]


Caso $Bas(I_{i}^{\mathcal{P}})=\mathrm{N}\bar{k}\leftarrow0.$ Entonces%
\[
S_{\mathcal{P}}(i,\vec{s},\vec{\sigma})=(i+1,(s_{1},...,s_{k-1},0,s_{k+1}%
,...),\vec{\sigma})
\]


Caso $Bas(I_{i}^{\mathcal{P}})=\mathrm{IF}$ $\mathrm{N}\bar{k}$ $\neq0$
$\mathrm{GOTO}$ $\mathrm{L}\bar{m}.$ Entonces tenemos dos subcasos.

Subcaso a. El valor de $\mathrm{N}\bar{k}$ en $(\vec{s},\vec{\sigma})$ es 0.
Entonces%
\[
S_{\mathcal{P}}(i,\vec{s},\vec{\sigma})=(i+1,\vec{s},\vec{\sigma})
\]


Subcaso b. El valor de $\mathrm{N}\bar{k}$ en $(\vec{s},\vec{\sigma})$ es no
nulo. Entonces%
\[
S_{\mathcal{P}}(i,\vec{s},\vec{\sigma})=(\min\{l:I_{l}^{\mathcal{P}%
}\ \mathrm{tiene\ label\ L}\bar{m}\},\vec{s},\vec{\sigma})
\]


Caso $Bas(I_{i}^{\mathcal{P}})=\mathrm{P}\bar{k}\leftarrow$
$^{\curvearrowright}\mathrm{P}\bar{k}.$ Entonces%
\[
S_{\mathcal{P}}(i,\vec{s},\vec{\sigma})=(i+1,\vec{s},(\sigma_{1}%
,...,\sigma_{k-1},^{\curvearrowright}\sigma_{k},\sigma_{k+1},...))
\]


Caso $Bas(I_{i}^{\mathcal{P}})=\mathrm{P}\bar{k}\leftarrow\mathrm{P}\bar{k}%
.a$. Entonces%
\[
S_{\mathcal{P}}(i,\vec{s},\vec{\sigma})=(i+1,\vec{s},(\sigma_{1}%
,...,\sigma_{k-1},\sigma_{k}a,\sigma_{k+1},...))
\]


Caso $Bas(I_{i}^{\mathcal{P}})=\mathrm{P}\bar{k}\leftarrow\mathrm{P}\bar{n}$.
Entonces%
\[
S_{\mathcal{P}}(i,\vec{s},\vec{\sigma})=(i+1,\vec{s},(\sigma_{1}%
,...,\sigma_{k-1},\sigma_{n},\sigma_{k+1},...))
\]


Caso $Bas(I_{i}^{\mathcal{P}})=\mathrm{P}\bar{k}\leftarrow\varepsilon.$
Entonces%
\[
S_{\mathcal{P}}(i,\vec{s},\vec{\sigma})=(i+1,\vec{s},(\sigma_{1}%
,...,\sigma_{k-1},\varepsilon,\sigma_{k+1},...))
\]


Caso $Bas(I_{i}^{\mathcal{P}})=\mathrm{IF}\;\mathrm{P}\bar{k}\;\mathrm{BEGINS}%
\;a\;\mathrm{GOTO}\;\mathrm{L}\bar{m}.$ Entonces tenemos dos subcasos.

Subcaso a. El valor de $\mathrm{P}\bar{k}$ en $(\vec{s},\vec{\sigma})$
comiensa con $a$. Entonces%
\[
S_{\mathcal{P}}(i,\vec{s},\vec{\sigma})=(\min\{l:I_{l}^{\mathcal{P}%
}\ \mathrm{tiene\ label\ L}\bar{m}\},\vec{s},\vec{\sigma})
\]


Subcaso b. El valor de $\mathrm{P}\bar{k}$ en $(\vec{s},\vec{\sigma})$ no
comiensa con $a$. Entonces%
\[
S_{\mathcal{P}}(i,\vec{s},\vec{\sigma})=(i+1,\vec{s},\vec{\sigma})
\]


Caso $Bas(I_{i}^{\mathcal{P}})=\mathrm{GOTO}\;\mathrm{L}\bar{m}$. Entonces%
\[
S_{\mathcal{P}}(i,\vec{s},\vec{\sigma})=(\min\{l:I_{l}^{\mathcal{P}%
}\ \mathrm{tiene\ label\ L}\bar{m}\},\vec{s},\vec{\sigma})
\]


Caso $Bas(I_{i}^{\mathcal{P}})=\mathrm{SKIP}$. Entonces%
\[
S_{\mathcal{P}}(i,\vec{s},\vec{\sigma})=(i+1,\vec{s},\vec{\sigma})
\]


\paragraph{La computacion partiendo de un estado}

Dado un programa $\mathcal{P}$ y un estado $(\vec{s},\vec{\sigma})$ a la
infinitupla%
\[
((1,\vec{s},\vec{\sigma}),S_{\mathcal{P}}(1,\vec{s},\vec{\sigma}%
),S_{\mathcal{P}}(S_{\mathcal{P}}(1,\vec{s},\vec{\sigma})),S_{\mathcal{P}%
}(S_{\mathcal{P}}(S_{\mathcal{P}}(1,\vec{s},\vec{\sigma}))),...)
\]
la llamaremos la \textit{computacion de }$\mathcal{P}$\textit{\ partiendo del
estado }$(\vec{s},\vec{\sigma})$. Diremos que%
\[
\overset{t\text{ veces}}{\overbrace{S_{\mathcal{P}}(...S_{\mathcal{P}%
}(S_{\mathcal{P}}(}}1,\vec{s},\vec{\sigma}))...)
\]
es la \textit{descripcion instantanea obtenida luego de }$t$ \textit{pasos,
partiendo del estado }$(\vec{s},\vec{\sigma})$. Si%
\[
\overset{t\text{ veces}}{\overbrace{S_{\mathcal{P}}(...S_{\mathcal{P}%
}(S_{\mathcal{P}}(}}1,\vec{s},\vec{\sigma}))...)=(j,\vec{u},\vec{\eta})
\]
diremos que $(\vec{u},\vec{\eta})$ es el \textit{estado obtenido luego de }$t
$ \textit{pasos, partiendo del estado }$(\vec{s},\vec{\sigma})$.

Es claro que en la infinitupla de mas arriba esta toda la informacion de la
"corrida" del programa $\mathcal{P}$ cuando partimos del estado $(\vec{s}%
,\vec{\sigma})$. Veamos un ejemplo. Sea $\Sigma=\{\blacktriangle,\#\}$ y sea
$\mathcal{P}$ el siguiente programa%
\[%
\begin{array}
[c]{ll}%
\mathrm{L}3 & \mathrm{N}4\leftarrow\mathrm{N}4+1\\
& \mathrm{P}1\leftarrow\ ^{\curvearrowright}\mathrm{P}1\\
& \mathrm{IF\ P}1\ \mathrm{BEGINS\ }\blacktriangle\ \mathrm{GOTO}%
\;\mathrm{L}3\\
& \mathrm{P}3\leftarrow\mathrm{P}3.\#
\end{array}
\]
Supongamos que tomamos $(\vec{s},\vec{\sigma})$ igual al estado%
\[
\left(  (2,1,0,5,3,0,0,0,...),(\#\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon
,...)\right)
\]
Tendremos entonces que la computacion de $\mathcal{P}$\ partiendo del estado
$(\vec{s},\vec{\sigma})$ es la siguiente sucesion (de arriba hacia abajo) de
descripciones instantaneas:%
\begin{gather*}
(1,(2,1,0,5,3,0,0,0,...),(\#\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{realizando }I_{1}^{\mathcal{P}}=\mathrm{N}4\leftarrow\mathrm{N}4+1\text{
obtenemos}\\
(2,(2,1,0,6,3,0,0,0,...),(\#\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{realizando }I_{2}^{\mathcal{P}}=\mathrm{P}1\leftarrow
\ ^{\curvearrowright}\mathrm{P}1\text{ obtenemos}\\
(3,(2,1,0,6,3,0,0,0,...),(\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{realizando }I_{3}^{\mathcal{P}}=\mathrm{IF\ P}1\ \mathrm{BEGINS\ }%
\blacktriangle\ \mathrm{GOTO}\;\mathrm{L}3\text{ obtenemos}\\
(1,(2,1,0,6,3,0,0,0,...),(\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{realizando }I_{1}^{\mathcal{P}}=\mathrm{N}4\leftarrow\mathrm{N}4+1\text{
obtenemos}\\
(2,(2,1,0,7,3,0,0,0,...),(\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{realizando }I_{2}^{\mathcal{P}}=\mathrm{P}1\leftarrow
\ ^{\curvearrowright}\mathrm{P}1\text{ obtenemos}\\
(3,(2,1,0,7,3,0,0,0,...),(\#\#,\varepsilon,\blacktriangle\blacktriangle
,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{realizando }I_{3}^{\mathcal{P}}=\mathrm{IF\ P}1\ \mathrm{BEGINS\ }%
\blacktriangle\ \mathrm{GOTO}\;\mathrm{L}3\text{ obtenemos}\\
(4,(2,1,0,7,3,0,0,0,...),(\#\#,\varepsilon,\blacktriangle\blacktriangle
,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{realizando }I_{4}^{\mathcal{P}}=\mathrm{P}3\leftarrow\mathrm{P}%
3.\#\text{ obtenemos}\\
(5,(2,1,0,7,3,0,0,0,...),(\#\#,\varepsilon,\blacktriangle\blacktriangle
\#,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{intentando realizar }I_{5}^{\mathcal{P}}=\varepsilon\text{ obtenemos}\\
(5,(2,1,0,7,3,0,0,0,...),(\#\#,\varepsilon,\blacktriangle\blacktriangle
\#,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{intentando realizar }I_{5}^{\mathcal{P}}=\varepsilon\text{ obtenemos}\\
(5,(2,1,0,7,3,0,0,0,...),(\#\#,\varepsilon,\blacktriangle\blacktriangle
\#,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{intentando realizar }I_{5}^{\mathcal{P}}=\varepsilon\text{ obtenemos}\\
(5,(2,1,0,7,3,0,0,0,...),(\#\#,\varepsilon,\blacktriangle\blacktriangle
\#,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\vdots
\end{gather*}
Notese que en este caso es natural decir que el programa $\mathcal{P}$ se
detiene, partiendo del estado inicial dado ya que llega a un punto en el que
queda intentando realizar $I_{n(\mathcal{P})+1}^{\mathcal{P}}$ lo cual no es
una instruccion. Veamos un ejemplo de no detencion. Sea $\mathcal{Q}$ el
siguiente programa%
\[%
\begin{array}
[c]{ll}%
\mathrm{L}3 & \mathrm{N}4\leftarrow\mathrm{N}4+1\\
& \mathrm{IF\ P}1\ \mathrm{BEGINS\ }\blacktriangle\ \mathrm{GOTO}\;\mathrm{L}3
\end{array}
\]
Supongamos que tomamos $(\vec{s},\vec{\sigma})$ igual al estado%
\[
\left(  (2,1,0,5,3,0,0,0,...),(\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon
,...)\right)
\]
Tendremos entonces que la computacion de $\mathcal{Q}$\ partiendo del estado
$(\vec{s},\vec{\sigma})$ es la siguiente sucesion (de arriba hacia abajo) de
descripciones instantaneas:%
\begin{gather*}
(1,(2,1,0,5,3,0,0,0,...),(\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{realizando }I_{1}^{\mathcal{P}}=\mathrm{N}4\leftarrow\mathrm{N}4+1\text{
obtenemos}\\
(2,(2,1,0,6,3,0,0,0,...),(\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{realizando }I_{2}^{\mathcal{P}}=\mathrm{IF\ P}1\ \mathrm{BEGINS\ }%
\blacktriangle\ \mathrm{GOTO}\;\mathrm{L}3\text{ obtenemos}\\
(1,(2,1,0,6,3,0,0,0,...),(\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{realizando }I_{1}^{\mathcal{P}}=\mathrm{N}4\leftarrow\mathrm{N}4+1\text{
obtenemos}\\
(2,(2,1,0,7,3,0,0,0,...),(\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{realizando }I_{2}^{\mathcal{P}}=\mathrm{IF\ P}1\ \mathrm{BEGINS\ }%
\blacktriangle\ \mathrm{GOTO}\;\mathrm{L}3\text{ obtenemos}\\
(1,(2,1,0,7,3,0,0,0,...),(\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{realizando }I_{1}^{\mathcal{P}}=\mathrm{N}4\leftarrow\mathrm{N}4+1\text{
obtenemos}\\
(2,(2,1,0,8,3,0,0,0,...),(\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{realizando }I_{2}^{\mathcal{P}}=\mathrm{IF\ P}1\ \mathrm{BEGINS\ }%
\blacktriangle\ \mathrm{GOTO}\;\mathrm{L}3\text{ obtenemos}\\
(1,(2,1,0,8,3,0,0,0,...),(\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{realizando }I_{1}^{\mathcal{P}}=\mathrm{N}4\leftarrow\mathrm{N}4+1\text{
obtenemos}\\
(2,(2,1,0,9,3,0,0,0,...),(\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\text{realizando }I_{2}^{\mathcal{P}}=\mathrm{IF\ P}1\ \mathrm{BEGINS\ }%
\blacktriangle\ \mathrm{GOTO}\;\mathrm{L}3\text{ obtenemos}\\
(1,(2,1,0,9,3,0,0,0,...),(\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#,\varepsilon,\varepsilon,\varepsilon,...))\\
\vdots
\end{gather*}
Notese que en este caso, es claro que el programa $\mathcal{Q}$ no se detiene
partiendo del estado inicial dado ya que sigue indefinidamente realizando instrucciones.

\bigskip

\paragraph*{Definicion matematica de detencion}

Ahora definiremos matematicamente el concepto de detencion. Cuando la primer
coordenada de%
\[
\overset{t\text{ veces}}{\overbrace{S_{\mathcal{P}}(...S_{\mathcal{P}%
}(S_{\mathcal{P}}(}}1,\vec{s},\vec{\sigma}))...)
\]
sea igual a $n(\mathcal{P})+1$, diremos que $\mathcal{P}$ \textit{se detiene
(luego de }$t$\textit{\ pasos), partiendo desde el estado }$(\vec{s}%
,\vec{\sigma})$. Si ninguna de las primeras coordenadas en la computacion%
\[
((1,\vec{s},\vec{\sigma}),S_{\mathcal{P}}(1,\vec{s},\vec{\sigma}%
),S_{\mathcal{P}}(S_{\mathcal{P}}(1,\vec{s},\vec{\sigma})),S_{\mathcal{P}%
}(S_{\mathcal{P}}(S_{\mathcal{P}}(1,\vec{s},\vec{\sigma}))),...)
\]
es igual a $n(\mathcal{P})+1$, diremos que $\mathcal{P}$ \textit{no se detiene
partiendo del estado }$(\vec{s},\vec{\sigma})$.

Cabe destacar que en los conceptos antes definidos por "1 paso" entendemos
"realizarp una instrucion", donde tal como se lo explico antes "realizarp"
significa "realizar si se puede". Otra observacion importante es que los
programas de $\mathcal{S}^{\Sigma}$ tienen una sola manera de detenerse, i.e.
siempre que se detienen lo hacen habiendo realizado la ultima de sus
instrucciones e intentando realizar la instruccion siguiente a su ultima instruccion.

\bigskip

\subsubsection{Funciones $\Sigma$-computables}

Ahora que hemos definido matematicamente la semantica de $\mathcal{S}^{\Sigma
}$ estamos en condiciones de definir el concepto de funcion $\Sigma
$-computable, el cual sera una modelizacion matematica del concepto de funcion
$\Sigma$-efectivamente computable. Intuitivamente hablando una funcion sera
$\Sigma$-computable cuando haya un programa que la compute. Para precisar este
concepto nos sera util la siguiente notacion. Dados $x_{1},...,x_{n}\in\omega$
y $\alpha_{1},...,\alpha_{m}\in\Sigma^{\ast} $, con $n,m\in\omega$, usaremos%
\[
\left\Vert x_{1},...,x_{n},\alpha_{1},...,\alpha_{m}\right\Vert
\]
para denotar el estado%
\[
\left(  (x_{1},...,x_{n},0,...),(\alpha_{1},...,\alpha_{m},\varepsilon
,...)\right)
\]
Esta notacion requiere aclarar un poco como debe interpretarse en los casos
limite, es decir cuando alguno de los numeros $n,m$ es igual a $0$. Notese que
por ejemplo%
\[
\left\Vert x\right\Vert =\left(  (x,0,...),(\varepsilon,...)\right)
\]
(es el caso $n=1$ y $m=0$). Tambien%
\[
\left\Vert \alpha\right\Vert =\left(  (0,...),(\alpha,\varepsilon,...)\right)
\]
(es el caso $n=0$ y $m=1$). En el caso $n=m=0$ pensaremos que $x_{1}%
,...,x_{n},\alpha_{1},...,\alpha_{m}$ se transforma en $\Diamond$ por lo que
se obtiene%
\[
\left\Vert \Diamond\right\Vert =\left(  (0,...),(\varepsilon,...)\right)
\]
Ademas es claro que%
\[
\left\Vert x_{1},...,x_{n},\alpha_{1},...,\alpha_{m}\right\Vert =\left\Vert
x_{1},...,x_{n},\overset{i}{\overbrace{0,...,0}},\alpha_{1},...,\alpha
_{m},\overset{j}{\overbrace{\varepsilon,...,\varepsilon}}\right\Vert
\]
cualesquiera sean $i,j\in\omega$.

Dado $\mathcal{P}\in\mathrm{Pro}^{\Sigma}$, definamos para cada par $n,m\geq
0$, la funcion $\Psi_{\mathcal{P}}^{n,m,\#}$ de la siguiente manera:%
\[%
\begin{array}
[c]{l}%
D_{\Psi_{\mathcal{P}}^{n,m,\#}}=\{(\vec{x},\vec{\alpha})\in\omega^{n}%
\times\Sigma^{\ast m}:\mathcal{P}\text{ termina, partiendo del}\\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\text{estado
}\left\Vert x_{1},...,x_{n},\alpha_{1},...,\alpha_{m}\right\Vert \}
\end{array}
\]%
\[%
\begin{array}
[c]{l}%
\Psi_{\mathcal{P}}^{n,m,\#}(\vec{x},\vec{\alpha})=\text{valor de }%
\mathrm{N}1\text{ en el estado obtenido cuando }\mathcal{P}\text{ termina,}\\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\text{partiendo de }\left\Vert
x_{1},...,x_{n},\alpha_{1},...,\alpha_{m}\right\Vert
\end{array}
\]
Analogamente definamos la funcion $\Psi_{\mathcal{P}}^{n,m,\ast}$ de la
siguiente manera:%
\[%
\begin{array}
[c]{l}%
D_{\Psi_{\mathcal{P}}^{n,m,\ast}}=\{(\vec{x},\vec{\alpha})\in\omega^{n}%
\times\Sigma^{\ast m}:\mathcal{P}\text{ termina, partiendo del}\\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\text{estado
}\left\Vert x_{1},...,x_{n},\alpha_{1},...,\alpha_{m}\right\Vert \}
\end{array}
\]%
\[%
\begin{array}
[c]{l}%
\Psi_{\mathcal{P}}^{n,m,\ast}(\vec{x},\vec{\alpha})=\text{valor de }%
\mathrm{P}1\text{ en el estado obtenido cuando }\mathcal{P}\text{ termina,}\\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\text{partiendo de }\left\Vert
x_{1},...,x_{n},\alpha_{1},...,\alpha_{m}\right\Vert
\end{array}
\]
Ahora si daremos la definicion precisa de funcion $\Sigma$-computable. Una
funcion $\Sigma$-mixta $f:S\subseteq\omega^{n}\times\Sigma^{\ast m}%
\rightarrow\omega$ sera llamada $\Sigma$\textit{-computable} si hay un
programa $\mathcal{P}$ tal que $f=\Psi_{\mathcal{P}}^{n,m,\#}$. En tal caso
diremos que la funcion $f$ es \textit{computada} por $\mathcal{P}$.
Analogamente una funcion $\Sigma$-mixta $f:S\subseteq\omega^{n}\times
\Sigma^{\ast m}\rightarrow\Sigma^{\ast}$ sera llamada $\Sigma$%
\textit{-computable} si hay un programa $\mathcal{P}$ tal que $f=\Psi
_{\mathcal{P}}^{n,m,\ast}$. En tal caso diremos que la funcion $f$ es
\textit{computada} por $\mathcal{P}$.

Algunos ejemplos:

\begin{enumerate}
\item[E$_{1}$] El programa%
\[%
\begin{array}
[c]{ll}%
\mathrm{L}2 & \mathrm{IF}\;\mathrm{N}1\neq0\;\mathrm{GOTO}\;\mathrm{L}1\\
& \mathrm{GOTO}\;\mathrm{L}2\\
\mathrm{L}1 & \mathrm{N}1\leftarrow\mathrm{N}1\dot{-}1
\end{array}
\]
computa la funcion $Pred$. Note que este programa tambien computa las
funciones $Pred\circ p_{1}^{n,m}$, para $n\geq1$ y $m\geq0$.

\item[E$_{2}$] Sea $\Sigma=\{\clubsuit,\triangle\}.$ El programa%
\[%
\begin{array}
[c]{ll}%
\mathrm{L}3 & \mathrm{IF}\;\mathrm{P}2\;\mathrm{BEGINS}\;\clubsuit
\;\mathrm{GOTO}\;\mathrm{L}1\\
& \mathrm{IF}\;\mathrm{P}2\;\mathrm{BEGINS}\;\triangle\;\mathrm{GOTO}%
\;\mathrm{L}2\\
& \mathrm{GOTO}\;\mathrm{L}4\\
\mathrm{L}1 & \mathrm{P}2\leftarrow\text{ }^{\curvearrowright}\mathrm{P}2\\
& \mathrm{P}1\leftarrow\mathrm{P}1\clubsuit\\
& \mathrm{GOTO}\;\mathrm{L}3\\
\mathrm{L}2 & \mathrm{P}2\leftarrow\text{ }^{\curvearrowright}\mathrm{P}2\\
& \mathrm{P}1\leftarrow\mathrm{P}1\triangle\\
& \mathrm{GOTO}\;\mathrm{L}3\\
\mathrm{L}4 & \mathrm{SKIP}%
\end{array}
\]
computa la funcion $\lambda\alpha\beta\left[  \alpha\beta\right]  .$
\end{enumerate}

Por supuesto para que el concepto de funcion $\Sigma$-computable tenga chance
de ser una modelizacion adecuada del concepto de funcion $\Sigma
$-efectivamente computable, tiene que ser cierto el siguiente resultado.

\begin{proposition}
Si $f$ es $\Sigma$-computable, entonces $f$ es $\Sigma$-efectivamente computable.
\end{proposition}

\begin{proof}
Supongamos por ejemplo que $f:S\subseteq\omega^{n}\times\Sigma^{\ast
m}\rightarrow\omega$ es computada por $\mathcal{P}\in\mathrm{Pro}^{\Sigma}$.
Un procedimiento efectivo que compute a $f$ puede consistir de realizar las
sucesivas instrucciones de $\mathcal{P}$ (partiendo de $\left\Vert
x_{1},...,x_{n},\alpha_{1},...,\alpha_{m}\right\Vert $) y eventualmente
terminar en caso de que nos toque realizar la instruccion $n(\mathcal{P})+1$,
y dar como salida el contenido de la variable $\mathrm{N}1$. Daremos a
continuacion una descripcion mas detallada de dicho procedimiento.

Fijemos primero un numero natural $k$ que sea mayor que $\max\{n,m\}$ y tal
que toda variable que ocurre en $\mathcal{P}$ este en la lista $\mathrm{N}%
1,...,\mathrm{N}\bar{k},\mathrm{P}1,...,\mathrm{P}\bar{k}$. Sea $\mathbb{P}$
el siguiente procedimiento efectivo:

- Conjunto de datos de entrada de $\mathbb{P}$ igual a $\omega^{n}\times
\Sigma^{\ast}{}^{m}$

- Conjunto de datos de salida de $\mathbb{P}$ contenido en $\omega$

- Funcionamiento:

Etapa 1

\noindent Asignar los siguientes valores a las variables $I,X_{1}%
,...,X_{k},A_{1},...,A_{k}$:%
\[%
\begin{array}
[c]{ccc}
& I\leftarrow1 & \\
X_{1}\leftarrow x_{1} &  & A_{1}\leftarrow\alpha_{1}\\
\vdots &  & \vdots\\
X_{n}\leftarrow x_{n} &  & A_{m}\leftarrow\alpha_{m}\\
X_{n+1}\leftarrow0 &  & A_{m+1}\leftarrow\varepsilon\\
\vdots &  & \vdots\\
X_{k}\leftarrow0 &  & A_{k}\leftarrow\varepsilon
\end{array}
\]


Etapa 2

\noindent Asignar:

$I\leftarrow$ 1er coordenada de $S_{\mathcal{P}}(I,(X_{1},...,X_{k}%
,0,...),(A_{1},...,A_{k},\varepsilon,...))$

\noindent Para $i=1,...,k$:

$X_{i}\leftarrow$ $i$-esima coordenada de la segunda coordenada de
$S_{\mathcal{P}}(I,(X_{1},...,X_{k},0,...),(A_{1},...,A_{k},\varepsilon,...))$

$A_{i}\leftarrow$ $i$-esima coordenada de la tercer coordenada de
$S_{\mathcal{P}}(I,(X_{1},...,X_{k},0,...),(A_{1},...,A_{k},\varepsilon,...))$

Etapa 3

\noindent Si $I=n(\mathcal{P})+1$, entonces dar $X_{1}$ como salida y
detenerse. En caso contrario ir a Etapa 2.

Se deja al lector corroborar que $\mathbb{P}$ es efectivo.
\end{proof}

\bigskip

Sin envargo nuestro modelo imperativo de funcion $\Sigma$-efectivamente
computable todavia podria no ser correcto ya que podria pasar que haya una
funcion $\Sigma$-mixta que sea computada por un procedimiento efectivo pero
que no exista un programa de $\mathcal{S}^{\Sigma}$ que la compute. En otras
palabras el modelo imperativo o Neumanniano podria ser incompleto. Por
supuesto este no es el caso y los desarrollos que veremos mas adelante nos
convenceran de que el paradigma imperativo es completo.

\bigskip

\subsubsection{Macros}

Supongamos que estamos escribiendo un programa $\mathcal{P}$ de $\mathcal{S}%
^{\Sigma}$ con el objeto de que realice cierta tarea. Supongamos ademas que
nos vendria muy bien para nuestros propositos poder usar una instruccion%
\[
\mathrm{N}5\leftarrow\mathrm{N}16+\mathrm{N}3
\]
la cual por supuesto al correr el programa, deberia producir el efecto de
dejar en la variable $\mathrm{N}5$ la suma de los contenidos de las variables
$\mathrm{N}16$ y $\mathrm{N}3$, sin modificar el contenido de las variables
distintas a $\mathrm{N}5$. Lamentablemente no tenemos en $\mathcal{S}^{\Sigma
}$ este tipo de instruccion pero podriamos reemplazarla por el siguiente
programa%
\[%
\begin{array}
[c]{ll}
& \mathrm{N}1111\leftarrow\mathrm{N}16\\
& \mathrm{N}2222\leftarrow\mathrm{N}3\\
& \mathrm{N}5\leftarrow\mathrm{N}1111\\
\mathrm{L}1000 & \mathrm{IF}\;\mathrm{N}2222\neq0\;\mathrm{GOTO}%
\;\mathrm{L}2000\\
& \mathrm{GOTO}\;\mathrm{L}3000\\
\mathrm{L}2000 & \mathrm{N}2222\leftarrow\mathrm{N}2222\dot{-}1\\
& \mathrm{N}5\leftarrow\mathrm{N}5+1\\
& \mathrm{GOTO}\;\mathrm{L}1000\\
\mathrm{L}3000 & \mathrm{SKIP}%
\end{array}
\]
donde las variables $\mathrm{N}1111$, $\mathrm{N}2222$ y los labels
$\mathrm{L}1000$, $\mathrm{L}2000$, $\mathrm{L}3000$ solo seran usados aqui,
es decir no apareceran en el resto de nuestro programa $\mathcal{P}$. Notese
que este programa cuando es corrido termina dejando en la variable
$\mathrm{N}5$ la suma de los contenidos de las variables $\mathrm{N}16$ y
$\mathrm{N}3$ y modifica el contenido de las variables $\mathrm{N}1111$ y
$\mathrm{N}2222$, lo cual no traera problemas ya que $\mathrm{N}1111$ y
$\mathrm{N}2222$ no se usan en el resto de $\mathcal{P}$. La variables
$\mathrm{N}1111$ y $\mathrm{N}2222$ son auxiliares y se usan justamente para
preservar el valor de las variables $\mathrm{N}16$ y $\mathrm{N}3$ ya que
ellas son variables protagonistas de nuestro programa $\mathcal{P}$ y en esta
instancia no queremos alterar su contenido sino solo realizar la asignacion
$\mathrm{N}5\leftarrow\mathrm{N}16+\mathrm{N}3$. Dejamos al lector explicar
por que es necesario para que la simulacion sea correcta que los labels
$\mathrm{L}1000$, $\mathrm{L}2000$ y $\mathrm{L}3000$ no sean usados en el
resto de $\mathcal{P}$.

Es decir el programa anterior simula la instruccion $\mathrm{N}5\leftarrow
\mathrm{N}16+\mathrm{N}3$ que no podiamos usar por no ser una instruccion de
$\mathcal{S}^{\Sigma}$, con un costo bastante bajo, es decir el costo de
convenir en no usar en el resto de $\mathcal{P}$ las variables $\mathrm{N}%
1111$ y $\mathrm{N}2222$ ni los labels $\mathrm{L}1000$, $\mathrm{L}2000$ y
$\mathrm{L}3000$.

Ahora supongamos que seguimos escribiendo el programa $\mathcal{P}$ y nos hace
falta simular la instruccion $\mathrm{N}20\leftarrow\mathrm{N}1+\mathrm{N}14$.
Entonces es claro que podriamos modificar el programa que simulaba
$\mathrm{N}5\leftarrow\mathrm{N}16+\mathrm{N}3$ haciendole reemplazos
adecuados a sus variables y labels. Por ejemplo podriamos escribir%
\[%
\begin{array}
[c]{ll}
& \mathrm{N}9999\leftarrow\mathrm{N}1\\
& \mathrm{N}8888\leftarrow\mathrm{N}14\\
& \mathrm{N}20\leftarrow\mathrm{N}9999\\
\mathrm{L}1001 & \mathrm{IF}\;\mathrm{N}8888\neq0\;\mathrm{GOTO}%
\;\mathrm{L}2002\\
& \mathrm{GOTO}\;\mathrm{L}3003\\
\mathrm{L}2002 & \mathrm{N}8888\leftarrow\mathrm{N}8888\dot{-}1\\
& \mathrm{N}20\leftarrow\mathrm{N}20+1\\
& \mathrm{GOTO}\;\mathrm{L}1001\\
\mathrm{L}3003 & \mathrm{SKIP}%
\end{array}
\]
donde $\mathrm{N}9999$, $\mathrm{N}8888$, $\mathrm{L}1001$, $\mathrm{L}2002$ y
$\mathrm{L}3003$ solo seran usados aqui, es decir no apareceran en el resto de
nuestro programa $\mathcal{P}$.

Consideremos el siguiente "molde" que llamaremos $M$%
\[%
\begin{array}
[c]{ll}
& \mathrm{V}4\leftarrow\mathrm{V}2\\
& \mathrm{V}5\leftarrow\mathrm{V}3\\
& \mathrm{V}1\leftarrow\mathrm{V}4\\
\mathrm{A}1 & \mathrm{IF}\;\mathrm{V}5\neq0\;\mathrm{GOTO}\;\mathrm{A}2\\
& \mathrm{GOTO}\;\mathrm{A}3\\
\mathrm{A}2 & \mathrm{V}5\leftarrow\mathrm{V}5\dot{-}1\\
& \mathrm{V}1\leftarrow\mathrm{V}1+1\\
& \mathrm{GOTO}\;\mathrm{A}1\\
\mathrm{A}3 & \mathrm{SKIP}%
\end{array}
\]
Como puede notarse, cuando reemplazamos en $M$

\begin{enumerate}
\item[-] cada ocurrencia de $\mathrm{V}1$ por $\mathrm{N}5$

\item[-] cada ocurrencia de $\mathrm{V}2$ por $\mathrm{N}16$

\item[-] cada ocurrencia de $\mathrm{V}3$ por $\mathrm{N}3$

\item[-] cada ocurrencia de $\mathrm{V}4$ por $\mathrm{N}1111$

\item[-] cada ocurrencia de $\mathrm{V}5$ por $\mathrm{N}2222$

\item[-] cada ocurrencia de $\mathrm{A}1$ por $\mathrm{L}1000$

\item[-] cada ocurrencia de $\mathrm{A}2$ por $\mathrm{L}2000$

\item[-] cada ocurrencia de $\mathrm{A}3$ por $\mathrm{L}3000$
\end{enumerate}

\noindent obtenemos el programa que simulaba la instruccion $\mathrm{N}%
5\leftarrow\mathrm{N}16+\mathrm{N}3$ dentro de $\mathcal{P}$. Similarmente,
cuando reemplazamos en $M$

\begin{enumerate}
\item[-] cada ocurrencia de $\mathrm{V}1$ por $\mathrm{N}20$

\item[-] cada ocurrencia de $\mathrm{V}2$ por $\mathrm{N}1$

\item[-] cada ocurrencia de $\mathrm{V}3$ por $\mathrm{N}14$

\item[-] cada ocurrencia de $\mathrm{V}4$ por $\mathrm{N}9999$

\item[-] cada ocurrencia de $\mathrm{V}5$ por $\mathrm{N}8888$

\item[-] cada ocurrencia de $\mathrm{A}1$ por $\mathrm{L}1001$

\item[-] cada ocurrencia de $\mathrm{A}2$ por $\mathrm{L}2002$

\item[-] cada ocurrencia de $\mathrm{A}3$ por $\mathrm{L}3003$
\end{enumerate}

\noindent obtenemos el programa que simulaba la instruccion $\mathrm{N}%
20\leftarrow\mathrm{N}1+\mathrm{N}14$ dentro de $\mathcal{P}$. La practicidad
de tener el molde $M$ cae de maduro. Ahora en caso de necesitar una
instruccion del tipo $\mathrm{N}\bar{k}\leftarrow\mathrm{N}\bar{n}%
+\mathrm{N}\bar{m}$ solo tenemos que reemplazar en $M$

\begin{enumerate}
\item[-] cada ocurrencia de $\mathrm{V}1$ por $\mathrm{N}\bar{k}$

\item[-] cada ocurrencia de $\mathrm{V}2$ por $\mathrm{N}\bar{n}$

\item[-] cada ocurrencia de $\mathrm{V}3$ por $\mathrm{N}\bar{m}$
\end{enumerate}

\noindent y reemplazar las "variables" $\mathrm{V}4$ y $\mathrm{V}5$ y los
"labels" $\mathrm{A}1$, $\mathrm{A}2$ y $\mathrm{A}3$, por dos variables
concretas y tres labels concretos que no se usen en el programa que estamos
realizando. El programa asi obtenido simulara a la instruccion $\mathrm{N}%
\bar{k}\leftarrow\mathrm{N}\bar{n}+\mathrm{N}\bar{m}$.

En la gerga computacional el molde $M$ suele llamarse \textit{macro} y los
programas obtenidos luego de realizar los reemplazos son llamados
\textit{expansiones de }$M$. Notese que $Ti(M)=\mathrm{PALABRA}$ ya que, como
en el caso de los programas, escribimos a $M$ linea por linea para facilitar
su manejo pero en realidad es una sola palabra, a saber:%
\[
\mathrm{V}1\mathrm{\leftarrow}\text{V}2\mathrm{V}4\mathrm{\leftarrow}%
\text{V}3\mathrm{A}1\mathrm{IFV}4\mathrm{\neq}0\mathrm{GOTOA}2\mathrm{GOTOA}%
3\mathrm{A}2\mathrm{V}4\mathrm{\leftarrow}\text{V}4\mathrm{\dot{-}}%
1\mathrm{V}1\mathrm{\leftarrow}\text{V}1\mathrm{+}1\mathrm{GOTOA}%
1\mathrm{A}3\mathrm{SKIP}%
\]
Es decir, como objeto matematico, $M$ es simplemente una palabra. A las
palabras de la forma $\mathrm{V}\bar{n}$, con $n\in\mathbf{N}$, las llamaremos
\textit{variables numericas de macro}. A las palabras de la forma
$\mathrm{W}\bar{n}$, con $n\in\mathbf{N}$, las llamaremos \textit{variables
alfabeticas de macro} y a las palabras de la forma $\mathrm{A}\bar{n}$, con
$n\in\mathbf{N}$, las llamaremos \textit{labels de macro}. Nuestro macro $M$
no tiene variables alfabeticas de macro pero otros macros por supuesto pueden
tener este tipo de variables.

Las variables $\mathrm{V}1$, $\mathrm{V}2$ y $\mathrm{V}3$ son llamadas
\textit{variables oficiales} de $M$ ya que son las variables que seran
reemplazadas por variables que son protagonistas dentro del programa
$\mathcal{P}$ que usara la expansion de $M$. Las palabras $\mathrm{V}4$ y
$\mathrm{V}5$ son llamadas \textit{variables auxiliares} de $M$ ya que seran
reemplazadas por variables que se usaran solo dentro de la expansion y no
intervienen en la "trama" del programa $\mathcal{P}$. Tambien $\mathrm{A}1$,
$\mathrm{A}2$ y $\mathrm{A}3$ son llamados \textit{labels auxiliares} de $M$
ya que son usados solo para su funcionamiento interno y no tienen vinculacion
con los labels del programa $\mathcal{P}$.

En el siguiente ejemplo veremos un macro que tiene un label que no es auxiliar
sino oficial. Sea $\Sigma=\{@,!\}$. Supongamos que estamos escribiendo un
programa $\mathcal{P}^{\prime}$ y nos hace falta simular instrucciones de la
forma%
\[
\mathrm{IF}\;\left\vert \mathrm{P}\bar{n}\right\vert \leq\mathrm{N}\bar
{m}\ \mathrm{GOTO}\;\mathrm{L}\bar{k}%
\]
(por supuesto estas instrucciones no pertenecen al lenguaje $\mathcal{S}%
^{\Sigma}$ pero deberia quedar claro como funcionan). Entonces podemos tomar
el macro $M^{\prime}$:%
\[%
\begin{array}
[c]{ll}
& \mathrm{W}2\leftarrow\mathrm{W}1\\
& \mathrm{V}2\leftarrow\mathrm{V}1\\
\mathrm{A}4 & \mathrm{IF}\;\mathrm{W}2\;\mathrm{BEGINS}\;@\;\mathrm{GOTO}%
\;\mathrm{A}2\\
& \mathrm{IF}\;\mathrm{W}2\;\mathrm{BEGINS}\;!\;\mathrm{GOTO}\;\mathrm{A}2\\
& \mathrm{GOTO}\;\mathrm{A}1\\
\mathrm{A}2 & \mathrm{IF}\;\mathrm{V}2\neq0\;\mathrm{GOTO}\;\mathrm{A}3\\
& \mathrm{GOTO}\;\mathrm{A}5\\
\mathrm{A}3 & \mathrm{W}2\leftarrow^{\curvearrowright}\mathrm{W}2\\
& \mathrm{V}2\leftarrow\mathrm{V}2\dot{-}1\\
& \mathrm{GOTO}\;\mathrm{A}4\\
\mathrm{A}5 & \mathrm{SKIP}%
\end{array}
\]
el cual tiene

\begin{enumerate}
\item[-] variables oficiales $\mathrm{W}1$ y $\mathrm{V}1$ (correspondientes a
$\mathrm{P}\bar{n}$ y $\mathrm{N}\bar{m}$)

\item[-] variables auxiliares $\mathrm{W}2$ y $\mathrm{V}2$

\item[-] labels auxiliares $\mathrm{A}2$, $\mathrm{A}3$, $\mathrm{A}4$ y
$\mathrm{A}5$

\item[-] un label oficial $\mathrm{A}1$ (correspondiente a $\mathrm{L}\bar{k}
$)
\end{enumerate}

\noindent Una descripcion intuitiva del macro $M^{\prime}$ seria%
\[
\mathrm{IF}\;\left\vert \mathrm{W}1\right\vert \leq\mathrm{V}1\ \mathrm{GOTO}%
\;\mathrm{A}1
\]
Notese que en las primeras dos lineas el macro $M^{\prime}$ guarda los valores
de las variables oficiales $\mathrm{W}1$ y $\mathrm{V}1$ en las variables
auxiliares $\mathrm{W}2$ y $\mathrm{V}2$, y sigue trabajando con las
auxiliares. Esto es para preservar el valor de las variables oficiales. Dado
que $\Sigma=\{@,!\}$, las dos siguientes lineas sirven para decidir si el
contenido de $\mathrm{W}2$ es $\varepsilon$ o no. Dejamos al lector entender
el resto del funcionamiento de $M^{\prime}$.

Para dar un ejemplo de como usariamos a $M^{\prime}$, supongamos que para
seguir escribiendo nuestro programa $\mathcal{P}^{\prime}$ nos hace falta
simular la instruccion%
\[
\mathrm{IF}\;\left\vert \mathrm{P}5\right\vert \leq\mathrm{N}14\ \mathrm{GOTO}%
\;\mathrm{L}1
\]
y supongamos que las variables $\mathrm{P}1000$ y $\mathrm{N}1000$ y los
labels $\mathrm{L}6666$, $\mathrm{L}7777$, $\mathrm{L}8888$ y $\mathrm{L}9999
$ no se usaron hasta el momento en $\mathcal{P}^{\prime}$. Entonces podemos
reemplazar en $M^{\prime}$

\begin{enumerate}
\item[-] cada ocurrencia de $\mathrm{W}1$ por $\mathrm{P}5$

\item[-] cada ocurrencia de $\mathrm{V}1$ por $\mathrm{N}14$

\item[-] cada ocurrencia de $\mathrm{W}2$ por $\mathrm{P}1000$

\item[-] cada ocurrencia de $\mathrm{V}2$ por $\mathrm{N}1000$

\item[-] cada ocurrencia de $\mathrm{A}1$ por $\mathrm{L}1$

\item[-] cada ocurrencia de $\mathrm{A}2$ por $\mathrm{L}6666$

\item[-] cada ocurrencia de $\mathrm{A}3$ por $\mathrm{L}7777$

\item[-] cada ocurrencia de $\mathrm{A}4$ por $\mathrm{L}8888$

\item[-] cada ocurrencia de $\mathrm{A}5$ por $\mathrm{L}9999$
\end{enumerate}

\noindent y la expansion de $M^{\prime}$ asi obtenida simulara la instruccion
$\mathrm{IF}\;\left\vert \mathrm{P}5\right\vert \leq\mathrm{N}%
14\ \mathrm{GOTO}\;\mathrm{L}1$. Cabe destacar que para asegurarnos que la
simulacion funcione, tambien deberemos no usar en el resto de $\mathcal{P}%
^{\prime}$ las variables $\mathrm{P}1000$ y $\mathrm{N}1000$ y los labels
$\mathrm{L}6666$, $\mathrm{L}7777$, $\mathrm{L}8888$ y $\mathrm{L}9999$.

Es decir $M^{\prime}$ funciona como un molde con el cual haciendo reemplazos
adecuados podemos simular cualquier instruccion del tipo $\mathrm{IF}%
\;\left\vert \mathrm{P}\bar{n}\right\vert \leq\mathrm{N}\bar{m}\ \mathrm{GOTO}%
\;\mathrm{L}\bar{k}$, con $n,m,k\in\mathbf{N}$.

Deberia quedar claro el caracter oficial del label $\mathrm{A}1$ en
$M^{\prime}$ ya que el label por el que se lo reemplaza para hacer la
expansion es uno de los labels protagonistas del programa que se esta escribiendo.

Cabe destacar que las expansiones de $M^{\prime}$ no son programas ya que si
bien son concatenaciones de instrucciones, no cumplen la ley de los GOTO
(llamada (G) en la definicion de programa) respecto del label que reemplazo a
$\mathrm{A}1$.

\bigskip

\textbf{Nota:} Siempre supondremos que la primera instruccion de los macros no
es labelada. Esto es porque muchas veces cuando expandamos un macro nos
interesara labelar la primera instruccion de dicha expansion. Por supuesto,
esto es facil de conseguir ya que si $M$ es un macro, entonces $\mathrm{SKIP}%
M$ es tambien un macro que posee las mismas propiedades.

\bigskip

Como hemos visto recien hay dos tipos de macros:

\begin{enumerate}
\item[-] los de asignacion que cuando son expandidos nos dan un programa que
simula la asignacion a una variable dada del resultado de aplicar una funcion
a los contenidos de ciertas otras variables; y

\item[-] los de tipo IF que cuando son expandidos nos dan un programa salvo
por la ley (G), el cual direcciona al label que fue a reemplazar a
$\mathrm{A}1$ cuando se cumple cierta propiedad (predicado) relativa a los
contenidos de las variables que fueron a reemplazar a las variables oficiales.
\end{enumerate}

\bigskip

\paragraph{Ejemplo concreto de uso de macros}

Ya vimos recien que la palabra%
\[%
\begin{array}
[c]{ll}
& \mathrm{V}4\leftarrow\mathrm{V}2\\
& \mathrm{V}5\leftarrow\mathrm{V}3\\
& \mathrm{V}1\leftarrow\mathrm{V}4\\
\mathrm{A}1 & \mathrm{IF}\;\mathrm{V}5\neq0\;\mathrm{GOTO}\;\mathrm{A}2\\
& \mathrm{GOTO}\;\mathrm{A}3\\
\mathrm{A}2 & \mathrm{V}5\leftarrow\mathrm{V}5\dot{-}1\\
& \mathrm{V}1\leftarrow\mathrm{V}1+1\\
& \mathrm{GOTO}\;\mathrm{A}1\\
\mathrm{A}3 & \mathrm{SKIP}%
\end{array}
\]
es un macro que sirve para simular instrucciones de la forma $\mathrm{N}%
\bar{k}\leftarrow\mathrm{N}\bar{n}+\mathrm{N}\bar{m}$. Notemos que este macro
es de asignacion ya que cuando es expandido nos da un programa que simula la
asignacion a una variable dada del resultado de aplicar una funcion a los
contenidos de ciertas otras variables. En este caso la funcion es
$SUMA=\lambda xy[x+y]$ por lo cual usaremos $\left[  \mathrm{V}1\leftarrow
SUMA(\mathrm{V}2,\mathrm{V}3)\right]  $ para denotar a dicho macro. Usaremos
este macro para dar un programa $\mathcal{P}$ que compute a la funcion
$\lambda xy[x.y]$. Notese que podemos tomar $\mathcal{P}$ igual al siguiente programa%

\[%
\begin{array}
[c]{ll}%
\mathrm{L}1 & \mathrm{IF}\;\mathrm{N}2\neq0\;\mathrm{GOTO}\;\mathrm{L}2\\
& \mathrm{GOTO}\;\mathrm{L}3\\
\mathrm{L}2 & \left[  \mathrm{N}3\leftarrow SUMA(\mathrm{N}3,\mathrm{N}%
1)\right] \\
& \mathrm{N}2\leftarrow\mathrm{N}2\dot{-}1\\
& \mathrm{GOTO}\;\mathrm{L}1\\
\mathrm{L}3 & \mathrm{N}1\leftarrow\mathrm{N}3
\end{array}
\]
donde $\left[  \mathrm{N}3\leftarrow SUMA(\mathrm{N}3,\mathrm{N}1)\right]  $
es una expansion del macro $\left[  \mathrm{V}1\leftarrow SUMA(\mathrm{V}%
2,\mathrm{V}3)\right]  $ hecha haciendo el reemplazo de las variables
oficiales $\mathrm{V}1,\mathrm{V}2$ y $\mathrm{V}3$ por $\mathrm{N}%
3,\mathrm{N}3$ y $\mathrm{N}1$, respectivamente, y haciendo reemplazos
adecuados de sus variables y labels auxiliares. Hay muchas formas de hacer los
reemplazos de variables y labels auxiliares pero en general no lo
especificaremos explicitamente cuando expandamos un macro ya que es facil
imaginar como hacerlo en funcion del programa que estemos realizando. Por
ejemplo en el caso de $\mathcal{P}$ podriamos hacer los siguientes reemplazos:

\begin{enumerate}
\item[-] cada ocurrencia de $\mathrm{V}4$ por $\mathrm{N}1111$

\item[-] cada ocurrencia de $\mathrm{V}5$ por $\mathrm{N}2222$

\item[-] cada ocurrencia de $\mathrm{A}1$ por $\mathrm{L}1000$

\item[-] cada ocurrencia de $\mathrm{A}2$ por $\mathrm{L}2000$

\item[-] cada ocurrencia de $\mathrm{A}3$ por $\mathrm{L}3000$
\end{enumerate}

\noindent y claramente esto no afectara la "logica" o "idea" de nuestro
programa $\mathcal{P}$. De esta forma la expansion $\left[  \mathrm{N}%
3\leftarrow SUMA(\mathrm{N}3,\mathrm{N}1)\right]  $ es el siguiente programa:%
\[%
\begin{array}
[c]{ll}
& \mathrm{N}1111\leftarrow\mathrm{N}3\\
& \mathrm{N}2222\leftarrow\mathrm{N}1\\
& \mathrm{N}3\leftarrow\mathrm{N}1111\\
\mathrm{L}1000 & \mathrm{IF}\;\mathrm{N}2222\neq0\;\mathrm{GOTO}%
\;\mathrm{L}2000\\
& \mathrm{GOTO}\;\mathrm{L}3000\\
\mathrm{L}2000 & \mathrm{N}2222\leftarrow\mathrm{N}2222\dot{-}1\\
& \mathrm{N}3\leftarrow\mathrm{N}3+1\\
& \mathrm{GOTO}\;\mathrm{L}1000\\
\mathrm{L}3000 & \mathrm{SKIP}%
\end{array}
\]
el cual por supuesto esta escrito con espacios y en forma vertical pero es una
mera palabra. Tenemos entonces que $\mathcal{P}$ es el programa:%

\[%
\begin{array}
[c]{ll}%
\mathrm{L}1 & \mathrm{IF}\;\mathrm{N}2\neq0\;\mathrm{GOTO}\;\mathrm{L}2\\
& \mathrm{GOTO}\;\mathrm{L}3\\
\mathrm{L}2 & \mathrm{N}1111\leftarrow\mathrm{N}1\\
& \mathrm{N}2222\leftarrow\mathrm{N}3\\
& \mathrm{N}3\leftarrow\mathrm{N}1111\\
\mathrm{L}1000 & \mathrm{IF}\;\mathrm{N}2222\neq0\;\mathrm{GOTO}%
\;\mathrm{L}2000\\
& \mathrm{GOTO}\;\mathrm{L}3000\\
\mathrm{L}2000 & \mathrm{N}2222\leftarrow\mathrm{N}2222\dot{-}1\\
& \mathrm{N}3\leftarrow\mathrm{N}3+1\\
& \mathrm{GOTO}\;\mathrm{L}1000\\
\mathrm{L}3000 & \mathrm{SKIP}\\
& \mathrm{N}2\leftarrow\mathrm{N}2\dot{-}1\\
& \mathrm{GOTO}\;\mathrm{L}1\\
\mathrm{L}3 & \mathrm{N}1\leftarrow\mathrm{N}3
\end{array}
\]
el cual por supuesto esta escrito con espacios y en forma vertical pero es una
mera palabra.

\bigskip

\paragraph{Macros asociados a funciones $\Sigma$-computables}

Dada una funcion $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast m}%
\rightarrow\omega$, usaremos%
\[
\left[  \mathrm{V}\overline{n+1}\leftarrow f(\mathrm{V}1,...,\mathrm{V}\bar
{n},\mathrm{W}1,...,\mathrm{W}\bar{m})\right]
\]
para denotar un macro $M$ el cual cumpla las siguientes propiedades. Cabe
destacar que no siempre existira dicho macro, es decir solo para ciertas
funciones $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow\omega$
habra un tal macro.

\begin{enumerate}
\item[(1)] Las variables oficiales de $M$ son $\mathrm{V}1,...,\mathrm{V}%
\bar{n},\mathrm{V}\overline{n+1},\mathrm{W}1,...,\mathrm{W}\bar{m}$

\item[(2)] $M$ no tiene labels oficiales

\item[(3)] Si reemplazamos:

\begin{enumerate}
\item las variables oficiales de $M$ (i.e. $\mathrm{V}1,...,\mathrm{V}\bar
{n},\mathrm{V}\overline{n+1},\mathrm{W}1,...,\mathrm{W}\bar{m}$) por variables
concretas%
\[
\mathrm{N}\overline{k_{1}},...,\mathrm{N}\overline{k_{n}},\mathrm{N}%
\overline{k_{n+1}},\mathrm{P}\overline{j_{1}},...,\mathrm{P}\overline{j_{m}}%
\]
(elejidas libremente, es decir los numeros $k_{1},...,k_{n+1},j_{1},...,j_{m}
$ son cualesquiera)

\item las variables auxiliares de $M$ por variables concretas (distintas de a
dos) y NO pertenecientes a la lista $\mathrm{N}\overline{k_{1}},...,\mathrm{N}%
\overline{k_{n}},\mathrm{N}\overline{k_{n+1}},\mathrm{P}\overline{j_{1}%
},...,\mathrm{P}\overline{j_{m}}$

\item los labels auxiliares de $M$ por labels concretos (distintos de a dos)
\end{enumerate}

\noindent Entonces la palabra asi obtenida es un programa de $\mathcal{S}%
^{\Sigma}$ que denotaremos con%
\[
\left[  \mathrm{N}\overline{k_{n+1}}\leftarrow f(\mathrm{N}\overline{k_{1}%
},...,\mathrm{N}\overline{k_{n}},\mathrm{P}\overline{j_{1}},...,\mathrm{P}%
\overline{j_{m}})\right]
\]
el cual debe tener la siguiente propiedad:

\begin{enumerate}
\item[-] Si hacemos correr $\left[  \mathrm{N}\overline{k_{n+1}}\leftarrow
f(\mathrm{N}\overline{k_{1}},...,\mathrm{N}\overline{k_{n}},\mathrm{P}%
\overline{j_{1}},...,\mathrm{P}\overline{j_{m}})\right]  $ partiendo de un
estado $e$ que le asigne a las variables $\mathrm{N}\overline{k_{1}%
},...,\mathrm{N}\overline{k_{n}},\mathrm{P}\overline{j_{1}},...,\mathrm{P}%
\overline{j_{m}}$ valores $x_{1},...,x_{n},\alpha_{1},...,\alpha_{m}$,
entonces independientemente de los valores que les asigne $e$ al resto de las
variables (incluidas las que fueron a reemplazar a las variables auxiliares de
$M$) se dara que

\begin{enumerate}
\item si $(x_{1},...,x_{n},\alpha_{1},...,\alpha_{m})\notin D_{f}$, entonces
$\left[  \mathrm{N}\overline{k_{n+1}}\leftarrow f(\mathrm{N}\overline{k_{1}%
},...,\mathrm{N}\overline{k_{n}},\mathrm{P}\overline{j_{1}},...,\mathrm{P}%
\overline{j_{m}})\right]  $ no se detiene

\item si $(x_{1},...,x_{n},\alpha_{1},...,\alpha_{m})\in D_{f}$, entonces
$\left[  \mathrm{N}\overline{k_{n+1}}\leftarrow f(\mathrm{N}\overline{k_{1}%
},...,\mathrm{N}\overline{k_{n}},\mathrm{P}\overline{j_{1}},...,\mathrm{P}%
\overline{j_{m}})\right]  $ se detiene (i.e. intenta realizar la siguiente a
su ultima instrucion) y llega a un estado $e^{\prime}$ el cual cumple:

\begin{enumerate}
\item $e^{\prime}$ le asigna a $\mathrm{N}\overline{k_{n+1}}$ el valor
$f(x_{1},...,x_{n},\alpha_{1},...,\alpha_{m})$

\item $e^{\prime}$ solo puede diferir de $e$ en los valores que le asigna a
$\mathrm{N}\overline{k_{n+1}}$ o a las variables que fueron a reemplazar a las
variables auxiliares de $M$. Al resto de las variables, incluidas
$\mathrm{N}\overline{k_{1}},...,\mathrm{N}\overline{k_{n}},\mathrm{P}%
\overline{j_{1}},...,\mathrm{P}\overline{j_{m}}$ no las modifica (salvo en el
caso de que alguna $\mathrm{N}\overline{k_{i}}$ sea la variable $\mathrm{N}%
\overline{k_{n+1}}$, situacion en la cual el valor final de la variable
$\mathrm{N}\overline{k_{i}}$ sera $f(x_{1},...,x_{n},\alpha_{1},...,\alpha
_{m})$)
\end{enumerate}
\end{enumerate}
\end{enumerate}
\end{enumerate}

\noindent El programa $\left[  \mathrm{N}\overline{k_{n+1}}\leftarrow
f(\mathrm{N}\overline{k_{1}},...,\mathrm{N}\overline{k_{n}},\mathrm{P}%
\overline{j_{1}},...,\mathrm{P}\overline{j_{m}})\right]  $ es comunmente
llamado la expansion del macro $\left[  \mathrm{V}\overline{n+1}\leftarrow
f(\mathrm{V}1,...,\mathrm{V}\bar{n},\mathrm{W}1,...,\mathrm{W}\bar{m})\right]
$ con respecto a la eleccion de variables y labels realizada.

\bigskip

Tambien, dada una funcion $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast
m}\rightarrow\Sigma^{\ast}$, con%
\[
\left[  \mathrm{W}\overline{m+1}\leftarrow f(\mathrm{V}1,...,\mathrm{V}\bar
{n},\mathrm{W}1,...,\mathrm{W}\bar{m})\right]
\]
denotaremos un macro el cual cumpla condiciones analogas a las descriptas
recien. Dejamos al lector escribirlas en detalle para este caso.

\begin{proposition}
\label{macro funciones}

\begin{enumerate}
\item[(a)] Sea $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow
\omega$ una funcion $\Sigma$\textit{-}computable. Entonces en $\mathcal{S}%
^{\Sigma}$ hay un macro%
\[
\left[  \mathrm{V}\overline{n+1}\leftarrow f(\mathrm{V}1,...,\mathrm{V}\bar
{n},\mathrm{W}1,...,\mathrm{W}\bar{m})\right]
\]


\item[(b)] Sea $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow
\Sigma^{\ast}$ una funcion $\Sigma$-computable. Entonces en $\mathcal{S}%
^{\Sigma}$ hay un macro%
\[
\left[  \mathrm{W}\overline{m+1}\leftarrow f(\mathrm{V}1,...,\mathrm{V}\bar
{n},\mathrm{W}1,...,\mathrm{W}\bar{m})\right]
\]

\end{enumerate}
\end{proposition}

\begin{proof}
Probaremos (b) La prueba de (a) es similar. Sea $\mathcal{P}$ un programa que
compute a $f$. Tomemos un $k$ tal que $k\geq n,m$ y tal que todas las
variables y labels de $\mathcal{P}$ estan en el conjunto%
\[
\{\mathrm{N}1,...,\mathrm{N}\bar{k},\mathrm{P}1,...,\mathrm{P}\bar
{k},\mathrm{L}1,...,\mathrm{L}\bar{k}\}\text{.}%
\]
Sea $\mathcal{P}^{\prime}$ la palabra que resulta de reemplazar en
$\mathcal{P}$:

\begin{enumerate}
\item[-] la variable $\mathrm{N}\overline{j}$ por $\mathrm{V}\overline{n+j}$,
para cada $j=1,...,k$

\item[-] la variable $\mathrm{P}\overline{j}$ por $\mathrm{W}\overline{m+j}$,
para cada $j=1,...,k$

\item[-] el label $\mathrm{L}\overline{j}$ por $\mathrm{A}\overline{j}$, para
cada $j=1,...,k$
\end{enumerate}

Notese que%
\[%
\begin{array}
[c]{l}%
\mathrm{V}\overline{n+1}\leftarrow\mathrm{V}1\\
\ \ \ \ \ \ \ \ \ \vdots\\
\mathrm{V}\overline{n+n}\leftarrow\mathrm{V}\overline{n}\\
\mathrm{V}\overline{n+n+1}\leftarrow0\\
\ \ \ \ \ \ \ \ \ \vdots\\
\mathrm{V}\overline{n+k}\leftarrow0\\
\mathrm{W}\overline{m+1}\leftarrow\mathrm{W}1\\
\ \ \ \ \ \ \ \ \ \vdots\\
\mathrm{W}\overline{m+m}\leftarrow\mathrm{W}\overline{m}\\
\mathrm{W}\overline{m+m+1}\leftarrow\varepsilon\\
\ \ \ \ \ \ \ \ \ \vdots\\
\mathrm{W}\overline{m+k}\leftarrow\varepsilon\\
\mathcal{P}^{\prime}%
\end{array}
\]
es el macro buscado, el cual tendra sus variables auxiliares y labels en la
lista%
\[
\mathrm{V}\overline{n+1},...,\mathrm{V}\overline{n+k},\mathrm{W}\overline
{m+2},...,\mathrm{W}\overline{m+k},\mathrm{A}1,...,\mathrm{A}\overline{k}.
\]

\end{proof}

\bigskip

Dejamos al lector probar la resiproca de la proposicion anterior, es decir que
si $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow\omega$ es tal
que en $\mathcal{S}^{\Sigma}$ hay un macro%
\[
\left[  \mathrm{V}\overline{n+1}\leftarrow f(\mathrm{V}1,...,\mathrm{V}\bar
{n},\mathrm{W}1,...,\mathrm{W}\bar{m})\right]
\]
entonces $f$ es $\Sigma$-computable

\bigskip

\paragraph{Macros asociados a predicados $\Sigma$-computables}

Dado un predicado $P:D_{P}\subseteq\omega^{n}\times\Sigma^{\ast m}%
\rightarrow\omega$, usaremos%
\[
\left[  \mathrm{IF}\;P(\mathrm{V}1,...,\mathrm{V}\bar{n},\mathrm{W}%
1,...,\mathrm{W}\bar{m})\;\mathrm{GOTO}\;\mathrm{A}1\right]
\]
para denotar un macro $M$ el cual cumpla las siguientes propiedades. Cabe
destacar que no siempre existira dicho macro, es decir solo para ciertos
predicados $P:D_{P}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow\omega$
habra un tal macro.

\begin{enumerate}
\item[(1)] Las variables oficiales de $M$ son $\mathrm{V}1,...,\mathrm{V}%
\bar{n},\mathrm{W}1,...,\mathrm{W}\bar{m}$

\item[(2)] $\mathrm{A}1$ es el unico label oficial de $M$

\item[(3)] Si reemplazamos:

\begin{enumerate}
\item las variables oficiales de $M$ (i.e. $\mathrm{V}1,...,\mathrm{V}\bar
{n},\mathrm{W}1,...,\mathrm{W}\bar{m}$) por variables concretas%
\[
\mathrm{N}\overline{k_{1}},...,\mathrm{N}\overline{k_{n}},\mathrm{P}%
\overline{j_{1}},...,\mathrm{P}\overline{j_{m}}%
\]
(elejidas libremente, es decir los numeros $k_{1},...,k_{n},j_{1},...,j_{m}$
son cualesquiera)

\item el label oficial $\mathrm{A}1$ por el label concreto $\mathrm{L}\bar{k}
$ (elejido libremente, es decir $k$ es cualquier elemento de $\mathbf{N}$)

\item las variables auxiliares de $M$ por variables concretas (distintas de a
dos) y NO pertenecientes a la lista $\mathrm{N}\overline{k_{1}},...,\mathrm{N}%
\overline{k_{n}},\mathrm{P}\overline{j_{1}},...,\mathrm{P}\overline{j_{m}}$

\item los labels auxiliares de $M$ por labels concretos (distintos de a dos) y
ninguno igual a $\mathrm{L}\bar{k}$
\end{enumerate}

\noindent Entonces la palabra asi obtenida es un programa de $\mathcal{S}%
^{\Sigma}$ (salvo por la ley de los GOTO respecto de $\mathrm{L}\bar{k}$) que
denotaremos con%
\[
\left[  \mathrm{IF\ }P(\mathrm{N}\overline{k_{1}},...,\mathrm{N}%
\overline{k_{n}},\mathrm{P}\overline{j_{1}},...,\mathrm{P}\overline{j_{m}%
})\ \mathrm{GOTO\ L}\bar{k}\right]
\]
el cual debe tener la siguiente propiedad:

\begin{enumerate}
\item[-] Si hacemos correr $\left[  \mathrm{IF\ }P(\mathrm{N}\overline{k_{1}%
},...,\mathrm{N}\overline{k_{n}},\mathrm{P}\overline{j_{1}},...,\mathrm{P}%
\overline{j_{m}})\ \mathrm{GOTO\ L}\bar{k}\right]  $ partiendo de un estado $e
$ que le asigne a las variables $\mathrm{N}\overline{k_{1}},...,\mathrm{N}%
\overline{k_{n}},\mathrm{P}\overline{j_{1}},...,\mathrm{P}\overline{j_{m}}$
valores $x_{1},...,x_{n},\alpha_{1},...,\alpha_{m}$, entonces
independientemente de los valores que les asigne $e$ al resto de las variables
(incluidas las que fueron a reemplazar a las variables auxiliares de $M$) se
dara que

\begin{enumerate}
\item si $(x_{1},...,x_{n},\alpha_{1},...,\alpha_{m})\notin D_{P}$, entonces
$\left[  \mathrm{IF\ }P(\mathrm{N}\overline{k_{1}},...,\mathrm{N}%
\overline{k_{n}},\mathrm{P}\overline{j_{1}},...,\mathrm{P}\overline{j_{m}%
})\ \mathrm{GOTO\ L}\bar{k}\right]  $ no se detiene

\item si $(x_{1},...,x_{n},\alpha_{1},...,\alpha_{m})\in D_{P}$ y
$P(x_{1},...,x_{n},\alpha_{1},...,\alpha_{m})=1$, entonces luego de una
cantidad finita de pasos, $\left[  \mathrm{IF\ }P(\mathrm{N}\overline{k_{1}%
},...,\mathrm{N}\overline{k_{n}},\mathrm{P}\overline{j_{1}},...,\mathrm{P}%
\overline{j_{m}})\ \mathrm{GOTO\ L}\bar{k}\right]  $ direcciona al label
$\mathrm{L}\bar{k}$ quedando en un estado $e^{\prime}$ el cual solo puede
diferir de $e$ en los valores que le asigna a las variables que fueron a
reemplazar a las variables auxiliares de $M$. Al resto de las variables,
incluidas $\mathrm{N}\overline{k_{1}},...,\mathrm{N}\overline{k_{n}%
},\mathrm{P}\overline{j_{1}},...,\mathrm{P}\overline{j_{m}}$ no las modifica

\item si $(x_{1},...,x_{n},\alpha_{1},...,\alpha_{m})\in D_{P}$ y
$P(x_{1},...,x_{n},\alpha_{1},...,\alpha_{m})=0$, entonces luego de una
cantidad finita de pasos, $\left[  \mathrm{IF\ }P(\mathrm{N}\overline{k_{1}%
},...,\mathrm{N}\overline{k_{n}},\mathrm{P}\overline{j_{1}},...,\mathrm{P}%
\overline{j_{m}})\ \mathrm{GOTO\ L}\bar{k}\right]  $ se detiene (i.e. intenta
realizar la siguiente a su ultima instruccion) quedando en un estado
$e^{\prime}$ el cual solo puede diferir de $e$ en los valores que le asigna a
las variables que fueron a reemplazar a las variables auxiliares de $M$. Al
resto de las variables, incluidas $\mathrm{N}\overline{k_{1}},...,\mathrm{N}%
\overline{k_{n}},\mathrm{P}\overline{j_{1}},...,\mathrm{P}\overline{j_{m}}$ no
las modifica
\end{enumerate}
\end{enumerate}
\end{enumerate}

\noindent La palabra $\left[  \mathrm{IF\ }P(\mathrm{N}\overline{k_{1}%
},...,\mathrm{N}\overline{k_{n}},\mathrm{P}\overline{j_{1}},...,\mathrm{P}%
\overline{j_{m}})\ \mathrm{GOTO\ L}\bar{k}\right]  $ es llamada la expansion
del macro con respecto a la eleccion de variables y labels realizada

\bigskip

\begin{proposition}
\label{macro predicados}Sea $P:D_{P}\subseteq\omega^{n}\times\Sigma^{\ast
m}\rightarrow\omega$ un predicado $\Sigma$\textit{-}computable. Entonces en
$\mathcal{S}^{\Sigma}$ hay un macro%
\[
\left[  \mathrm{IF}\;P(\mathrm{V}1,...,\mathrm{V}\bar{n},\mathrm{W}%
1,...,\mathrm{W}\bar{m})\;\mathrm{GOTO}\;\mathrm{A}1\right]
\]

\end{proposition}

\begin{proof}
Por (a) de la proposicion anterior tenemos un macro $\left[  \mathrm{V}%
\overline{n+1}\leftarrow P(\mathrm{V}1,...,\mathrm{V}\bar{n},\mathrm{W}%
1,...,\mathrm{W}\bar{m})\right]  $. Notese que la palabra%
\[
\left[  \mathrm{V}\overline{n+1}\leftarrow P(\mathrm{V}1,...,\mathrm{V}\bar
{n},\mathrm{W}1,...,\mathrm{W}\bar{m})\right]  \mathrm{IFV}\overline
{n+1}\mathrm{\neq}0\mathrm{GOTOA}1
\]
es el macro buscado.
\end{proof}

\bigskip

Dejamos al lector probar la resiproca de la proposicion anterior, es decir si
$P:D_{P}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow\omega$ es tal que
en $\mathcal{S}^{\Sigma}$ hay un macro%
\[
\left[  \mathrm{IF}\;P(\mathrm{V}1,...,\mathrm{V}\bar{n},\mathrm{W}%
1,...,\mathrm{W}\bar{m})\;\mathrm{GOTO}\;\mathrm{A}1\right]
\]
entonces $P$ es $\Sigma$-computable.

\bigskip

\subsubsection{Conjuntos $\Sigma$-enumerables}

Ya que la nocion de funcion $\Sigma$-computable es el modelo matematico
Neumanniano o imperativo del concepto de funcion $\Sigma$-efectivamente
computable, nos podriamos preguntar entonces cual es el modelo matematico
Neumanniano del concepto de conjunto $\Sigma$-efectivamente enumerable. Si
prestamos atencion a la definicion de conjunto $\Sigma$-efectivamente
enumerable, notaremos que depende de la existencia de ciertas funciones
$\Sigma$-efectivamente computables por lo cual la siguiente definicion cae de maduro:

Un conjunto $S\subseteq\omega^{n}\times\Sigma^{\ast m}$ sera llamado $\Sigma
$\textit{-enumerable} cuando sea vacio o haya una funcion $F:\omega
\rightarrow\omega^{n}\times\Sigma^{\ast m}$ tal que $I_{F}=S$ y $F_{(i)}$ sea
$\Sigma$-computable, para cada $i\in\{1,...,n+m\}$.

Deberia entonces quedar claro que si el concepto de funcion $\Sigma
$-computable modeliza correctamente al concepto de funcion $\Sigma
$-efectivamente computable, entonces el concepto de conjunto $\Sigma
$-enumerable recien definido modeliza correctamente al concepto de conjunto
$\Sigma$-efectivamente enumerable. Notese que segun la definicion que acabamos
de escribir, un conjunto no vacio $S\subseteq\omega^{n}\times\Sigma^{\ast m}$
es $\Sigma$-enumerable si y solo si hay programas $\mathcal{P}_{1}%
,...,\mathcal{P}_{n+m}$ tales que

\begin{enumerate}
\item[-] $\mathrm{Dom}(\Psi_{\mathcal{P}_{1}}^{1,0,\#})=...=\mathrm{Dom}%
(\Psi_{\mathcal{P}_{n}}^{1,0,\#})=\omega$

\item[-] $\mathrm{Dom}(\Psi_{\mathcal{P}_{n+1}}^{1,0,\ast})=...=\mathrm{Dom}%
(\Psi_{\mathcal{P}_{n}+m}^{1,0,\ast})=\omega$

\item[-] $S=\operatorname{Im}[\Psi_{\mathcal{P}_{1}}^{1,0,\#},...,\Psi
_{\mathcal{P}_{n}}^{1,0,\#},\Psi_{\mathcal{P}_{n+1}}^{1,0,\ast},...,\Psi
_{\mathcal{P}_{n}+m}^{1,0,\ast}]$
\end{enumerate}

\noindent Como puede notarse los programas $\mathcal{P}_{1},...,\mathcal{P}%
_{n+m}$ puestos en paralelo a funcionar desde el estado $\left\Vert
x\right\Vert $ producen en forma natural un procedimiento efectivo (con dato
de entrada $x\in\omega$) que enumera a $S$. Por supuesto podemos decir que en
tal caso los programas $\mathcal{P}_{1},...,\mathcal{P}_{n+m}$ enumeran a $S$.
La siguiente proposicion muestra que tambien las cosas se pueden hacer con un
solo programa

\bigskip

\begin{proposition}
\label{P enumera a S}Sea $S\subseteq\omega^{n}\times\Sigma^{\ast m}$ un
conjunto no vacio. Entonces son equivalentes:

\begin{enumerate}
\item[(1)] $S$ es $\Sigma$-enumerable

\item[(2)] Hay un programa $\mathcal{P}\in\mathrm{Pro}^{\Sigma}$ tal que:

\begin{enumerate}
\item Para cada $x\in\omega$, tenemos que $\mathcal{P}$ se detiene partiendo
desde el estado $\left\Vert x\right\Vert $ y llega a un estado $\left\Vert
x_{1},...x_{n},\alpha_{1},...,\alpha_{m}\right\Vert $, donde $(\vec{x}%
,\vec{\alpha})\in S$.

\item Para cada $(\vec{x},\vec{\alpha})\in S$ hay un $x\in\omega$ tal que
$\mathcal{P}$ se detiene partiendo desde el estado $\left\Vert x\right\Vert $
y llega al estado $\left\Vert x_{1},...x_{n},\alpha_{1},...,\alpha
_{m}\right\Vert $
\end{enumerate}

\item[(3)] Hay un programa $\mathcal{P}\in\mathrm{Pro}^{\Sigma}$ tal que:

\begin{enumerate}
\item Para cada $x\in\omega$, tenemos que $\mathcal{P}$ se detiene partiendo
desde el estado $\left\Vert x\right\Vert $ y llega a un estado de la forma
$((x_{1},...,x_{n},y_{1},...),(\alpha_{1},...,\alpha_{m},\beta_{1},...))$,
donde $(x_{1},...,x_{n},\alpha_{1},...,\alpha_{m})\in S$.

\item Para cada $(x_{1},...x_{n},\alpha_{1},...,\alpha_{m})\in S$ hay un
$x\in\omega$ tal que $\mathcal{P}$ se detiene partiendo desde el estado
$\left\Vert x\right\Vert $ y llega a un estado de la forma $((x_{1}%
,...,x_{n},y_{1},...),(\alpha_{1},...,\alpha_{m},\beta_{1},...))$
\end{enumerate}
\end{enumerate}
\end{proposition}

\begin{proof}
(1)$\Rightarrow$(2). Ya que $S$ es no vacio, por definicion tenemos que hay
una $F:\omega\rightarrow\omega^{n}\times\Sigma^{\ast m}$ tal que $I_{F}=S$ y
$F_{(i)}$ es $\Sigma$-computable, para cada $i\in\{1,...,n+m\}$. Por la
Proposicion \ref{macro funciones} tenemos que existen macros:%
\begin{align*}
& \left[  \mathrm{V}2\leftarrow F_{(1)}(\mathrm{V}1)\right] \\
& \ \ \ \ \ \ \ \ \ \ \ \ \vdots\\
& \left[  \mathrm{V}2\leftarrow F_{(n)}(\mathrm{V}1)\right] \\
& \left[  \mathrm{W}1\leftarrow F_{(n+1)}(\mathrm{V}1)\right] \\
& \ \ \ \ \ \ \ \ \ \ \ \ \vdots\\
& \left[  \mathrm{W}1\leftarrow F_{(n+m)}(\mathrm{V}1)\right]
\end{align*}
Sea $\mathcal{Q}$ el siguiente programa:%
\begin{align*}
& \left[  \mathrm{P}\overline{m}\leftarrow F_{(n+m)}(\mathrm{N}1)\right] \\
& \ \ \ \ \ \ \ \ \ \ \ \ \vdots\\
& \left[  \mathrm{P}1\leftarrow F_{(n+1)}(\mathrm{N}1)\right] \\
& \left[  \mathrm{N}\overline{n}\leftarrow F_{(n)}(\mathrm{N}1)\right] \\
& \ \ \ \ \ \ \ \ \ \ \ \ \vdots\\
& \left[  \mathrm{N}1\leftarrow F_{(1)}(\mathrm{N}1)\right]
\end{align*}
donde se supone que las expansiones de los macros usados son hechas usando
variables auxiliares no pertenecientes a la lista $\mathrm{N}1,...,\mathrm{N}%
\overline{n},\mathrm{P}1,...,\mathrm{P}\overline{m}$ (por supuesto, dada la
fortaleza de nuestros macros se puede usa una misma variable auxiliar para dos
distintas expansiones), y tambien se supone que los labels auxiliares usados
en dichas expansiones son todos distintos, es decir no usamos el mismo label
auxiliar en dos expansiones distintas (por que?).

Sea $k$ tal que las variables de $\mathcal{Q}$ estan todas en la lista
$\mathrm{N}1,...,\mathrm{N}\bar{k},\mathrm{P}1,...,\mathrm{P}\bar{k}$. Sea
$\mathcal{P}$ el siguiente programa:%
\[
\mathcal{Q}\mathrm{N}\overline{n+1}\leftarrow0\mathrm{N}\overline
{n+2}\leftarrow0...\mathrm{N}\overline{k}\leftarrow0\mathrm{P}\overline
{m+1}\leftarrow\varepsilon\mathrm{P}\overline{m+2}\leftarrow\varepsilon
...\mathrm{P}\overline{k}\leftarrow\varepsilon
\]
Dejamos al lector corroborar que el programa $\mathcal{P}$ cumple las
propiedades a y b

(2)$\Rightarrow$(3). Directo.

(3)$\Rightarrow$(1). Supongamos $\mathcal{P}\in\mathrm{Pro}^{\Sigma}$ cumple a
y b de (3). Sean%
\begin{align*}
\mathcal{P}_{1}  & =\mathcal{P}\mathrm{N}1\leftarrow\mathrm{N}1\\
\mathcal{P}_{2}  & =\mathcal{P}\mathrm{N}1\leftarrow\mathrm{N}2\\
& \vdots\\
\mathcal{P}_{n}  & =\mathcal{P}\mathrm{N}1\leftarrow\mathrm{N}\overline{n}\\
\mathcal{P}_{n+1}  & =\mathcal{P}\mathrm{P}1\leftarrow\mathrm{P}1\\
\mathcal{P}_{n+2}  & =\mathcal{P}\mathrm{P}1\leftarrow\mathrm{P}2\\
& \vdots\\
\mathcal{P}_{n+m}  & =\mathcal{P}\mathrm{P}1\leftarrow\mathrm{P}\overline{m}%
\end{align*}
Definamos%
\begin{align*}
F_{1}  & =\Psi_{\mathcal{P}_{1}}^{1,0,\#}\\
F_{2}  & =\Psi_{\mathcal{P}_{2}}^{1,0,\#}\\
& \vdots\\
F_{n}  & =\Psi_{\mathcal{P}_{n}}^{1,0,\#}\\
F_{n+1}  & =\Psi_{\mathcal{P}_{n+1}}^{1,0,\ast}\\
F_{n+2}  & =\Psi_{\mathcal{P}_{n+2}}^{1,0,\ast}\\
& \vdots\\
F_{n+m}  & =\Psi_{\mathcal{P}_{n+m}}^{1,0,\ast}%
\end{align*}
Notese que cada $F_{i}$ es $\Sigma$-computable y tiene dominio igual a
$\omega$. Sea $F=[F_{1},...,F_{n+m}]$. Tenemos por definicion que
$D_{F}=\omega$ y ya que $F_{(i)}=F_{i}$, para cada $i=1,...,n+m$ tenemos que
cada $F_{(i)}$ es $\Sigma$-computable. Dejamos al lector verificar que
$I_{F}=S$
\end{proof}

\bigskip

Cuando un programa $\mathcal{P}$ cumpla las propiedades dadas en (3) de la
proposicion anterior respecto de un conjunto $S$, diremos que $\mathcal{P}$
\textit{enumera }a $S$.

Cabe destacar que (3)$\Rightarrow$(1) de la proposicion anterior es muy util a
la hora de probar que un conjunto dado es $\Sigma$-enumerable.

\bigskip

\subsubsection{Conjuntos $\Sigma$-computables}

La version imperativa del concepto de conjunto $\Sigma$-efectivamente
computable es facil de dar: un conjunto $S\subseteq\omega^{n}\times
\Sigma^{\ast m}$ sera llamado $\Sigma$\textit{-computable} cuando la funcion
$\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}}$ sea $\Sigma$-computable. O sea
que $S\subseteq\omega^{n}\times\Sigma^{\ast m}$ es $\Sigma$-computable sii hay
un programa $\mathcal{P}\in\mathrm{Pro}^{\Sigma}$ el cual computa a $\chi
_{S}^{\omega^{n}\times\Sigma^{\ast m}}$, es decir:

\begin{enumerate}
\item[-] Si $(\vec{x},\vec{\alpha})\in S$, entonces $\mathcal{P}$ se detiene
partiendo desde $\left\Vert x_{1},...x_{n},\alpha_{1},...,\alpha
_{m}\right\Vert $ y la variable $\mathrm{N}1$ queda con contenido igual a $1$

\item[-] Si $(\vec{x},\vec{\alpha})\in(\omega^{n}\times\Sigma^{\ast m})-S $,
entonces $\mathcal{P}$ se detiene partiendo desde $\left\Vert x_{1}%
,...x_{n},\alpha_{1},...,\alpha_{m}\right\Vert $ y la variable $\mathrm{N}1$
queda con contenido igual a $0$
\end{enumerate}

Si $\mathcal{P}$ es un programa el cual computa a $\chi_{S}^{\omega^{n}%
\times\Sigma^{\ast m}}$, diremos que $\mathcal{P}$ \textit{decide la
pertenecia a }$S$, con respecto al conjunto $\omega^{n}\times\Sigma^{\ast m}$.

\bigskip

\paragraph*{Macros asociados a conjuntos $\Sigma$-computables}

La proposicion anterior nos dice que si $S\subseteq\omega^{n}\times
\Sigma^{\ast m}$ es un conjunto $\Sigma$-computable, entonces, ya que
$\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}}$ es $\Sigma$-computable, hay un
macro%
\[
\left[  \mathrm{IF}\;\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}}(\mathrm{V}%
1,...,\mathrm{V}\bar{n},\mathrm{W}1,...,\mathrm{W}\bar{m})\;\mathrm{GOTO}%
\;\mathrm{A}1\right]
\]
Escribiremos el nombre de este macro de la siguiente manera mas intuitiva:%
\[
\left[  \mathrm{IF}\;(\mathrm{V}1,...,\mathrm{V}\bar{n},\mathrm{W}%
1,...,\mathrm{W}\bar{m})\in S\;\mathrm{GOTO}\;\mathrm{A}1\right]
\]
Notese que las expanciones de este macro, dado que $\chi_{S}^{\omega^{n}%
\times\Sigma^{\ast m}}$ es $\Sigma$-total, ya sea terminan por la ultima
instruccion de la expansion o direccionan a la primera instruccion que tenga
label igual al label que reemplazo a $\mathrm{A}1$ en la expansion. Es
importante notar que para asegurar la existencia de este macro utilizamos que
$S$ es $\Sigma$-computable lo cual no siempre sucedera para un conjunto $S$.
Por ejemplo, puede pasar que $S$ sea el dominio de una funcion $\Sigma
$\textit{-}computable pero que $S$ no sea $\Sigma$-computable (esto se vera
mas adelante) y en tal caso no existira un macro%
\[
\left[  \mathrm{IF}\;(\mathrm{V}1,...,\mathrm{V}\bar{n},\mathrm{W}%
1,...,\mathrm{W}\bar{m})\in S\;\mathrm{GOTO}\;\mathrm{A}1\right]
\]
ya que si tal macro existiera seria facil hacer un programa que compute a
$\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}}$ y $S$ seria $\Sigma$-computable.
Es muy comun el error de suponer que existe un macro $\left[  \mathrm{IF}%
\;(\mathrm{V}1,...,\mathrm{V}\bar{n},\mathrm{W}1,...,\mathrm{W}\bar{m})\in
S\;\mathrm{GOTO}\;\mathrm{A}1\right]  $ cuando $S$ es el dominio de una
funcion $\Sigma$-computable.

\bigskip

\subsection{Batallas entre paradigmas}

En esta seccion compararemos los tres paradigmas de computabilidad efectiva
que hemos desarrollado anteriormente. Para esto probaremos que cada uno de
dichos paradigmas "vence" al otro en el sentido que incluye por lo menos todas
las funciones que incluye el otro en su modelizacion del concepto de funcion
$\Sigma$-efectivamente computable.

\bigskip

\subsubsection{Neumann vence a Godel}

Usando macros podemos ahora probar que el paradigma imperativo de Neumann es
por lo menos tan abarcativo como el funcional de Godel. Mas concretamente:

\begin{theorem}
\label{RimplicaComp}Si $h$ es $\Sigma$-recursiva, entonces $h$ es $\Sigma
$\textit{-}computable.
\end{theorem}

\begin{proof}
Probaremos por induccion en $k$ que

\begin{enumerate}
\item[(*)] Si $h\in\mathrm{R}_{k}^{\Sigma}$, entonces $h$ es $\Sigma
$\textit{-}computable.
\end{enumerate}

\noindent El caso $k=0$ es dejado al lector. Supongamos (*) vale para $k$,
veremos que vale para $k+1$. Sea $h\in\mathrm{R}_{k+1}^{\Sigma}-\mathrm{R}%
_{k}^{\Sigma}.$ Hay varios casos

Caso 1. Supongamos $h=M(P)$, con $P:\omega\times\omega^{n}\times\Sigma^{\ast
m}\rightarrow\omega$, un predicado perteneciente a $\mathrm{R}_{k}^{\Sigma}$.
Por hipotesis inductiva, $P$ es $\Sigma$\textit{-}computable y por lo tanto
tenemos un macro%
\[
\left[  \mathrm{IF}\;P(\mathrm{V}1,...,\mathrm{V}\overline{n+1},\mathrm{W}%
1,...,\mathrm{W}\bar{m})\;\mathrm{GOTO}\;\mathrm{A}1\right]
\]
lo cual nos permite realizar el siguiente programa%
\[%
\begin{array}
[c]{ll}%
\mathrm{L}2 & \left[  \mathrm{IF}\;P(\mathrm{N}\overline{n+1},\mathrm{N}%
1,...,\mathrm{N}\bar{n},\mathrm{P}1,...,\mathrm{P}\bar{m})\;\mathrm{GOTO}%
\;\mathrm{L}1\right] \\
& \mathrm{N}\overline{n+1}\leftarrow\mathrm{N}\overline{n+1}+1\\
& \mathrm{GOTO}\;\mathrm{L}2\\
\mathrm{L}1 & \mathrm{N}1\leftarrow\mathrm{N}\overline{n+1}%
\end{array}
\]
Es facil chequear que este programa computa $h.$

Caso 2. Supongamos $h=R(f,\mathcal{G})$, con%
\begin{align*}
f  & :S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m}%
\rightarrow\Sigma^{\ast}\\
\mathcal{G}_{a}  & :S_{1}\times...\times S_{n}\times L_{1}\times...\times
L_{m}\times\Sigma^{\ast}\times\Sigma^{\ast}\rightarrow\Sigma^{\ast}\text{,
}a\in\Sigma
\end{align*}
elementos de $\mathrm{R}_{k}^{\Sigma}$. Sea $\Sigma=\{a_{1},...,a_{r}\}.$ Por
hipotesis inductiva, las funciones $f$, $\mathcal{G}_{a}$, $a\in\Sigma$, son
$\Sigma$\textit{-}computables y por lo tanto podemos hacer el siguiente
programa via el uso de macros%
\[%
\begin{array}
[c]{rl}
& \left[  \mathrm{P}\overline{m+3}\leftarrow f(\mathrm{N}1,...,\mathrm{N}%
\bar{n},\mathrm{P}1,...,\mathrm{P}\bar{m})\right] \\
\mathrm{L}\overline{r+1} & \mathrm{IF}\;\mathrm{P}\overline{m+1}%
\ \mathrm{BEGINS\ }a_{1}\text{ }\mathrm{GOTO}\;\mathrm{L}1\\
& \ \ \ \ \ \ \ \ \ \ \ \ \vdots\\
& \mathrm{IF}\;\mathrm{P}\overline{m+1}\ \mathrm{BEGINS\ }a_{r}\text{
}\mathrm{GOTO}\;\mathrm{L}\bar{r}\\
& \mathrm{GOTO}\;\mathrm{L}\overline{r+2}\\
\mathrm{L}1 & \mathrm{P}\overline{m+1}\leftarrow\text{ }^{\curvearrowright
}\mathrm{P}\overline{m+1}\\
& \left[  \mathrm{P}\overline{m+3}\leftarrow\mathcal{G}_{a_{1}}(\mathrm{N}%
1,...,\mathrm{N}\bar{n},\mathrm{P}1,...,\mathrm{P}\bar{m},\mathrm{P}%
\overline{m+2},\mathrm{P}\overline{m+3})\right] \\
& \mathrm{P}\overline{m+2}\leftarrow\mathrm{P}\overline{m+2}.a_{1}\\
& \mathrm{GOTO}\;\mathrm{L}\overline{r+1}\\
& \ \ \ \ \ \ \ \ \ \ \ \ \vdots\\
\mathrm{L}\bar{r} & \mathrm{P}\overline{m+1}\leftarrow\text{ }%
^{\curvearrowright}\mathrm{P}\overline{m+1}\\
& \left[  \mathrm{P}\overline{m+3}\leftarrow\mathcal{G}_{a_{r}}(\mathrm{N}%
1,...,\mathrm{N}\bar{n},\mathrm{P}1,...,\mathrm{P}\bar{m},\mathrm{P}%
\overline{m+2},\mathrm{P}\overline{m+3})\right] \\
& \mathrm{P}\overline{m+2}\leftarrow\mathrm{P}\overline{m+2}.a_{r}\\
& \mathrm{GOTO}\;\mathrm{L}\overline{r+1}\\
\mathrm{L}\overline{r+2} & \mathrm{P}1\leftarrow\mathrm{P}\overline{m+3}%
\end{array}
\]
Es facil chequear que este programa computa $h.$

El resto de los casos son dejados al lector.
\end{proof}

\begin{corollary}
\label{recursivo implica macro}Si%
\begin{align*}
f  & :D_{f}\subseteq\omega^{n}\times\Sigma^{\ast}{}^{m}\rightarrow\omega\\
g  & :D_{g}\subseteq\omega^{n}\times\Sigma^{\ast}{}^{m}\rightarrow\Sigma
^{\ast}\\
P  & :D_{P}\subseteq\omega^{n}\times\Sigma^{\ast}{}^{m}\rightarrow\{0,1\}
\end{align*}
son $\Sigma$-recursivas, entonces hay macros%
\begin{align*}
& \left[  \mathrm{V}\overline{n+1}\leftarrow f(\mathrm{V}1,...,\mathrm{V}%
\bar{n},\mathrm{W}1,...,\mathrm{W}\bar{m})\right] \\
& \left[  \mathrm{W}\overline{m+1}\leftarrow g(\mathrm{V}1,...,\mathrm{V}%
\bar{n},\mathrm{W}1,...,\mathrm{W}\bar{m})\right] \\
& \left[  \mathrm{IF}\;P(\mathrm{V}1,...,\mathrm{V}\bar{n},\mathrm{W}%
1,...,\mathrm{W}\bar{m})\;\mathrm{GOTO}\;\mathrm{A}1\right]
\end{align*}

\end{corollary}

\bigskip

\paragraph{Se lleno de macros}

Cabe destacar que el corolario anterior nos dice que hay macros%
\begin{align*}
& \left[  \mathrm{V}\overline{n+1}\leftarrow f(\mathrm{V}1,...,\mathrm{V}%
\bar{n},\mathrm{W}1,...,\mathrm{W}\bar{m})\right] \\
& \left[  \mathrm{W}\overline{m+1}\leftarrow g(\mathrm{V}1,...,\mathrm{V}%
\bar{n},\mathrm{W}1,...,\mathrm{W}\bar{m})\right] \\
& \left[  \mathrm{IF}\;P(\mathrm{V}1,...,\mathrm{V}\bar{n},\mathrm{W}%
1,...,\mathrm{W}\bar{m})\;\mathrm{GOTO}\;\mathrm{A}1\right]
\end{align*}
para todas las funciones $\Sigma$-mixtas y predicados $\Sigma$-mixtos que
hemos trabajado hasta el momento en la materia ya que todas eran $\Sigma
$-p.r.. Esto transforma al lenguaje $\mathcal{S}^{\Sigma}$ en un potente y
relativamente comodo lenguaje de programacion ya que ahora tenemos macros para
todas las funciones y predicados cotidianos en la matematica. Por ejemplo a
continuacion usaremos la existencia de los macros $[\mathrm{IF\ V}1 $ es
par$\ \mathrm{GOTO\ A}1]$ y $[\mathrm{V}2\leftarrow\lfloor\mathrm{V}%
1/2\rfloor]$ para probar el siguiente resultado cuya prueba esta inspirada en
su analoga del paradigma de computabilidad efectiva.

\bigskip

\begin{lemma}
Supongamos $S_{1},S_{2}\subseteq\omega^{n}\times\Sigma^{\ast m}$ son conjuntos
$\Sigma$-enumerables. Entonces $S_{1}\cup S_{2}$ es $\Sigma$-enumerable.
\end{lemma}

\begin{proof}
Podemos suponer que ni $S_{1}$ ni $S_{2}$ son vacios ya que de lo contrario
los resultados son triviales. Ademas supondremos que $n=2$ y $m=1$.

La idea de la prueba es la misma que la que usamos para probar que la union de
conjuntos $\Sigma$-efectivamente enumerables es $\Sigma$-efectivamente
enumerable. Daremos usando macros un programa que enumera a $S_{1}\cup S_{2}$
y luego aplicaremos la Proposicion \ref{P enumera a S}. Por hipotesis hay
funciones $F:\omega\rightarrow\omega\times\omega\times\Sigma^{\ast}$ y
$G:\omega\rightarrow\omega\times\omega\times\Sigma^{\ast}$ tales que $F_{(1)}%
$, $F_{(2)}$, $F_{(3)}$, $G_{(1)}$, $G_{(2)}$ y $G_{(3)}$ son $\Sigma
$-computables, $\operatorname{Im}(F)=S_{1}$ y $\operatorname{Im}(G)=S_{2}$. O
sea que hay macros%
\begin{align*}
& \left[  \mathrm{V}2\leftarrow F_{(1)}(\mathrm{V}1)\right] \\
& \left[  \mathrm{V}2\leftarrow F_{(2)}(\mathrm{V}1)\right] \\
& \left[  \mathrm{W}1\leftarrow F_{(3)}(\mathrm{V}1)\right] \\
& \left[  \mathrm{V}2\leftarrow G_{(1)}(\mathrm{V}1)\right] \\
& \left[  \mathrm{V}2\leftarrow G_{(2)}(\mathrm{V}1)\right] \\
& \left[  \mathrm{W}1\leftarrow G_{(3)}(\mathrm{V}1)\right]
\end{align*}
Ya que el predicado $Par=\lambda x[x$ es par$]$ es $\Sigma$-p.r., el Corolario
\ref{recursivo implica macro} nos dice que hay un macro:%
\[
\lbrack\mathrm{IF\ }Par(\mathrm{V}1)\ \mathrm{GOTO\ A}1]
\]
el cual escribiremos de la siguiente manera mas intuitiva%
\[
\lbrack\mathrm{IF\ V}1\text{ es par }\mathrm{GOTO\ A}1]
\]
Ya que la funcion $D=\lambda x[\lfloor x/2\rfloor]$ es $\Sigma$-p.r., el
Corolario \ref{recursivo implica macro} nos dice que hay un macro:%
\[
\lbrack\mathrm{V}2\leftarrow D(\mathrm{V}1)]
\]
el cual escribiremos de la siguiente manera mas intuitiva%
\[
\lbrack\mathrm{V}2\leftarrow\lfloor\mathrm{V}1/2\rfloor]
\]
Sea $\mathcal{P}$ el siguiente programa:%
\[%
\begin{array}
[c]{ll}
& [\mathrm{IF\ N}1\text{ es par }\mathrm{GOTO\ L}1\\
& \mathrm{N}1\leftarrow\mathrm{N}1\dot{-}1\\
& [\mathrm{N}1111\leftarrow\lfloor\mathrm{N}1/2\rfloor]\\
& \left[  \mathrm{N}1\leftarrow G_{(1)}(\mathrm{N}1111)\right] \\
& \left[  \mathrm{N}2\leftarrow G_{(2)}(\mathrm{N}1111)\right] \\
& \left[  \mathrm{P}1\leftarrow G_{(3)}(\mathrm{N}1111)\right] \\
& \mathrm{GOTO\ L}2\\
\mathrm{L}1 & [\mathrm{N}1111\leftarrow\lfloor\mathrm{N}1/2\rfloor]\\
& \left[  \mathrm{N}1\leftarrow F_{(1)}(\mathrm{N}1111)\right] \\
& \left[  \mathrm{N}2\leftarrow F_{(2)}(\mathrm{N}1111)\right] \\
& \left[  \mathrm{P}1\leftarrow F_{(3)}(\mathrm{N}1111)\right] \\
\mathrm{L}2 & \mathrm{SKIP}%
\end{array}
\]
Es facil ver que $\mathcal{P}$ cumple a y b de (3) de la Proposicion
\ref{P enumera a S} por lo cual $S_{1}\cup S_{2}$ es $\Sigma$-enumerable.
\end{proof}

\bigskip

Tal como se vio en este ejemplo, el Corolario \ref{recursivo implica macro}
junto con nuestra gran coleccion de funciones ya probadamente $\Sigma
$-recursivas, nos permite simular con programas muchos de los procedimientos
efectivos realizados anteriormente. Mas capacidad de simulacion obtendremos
luego de ver que Godel vence a Neumann ya que la equivalencia de estos dos
paradigmas nos asegura la existencia de macros que permitiran dentro de un
programa hablar acerca del funcionamiento de otro programa. Esto sera clave a
la hora de simular con programas a procedimientos efectivos que en su
funcionamiento involucran el funcionamiento de otros procedimientos.

\bigskip

\subsubsection{Godel vence a Neumann}

Para probar que toda funcion $\Sigma$-computable es $\Sigma$-recursiva debemos
hacer un profundo estudio de la recursividad del lenguaje $\mathcal{S}%
^{\Sigma}$. Primero analizaremos la recursividad de la sintaxis de
$\mathcal{S}^{\Sigma}$.

\bigskip

\paragraph{Analisis de la recursividad de la sintaxis de $\mathcal{S}^{\Sigma
}$}

Primero probaremos dos lemas que muestran que la sintaxis de $\mathcal{S}%
^{\Sigma}$ es $(\Sigma\cup\Sigma_{p})$-recursiva primitiva. Recordemos que
$Sig:Num^{\ast}\rightarrow Num^{\ast}$ fue definida de la siguiente manera%
\begin{align*}
Sig(\varepsilon)  & =1\\
Sig(\alpha0)  & =\alpha1\\
Sig(\alpha1)  & =\alpha2\\
Sig(\alpha2)  & =\alpha3\\
Sig(\alpha3)  & =\alpha4\\
Sig(\alpha4)  & =\alpha5\\
Sig(\alpha5)  & =\alpha6\\
Sig(\alpha6)  & =\alpha7\\
Sig(\alpha7)  & =\alpha8\\
Sig(\alpha8)  & =\alpha9\\
Sig(\alpha9)  & =Sig(\alpha)0
\end{align*}
Y tambien definimos $Dec:\omega\rightarrow Num^{\ast}$ de la siguiente manera%
\begin{align*}
Dec(0)  & =\varepsilon\\
Dec(n+1)  & =Sig(Dec(n))
\end{align*}
Recordemos tambien que para hacer mas agil la notacion escribimos $\bar{n}$ en
lugar de $Dec(n)$.

Es obvio de las definiciones que ambas funciones son $Num$-p.r.. Mas aun tenemos

\begin{lemma}
Sea $\Sigma$ un alfabeto cualquiera. Las funciones $Sig$ y $Dec$ son
$(\Sigma\cup\Sigma_{p})$-p.r..
\end{lemma}

\begin{proof}
Es facil ver que $Sig$ y $Dec$ son $Num$-p.r.. Ya que tambien son $(\Sigma
\cup\Sigma_{p})$-mixtas, el Teorema \ref{independencia} nos dice que ambas son
$(\Sigma\cup\Sigma_{p})$-p.r..
\end{proof}

\bigskip

Recordemos que $Bas:\mathrm{Ins}^{\Sigma}\rightarrow(\Sigma\cup\Sigma
_{p})^{\ast}$, fue definida por%
\[
Bas(I)=\left\{
\begin{array}
[c]{ccl}%
J &  & \text{si }I\text{ es de la forma }\mathrm{L}\bar{k}J\text{, con }%
k\in\mathbf{N}\text{ y }J\in\mathrm{Ins}^{\Sigma}\\
I &  & \text{caso contrario}%
\end{array}
\right.
\]
Definamos $Lab:\mathrm{Ins}^{\Sigma}\rightarrow(\Sigma\cup\Sigma_{p})^{\ast}$
de la siguiente manera%
\[
Lab(I)=\left\{
\begin{array}
[c]{lll}%
\mathrm{L}\bar{k} &  & \text{si }I\text{ es de la forma }\mathrm{L}\bar
{k}J\text{, con }k\in\mathbf{N}\text{ y }J\in\mathrm{Ins}^{\Sigma}\\
\varepsilon &  & \text{caso contrario}%
\end{array}
\right.
\]


\begin{lemma}
\label{cota longitud de la barra}Para cada $n,x\in\omega$, tenemos que
$\left\vert \bar{n}\right\vert \leq x$ si y solo si $n\leq10^{x}-1$
\end{lemma}

\begin{lemma}
\label{Ins-es-pr}$\mathrm{Ins}^{\Sigma}$ es un conjunto $(\Sigma\cup\Sigma
_{p})$-p.r..
\end{lemma}

\begin{proof}
Para simplificar la prueba asumiremos que $\Sigma=\{@,\blacktriangle\}$. Ya
que $\mathrm{Ins}^{\Sigma}$ es union de los siguientes conjuntos%
\begin{align*}
L_{1}  & =\left\{  \mathrm{N}\bar{k}\leftarrow\mathrm{N}\bar{k}+1:k\in
\mathbf{N}\right\} \\
L_{2}  & =\left\{  \mathrm{N}\bar{k}\leftarrow\mathrm{N}\bar{k}\dot{-}%
1:k\in\mathbf{N}\right\} \\
L_{3}  & =\left\{  \mathrm{N}\bar{k}\leftarrow\mathrm{N}\bar{n}:k,n\in
\mathbf{N}\right\} \\
L_{4}  & =\left\{  \mathrm{N}\bar{k}\leftarrow0:k\in\mathbf{N}\right\} \\
L_{5}  & =\left\{  \mathrm{IF}\;\mathrm{N}\bar{k}\neq0\;\mathrm{GOTO}%
\;\mathrm{L}\bar{m}:k,m\in\mathbf{N}\right\} \\
L_{6}  & =\left\{  \mathrm{P}\bar{k}\leftarrow\mathrm{P}\bar{k}.@:k\in
\mathbf{N}\right\} \\
L_{7}  & =\left\{  \mathrm{P}\bar{k}\leftarrow\mathrm{P}\bar{k}.\blacktriangle
:k\in\mathbf{N}\right\} \\
L_{8}  & =\left\{  \mathrm{P}\bar{k}\leftarrow\text{ }^{\curvearrowright
}\mathrm{P}\bar{k}:k\in\mathbf{N}\right\} \\
L_{9}  & =\left\{  \mathrm{P}\bar{k}\leftarrow\mathrm{P}\bar{n}:k,n\in
\mathbf{N}\right\} \\
L_{10}  & =\left\{  \mathrm{P}\bar{k}\leftarrow\varepsilon:k\in\mathbf{N}%
\right\} \\
L_{11}  & =\left\{  \mathrm{IF}\;\mathrm{P}\bar{k}\;\mathrm{BEGINS}%
\;@\;\mathrm{GOTO}\;\mathrm{L}\bar{m}:k,m\in\mathbf{N}\right\} \\
L_{12}  & =\left\{  \mathrm{IF}\;\mathrm{P}\bar{k}\;\mathrm{BEGINS}%
\;\blacktriangle\;\mathrm{GOTO}\;\mathrm{L}\bar{m}:k,m\in\mathbf{N}\right\} \\
L_{13}  & =\left\{  \mathrm{GOTO}\;\mathrm{L}\bar{m}:m\in\mathbf{N}\right\} \\
L_{14}  & =\left\{  \mathrm{SKIP}\right\} \\
L_{15}  & =\left\{  \mathrm{L}\bar{k}\alpha:k\in\mathbf{N\;}\text{y }\alpha\in
L_{1}\cup...\cup L_{14}\right\}
\end{align*}
solo debemos probar que $L_{1},...,L_{15}$ son $(\Sigma\cup\Sigma_{p})$-p.r..
Veremos primero por ejemplo que%
\[
L_{11}=\left\{  \mathrm{IFP}\bar{k}\mathrm{BEGINS}@\mathrm{GOTOL}\bar
{m}:k,m\in\mathbf{N}\right\}
\]
es $(\Sigma\cup\Sigma_{p})$-p.r.. Primero notese que $\alpha\in L_{11}$ si y
solo si existen $k,m\in\mathbf{N}$ tales que%
\[
\alpha=\mathrm{IFP}\bar{k}\mathrm{BEGINS}@\mathrm{GOTOL}\bar{m}%
\]
Mas formalmente tenemos que $\alpha\in L_{11}$ si y solo si%
\[
(\exists k\in\mathbf{N})(\exists m\in\mathbf{N})\;\alpha=\mathrm{IFP}\bar
{k}\mathrm{BEGINS}@\mathrm{GOTOL}\bar{m}%
\]
Ya que cuando existen tales $k,m$ tenemos que $\bar{k}$ y $\bar{m}$ son
subpalabras de $\alpha$, el lema anterior nos dice que $\alpha\in L_{11}$ si y
solo si%
\[
(\exists k\in\mathbf{N})_{k\leq10^{\left\vert \alpha\right\vert }}(\exists
m\in\mathbf{N})_{m\leq10^{\left\vert \alpha\right\vert }}\;\alpha
=\mathrm{IFP}\bar{k}\mathrm{BEGINS}@\mathrm{GOTOL}\bar{m}%
\]
Sea%
\[
P=\lambda mk\alpha\left[  \alpha=\mathrm{IFP}\bar{k}\mathrm{BEGINS}%
@\mathrm{GOTOL}\bar{m}\right]
\]
Ya que $D_{\lambda k\left[  \bar{k}\right]  }=\omega$, tenemos que
$D_{P}=\omega^{2}\times(\Sigma\cup\Sigma_{p})^{\ast}$. Notese que%
\[
P=\lambda\alpha\beta\left[  \alpha=\beta\right]  \circ\left[  p_{3}%
^{2,1},f\right]
\]
donde%
\[
f=\lambda\alpha_{1}\alpha_{2}\alpha_{3}\alpha_{4}\left[  \alpha_{1}\alpha
_{2}\alpha_{3}\alpha_{4}\right]  \circ\left[  C_{\mathrm{IFP}}^{2,1},\lambda
k\left[  \bar{k}\right]  \circ p_{2}^{2,1},C_{\mathrm{BEGINS}@\mathrm{GOTOL}%
}^{2,1},\lambda k\left[  \bar{k}\right]  \circ p_{1}^{2,1}\right]
\]
lo cual nos dice que $P$ es $(\Sigma\cup\Sigma_{p})$-p.r..

Notese que%
\[
\chi_{L_{11}}^{(\Sigma\cup\Sigma_{p})^{\ast}}=\lambda\alpha\left[  (\exists
k\in\mathbf{N})_{k\leq10^{\left\vert \alpha\right\vert }}(\exists
m\in\mathbf{N})_{m\leq10^{\left\vert \alpha\right\vert }}\;P(m,k,\alpha
)\right]
\]
Esto nos dice que podemos usar dos veces el Lema \ref{cuantificacion} para ver
que $\chi_{L_{11}}^{(\Sigma\cup\Sigma_{p})^{\ast}}$ es $(\Sigma\cup\Sigma
_{p})$-p.r.. Veamos como. Sea%
\[
Q=\lambda k\alpha\left[  (\exists m\in\mathbf{N})_{m\leq10^{\left\vert
\alpha\right\vert }}\;P(m,k,\alpha)\right]
\]
Por el Lema \ref{cuantificacion} tenemos que%
\[
\lambda xk\alpha\left[  (\exists m\in\mathbf{N})_{m\leq x}\;P(m,k,\alpha
)\right]
\]
es $(\Sigma\cup\Sigma_{p})$-p.r. lo cual nos dice que%
\[
Q=\lambda xk\alpha\left[  (\exists m\in\mathbf{N})_{m\leq x}\;P(m,k,\alpha
)\right]  \circ\left[  \lambda\alpha\left[  10^{\left\vert \alpha\right\vert
}\right]  \circ p_{2}^{1,1},p_{1}^{1,1},p_{2}^{1,1}\right]
\]
lo es. Ya que%
\[
\chi_{L_{11}}^{(\Sigma\cup\Sigma_{p})^{\ast}}=\lambda\alpha\left[  (\exists
k\in\mathbf{N})_{k\leq10^{\left\vert \alpha\right\vert }}\;Q(k,\alpha)\right]
\]
podemos en forma similar aplicar el Lema \ref{cuantificacion} y obtener
finalmente que $\chi_{L_{11}}^{(\Sigma\cup\Sigma_{p})^{\ast}}$ es $(\Sigma
\cup\Sigma_{p})$-p.r..

En forma similar podemos probar que $L_{1},...,L_{14}$ son $(\Sigma\cup
\Sigma_{p})$-p.r.. Esto nos dice que $L_{1}\cup...\cup L_{14}$ es $(\Sigma
\cup\Sigma_{p})$-p.r.. Notese que $L_{1}\cup...\cup L_{14}$ es el conjunto de
las instrucciones basicas de $\mathcal{S}^{\Sigma}$. Llamemos $\mathrm{InsBas}%
^{\Sigma}$ a dicho conjunto. Para ver que $L_{15}$ es $(\Sigma\cup\Sigma_{p}%
)$-p.r. notemos que%
\[
\chi_{L_{15}}^{(\Sigma\cup\Sigma_{p})^{\ast}}=\lambda\alpha\left[  (\exists
k\in\mathbf{N})_{k\leq10^{\left\vert \alpha\right\vert }}(\exists\beta
\in\mathrm{InsBas}^{\Sigma})_{\left\vert \beta\right\vert \leq\left\vert
\alpha\right\vert }\;\alpha=\mathrm{L}\bar{k}\beta\right]
\]
lo cual nos dice que aplicando dos veces el Lema \ref{cuantificacion}
obtenemos que $\chi_{L_{15}}^{(\Sigma\cup\Sigma_{p})^{\ast}}$ es $(\Sigma
\cup\Sigma_{p})$-p.r.. Ya que $\mathrm{Ins}^{\Sigma}=\mathrm{InsBas}^{\Sigma
}\cup L_{15}$ tenemos que $\mathrm{Ins}^{\Sigma}$ es $(\Sigma\cup\Sigma_{p})$-p.r..
\end{proof}

@@finpagina@@

\begin{lemma}
$Bas$ y $Lab$ son funciones $(\Sigma\cup\Sigma_{p})$-p.r.
\end{lemma}

\begin{proof}
Sea $\leq$ un orden total sobre $\Sigma\cup\Sigma_{p}$. Sea $L=\{\mathrm{L}%
\bar{k}:k\in\mathbf{N}\}\cup\{\varepsilon\}$. Dejamos al lector probar que $L$
es un conjunto $(\Sigma\cup\Sigma_{p})$-p.r.. Sea%
\[
P=\lambda I\alpha\left[  \alpha\in\mathrm{Ins}^{\Sigma}\wedge I\in
\mathrm{Ins}^{\Sigma}\wedge\lbrack\alpha]_{1}\neq\mathrm{L}\wedge(\exists
\beta\in L)\ I=\beta\alpha\right]
\]
Note que $D_{P}=(\Sigma\cup\Sigma_{p})^{\ast2}$. Dejamos al lector probar que
$P$ es $(\Sigma\cup\Sigma_{p})$-p.r.. Notese ademas que cuando $I\in
\mathrm{Ins}^{\Sigma}$ tenemos que $P(I,\alpha)=1$ sii $\alpha=Bas(I)$.
Dejamos al lector probar que $Bas=M^{\leq}\left(  P\right)  $ por lo que para
ver que $Bas$ es $(\Sigma\cup\Sigma_{p})$-p.r., solo nos falta ver que la
funcion $Bas$ es acotada por alguna funcion $(\Sigma\cup\Sigma_{p})$-p.r. y
$(\Sigma\cup\Sigma_{p})$-total. Pero esto es trivial ya que $\left\vert
Bas(I)\right\vert \leq\left\vert I\right\vert =\lambda\alpha\lbrack\left\vert
\alpha\right\vert ](I)$, para cada $I\in\mathrm{Ins}^{\Sigma}$.

Finalmente note que%
\[
Lab=M^{\leq}\left(  \lambda I\alpha\left[  \alpha Bas(I)=I\right]  \right)
\]
lo cual nos dice que $Lab$ es $(\Sigma\cup\Sigma_{p})$-p.r..
\end{proof}

\bigskip

Recordemos que dado un programa $\mathcal{P}$ habiamos definido $I_{i}%
^{\mathcal{P}}=\varepsilon$, para $i=0$ o $i>n(\mathcal{P}).$ O sea que la
funcion $(\Sigma\cup\Sigma_{p})$-mixta $\lambda i\mathcal{P}\left[
I_{i}^{\mathcal{P}}\right]  $ tiene dominio igual a $\omega\times
\mathrm{Pro}^{\Sigma}$. Notese que usamos notacion lambda respecto del
alfabeto $\Sigma\cup\Sigma_{p}$. Ademas notese que usamos la variable
$\mathcal{P}$ en la notacion lambda por un tema de comodidad psicologica dado
que la expresion $I_{i}^{\alpha}$ esta definida solo cuando $\alpha$ es un
programa pero podriamos haber escrito $\lambda i\alpha\left[  I_{i}^{\alpha
}\right]  $ y sigue siendo la misma funcion.

\begin{lemma}
\label{Pro-es-pr}

\begin{enumerate}
\item[(a)] $\mathrm{Pro}^{\Sigma}$ es un conjunto $(\Sigma\cup\Sigma_{p}) $-p.r.

\item[(b)] $\lambda\mathcal{P}\left[  n(\mathcal{P})\right]  $ y $\lambda
i\mathcal{P}\left[  I_{i}^{\mathcal{P}}\right]  $ son funciones $(\Sigma
\cup\Sigma_{p})$-p.r..
\end{enumerate}
\end{lemma}

\begin{proof}
Ya que $\mathrm{Pro}^{\Sigma}=D_{\lambda\mathcal{P}\left[  n(\mathcal{P}%
)\right]  }$ tenemos que (b) implica (a). Para probar (b) Sea $\leq$ un orden
total sobre $\Sigma\cup\Sigma_{p}$. Sea $P$ el siguiente predicado

$\lambda x\left[  Lt(x)>0\wedge(\forall t\in\mathbf{N})_{t\leq Lt(x)}%
\;\ast^{\leq}((x)_{t})\in\mathrm{Ins}^{\Sigma}\wedge\right.  $

$\ \ \ \ \ \ \ \ \ \ \ \ (\forall t\in\mathbf{N})_{t\leq Lt(x)}(\forall
m\in\mathbf{N})\;\lnot(\mathrm{L}\bar{m}\ $t-final $\ast^{\leq}((x)_{t}))\vee$

$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.  (\exists j\in\mathbf{N})_{j\leq
Lt(x)}(\exists\alpha\in(\Sigma\cup\Sigma_{p})-Num)\;\mathrm{L}\bar{m}%
\alpha\ \text{t-inicial}\ast^{\leq}((x)_{j})\right]  $

\noindent Notese que $D_{P}=\mathbf{N}$ y que $P(x)=1$ sii $Lt(x)>0$,
$\ast^{\leq}((x)_{t})\in\mathrm{Ins}^{\Sigma}$, para cada $t=1,...,Lt(x)$ y
ademas $\subset_{t=1}^{t=Lt(x)}\ast^{\leq}((x)_{t})\in\mathrm{Pro}^{\Sigma}$.
Para ver que $P$ es $(\Sigma\cup\Sigma_{p})$-p.r. solo nos falta acotar el
cuantificador $(\forall m\in\mathbf{N})$ de la expresion lambda que define a
$P$. Ya que nos interesan los valores de $m$ para los cuales $\bar{m}$ es
posiblemente una subpalabra de alguna de las palabras $\ast^{\leq}((x)_{j})$,
el Lema \ref{cota longitud de la barra} nos dice que una cota posible es
$10^{\max\{\left\vert \ast^{\leq}((x)_{j})\right\vert :1\leq j\leq Lt(x)\}}%
-1$. Dejamos al lector los detalles de la prueba de que $P$ es $(\Sigma
\cup\Sigma_{p})$-p.r.. Sea%
\[
Q=\lambda x\alpha\left[  P(x)\wedge\alpha=\subset_{t=1}^{t=Lt(x)}\ast^{\leq
}((x)_{t})\right]  \text{.}%
\]
Note que $D_{Q}=\mathbf{N}\times(\Sigma\cup\Sigma_{p})^{\ast}$. Claramente $Q$
es $(\Sigma\cup\Sigma_{p})$-p.r.. Ademas note que $D_{M(Q)}=\mathrm{Pro}%
^{\Sigma}$. Notese que para $\mathcal{P}\in\mathrm{Pro}^{\Sigma}$, tenemos que
$M(Q)(\mathcal{P})$ es aquel numero tal que pensado como infinitupla (via
mirar su secuencia de exponentes) codifica la secuencia de instrucciones que
forman a $\mathcal{P}$. Es decir%
\[
M(Q)(\mathcal{P})=\left\langle \#^{\leq}(I_{1}^{\mathcal{P}}),\#^{\leq}%
(I_{2}^{\mathcal{P}}),...,\#^{\leq}(I_{n(\mathcal{P})}^{\mathcal{P}%
}),0,0,...\right\rangle
\]
Por (b) del Lema \ref{minimizacion}, $M(Q)$ es $(\Sigma\cup\Sigma_{p})$-p.r.
ya que para cada $\mathcal{P}\in\mathrm{Pro}^{\Sigma}$ tenemos que%
\begin{align*}
M(Q)(\mathcal{P})  & =\left\langle \#^{\leq}(I_{1}^{\mathcal{P}}),\#^{\leq
}(I_{2}^{\mathcal{P}}),...,\#^{\leq}(I_{n(\mathcal{P})}^{\mathcal{P}%
}),0,0,...\right\rangle \\
& =\underset{i=1}{\overset{n(\mathcal{P})}{\Pi}}pr(i)^{\#^{\leq}%
(I_{1}^{\mathcal{P}})}\\
& \leq\underset{i=1}{\overset{\left\vert \mathcal{P}\right\vert }{\Pi}%
}pr(i)^{\#^{\leq}(\mathcal{P})}%
\end{align*}
Ademas tenemos que%
\begin{align*}
\lambda\mathcal{P}\left[  n(\mathcal{P})\right]   & =\lambda x\left[
Lt(x)\right]  \circ M(Q)\\
\lambda i\mathcal{P}\left[  I_{i}^{\mathcal{P}}\right]   & =\ast^{\leq}\circ
g\circ\left[  p_{1}^{1,1},M(Q)\circ p_{2}^{1,1}\right]
\end{align*}
donde $g=C_{0}^{1,1}|_{\{0\}\times\omega}\cup\lambda ix\left[  (x)_{i}\right]
$, lo cual dice que $\lambda\mathcal{P}\left[  n(\mathcal{P})\right]  $ y
$\lambda i\mathcal{P}\left[  I_{i}^{\mathcal{P}}\right]  $ son funciones
$(\Sigma\cup\Sigma_{p})$-p.r..
\end{proof}

\bigskip

\paragraph{Analisis de la recursividad de la semantica de $\mathcal{S}%
^{\Sigma}$}

Para estudiar la recursividad de la semantica de $\mathcal{S}^{\Sigma}$
deberemos definir varias funciones que tienen que ver con el funcionamiento de
un programa y estudiar su recursividad.

\subparagraph{Las funciones $i^{n,m}$, $E_{\#}^{n,m}$ y $E_{\ast}^{n,m}$}

Sean $n,m\geq0$ fijos. Definamos entonces las funciones%

\begin{align*}
i^{n,m}  & :\omega\times\omega^{n}\times\Sigma^{\ast m}\times\mathrm{Pro}%
^{\Sigma}\rightarrow\omega\\
E_{\#}^{n,m}  & :\omega\times\omega^{n}\times\Sigma^{\ast m}\times
\mathrm{Pro}^{\Sigma}\rightarrow\omega^{\lbrack\mathbf{N}]}\\
E_{\ast}^{n,m}  & :\omega\times\omega^{n}\times\Sigma^{\ast m}\times
\mathrm{Pro}^{\Sigma}\rightarrow\Sigma^{\ast\lbrack\mathbf{N}]}%
\end{align*}
de la siguiente manera%

\begin{align*}
(i^{n,m}(0,\vec{x},\vec{\alpha},\mathcal{P}),E_{\#}^{n,m}(0,\vec{x}%
,\vec{\alpha},\mathcal{P}),E_{\ast}^{n,m}(0,\vec{x},\vec{\alpha}%
,\mathcal{P}))  & =(1,(x_{1},...,x_{n},0,...),(\alpha_{1},...,\alpha
_{m},\varepsilon,...))\\
(i^{n,m}(t+1,\vec{x},\vec{\alpha},\mathcal{P}),E_{\#}^{n,m}(t+1,\vec{x}%
,\vec{\alpha},\mathcal{P}),E_{\ast}^{n,m}(t+1,\vec{x},\vec{\alpha}%
,\mathcal{P}))  & =S_{\mathcal{P}}(i^{n,m}(t,\vec{x},\vec{\alpha}%
,\mathcal{P}),E_{\#}^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P}),E_{\ast}%
^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P}))
\end{align*}
Notese que%
\[
(i^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P}),E_{\#}^{n,m}(t,\vec{x}%
,\vec{\alpha},\mathcal{P}),E_{\ast}^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P}))
\]
es la descripcion instantanea que se obtiene luego de correr $\mathcal{P}$ una
cantidad $t$ de pasos partiendo del estado%
\[
((x_{1},...,x_{n},0,...),(\alpha_{1},...,\alpha_{m},\varepsilon,...))
\]
Es importante notar que si bien $i^{n,m}$ es una funcion $(\Sigma\cup
\Sigma_{p})$-mixta, ni $E_{\#}^{n,m}$ ni $E_{\ast}^{n,m}$ lo son.

Definamos para cada $j\in\mathbf{N}$, funciones%

\begin{align*}
E_{\#j}^{n,m}  & :\omega\times\omega^{n}\times\Sigma^{\ast m}\times
\mathrm{Pro}^{\Sigma}\rightarrow\omega\\
E_{\ast j}^{n,m}  & :\omega\times\omega^{n}\times\Sigma^{\ast m}%
\times\mathrm{Pro}^{\Sigma}\rightarrow\Sigma^{\ast}%
\end{align*}
de la siguiente manera%
\begin{align*}
E_{\#j}^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P})  & =j\text{-esima coordenada
de }E_{\#}^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P})\\
E_{\ast j}^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P})  & =j\text{-esima
coordenada de }E_{\ast}^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P})
\end{align*}
Notese que%
\begin{align*}
E_{\#}^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P})  & =(E_{\#1}^{n,m}(t,\vec
{x},\vec{\alpha},\mathcal{P}),E_{\#2}^{n,m}(t,\vec{x},\vec{\alpha}%
,\mathcal{P}),...)\\
E_{\ast}^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P})  & =(E_{\ast1}^{n,m}%
(t,\vec{x},\vec{\alpha},\mathcal{P}),E_{\ast2}^{n,m}(t,\vec{x},\vec{\alpha
},\mathcal{P}),...)
\end{align*}
Nuestro proximo objetivo es mostrar que las funciones $i^{n,m}$,
$E_{\#j}^{n,m}$, $E_{\ast j}^{n,m}$ son $(\Sigma\cup\Sigma_{p})$-p.r.

Para esto primero debemos probar un lema el cual muestre que una ves
codificadas las descripciones instantaneas en forma numerica, las funciones
que dan la descripcion instantanea sucesora son $(\Sigma\cup\Sigma_{p})$-p.r..
Dado un orden total $\leq$ sobre $\Sigma\cup\Sigma_{p}$, codificaremos las
descripciones instantaneas haciendo uso de las biyecciones%
\[%
\begin{array}
[c]{rcl}%
\omega^{\left[  \mathbf{N}\right]  } & \rightarrow & \mathbf{N}\\
(s_{1},s_{2},...) & \rightarrow & \left\langle s_{1},s_{2},...\right\rangle
\end{array}
\;\;\;\;\;\;\;\;\;\;\;\;%
\begin{array}
[c]{rcl}%
\Sigma^{\ast\left[  \mathbf{N}\right]  } & \rightarrow & \mathbf{N}\\
(\sigma_{1},\sigma_{2},...) & \rightarrow & \left\langle \#^{\leq}(\sigma
_{1}),\#^{\leq}(\sigma_{2}),...\right\rangle
\end{array}
\]
Es decir que a la descripcion instantanea%
\[
(i,(s_{1},s_{2},...),(\sigma_{1},\sigma_{2},...))
\]
la codificaremos con la terna%
\[
(i,\left\langle s_{1},s_{2},...\right\rangle ,\left\langle \#^{\leq}%
(\sigma_{1}),\#^{\leq}(\sigma_{2}),...\right\rangle )\in\omega\times
\mathbf{N}\times\mathbf{N}%
\]
Es decir que una terna $(i,x,y)\in\omega\times\mathbf{N}\times\mathbf{N}$
codificara a la descripcion instantanea%
\[
(i,((x)_{1},(x)_{2},...),(\ast^{\leq}((y)_{1}),\ast^{\leq}((y)_{2}),...))
\]
Definamos%

\begin{align*}
s  & :\omega\times\mathbf{N}\times\mathbf{N}\times\mathrm{Pro}^{\Sigma
}\rightarrow\omega\\
S_{\#}  & :\omega\times\mathbf{N}\times\mathbf{N}\times\mathrm{Pro}^{\Sigma
}\rightarrow\omega\\
S_{\ast}  & :\omega\times\mathbf{N}\times\mathbf{N}\times\mathrm{Pro}^{\Sigma
}\rightarrow\omega
\end{align*}
de la siguiente manera%

\[%
\begin{array}
[t]{ll}%
s(i,x,y,\mathcal{P})= & \text{primera coordenada de la codificacion de la
descripcion instantanea}\\
& \text{sucesora de }(i,((x)_{1},(x)_{2},...),(\ast^{\leq}((y)_{1}),\ast
^{\leq}((y)_{2}),...))\text{ en }\mathcal{P}%
\end{array}
\]
%

\[%
\begin{array}
[t]{ll}%
S_{\#}(i,x,y,\mathcal{P})= & \text{segunda coordenada de la codificacion de la
descripcion instantanea}\\
& \text{sucesora de }(i,((x)_{1},(x)_{2},...),(\ast^{\leq}((y)_{1}),\ast
^{\leq}((y)_{2}),...))\text{ en }\mathcal{P}%
\end{array}
\]
%

\[%
\begin{array}
[t]{ll}%
S_{\ast}(i,x,y,\mathcal{P})= & \text{tercera coordenada de la codificacion de
la descripcion instantanea}\\
& \text{sucesora de }(i,((x)_{1},(x)_{2},...),(\ast^{\leq}((y)_{1}),\ast
^{\leq}((y)_{2}),...))\text{ en }\mathcal{P}%
\end{array}
\]
Notese que la definicion de estas funciones depende del orden total $\leq$
sobre $\Sigma\cup\Sigma_{p}$.

@@finpagina@@

\begin{lemma}
\label{descripcion instantanea sucesora es PR}Dado un orden total $\leq$ sobre
$\Sigma\cup\Sigma_{p}$, las funciones $s$, $S_{\#}$ y $S_{\ast}$ son
$(\Sigma\cup\Sigma_{p})$-p.r..
\end{lemma}

\begin{proof}
Necesitaremos algunas funciones $(\Sigma\cup\Sigma_{p})$-p.r.. Dada una
instruccion $I$ en la cual al menos ocurre una variable, usaremos $\#Var1(I)$
para denotar el numero de la primer variable que ocurre en $I$. Por ejemplo%
\[
\#Var1\left(  \mathrm{L}\bar{n}\;\mathrm{IF\;N}\bar{k}\neq0\;\mathrm{GOTO\;L}%
\bar{m}\right)  =k
\]
Notese que $\lambda I[\#Var1(I)]$ tiene dominio igual a $\mathrm{Ins}^{\Sigma
}-L$, donde $L$ es la union de los siguientes conjuntos%
\begin{gather*}
\{\mathrm{GOTO\ L}\bar{m}:m\in\mathbf{N\}\cup}\{\mathrm{L}\bar{k}%
\ \mathrm{GOTO\ L}\bar{m}:k,m\in\mathbf{N\}}\\
\left\{  \mathrm{SKIP}\right\}  \mathbf{\cup}\{\mathrm{L}\bar{k}%
\ \mathrm{SKIP}:k\in\mathbf{N\}}%
\end{gather*}
Dada una instruccion $I$ en la cual ocurren dos variables, usaremos
$\#Var2(I)$ para denotar el numero de la segunda variable que ocurre en $I$.
Por ejemplo%
\[
\#Var2\left(  \mathrm{N}\bar{k}\leftarrow\mathrm{N}\bar{m}\right)  =m
\]
Notese que el dominio de $\lambda I[\#Var2(I)]$ es igual a la union de los
siguientes conjuntos%
\begin{align*}
\{\mathrm{N}\bar{k}  & \leftarrow\mathrm{N}\bar{m}:k,m\in\mathbf{N\}\cup
}\{\mathrm{L}\bar{j}\ \mathrm{N}\bar{k}\leftarrow\mathrm{N}\bar{m}%
:j,k,m\in\mathbf{N\}}\\
\{\mathrm{P}\bar{k}  & \leftarrow\mathrm{P}\bar{m}:k,m\in\mathbf{N\}\cup
}\{\mathrm{L}\bar{j}\ \mathrm{P}\bar{k}\leftarrow\mathrm{P}\bar{m}%
:j,k,m\in\mathbf{N\}}%
\end{align*}
Ademas notese que para una instruccion $I$ tenemos que%
\begin{align*}
\#Var1(I)  & =\min_{k}(\mathrm{N}\bar{k}\mathrm{\leftarrow}\text{
}\mathrm{ocu}\text{ }I\vee\mathrm{N}\bar{k}\mathrm{\neq}\text{ }%
\mathrm{ocu}\text{ }I\vee\mathrm{P}\bar{k}\mathrm{\leftarrow}\text{
}\mathrm{ocu}\text{ }I\vee\mathrm{P}\bar{k}\mathrm{B}\;\mathrm{ocu}\text{
}I)\\
\#Var2(I)  & =\min_{k}(\mathrm{N}\bar{k}\ \text{t-final }I\vee\mathrm{N}%
\bar{k}\mathrm{+}\text{ }\mathrm{ocu}\text{ }I\vee\mathrm{N}\bar
{k}\mathrm{\dot{-}}\text{ }\mathrm{ocu}\text{ }I\vee\mathrm{P}\bar
{k}\ \text{t-final }I\vee\mathrm{P}\bar{k}.\text{ }\mathrm{ocu}\text{ }I)
\end{align*}
Esto nos dice que si llamamos $P$ al predicado%
\[
\lambda k\alpha\left[  \alpha\in\mathrm{Ins}^{\Sigma}\wedge(\mathrm{N}\bar
{k}\mathrm{\leftarrow}\text{ }\mathrm{ocu}\text{ }\alpha\vee\mathrm{N}\bar
{k}\mathrm{\neq}\text{ }\mathrm{ocu}\text{ }\alpha\vee\mathrm{P}\bar
{k}\mathrm{\leftarrow}\text{ }\mathrm{ocu}\text{ }\alpha\vee\mathrm{P}\bar
{k}\mathrm{B}\;\mathrm{ocu}\text{ }\alpha)\right]
\]
entonces $\lambda I[\#Var1(I)]=M(P)$ por lo cual $\lambda I[\#Var1(I)]$ es
$(\Sigma\cup\Sigma_{p})$-p.r. Similarmente se puede ver que $\lambda
I[\#Var2(I)]$ es $(\Sigma\cup\Sigma_{p})$-p.r.. Sea%
\[%
\begin{array}
[c]{rll}%
F_{\dot{-}}:\mathbf{N}\times\mathbf{N} & \rightarrow & \omega\\
(x,j) & \rightarrow & \left\langle (x)_{1},....,(x)_{j-1},(x)_{j}\dot
{-}1,(x)_{j+1},...\right\rangle
\end{array}
\]
Ya que%
\[
F_{\dot{-}}(x,j)=\left\{
\begin{array}
[c]{lll}%
Q(x,pr(j)) &  & \text{si }pr(j)\text{ divide }x\\
x &  & \text{caso contrario}%
\end{array}
\right.
\]
tenemos que $F_{\dot{-}}$ es $(\Sigma\cup\Sigma_{p})$-p.r.. Sea%
\[%
\begin{array}
[c]{rll}%
F_{+}:\mathbf{N}\times\mathbf{N} & \rightarrow & \omega\\
(x,j) & \rightarrow & \left\langle (x)_{1},....,(x)_{j-1},(x)_{j}%
+1,(x)_{j+1},...\right\rangle
\end{array}
\]
Ya que $F_{+}(x,j)=x.pr(j)$ tenemos que $F_{+}$ es $(\Sigma\cup\Sigma_{p})
$-p.r.. Sea%
\[%
\begin{array}
[c]{rll}%
F_{\leftarrow}:\mathbf{N}\times\mathbf{N}\times\mathbf{N} & \rightarrow &
\omega\\
(x,j,k) & \rightarrow & \left\langle (x)_{1},....,(x)_{j-1},(x)_{k}%
,(x)_{j+1},...\right\rangle
\end{array}
\]
Ya que $F_{\leftarrow}(x,j,k)=Q(x,pr(j)^{(x)_{j}}).pr(j)^{(x)_{k}}$ tenemos
que $F_{\leftarrow}$ es $(\Sigma\cup\Sigma_{p})$-p.r.. Sea%
\[%
\begin{array}
[c]{rll}%
F_{0}:\mathbf{N}\times\mathbf{N} & \rightarrow & \omega\\
(x,j) & \rightarrow & \left\langle (x)_{1},....,(x)_{j-1},0,(x)_{j+1}%
,...\right\rangle
\end{array}
\]
Es facil ver que $F_{0}$ es $(\Sigma\cup\Sigma_{p})$-p.r.. Para cada
$a\in\Sigma$, sea%
\[%
\begin{array}
[c]{rll}%
F_{a}:\mathbf{N}\times\mathbf{N} & \rightarrow & \omega\\
(x,j) & \rightarrow & \left\langle (x)_{1},....,(x)_{j-1},\#^{\leq}(\ast
^{\leq}((x)_{j})a),(x)_{j+1},...\right\rangle
\end{array}
\]
Es facil ver que $F_{a}$ es $(\Sigma\cup\Sigma_{p})$-p.r.. En forma similar
puede ser probado que%
\[%
\begin{array}
[c]{rll}%
F_{\curvearrowright}:\mathbf{N}\times\mathbf{N} & \rightarrow & \omega\\
(x,j) & \rightarrow & \left\langle (x)_{1},....,(x)_{j-1},\#^{\leq
}(^{\curvearrowright}(\ast^{\leq}((x)_{j}))),(x)_{j+1},...\right\rangle
\end{array}
\]
es $(\Sigma\cup\Sigma_{p})$-p.r.

Dado $(i,x,y,\mathcal{P})\in\omega\times\mathbf{N}\times\mathbf{N}%
\times\mathrm{Pro}^{\Sigma}$, tenemos varios casos en los cuales los valores
$s(i,x,y,\mathcal{P}),S_{\#}(i,x,y,\mathcal{P})$ y $S_{\ast}(i,x,y,\mathcal{P}%
)$ pueden ser obtenidos usando las funciones antes definidas:

\begin{enumerate}
\item[(1)] CASO $i=0\vee i>n(\mathcal{P})$. Entonces%
\begin{align*}
s(i,x,y,\mathcal{P})  & =i\\
S_{\#}(i,x,y,\mathcal{P})  & =x\\
S_{\ast}(i,x,y,\mathcal{P})  & =y
\end{align*}


\item[(2)] CASO $(\exists j\in\omega)\;Bas(I_{i}^{\mathcal{P}})=\mathrm{N}%
\bar{j}\leftarrow\mathrm{N}\bar{j}+1$. Entonces%
\begin{align*}
s(i,x,y,\mathcal{P})  & =i+1\\
S_{\#}(i,x,y,\mathcal{P})  & =F_{+}(x,\#Var1(I_{i}^{\mathcal{P}}))\\
S_{\ast}(i,x,y,\mathcal{P})  & =y
\end{align*}


\item[(3)] CASO $(\exists j\in\omega)\;Bas(I_{i}^{\mathcal{P}})=\mathrm{N}%
\bar{j}\leftarrow\mathrm{N}\bar{j}\dot{-}1$. Entonces%
\begin{align*}
s(i,x,y,\mathcal{P})  & =i+1\\
S_{\#}(i,x,y,\mathcal{P})  & =F_{\dot{-}}(x,\#Var1(I_{i}^{\mathcal{P}}))\\
S_{\ast}(i,x,y,\mathcal{P})  & =y
\end{align*}


\item[(4)] CASO $(\exists j,k\in\omega)\;Bas(I_{i}^{\mathcal{P}}%
)=\mathrm{N}\bar{j}\leftarrow\mathrm{N}\bar{k}$. Entonces%
\begin{align*}
s(i,x,y,\mathcal{P})  & =i+1\\
S_{\#}(i,x,y,\mathcal{P})  & =F_{\leftarrow}(x,\#Var1(I_{i}^{\mathcal{P}%
}),\#Var2(I_{i}^{\mathcal{P}}))\\
S_{\ast}(i,x,y,\mathcal{P})  & =y
\end{align*}


\item[(5)] CASO $(\exists j,k\in\omega)\;Bas(I_{i}^{\mathcal{P}}%
)=\mathrm{N}\bar{j}\leftarrow0$. Entonces%
\begin{align*}
s(i,x,y,\mathcal{P})  & =i+1\\
S_{\#}(i,x,y,\mathcal{P})  & =F_{0}(x,\#Var1(I_{i}^{\mathcal{P}}))\\
S_{\ast}(i,x,y,\mathcal{P})  & =y
\end{align*}


\item[(6)] CASO $(\exists j,m\in\omega)\;\left(  Bas(I_{i}^{\mathcal{P}%
})=\mathrm{IF}\;\mathrm{N}\bar{j}\neq0\;\mathrm{GOTO}\;\mathrm{L}\bar{m}%
\wedge(x)_{j}=0\right)  $. Entonces%
\begin{align*}
s(i,x,y,\mathcal{P})  & =i+1\\
S_{\#}(i,x,y,\mathcal{P})  & =x\\
S_{\ast}(i,x,y,\mathcal{P})  & =y
\end{align*}


\item[(7)] CASO $(\exists j,m\in\omega)\;\left(  Bas(I_{i}^{\mathcal{P}%
})=\mathrm{IF}\;\mathrm{N}\bar{j}\neq0\;\mathrm{GOTO}\;\mathrm{L}\bar{m}%
\wedge(x)_{j}\neq0\right)  $. Entonces%
\begin{align*}
s(i,x,y,\mathcal{P})  & =\min_{l}\left(  Lab(I_{l}^{\mathcal{P}}%
)\neq\varepsilon\wedge Lab(I_{l}^{\mathcal{P}})\text{ }\mathrm{t}%
\text{\textrm{-final} }I_{i}^{\mathcal{P}}\right) \\
S_{\#}(i,x,y,\mathcal{P})  & =x\\
S_{\ast}(i,x,y,\mathcal{P})  & =y
\end{align*}


\item[(8)] CASO $(\exists j\in\omega)\;Bas(I_{i}^{\mathcal{P}})=\mathrm{P}%
\bar{j}\leftarrow\mathrm{P}\bar{j}.a$. Entonces%
\begin{align*}
s(i,x,y,\mathcal{P})  & =i+1\\
S_{\#}(i,x,y,\mathcal{P})  & =x\\
S_{\ast}(i,x,y,\mathcal{P})  & =F_{a}(y,\#Var1(I_{i}^{\mathcal{P}}))
\end{align*}


\item[(9)] CASO $(\exists j\in\omega)\;Bas(I_{i}^{\mathcal{P}})=\mathrm{P}%
\bar{j}\leftarrow$ $^{\curvearrowright}\mathrm{P}\bar{j}$. Entonces%
\begin{align*}
s(i,x,y,\mathcal{P})  & =i+1\\
S_{\#}(i,x,y,\mathcal{P})  & =x\\
S_{\ast}(i,x,y,\mathcal{P})  & =F_{\curvearrowright}(y,\#Var1(I_{i}%
^{\mathcal{P}}))
\end{align*}


\item[(10)] CASO $(\exists j,k\in\omega)\;Bas(I_{i}^{\mathcal{P}}%
)=\mathrm{P}\bar{j}\leftarrow\mathrm{P}\bar{k}$. Entonces%
\begin{align*}
s(i,x,y,\mathcal{P})  & =i+1\\
S_{\#}(i,x,y,\mathcal{P})  & =x\\
S_{\ast}(i,x,y,\mathcal{P})  & =F_{\leftarrow}(y,\#Var1(I_{i}^{\mathcal{P}%
}),\#Var2(I_{i}^{\mathcal{P}}))
\end{align*}


\item[(11)] CASO $(\exists j\in\omega)\;Bas(I_{i}^{\mathcal{P}})=\mathrm{P}%
\bar{j}\leftarrow\varepsilon$. Entonces%
\begin{align*}
s(i,x,y,\mathcal{P})  & =i+1\\
S_{\#}(i,x,y,\mathcal{P})  & =x\\
S_{\ast}(i,x,y,\mathcal{P})  & =F_{0}(y,\#Var1(I_{i}^{\mathcal{P}}))
\end{align*}


\item[(12)] CASO $(\exists j,m\in\omega)(\exists a\in\Sigma)\;\left(
Bas(I_{i}^{\mathcal{P}})=\mathrm{IF}\;\mathrm{P}\bar{j}\;\mathrm{BEGINS}%
\;a\;\mathrm{GOTO}\;\mathrm{L}\bar{m}\wedge\lbrack\ast^{\leq}((y)_{j}%
)]_{1}\neq a\right)  $. Entonces%
\begin{align*}
s(i,x,y,\mathcal{P})  & =i+1\\
S_{\#}(i,x,y,\mathcal{P})  & =x\\
S_{\ast}(i,x,y,\mathcal{P})  & =y
\end{align*}


\item[(13)] CASO $(\exists j,m\in\omega)(\exists a\in\Sigma)\;\left(
Bas(I_{i}^{\mathcal{P}})=\mathrm{IF\;P}\bar{j}\;\mathrm{BEGINS\;}%
a\;\mathrm{GOTO\;L}\bar{m}\wedge\lbrack\ast^{\leq}((y)_{j})]_{1}=a\right)  $.
Entonces%
\begin{align*}
s(i,x,y,\mathcal{P})  & =\min_{l}\left(  Lab(I_{l}^{\mathcal{P}}%
)\neq\varepsilon\wedge Lab(I_{l}^{\mathcal{P}})\text{ }\mathrm{t}%
\text{\textrm{-final} }I_{i}^{\mathcal{P}}\right) \\
S_{\#}(i,x,y,\mathcal{P})  & =x\\
S_{\ast}(i,x,y,\mathcal{P})  & =y
\end{align*}


\item[(14)] CASO $(\exists j\in\omega)\;Bas(I_{i}^{\mathcal{P}})=\mathrm{GOTO}%
$ $\mathrm{L}\bar{j}$. Entonces%
\begin{align*}
s(i,x,y,\mathcal{P})  & =\min_{l}\left(  Lab(I_{l}^{\mathcal{P}}%
)\neq\varepsilon\wedge Lab(I_{l}^{\mathcal{P}})\text{ }\mathrm{t}%
\text{\textrm{-final} }I_{i}^{\mathcal{P}}\right) \\
S_{\#}(i,x,y,\mathcal{P})  & =x\\
S_{\ast}(i,x,y,\mathcal{P})  & =y
\end{align*}


\item[(15)] CASO $Bas(I_{i}^{\mathcal{P}})=\mathrm{SKIP}$. Entonces%
\begin{align*}
s(i,x,y,\mathcal{P})  & =i+1\\
S_{\#}(i,x,y,\mathcal{P})  & =x\\
S_{\ast}(i,x,y,\mathcal{P})  & =y
\end{align*}

\end{enumerate}

O sea que los casos anteriores nos permiten definir conjuntos $S_{1}%
,...,S_{15}$, los cuales son disjuntos de a pares y cuya union da el conjunto
$\omega\times\mathbf{N}\times\mathbf{N}\times\mathrm{Pro}^{\Sigma}$, de manera
que cada una de las funciones $s,S_{\#}$ y $S_{\ast}$ pueden escribirse como
union disjunta de funciones $(\Sigma\cup\Sigma_{p}) $-p.r. restrinjidas
respectivamente a los conjuntos $S_{1},...,S_{15}$. Ya que los conjuntos
$S_{1},...,S_{15}$ son $(\Sigma\cup\Sigma_{p})$-p.r. el Lema \ref{dpc} nos
dice que $s,S_{\#}$ y $S_{\ast}$ lo son.
\end{proof}

\bigskip

Aparte del lema anterior, para probar que las funciones $i^{n,m}$,
$E_{\#j}^{n,m}$ y $E_{\ast j}^{n,m}$ son $(\Sigma\cup\Sigma_{p})$-p.r., nos
sera necesario el siguiente resultado. Recordemos que para $x_{1},...,x_{n}%
\in\omega$, usabamos $\left\langle x_{1},...,x_{n}\right\rangle $ para denotar
$\left\langle x_{1},...,x_{n},0,...\right\rangle $. Ademas recordemos que en
el Lema \ref{CodificadorasSonPR} probamos que para cada $n\geq1$, la funcion
$\lambda x_{1}...x_{n}\left[  \left\langle x_{1},...,x_{n}\right\rangle
\right]  $ es $\emptyset$-p.r.

\begin{lemma}
\label{recursion primitiva multiple}Sean%
\begin{align*}
f_{i}  & :S_{1}\times...\times S_{n}\times L_{1}\times...\times L_{m}%
\rightarrow\omega\\
g_{i}  & :\omega^{k}\times\omega\times S_{1}\times...\times S_{n}\times
L_{1}\times...\times L_{m}\rightarrow\omega\\
F_{i}  & :\omega\times S_{1}\times...\times S_{n}\times L_{1}\times...\times
L_{m}\rightarrow\omega
\end{align*}
con $i=1,...,k$, funciones $\Sigma$-mixtas. Supongamos que%
\begin{align*}
F_{i}(0,\vec{x},\vec{\alpha})  & =f_{i}(0,\vec{x},\vec{\alpha})\\
F_{i}(t+1,\vec{x},\vec{\alpha})  & =g_{i}(F_{1}(t,\vec{x},\vec{\alpha
}),...,F_{k}(t,\vec{x},\vec{\alpha}),t,\vec{x},\vec{\alpha})
\end{align*}
para cada $i=1,...,k$, $t\in\omega$ y $(\vec{x},\vec{\alpha})\in S_{1}%
\times...\times S_{n}\times L_{1}\times...\times L_{m}$. Entonces si las
funciones $f_{1},...,f_{k},g_{1},...,g_{k}$ son $\Sigma$-p.r., las funciones
$F_{1},...,F_{k}$ lo son.
\end{lemma}

\begin{proof}
Para mayor claridad haremos el caso $k=2$. Sea%
\[
F=\lambda t\vec{x}\vec{\alpha}\left[  \left\langle F_{1}(t,\vec{x},\vec
{\alpha}),F_{2}(t,\vec{x},\vec{\alpha})\right\rangle \right]
\]
Es claro que si $F$ es $\Sigma$-p.r., entonces lo es cada $F_{i}$. Notese que%
\begin{align*}
F(0,\vec{x},\vec{\alpha})  & =\left\langle f_{1}(\vec{x},\vec{\alpha}%
),f_{2}(\vec{x},\vec{\alpha})\right\rangle \\
F(t+1,\vec{x},\vec{\alpha})  & =\left\langle g_{1}((F(t,\vec{x},\vec{\alpha
}))_{1},(F(t,\vec{x},\vec{\alpha}))_{2},t,\vec{x},\vec{\alpha}),g_{2}%
((F(t,\vec{x},\vec{\alpha}))_{1},(F(t,\vec{x},\vec{\alpha}))_{2},t,\vec
{x},\vec{\alpha})\right\rangle
\end{align*}
lo cual nos dice que $F=R(f,g)$ donde%
\begin{align*}
f  & =\lambda\vec{x}\vec{\alpha}\left[  \left\langle f_{1}(\vec{x},\vec
{\alpha}),f_{2}(\vec{x},\vec{\alpha})\right\rangle \right] \\
g  & =\lambda At\vec{x}\vec{\alpha}\left[  \left\langle g_{1}((A)_{1}%
,(A)_{2},t,\vec{x},\vec{\alpha}),g_{2}((A)_{1},(A)_{2},t,\vec{x},\vec{\alpha
})\right\rangle \right]
\end{align*}

\end{proof}

@@finpagina@@

Ahora usando los dos lemas anteriores podemos probar el siguiente importante resultado.

\begin{proposition}
Sean $n,m\geq0$. Las funciones $i^{n,m}$, $E_{\#j}^{n,m}$, $E_{\ast j}^{n,m}
$, $j=1,2,...$, son $(\Sigma\cup\Sigma_{p})$-p.r.
\end{proposition}

\begin{proof}
Sea $\leq$ un orden total sobre $\Sigma\cup\Sigma_{p}$ y sean $s$, $S_{\#}$ y
$S_{\ast}$ las funciones definidas previamente al Lema
\ref{descripcion instantanea sucesora es PR}. Definamos%
\begin{align*}
K_{\#}^{n,m}  & =\lambda t\vec{x}\vec{\alpha}\mathcal{P}\left[  \left\langle
E_{\#1}^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P}),E_{\#2}^{n,m}(t,\vec{x}%
,\vec{\alpha},\mathcal{P}),...\right\rangle \right] \\
K_{\ast}^{n,m}  & =\lambda t\vec{x}\vec{\alpha}\mathcal{P}\left[  \left\langle
\#^{\leq}(E_{\ast1}^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P})),\#^{\leq
}(E_{\ast2}^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P})),...\right\rangle
\right]
\end{align*}
Notese que%
\begin{align*}
i^{n,m}(0,\vec{x},\vec{\alpha},\mathcal{P})  & =1\\
K_{\#}^{n,m}(0,\vec{x},\vec{\alpha},\mathcal{P})  & =\left\langle
x_{1},...,x_{n}\right\rangle \\
K_{\ast}^{n,m}(0,\vec{x},\vec{\alpha},\mathcal{P})  & =\left\langle \#^{\leq
}(\alpha_{1}),...,\#^{\leq}(\alpha_{m})\right\rangle \\
i^{n,m}(t+1,\vec{x},\vec{\alpha},\mathcal{P})  & =s(i^{n,m}(t,\vec{x}%
,\vec{\alpha},\mathcal{P}),K_{\#}^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P}%
),K_{\ast}^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P}))\\
K_{\#}^{n,m}(t+1,\vec{x},\vec{\alpha},\mathcal{P})  & =S_{\#}(i^{n,m}%
(t,\vec{x},\vec{\alpha},\mathcal{P}),K_{\#}^{n,m}(t,\vec{x},\vec{\alpha
},\mathcal{P}),K_{\ast}^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P}))\\
K_{\ast}^{n,m}(t+1,\vec{x},\vec{\alpha},\mathcal{P})  & =S_{\ast}%
(i^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P}),K_{\#}^{n,m}(t,\vec{x}%
,\vec{\alpha},\mathcal{P}),K_{\ast}^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P}))
\end{align*}
Por el Lema \ref{recursion primitiva multiple} tenemos que $i^{n,m}$,
$K_{\#}^{n,m}$ y $K_{\ast}^{n,m}$ son $(\Sigma\cup\Sigma_{p})$-p.r.. Ademas
notese que%
\begin{align*}
E_{\#j}^{n,m}  & =\lambda t\vec{x}\vec{\alpha}\mathcal{P}\left[  (K_{\#}%
^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P}))_{j}\right] \\
E_{\ast j}^{n,m}  & =\lambda t\vec{x}\vec{\alpha}\mathcal{P}\left[  \ast
^{\leq}((K_{\ast}^{n,m}(t,\vec{x},\vec{\alpha},\mathcal{P}))_{j})\right]
\end{align*}
por lo cual las funciones $E_{\#j}^{n,m}$, $E_{\ast j}^{n,m}$, $j=1,2,...$,
son $(\Sigma\cup\Sigma_{p})$-p.r.
\end{proof}

\bigskip

\subparagraph{Las funciones $Halt^{n,m}$ y $T^{n,m}$}

Dados $n,m\in\omega$, definamos:%
\[
Halt^{n,m}=\lambda t\vec{x}\vec{\alpha}\mathcal{P}\left[  i^{n,m}(t,\vec
{x},\vec{\alpha},\mathcal{P})=n(\mathcal{P})+1\right]
\]
Notese que $D_{Halt^{n,m}}=\omega\times\omega^{n}\times\Sigma^{\ast m}%
\times\mathrm{Pro}^{\Sigma}$ (ojo que aqui la notacion lambda es respecto del
alfabeto $\Sigma\cup\Sigma_{p}$). Ademas notese que usamos la variable
$\mathcal{P}$ en la notacion lambda por un tema de comodidad psicologica dado
que $i^{n,m}$ esta definida solo cuando la ultima coordenada es un programa
pero podriamos haber escrito $\lambda t\vec{x}\vec{\alpha}\alpha\left[
i^{n,m}(t,\vec{x},\vec{\alpha},\alpha)=n(\alpha)+1\right]  $ y sigue siendo la
misma funcion.

Cabe destacar que $Halt^{n,m}$ tiene una descripcion muy intuitiva, ya que
dado $(t,\vec{x},\vec{\alpha},\mathcal{P})\in\omega\times\omega^{n}%
\times\Sigma^{\ast m}\times\mathrm{Pro}^{\Sigma}$, tenemos que $Halt^{n,m}%
(t,\vec{x},\vec{\alpha},\mathcal{P})=1$ si y solo si el programa $\mathcal{P}$
se detiene luego de $t$ pasos partiendo desde el estado $\left\Vert
x_{1},...,x_{n},\alpha_{1},...,\alpha_{m}\right\Vert $.

\begin{lemma}
$Halt^{n,m}$ es $(\Sigma\cup\Sigma_{p})$-p.r.
\end{lemma}

\begin{proof}
Notar que $Halt^{n,m}=\lambda xy[x=y]\circ\left[  i^{n,m},\lambda
\mathcal{P}[n(\mathcal{P})+1]\circ p_{1+n+m+1}^{1+n,m+1}\right]  $.
\end{proof}

\bigskip

Ahora definamos $T^{n,m}=M(Halt^{n,m})$. Notese que%
\[
D_{T^{n,m}}=\{(\vec{x},\vec{\alpha},\mathcal{P}):\mathcal{P}\text{ se detiene
partiendo de }\left\Vert x_{1},...,x_{n},\alpha_{1},...,\alpha_{m}\right\Vert
\}
\]
y para $(\vec{x},\vec{\alpha},\mathcal{P})\in D_{T^{n,m}}$ tenemos que
$T^{n,m}(\vec{x},\vec{\alpha},\mathcal{P})=$ cantidad de pasos necesarios para
que $\mathcal{P}$ se detenga partiendo de $\left\Vert x_{1},...,x_{n}%
,\alpha_{1},...,\alpha_{m}\right\Vert $. En algun sentido, la funcion
$T^{n,m}$ mide el tiempo que tarda en detenerse $\mathcal{P}$

\begin{proposition}
$T^{n,m}$ es $(\Sigma\cup\Sigma_{p})$-recursiva
\end{proposition}

\begin{proof}
Es directo del Lema \ref{minimizacion} ya que $Halt^{n,m}$ es $(\Sigma
\cup\Sigma_{p})$-p.r.
\end{proof}

\bigskip

\subparagraph{Las funciones $\Phi_{\#}^{n,m}$ y $\Phi_{\ast}^{n,m}$}

Para $n,m\in\omega$ definamos la funcion $\Phi_{\#}^{n,m}$ de la siguiente
manera:%
\begin{align*}
D_{\Phi_{\#}^{n,m}}  & =\left\{  (\vec{x},\vec{\alpha},\mathcal{P})\in
\omega^{n}\times\Sigma^{\ast m}\times\mathrm{Pro}^{\Sigma}:(\vec{x}%
,\vec{\alpha})\in D_{\Psi_{\mathcal{P}}^{n,m,\#}}\right\} \\
\Phi_{\#}^{n,m}(\vec{x},\vec{\alpha},\mathcal{P})  & =\Psi_{\mathcal{P}%
}^{n,m,\#}(\vec{x},\vec{\alpha})\text{, para cada }(\vec{x},\vec{\alpha
},\mathcal{P})\in D_{\Phi_{\#}^{n,m}}%
\end{align*}
Notese que%
\[
D_{\Phi_{\#}^{n,m}}=\{(\vec{x},\vec{\alpha},\mathcal{P}):\mathcal{P}\text{ se
detiene partiendo de }\left\Vert x_{1},...,x_{n},\alpha_{1},...,\alpha
_{m}\right\Vert \}
\]
y para cada $(\vec{x},\vec{\alpha},\mathcal{P})\in D_{\Phi_{\#}^{n,m}}$, se
tiene que $\Phi_{\#}^{n,m}(\vec{x},\vec{\alpha},\mathcal{P})=$ valor que queda
en la variable $\mathrm{N}1$ cuando $\mathcal{P}$ se detiene partiendo de
$\left\Vert x_{1},...,x_{n},\alpha_{1},...,\alpha_{m}\right\Vert $.

Similarmente, definamos la funcion $\Phi_{\ast}^{n,m}$ de la siguiente manera:%
\begin{align*}
D_{\Phi_{\ast}^{n,m}}  & =\left\{  (\vec{x},\vec{\alpha},\mathcal{P})\in
\omega^{n}\times\Sigma^{\ast m}\times\mathrm{Pro}^{\Sigma}:(\vec{x}%
,\vec{\alpha})\in D_{\Psi_{\mathcal{P}}^{n,m,\ast}}\right\} \\
\Phi_{\ast}^{n,m}(\vec{x},\vec{\alpha},\mathcal{P})  & =\Psi_{\mathcal{P}%
}^{n,m,\ast}(\vec{x},\vec{\alpha})\text{, para cada }(\vec{x},\vec{\alpha
},\mathcal{P})\in D_{\Phi_{\ast}^{n,m}}%
\end{align*}
Notese que%
\begin{align*}
\Phi_{\#}^{n,m}  & =\lambda\vec{x}\vec{\alpha}\mathcal{P}\left[
\Psi_{\mathcal{P}}^{n,m,\#}(\vec{x},\vec{\alpha})\right] \\
\Phi_{\ast}^{n,m}  & =\lambda\vec{x}\vec{\alpha}\mathcal{P}\left[
\Psi_{\mathcal{P}}^{n,m,\ast}(\vec{x},\vec{\alpha})\right]
\end{align*}


\begin{theorem}
Las funciones $\Phi_{\#}^{n,m}$ y $\Phi_{\ast}^{n,m}$ son $(\Sigma\cup
\Sigma_{p})$-recursivas.
\end{theorem}

\begin{proof}
Veremos que $\Phi_{\#}^{n,m}$ es $(\Sigma\cup\Sigma_{p})$-recursiva. Notar que
$D_{T^{n,m}}=D_{\Phi_{\#}^{n,m}}$. Notese que para $(\vec{x},\vec{\alpha
},\mathcal{P})\in D_{T^{n,m}}=D_{\Phi_{\#}^{n,m}}$ tenemos que%
\[
\Phi_{\#}^{n,m}(\vec{x},\vec{\alpha},\mathcal{P})=E_{\#1}^{n,m}\left(
T^{n,m}(\vec{x},\vec{\alpha},\mathcal{P}),\vec{x},\vec{\alpha},\mathcal{P}%
\right)
\]
lo cual con un poco mas de trabajo nos permite probar que%
\[
\Phi_{\#}^{n,m}=E_{\#1}^{n,m}\circ\left[  T^{n,m},p_{1}^{n,m+1},...,p_{n+m+1}%
^{n,m+1}\right]
\]
Ya que la funcion $E_{\#1}^{n,m}$ es $(\Sigma\cup\Sigma_{p})$-p.r. y $T^{n,m}$
es $(\Sigma\cup\Sigma_{p})$-r., tenemos que $\Phi_{\#}^{n,m}$ es $(\Sigma
\cup\Sigma_{p})$-r.
\end{proof}

\bigskip

\paragraph{Godel vence a Neumann}

Ahora nos sera facil probar que el paradigma de Godel es por lo menos tan
abarcativo como el imperativo de Von Neumann. Mas concretamente:

\begin{theorem}
\label{computable-implica-recursiva}Si $f:D_{f}\subseteq\omega^{n}\times
\Sigma^{\ast m}\rightarrow O$ es $\Sigma$-computable, entonces $f$ es $\Sigma$-recursiva.
\end{theorem}

\begin{proof}
Haremos el caso $O=\Sigma^{\ast}$. Sea $\mathcal{P}_{0}$ un programa que
compute a $f$. Primero veremos que $f$ es $(\Sigma\cup\Sigma_{p})$-recursiva.
Note que%
\[
f=\Phi_{\ast}^{n,m}\circ\left[  p_{1}^{n,m},...,p_{n+m}^{n,m},C_{\mathcal{P}%
_{0}}^{n,m}\right]
\]
donde cabe destacar que $p_{1}^{n,m},...,p_{n+m}^{n,m}$ son las proyecciones
respecto del alfabeto $\Sigma\cup\Sigma_{p}$, es decir que tienen dominio
$\omega^{n}\times(\Sigma\cup\Sigma_{p})^{\ast m}$. Ya que $\Phi_{\ast}^{n,m}$
es $(\Sigma\cup\Sigma_{p})$-recursiva tenemos que $f$ lo es. O sea que el
Teorema \ref{independencia} nos dice que $f$ es $\Sigma$-recursiva.
\end{proof}

\bigskip

Un corolario interesante que se puede obtener del teorema anterior es que toda
funcion $\Sigma$-recursiva puede obtenerse combinando las reglas basicas en
una forma muy particular.

\begin{corollary}
Si $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow O$ es $\Sigma
$-recursiva, entonces existe un predicado $\Sigma$-p.r. $P:\mathbf{N}%
\times\omega^{n}\times\Sigma^{\ast m}\rightarrow\omega$ y una funcion $\Sigma
$-p.r. $g:\mathbf{N}\rightarrow O$ tales que $f=g\circ M(P).$
\end{corollary}

\begin{proof}
Supongamos que $O=\Sigma^{\ast}$. Sea $\mathcal{P}_{0}$ un programa el cual
compute a $f$. Sea $\leq$ un orden total sobre $\Sigma$. Note que podemos
tomar%
\begin{align*}
P  & =\lambda t\vec{x}\vec{\alpha}[Halt^{n,m}\left(  (t)_{1},\vec{x}%
,\vec{\alpha},\mathcal{P}_{0}\right)  \wedge(t)_{2}=\#^{\leq}(E_{\ast1}%
^{n,m}((t)_{1},\vec{x},\vec{\alpha},\mathcal{P}_{0}))]\\
g  & =\lambda t\left[  \ast^{\leq}((t)_{2})\right]
\end{align*}
(Justifique por que $P$ es $\Sigma$-p.r..)
\end{proof}

\bigskip

A continuacion veremos ejemplos naturales de funciones $(\Sigma\cup\Sigma
_{p})$-recursivas que no son $(\Sigma\cup\Sigma_{p})$-recursivas primitivas.
Cabe destacar que la prueba se basa en la Proposicion
\ref{recursivo no implica PR} (enunciada sin demostracion) la cual nos dice
que cualquiera sea el alfabeto finito $\Sigma$, siempre hay una funcion que es
$\Sigma$-recursiva y no es $\Sigma$-recursiva primitiva

\bigskip

\begin{proposition}
\label{T y FI no son PR}Cualesquiera sean $n,m\in\omega$, se tiene que las
funciones $T^{n,m}$, $\Phi_{\#}^{n,m}$ y $\Phi_{\ast}^{n,m}$ no son
$(\Sigma\cup\Sigma_{p})$-p.r.
\end{proposition}

\begin{proof}
Fijemos $n,m\in\omega$. Probaremos que $\Phi_{\#}^{n,m}$ es $(\Sigma\cup
\Sigma_{p})$-p.r. sii $\Phi_{\#}^{0,0}$ es $(\Sigma\cup\Sigma_{p})$-p.r.. Sean
$f_{1},f_{2}:\omega^{n}\times\Sigma^{\ast m}\rightarrow(\Sigma\cup\Sigma
_{p})^{\ast}$ dadas por%
\begin{align*}
f_{1}(\vec{x},\vec{\alpha})  & =(\mathrm{N}1\leftarrow\mathrm{N}1+1)^{x_{1}%
}...(\mathrm{N}\bar{n}\leftarrow\mathrm{N}\bar{n}+1)^{x_{n}}\\
f_{1}(\vec{x},\vec{\alpha})  & =\left(  \subset_{i=1}^{i=\left\vert \alpha
_{1}\right\vert }\mathrm{P}1\leftarrow\mathrm{P}1.[\alpha_{1}]_{i}\right)
...\left(  \subset_{i=1}^{i=\left\vert \alpha_{m}\right\vert }\mathrm{P}%
1\leftarrow\mathrm{P}1.[\alpha_{m}]_{i}\right)
\end{align*}
Sea $f:\omega^{n}\times\Sigma^{\ast m}\times\mathrm{Pro}^{\Sigma}%
\rightarrow(\Sigma\cup\Sigma_{p})^{\ast}$ dada por%
\[
f(\vec{x},\vec{\alpha},\mathcal{P})=f_{1}(\vec{x},\vec{\alpha})f_{1}(\vec
{x},\vec{\alpha})\mathcal{P}%
\]
Es facil ver que $f$ es $(\Sigma\cup\Sigma_{p})$-p.r.. Notese que $\Phi
_{\#}^{n,m}=\Phi_{\#}^{0,0}\circ f$. Ademas notese que%
\[
\Phi_{\#}^{0,0}=\Phi_{\#}^{n,m}\circ\left[  C_{0}^{0,1},...,C_{0}%
^{0,1},C_{\varepsilon}^{0,1},...,C_{\varepsilon}^{0,1},p_{1}^{0,1}\right]
\]
Ya que $f$ y las funciones $C_{0}^{0,1},C_{\varepsilon}^{0,1},p_{1}^{0,1}$ son
$(\Sigma\cup\Sigma_{p})$-p.r., tenemos que $\Phi_{\#}^{n,m}$ es $(\Sigma
\cup\Sigma_{p})$-p.r. sii $\Phi_{\#}^{0,0}$ lo es.

Supongamos ahora que para algunos $k,l\in\omega$ se tiene que $\Phi_{\#}%
^{k,l}$ es $(\Sigma\cup\Sigma_{p})$-p.r.. Llegaremos a un absurdo. Por lo
antes probado tenemos que $\Phi_{\#}^{n,m}$ es $(\Sigma\cup\Sigma_{p})$-p.r.,
cualesquiera sean $n,m\in\omega$. Notese que de la prueba del teorema anterior
sigue que toda funcion $\Sigma$-computable con imagen contenida en $\omega$ es
de la forma $\Phi_{\#}^{n,m}\circ\left[  p_{1}^{n,m},...,p_{n+m}%
^{n,m},C_{\mathcal{P}_{0}}^{n,m}\right]  $, para algunos $n,m\in\omega$ y
$\mathcal{P}_{0}\in\mathrm{Pro}^{\Sigma}$. Pero entonces toda funcion $\Sigma
$-computable con imagen contenida en $\omega$ es $(\Sigma\cup\Sigma_{p}%
)$-p.r., lo cual por el Teorema \ref{computable-implica-recursiva} nos dice
que toda funcion $\Sigma$-computable con imagen contenida en $\omega$ es
$(\Sigma\cup\Sigma_{p})$-p.r.. Esto contradice la Proposicion
\ref{recursivo no implica PR}.

Ahora supongamos que hay $n,m\in\omega$ tales que $T^{n,m}$ es $(\Sigma
\cup\Sigma_{p})$-p.r.. Llegaremos a un absurdo. Como ya vimos en la prueba de
un teorema reciente, se tiene que%
\[
\Phi_{\#}^{n,m}=E_{\#1}^{n,m}\circ\left[  T^{n,m},p_{1}^{n,m+1},...,p_{n+m+1}%
^{n,m+1}\right]
\]
Pero entonces ya que $E_{\#1}^{n,m}$ es $(\Sigma\cup\Sigma_{p})$-p.r., tenemos
que $\Phi_{\#}^{n,m}$ es $(\Sigma\cup\Sigma_{p})$-p.r., lo cual como ya vimos
recien no es cierto. El absurdo nos dice que $T^{n,m}$ no es $(\Sigma
\cup\Sigma_{p})$-p.r..
\end{proof}

\begin{corollary}
\label{minimizacion de PR que no es PR}La minimizacion de un predicado
$\Sigma$-p.r. no necesariamente es $\Sigma$-p.r.
\end{corollary}

\begin{proof}
Por definicion $T^{n,m}=M(Halt^{n,m})$.
\end{proof}

\bigskip

\paragraph{Uso de macros asociados a las funciones $Halt^{n,m}$, $E_{\#}%
^{n,m}$ y $E_{\ast}^{n,m}$}

Aqui veremos, con ejemplos, como ciertos macros nos permitiran dentro de un
programa hablar acerca del funcionamiento de otro programa. Esto junto con el
hecho que cada funcion $\Sigma$-recursiva y cada predicado $\Sigma$-recursivo
tienen su macro asociado (Corolario \ref{recursivo implica macro}), sera muy
util a la hora del dise\~{n}o de programas y nos permitira simular dentro del
paradigma imperativo muchas ideas usadas para el dise\~{n}o de procedimientos
efectivos. En este sentido la convinacion de los dos paradigmas (recursivo e
imperativo) nos permite fortalecer notablemente al paradigma imperativo en su
roll modelizador (o simulador) de los procedimientos efectivos. Esto es
importante ya que el paradigma mas comodo, amplio e intuitivo, a la hora de
decidir si algo es o no computable, es sin duda el filosofico o efectivo.

Veamos el primer ejemplo. Sea $\Sigma=\{@,!\}$ y sea $\mathcal{P}_{0}%
\in\mathrm{Pro}^{\Sigma}$ tal que $0\in\mathrm{Dom}\Psi_{\mathcal{P}_{0}%
}^{1,0,\#}$ y $\Psi_{\mathcal{P}_{0}}^{1,0,\#}(0)=2$. Probaremos que%
\[
S=\{x\in\mathrm{Dom}\Psi_{\mathcal{P}_{0}}^{1,0,\#}:\Psi_{\mathcal{P}_{0}%
}^{1,0,\#}(x)\neq0\}
\]
es $\Sigma$-enumerable. Notese que $0\in S$. Por definicion de conjunto
$\Sigma$-enumerable, deberemos encontrar un programa $\mathcal{P}%
\in\mathrm{Pro}^{\Sigma}$ tal que $\mathrm{Dom}\Psi_{\mathcal{P}}%
^{1,0,\#}=\omega$ y $\operatorname{Im}\Psi_{\mathcal{P}}^{1,0,\#}=S$. Dicho en
palabras, el programa $\mathcal{P}$ debera cumplir:

\begin{enumerate}
\item[-] siempre que lo corramos desde un estado de la forma $\left\Vert
x\right\Vert $, con $x\in\omega$, debe detenerse y el contenido de la variable
$\mathrm{N}1$ bajo detencion debera ser un elemento de $S$

\item[-] para cada $s\in S$ debera haber un $x\in\omega$ tal que $s$ es el
valor de la variable $\mathrm{N}1$ bajo detencion cuando corremos
$\mathcal{P}$ desde $\left\Vert x\right\Vert $
\end{enumerate}

\bigskip

A continuacion daremos una descripcion intuitiva del funcionamiento de
$\mathcal{P}$ (pseudocodigo) para luego escribirlo correctamente usando
macros. El programa $\mathcal{P}$ comenzara del estado $\left\Vert
x\right\Vert $ y hara las siguientes tareas

\bigskip

\begin{enumerate}
\item[ ] Etapa 1: si $x=0$ ir a Etapa 6, en caso contrario ir a Etapa 2.

\item[ ] Etapa 2: calcular $(x)_{1}$ y $(x)_{2}$ e ir a Etapa 3.

\item[ ] Etapa 3: si $\mathcal{P}_{0}$ termina desde $\left\Vert
(x)_{1}\right\Vert $ en $(x)_{2}$ pasos ir a Etapa 4, en caso contrario ir a
Etapa 6

\item[ ] Etapa 4: si el valor que queda en $\mathrm{N}1$ luego de correr
$\mathcal{P}_{0}$ una cantidad $(x)_{2}$ de pasos, partiendo de $\left\Vert
(x)_{1}\right\Vert $, es distinto de 0, entonces ir a Etapa 5. En caso
contrario ir a Etapa 6.

\item[ ] Etapa 5: asignar a $\mathrm{N}1$ el valor $(x)_{1}$ y terminar

\item[ ] Etapa 6: asignar a $\mathrm{N}1$ el valor $0$ y terminar
\end{enumerate}

\bigskip

Notese que la descripcion anterior no es ni mas ni menos que un procedimiento
efectivo que enumera a $S$, y nuestra mision es simularlo dentro del lenguaje
$\mathcal{S}^{\Sigma}$. Para esto usaremos varios macros. Ya que la funcion
$f=\lambda x[(x)_{1}]$ es $\Sigma$-p.r., el Corolario
\ref{recursivo implica macro} nos dice que hay un macro:%
\[
\lbrack\mathrm{V}2\leftarrow f(\mathrm{V}1)]
\]
el cual escribiremos de la siguiente manera mas intuitiva:%
\[
\lbrack\mathrm{V}2\leftarrow(\mathrm{V}1)_{1}]
\]
Similarmente hay un macro:%
\[
\lbrack\mathrm{V}2\leftarrow(\mathrm{V}1)_{2}]
\]
Tambien, ya que el predicado $P=\lambda x[x=0]$ es $\Sigma$-recursivo, hay un
macro:%
\[
\left[  \mathrm{IF}\;P(\mathrm{V}1)\;\mathrm{GOTO}\;\mathrm{A}1\right]
\]
el cual escribiremos de la siguiente manera:%
\[
\left[  \mathrm{IF}\;\mathrm{V}1=0\;\mathrm{GOTO}\;\mathrm{A}1\right]
\]
Definamos%
\[
H=\lambda tx\left[  Halt^{1,0}(t,x,\mathcal{P}_{0})\right]
\]
Notar que $D_{H}=\omega^{2}$ y que $H$ es $\Sigma$-mixta. Ademas sabemos que
la funcion $Halt^{1,0}$ es $(\Sigma\cup\Sigma_{p})$-p.r. por lo cual resulta
facilmente que $H$ es $(\Sigma\cup\Sigma_{p})$-p.r.. Por la Proposicion de
Independencia del Alfabeto tenemos que $H$ es $\Sigma$-p.r.. O sea que el
Corolario \ref{recursivo implica macro} nos dice que hay un macro:%
\[
\left[  \mathrm{IF}\;H(\mathrm{V}1,\mathrm{V}2)\;\mathrm{GOTO}\;\mathrm{A}%
1\right]
\]
Para hacer mas intuitivo el uso de este macro lo escribiremos de la siguiente
manera%
\[
\left[  \mathrm{IF}\;Halt^{1,0}(\mathrm{V}1,\mathrm{V}2,\mathcal{P}%
_{0})\;\mathrm{GOTO}\;\mathrm{A}1\right]
\]
Sea%
\[
g=\lambda tx\left[  E_{\#1}^{1,0}(t,x,\mathcal{P}_{0})\right]
\]
Ya que $g$ es $\Sigma$-recursiva (por que?), hay un macro:%
\[
\left[  \mathrm{V}3\leftarrow g(\mathrm{V}1,\mathrm{V}2)\right]
\]
Para hacer mas intuitivo el uso de este macro lo escribiremos de la siguiente
manera%
\[
\left[  \mathrm{V}3\leftarrow E_{\#1}^{1,0}(\mathrm{V}1,\mathrm{V}%
2,\mathcal{P}_{0})\right]
\]
Ahora si podemos dar nuestro progama $\mathcal{P}$ que enumera a $S$:%
\[%
\begin{array}
[c]{ll}
& \mathrm{IF}\;\mathrm{N}1\neq0\;\mathrm{GOTO}\;\mathrm{L}1\\
& \mathrm{GOTO}\;\mathrm{L}2\\
\mathrm{L}1 & [\mathrm{N}3\leftarrow(\mathrm{N}1)_{1}]\\
& [\mathrm{N}4\leftarrow(\mathrm{N}1)_{2}]\\
& \left[  \mathrm{IF}\;Halt^{1,0}(\mathrm{N}4,\mathrm{N}3,\mathcal{P}%
_{0})\;\mathrm{GOTO}\;\mathrm{L}3\right] \\
& \mathrm{GOTO}\;\mathrm{L}2\\
\mathrm{L}3 & \left[  \mathrm{N}5\leftarrow E_{\#1}^{1,0}(\mathrm{N}%
4,\mathrm{N}3,\mathcal{P}_{0})\right] \\
& [\mathrm{IF}\;\mathrm{N}5=0\;\mathrm{GOTO}\;\mathrm{L}2]\\
& \mathrm{N}1\leftarrow\mathrm{N}3\\
& \mathrm{GOTO}\;\mathrm{L}4\\
\mathrm{L}2 & \mathrm{N}1\leftarrow0\\
\mathrm{L}4 & \mathrm{SKIP}%
\end{array}
\]


\bigskip

\subparagraph{Enumeracion de conjuntos de programas}

Ya que los programas de $\mathcal{S}^{\Sigma}$ son palabras del alfabeto
$\Sigma\cup\Sigma_{p}$, nos podemos preguntar cuando un conjunto $L$ de
programas es $(\Sigma\cup\Sigma_{p})$-enumerable. Daremos un ejemplo. Sea
$\Sigma=\{@,!\}$ y sea%
\[
L=\{\mathcal{P}\in\mathrm{Pro}^{\Sigma}:\Psi_{\mathcal{P}}^{1,0,\#}(10)=10\}
\]
Veremos que $L$ es $(\Sigma\cup\Sigma_{p})$-enumerable, dando un programa
$\mathcal{Q}\in\mathrm{Pro}^{\Sigma\cup\Sigma_{p}}$ que enumere a $L$, es
decir tal que \textrm{Dom}$(\Psi_{\mathcal{Q}}^{1,0,\ast})=\omega$ y
$\operatorname{Im}(\Psi_{\mathcal{Q}}^{1,0,\ast})=L$. Cabe destacar que aqui
hay en juego dos versiones de nuestro lenguaje imperativo, es decir
enumeraremos un conjunto de programas de $\mathcal{S}^{\Sigma}$ usando un
programa de $\mathcal{S}^{\Sigma\cup\Sigma_{p}}$. Sea $\leq$ un orden total
sobre el conjunto $\Sigma\cup\Sigma_{p}$.

A continuacion daremos una descripcion intuitiva del funcionamiento de
$\mathcal{Q}$ (pseudocodigo) para luego escribirlo correctamente usando
macros. Notese que $\mathrm{SKIP}\in L$. El programa $\mathcal{Q}$ comenzara
del estado $\left\Vert x\right\Vert $ y hara las siguientes tareas

\bigskip

\begin{enumerate}
\item[ ] Etapa 1: si $x=0$ ir a Etapa 6, en caso contrario ir a Etapa 2.

\item[ ] Etapa 2: calcular $(x)_{1}$, $(x)_{2}$ y $\ast^{\leq}((x)_{1})$ e ir
a Etapa 3.

\item[ ] Etapa 3: si $\ast^{\leq}((x)_{1})\in\mathrm{Pro}^{\Sigma}$ y termina
partiendo desde $\left\Vert 10\right\Vert $ en $(x)_{2}$ pasos ir a Etapa 4,
en caso contrario ir a Etapa 6

\item[ ] Etapa 4: si el valor que queda en $\mathrm{N}1$ luego de correr
$\ast^{\leq}((x)_{1})$ una cantidad $(x)_{2}$ de pasos, partiendo de
$\left\Vert 10\right\Vert $ es igual a 10, entonces ir a Etapa 5. En caso
contrario ir a Etapa 6.

\item[ ] Etapa 5: asignar a $\mathrm{P}1$ la palabra $\ast^{\leq}((x)_{1})$ y terminar

\item[ ] Etapa 6: asignar a $\mathrm{P}1$ la palabra $\mathrm{SKIP}$ y terminar
\end{enumerate}

\bigskip

Notese que la descripcion anterior no es ni mas ni menos que un procedimiento
efectivo que enumera a $L$, y nuestra mision es simularlo dentro del lenguaje
$\mathcal{S}^{\Sigma\cup\Sigma_{p}}$. Para esto usaremos varios macros. Es
importante notar que los macros que usaremos corresponden al lenguaje
$\mathcal{S}^{\Sigma\cup\Sigma_{p}}$ ya que los usaremos en $\mathcal{Q}$ el
cual sera un programa de $\mathcal{S}^{\Sigma\cup\Sigma_{p}}$.

Ya que las funciones $\lambda x[(x)_{1}]$ y $\lambda x[(x)_{2}]$ son
$(\Sigma\cup\Sigma_{p})$-recursivas el Corolario \ref{recursivo implica macro}
nos dice que hay macros asociados a estas funciones los cuales escribiremos de
la siguiente manera mas intuitiva:%
\begin{align*}
\lbrack\mathrm{V}2  & \leftarrow(\mathrm{V}1)_{1}]\\
\lbrack\mathrm{V}2  & \leftarrow(\mathrm{V}1)_{2}]
\end{align*}
Ya que el predicado $P=\lambda x[x=10]$ es $(\Sigma\cup\Sigma_{p})$-recursivo
tenemos su macro asociado el cual escribiremos de la siguiente manera:%
\[
\left[  \mathrm{IF}\;\mathrm{V}1=10\;\mathrm{GOTO}\;\mathrm{A}1\right]
\]
Por un lema anterior sabemos que $\mathrm{Pro}^{\Sigma}$ es un conjunto
$(\Sigma\cup\Sigma_{p})$-p.r.%
%TCIMACRO{\U{b4}}%
%BeginExpansion
\'{}%
%EndExpansion
por lo cual $\chi_{\mathrm{Pro}^{\Sigma}}^{(\Sigma\cup\Sigma_{p})^{\ast}}$ es
$(\Sigma\cup\Sigma_{p})$-p.r., por lo cual hay un macro%
\[
\left[  \mathrm{IF}\;\chi_{\mathrm{Pro}^{\Sigma}}^{(\Sigma\cup\Sigma
_{p})^{\ast}}(\mathrm{W}1)\;\mathrm{GOTO}\;\mathrm{A}1\right]
\]
el cual escribiremos de la siguiente manera%
\[
\left[  \mathrm{IF}\;\mathrm{W}1\in\mathrm{Pro}^{\Sigma}\;\mathrm{GOTO}%
\;\mathrm{A}1\right]
\]
Ya que el predicado $Halt^{1,0}$ es $(\Sigma\cup\Sigma_{p})$-recursivo tenemos
un macro asociado a el, el cual escribiremos de la siguiente forma%
\[
\left[  \mathrm{IF}\;Halt^{1,0}(\mathrm{V}1,\mathrm{V}2,\mathrm{W}%
1)\;\mathrm{GOTO}\;\mathrm{A}1\right]
\]
Ya que $E_{\#1}^{1,0}$ es $(\Sigma\cup\Sigma_{p})$-recursivo tenemos un macro
asociado a ella, el cual escribiremos de la siguiente forma%
\[
\left[  \mathrm{V}3\leftarrow E_{\#1}^{1,0}(\mathrm{V}1,\mathrm{V}%
2,\mathrm{W}1)\right]
\]
Tambien usaremos macros%
\begin{gather*}
\lbrack\mathrm{V}1\leftarrow10]\\
\left[  \mathrm{W}1\leftarrow\mathrm{SKIP}\right]
\end{gather*}
(dejamos al lector hacerlos a mano o tambien se puede justificar su existencia
via la proposicion de existencia de macros aplicada a las funciones
$C_{10}^{0,0}$ y $C_{\mathrm{SKIP}}^{0,0}$).

Ahora si podemos hacer el programa $\mathcal{Q}$ que enumera a $L$:%

\[%
\begin{array}
[c]{ll}
& \mathrm{IF}\;\mathrm{N}1\neq0\;\mathrm{GOTO}\;\mathrm{L}1\\
& \mathrm{GOTO}\;\mathrm{L}2\\
\mathrm{L}1 & [\mathrm{N}2\leftarrow(\mathrm{N}1)_{1}]\\
& [\mathrm{N}3\leftarrow(\mathrm{N}1)_{2}]\\
& [\mathrm{P}1\leftarrow\ast^{\leq}(\mathrm{N}2)]\\
& \left[  \mathrm{IF}\;\mathrm{P}1\in\mathrm{Pro}^{\Sigma}\;\mathrm{GOTO}%
\;\mathrm{L}3\right] \\
& \mathrm{GOTO}\;\mathrm{L}2\\
\mathrm{L}3 & [\mathrm{N}4\leftarrow10]\\
& \left[  \mathrm{IF}\;Halt^{1,0}(\mathrm{N}3,\mathrm{N}4,\mathrm{P}%
1)\;\mathrm{GOTO}\;\mathrm{L}4\right] \\
& \mathrm{GOTO}\;\mathrm{L}2\\
\mathrm{L}4 & \left[  \mathrm{N}5\leftarrow E_{\#1}^{1,0}(\mathrm{N}%
3,\mathrm{N}4,\mathrm{P}1)\right] \\
& [\mathrm{IF}\;\mathrm{N}5=10\;\mathrm{GOTO}\;\mathrm{L}4]\\
\mathrm{L}2 & \left[  \mathrm{P}1\leftarrow\mathrm{SKIP}\right] \\
\mathrm{L}4 & \mathrm{SKIP}%
\end{array}
\]
\bigskip

\bigskip

Cuando $\Sigma\supseteq\Sigma_{p}$ podemos correr un programa $\mathcal{P}%
\in\mathrm{Pro}^{\Sigma}$ partiendo de un estado que asigne a sus variables
alfabeticas programas (ya que los programas son meras palabras de
$\Sigma^{\ast}$). En particular podriamos correr un programa $\mathcal{P}$
desde el estado $\left\Vert \mathcal{P}\right\Vert $. Llamaremos $A$ al
conjunto formado por aquellos programas $\mathcal{P}$ tales que $\mathcal{P}$
se detiene partiendo del estado $\left\Vert \mathcal{P}\right\Vert $. Es decir%
\[
A=\{\mathcal{P}\in\mathrm{Pro}^{\Sigma}:\exists t\in\omega\text{ tal que
}Halt^{0,1}(t,\mathcal{P},\mathcal{P})=1\}
\]
Por ejemplo $\mathrm{SKIP}\in A$. Dicho rapida y sugestivamente $A$ es el
conjunto formado por aquellos programas que se detienen partiendo de si
mismos. Dejamos al lector hacer un programa que enumere a $A$. Como veremos
mas adelante este conjunto, si bien es $\Sigma$-enumerable, no es $\Sigma$-computable.

\bigskip

\subsubsection{Godel vence a Turing}

Para probar que toda funcion $\Sigma$-Turing computable es $\Sigma$-recursiva
debemos estudiar la recursividad del funcionamiento de las maquinas de Turing.
Cabe destacar que tal como se lo explico en la Subseccion
\ref{BasicosMaquinasDeTuring} supondremos siempre que el conjunto de estados
de una maquina de Turing $M=\left(  Q,\Sigma,\Gamma,\delta,q_{0},B,F\right)  $
es un alfabeto disjunto con $\Gamma$.

Primero probaremos algunos lemas basicos.

\begin{lemma}
Sea $M=\left(  Q,\Sigma,\Gamma,\delta,q_{0},B,F\right)  $ una maquina de
Turing. Entonces

\begin{enumerate}
\item[(1)] $Des$ es un conjunto $(\Gamma\cup Q)$-p.r.

\item[(2)] $St:Des\rightarrow Q$ es una funcion $(\Gamma\cup Q)$-p.r.
\end{enumerate}
\end{lemma}

\bigskip

Notese que la funcion $\delta$ de una maquina de Turing $M=\left(
Q,\Sigma,\Gamma,\delta,q_{0},B,F\right)  $ no es $(\Gamma\cup Q)$-mixta. Sin
envargo los siguientes predicados $(\Gamma\cup Q)$-mixtos contienen toda la
informacion de $\delta$:%
\[%
\begin{array}
[c]{rcl}%
P_{L}:Q\times\Gamma\times Q\times\Gamma & \rightarrow & \omega\\
(p,\sigma,q,\gamma) & \rightarrow & \left\{
\begin{array}
[c]{ccl}%
1 &  & \text{si }\delta(q,\gamma)=(p,\sigma,L)\\
0 &  & \text{caso contrario}%
\end{array}
\right.
\end{array}
\]%
\[%
\begin{array}
[c]{rcl}%
P_{L}:Q\times\Gamma\times Q\times\Gamma & \rightarrow & \omega\\
(p,\sigma,q,\gamma) & \rightarrow & \left\{
\begin{array}
[c]{ccl}%
1 &  & \text{si }(p,\sigma,L)\in\delta(q,\gamma)\\
0 &  & \text{caso contrario}%
\end{array}
\right.
\end{array}
\]%
\[%
\begin{array}
[c]{rcl}%
P_{R}:Q\times\Gamma\times Q\times\Gamma & \rightarrow & \omega\\
(p,\sigma,q,\gamma) & \rightarrow & \left\{
\begin{array}
[c]{ccl}%
1 &  & \text{si }\delta(q,\gamma)=(p,\sigma,R)\\
0 &  & \text{caso contrario}%
\end{array}
\right.
\end{array}
\]%
\[%
\begin{array}
[c]{rcl}%
P_{K}:Q\times\Gamma\times Q\times\Gamma & \rightarrow & \omega\\
(p,\sigma,q,\gamma) & \rightarrow & \left\{
\begin{array}
[c]{ccl}%
1 &  & \text{si }\delta(q,\gamma)=(p,\sigma,K)\\
0 &  & \text{caso contrario}%
\end{array}
\right.
\end{array}
\]


\begin{lemma}
Sea $M=\left(  Q,\Sigma,\Gamma,\delta,q_{0},B,F\right)  $ una maquina de
Turing. Entonces los predicados $P_{L},P_{R}$ y $P_{K}$ son $(\Gamma\cup Q)$-p.r.
\end{lemma}

\begin{proof}
Ya que los tres predicados tienen dominio finito, el Corolario
\ref{dom-finito} nos dice que son $(\Gamma\cup Q)$-p.r.
\end{proof}

\bigskip

Recordemos que dado $\alpha\in(Q\cup\Gamma)^{\ast}$, definimos $\left\lfloor
\alpha\right\rfloor $ de la siguiente manera%
\begin{align*}
\left\lfloor \varepsilon\right\rfloor  & =\varepsilon\\
\left\lfloor \alpha\sigma\right\rfloor  & =\alpha\sigma\text{, si }\sigma\neq
B\\
\left\lfloor \alpha B\right\rfloor  & =\left\lfloor \alpha\right\rfloor
\end{align*}
Es decir $\left\lfloor \alpha\right\rfloor $ es el resultado de remover de
$\alpha$ el tramo final mas grande de la forma $B^{n}$.

Tambien dada cualquier palabra $\alpha$ definimos%

\begin{align*}
^{\curvearrowright}\alpha & =\left\{
\begin{array}
[c]{lll}%
\left[  \alpha\right]  _{2}...\left[  \alpha\right]  _{\left\vert
\alpha\right\vert } & \text{si} & \left\vert \alpha\right\vert \geq2\\
\varepsilon & \text{si} & \left\vert \alpha\right\vert \leq1
\end{array}
\right. \\
\alpha^{\curvearrowleft}  & =\left\{
\begin{array}
[c]{lll}%
\left[  \alpha\right]  _{1}...\left[  \alpha\right]  _{\left\vert
\alpha\right\vert -1} & \text{si} & \left\vert \alpha\right\vert \geq2\\
\varepsilon & \text{si} & \left\vert \alpha\right\vert \leq1
\end{array}
\right.
\end{align*}


\begin{lemma}
Las funciones $\lambda\alpha\lbrack\left\lfloor \alpha\right\rfloor ]$,
$\lambda\alpha\lbrack^{\curvearrowright}\alpha]$ y $\lambda\alpha\lbrack
\alpha^{\curvearrowleft}]$ son $(\Gamma\cup Q)$-p.r.. (Notar que la notacion
$\lambda$ aqui es respecto del alfabeto $\Gamma\cup Q$ por lo cual las tres
funciones tienen dominio igual a $(\Gamma\cup Q)^{\ast}$.)
\end{lemma}

\bigskip

Notese que dada una maquina de Turing $M$, la expresion $d\underset{M}{\vdash
}d^{\prime}$ fue definida solo en el caso en que $d$ y $d^{\prime}$ son
descripciones instantaneas. Es decir que el predicado $\lambda dd^{\prime
}\left[  d\vdash d^{\prime}\right]  $ tiene dominio igual a $Des\times Des$.

\begin{lemma}
El predicado $\lambda dd^{\prime}\left[  d\underset{M}{\vdash}d^{\prime
}\right]  $ es $(\Gamma\cup Q)$-p.r..
\end{lemma}

\begin{proof}
Sea $\tilde{P}_{L}:Des\times Des\times\Gamma\times\Gamma^{\ast}\times
\Gamma^{\ast}\times Q\times Q\rightarrow\omega$ definido por $\tilde{P}%
_{L}(d,d^{\prime},\sigma,\alpha,\beta,p,q)=1$ sii%
\[
d=\alpha p\beta\wedge(q,\sigma,L)=\delta\left(  p,\left[  \beta B\right]
_{1}\right)  \wedge\alpha\neq\varepsilon\wedge d^{\prime}=\left\lfloor
\alpha^{\curvearrowleft}q\left[  \alpha\right]  _{\left\vert \alpha\right\vert
}\sigma^{\curvearrowright}\beta\right\rfloor
\]
Sea $\tilde{P}_{R}:Des\times Des\times\Gamma\times\Gamma^{\ast}\times
\Gamma^{\ast}\times Q\times Q\rightarrow\omega$ definido por $\tilde{P}%
_{R}(d,d^{\prime},\sigma,\alpha,\beta,p,q)=1$ sii%
\[
d=\alpha p\beta\wedge(q,\sigma,R)=\delta\left(  p,\left[  \beta B\right]
_{1}\right)  \wedge d^{\prime}=\alpha\sigma q^{\curvearrowright}\beta
\]
Sea $\tilde{P}_{K}:Des\times Des\times\Gamma\times\Gamma^{\ast}\times
\Gamma^{\ast}\times Q\times Q\rightarrow\omega$ definido por $\tilde{P}%
_{K}(d,d^{\prime},\sigma,\alpha,\beta,p,q)=1$ sii%
\[
d=\alpha p\beta\wedge(q,\sigma,K)=\delta\left(  p,\left[  \beta B\right]
_{1}\right)  \wedge d^{\prime}=\left\lfloor \alpha q\sigma^{\curvearrowright
}\beta\right\rfloor
\]
Se deja al lector la verificacion de que estos predicados son $(\Gamma\cup
Q)$-p.r.. Notese que $\lambda dd^{\prime}\left[  d\underset{M}{\vdash
}d^{\prime}\right]  $ es igual al predicado%
\[
\lambda dd^{\prime}\left[  (\exists\sigma\in\Gamma)(\exists\alpha,\beta
\in\Gamma^{\ast})(\exists p,q\in Q)(\tilde{P}_{R}\vee\tilde{P}_{L}\vee
\tilde{P}_{K})(d,d^{\prime},\sigma,\alpha,\beta,p,q)\right]
\]
lo cual por el Lema \ref{cuantificacion} nos dice que $\lambda dd^{\prime
}\left[  d\underset{M}{\vdash}d^{\prime}\right]  $ es $(\Gamma\cup Q)$-p.r.
\end{proof}

\begin{proposition}
$\lambda ndd^{\prime}\left[  d\underset{M}{\overset{n}{\vdash}}d^{\prime
}\right]  $ es $(\Gamma\cup Q)$-p.r..
\end{proposition}

\begin{proof}
Sea $Q=\lambda dd^{\prime}\left[  d\underset{M}{\vdash}d^{\prime}\right]  \cup
C_{0}^{0,2}|_{(\Gamma\cup Q)^{\ast2}-Des^{2}}$ es decir $Q$ es el resultado de
extender con el valor $0$ al predicado $\lambda dd^{\prime}\left[
d\underset{M}{\vdash}d^{\prime}\right]  $ de manera que este definido en todo
$(\Gamma\cup Q)^{\ast2}$. Sea $\leq$ un orden total sobre $\Gamma\cup Q$ y sea
$Q_{1}:\mathbf{N}\times Des\times Des\rightarrow\omega$ definido por
$Q_{1}(x,d,d^{\prime})=1$ sii

$\left(  (\forall i\in\mathbf{N})_{i\leq Lt(x)}\ast^{\leq}((x)_{i})\in
Des\right)  \wedge\ast^{\leq}((x)_{1})=d\wedge$

$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ast^{\leq}((x)_{Lt(x)}%
)=d^{\prime}\wedge\left(  (\forall i\in\mathbf{N})_{i\leq Lt(x)\dot{-}%
1}\;Q(\ast^{\leq}((x)_{i}),\ast^{\leq}((x)_{i+1}))\right)  $

\noindent Notese que dicho rapidamente $Q_{1}(x,d,d^{\prime})=1$ sii $x$
codifica una computacion que parte de $d$ y llega a $d^{\prime}$. Se deja al
lector la verificacion de que este predicado es $(\Gamma\cup Q)$-p.r.. Notese
que%
\[
\lambda ndd^{\prime}\left[  d\underset{M}{\overset{n}{\vdash}}d^{\prime
}\right]  =\lambda ndd^{\prime}\left[  \left(  \exists x\in\mathbf{N}\right)
\;Lt(x)=n+1\wedge Q_{1}(x,d,d^{\prime})\right]
\]
Es decir que solo nos falta acotar el cuantificador existencial, para poder
aplicar el lema de cuantificacion acotada. Ya que cuando $d_{1},...,d_{n+1}\in
Des$ son tales que $d_{1}\underset{M}{\vdash}d_{2}\underset{M}{\vdash}%
\cdots\underset{M}{\vdash}d_{n+1}$ tenemos que%
\[
\left\vert d_{i}\right\vert \leq\left\vert d_{1}\right\vert +n\text{, para
}i=1,...,n
\]
una posible cota para dicho cuantificador es%
\[
\prod_{i=1}^{n+1}pr(i)^{\left\vert \Gamma\cup Q\right\vert ^{\left\vert
d\right\vert +n}}\text{.}%
\]
O sea que, por el lema de cuantificacion acotada, tenemos que el predicado
$\lambda ndd^{\prime}\left[  d\underset{M}{\overset{n}{\vdash}}d^{\prime
}\right]  $ es $(\Gamma\cup Q)$-p.r.
\end{proof}

\bigskip

@@finpagina@@

\begin{theorem}
\label{TuringComputableImplicaRecursiva}Supongamos $f:S\subseteq\omega
^{n}\times\Sigma^{\ast}{}^{m}\rightarrow O$ es $\Sigma$-Turing computable.
Entonces $f$ es $\Sigma$-recursiva.
\end{theorem}

\begin{proof}
Supongamos $O=\Sigma^{\ast}$ y sea $M=\left(  Q,\Sigma,\Gamma,\delta
,q_{0},B,\shortmid,F\right)  $ una maquina de Turing deterministica con unit
la cual compute a $f$. Sea $\leq$ un orden total sobre $\Sigma$. Notese que
por el Teorema \ref{independencia}, la funcion $\ast^{\leq}$ es $(\Gamma\cup
Q)$-p.r.. Sea $P:\mathbf{N}\times\omega^{n}\times\Sigma^{\ast m}%
\rightarrow\omega$ dado por $P(x,\vec{x},\vec{\alpha})=1$ sii

\begin{enumerate}
\item $(\exists q\in Q)\;\left\lfloor q_{0}B\shortmid^{x_{1}}...B\shortmid
^{x_{n}}B\alpha_{1}...B\alpha_{m}\right\rfloor \underset{M}{\overset{(x)_{1}%
}{\vdash}}\left\lfloor qB\ast^{\leq}((x)_{2})\right\rfloor \wedge$

\item $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \wedge(\forall d\in
Des)_{\left\vert d\right\vert \leq\left\vert \ast^{\leq}((x)_{2})\right\vert
+2}\;\lnot\left(  \left\lfloor qB\ast^{\leq}((x)_{2})\right\rfloor
\underset{M}{\vdash}d\right)  $
\end{enumerate}

\noindent Dejamos al lector la prueba de que $P$ es $(\Gamma\cup Q)$-p.r.. Ya
que $P$ es $\Sigma$-mixto, el Teorema \ref{independencia} nos dice que $P $ es
$\Sigma$-p.r.. Notese que%
\[
f=\lambda\vec{x}\vec{\alpha}\left[  \ast^{\leq}\left(  \left(  \min
_{x}P(x,\vec{x},\vec{\alpha})\right)  _{2}\right)  \right]  \text{,}%
\]
lo cual nos dice que $f$ es $\Sigma$-recursiva.
\end{proof}

\bigskip

\subsubsection{Turing vence a Neumann}

Probaremos que toda funcion $\Sigma$-computable es $\Sigma$-Turing computable.
Para esto probaremos un resultado general que nos ense\~{n}ara a simular el
comportamiento de un programa con una maquina de Turing. Es importante notar
que la simulacion que nos interesa que haga la maquina simuladora no es a
nivel de la funcion que computa el programa sino a un nivel mas general, es
decir nos interesa que simule a dicho programa como transformador de estados.
En particular y usada adecuadamente, la maquina simuladora nos servira para
confeccionar una maquina que compute una funcion computada por un programa dado.

\paragraph{Construccion de la maquina simuladora de un programa}

Dado $\mathcal{P}\in\mathrm{Pro}^{\Sigma}$, definamos%

\begin{align*}
N(\mathcal{P})  & =\mathrm{menor}\ k\in\mathbf{N}%
\ \mathrm{tal\ que\ las\ variables\ que\ ocurren\ en\ }\mathcal{P}\\
& \mathrm{esta}\text{n}\mathrm{\ todas\ en\ la\ lista\ N}1,...,\mathrm{N}%
\bar{k},\mathrm{P}1,...,\mathrm{P}\bar{k}%
\end{align*}
Por ejemplo si $\mathcal{P}$ es el siguiente programa (aqui $\Sigma
=\{\blacktriangle,\#\}$)%
\[%
\begin{array}
[c]{ll}%
\mathrm{L}1 & \mathrm{N}4\leftarrow\mathrm{N}4+1\\
& \mathrm{P}1\leftarrow\mathrm{P}1.\blacktriangle\\
& \mathrm{IF\ N}1\neq0\ \mathrm{GOTO}\;\mathrm{L}1
\end{array}
\]
entonces tenemos $N(\mathcal{P})=4$

Sea $\mathcal{P}$ un programa y sea $k$ fijo y mayor o igual a $N(\mathcal{P}%
)$. La construccion de la maquina simuladora dependera de $\mathcal{P}$ y de
$k$. Notese que cuando $\mathcal{P}$ se corre desde algun estado de la forma%
\[
\left\Vert x_{1},...,x_{k},\alpha_{1},...,\alpha_{k}\right\Vert
\]
los sucesivos estados por los que va pasando son todos de la forma%

\[
\left\Vert y_{1},...,y_{k},\beta_{1},...,\beta_{k}\right\Vert
\]
es decir en todos ellos las variables con indice mayor que $k$ valen $0$ o
$\varepsilon$. La razon es simple: ya que en $\mathcal{P}$ no figuran las
variables%
\begin{align*}
& \mathrm{N}\overline{k+1},\mathrm{N}\overline{k+2},...\\
& \mathrm{P}\overline{k+1},\mathrm{P}\overline{k+2},...
\end{align*}
estas variables quedan con valores $0$ y $\varepsilon$, respectivamente a lo
largo de toda la computacion.

La maquina simuladora que construiremos simulara a $\mathcal{P}$ en cuanto a
su funcionamiento cuando partimos de estados de la forma $\left\Vert
x_{1},...,x_{k},\alpha_{1},...,\alpha_{k}\right\Vert $. Necesitaremos tener
alguna manera de representar en la cinta los diferentes estados por los cuales
se va pasando, a medida que corremos a $\mathcal{P}$. Esto lo haremos de la
siguiente forma: al estado%

\[
\left\Vert x_{1},...,x_{k},\alpha_{1},...,\alpha_{k}\right\Vert
\]
lo representaremos en la cinta de la siguiente manera%
\[
B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha_{1}...B\alpha_{k}BBBB....
\]
Por ejemplo consideremos el programa $\mathcal{P}$ mostrado recien y fijemos
$k=6$. Entonces al estado%
\[
\left\Vert 3,2,5,0,4,2,\blacktriangle,\blacktriangle\blacktriangle
,\varepsilon,\#\blacktriangle,\#,\#\#\#\right\Vert
\]
lo representaremos en la cinta de la siguiente manera%
\[
B\mathrm{\shortmid\shortmid\shortmid}B\mathrm{\shortmid\shortmid
}B\mathrm{\shortmid\shortmid\shortmid\shortmid\shortmid}BB\mathrm{\shortmid
\shortmid\shortmid\shortmid}B\mathrm{\shortmid\shortmid}B\blacktriangle
B\blacktriangle\blacktriangle BB\#\blacktriangle B\#B\#\#\#BBBBB....
\]
A lo que queda entre dos blancos consecutivos (es decir que no hay ningun
blanco entre ellos) lo llamaremos "bloque", por ejemplo en la cinta de arriba
tenemos que los primeros 12 bloques son%
\[
\shortmid\shortmid\shortmid\ \ \ \shortmid\shortmid\ \ \ \shortmid
\shortmid\shortmid\shortmid\shortmid\ \ \ \ \varepsilon\ \ \ \ \shortmid
\shortmid\shortmid\shortmid\ \ \ \shortmid\shortmid\ \ \ \blacktriangle
\ \ \ \ \ \blacktriangle\blacktriangle\ \ \ \ \ \varepsilon
\ \ \ \ \ \#\blacktriangle\ \ \ \ \ \#\ \ \ \ \ \#\#\#
\]
y despues los bloques siguientes (que son infinitos ya que la cinta es
infinita hacia la derecha) son todos iguales a $\varepsilon$.

Una observacion importante es que esta forma de representacion de estados en
la cinta depende del $k$ elejido, es decir si tomaramos otro $k$, por ejemplo
$k=9$, entonces el estado anterior se representaria de otra forma en la cinta.
Aqui se ve claramente que la maquina simuladora que construiremos dependera
del $k$ elejido.

\bigskip

\subparagraph{Construccion de las maquinas simuladoras de instrucciones}

Armaremos la maquina simuladora como concatenacion de maquinas las cuales
simularan, via la representacion anterior, el funcionamiento de las distintas
instrucciones de $\mathcal{P}$. Asumiremos que en $\mathcal{P}$ no hay
instrucciones de la forma $\mathrm{GOTO}\;\mathrm{L}\bar{m}$ ni de la forma
$\mathrm{L}\bar{n}\ \mathrm{GOTO}\;\mathrm{L}\bar{m}$. Esto simplificara un
poco la construccion de la maquina simuladora y de hecho lo podemos hacer ya
que toda funcion $\Sigma$-computable puede ser computada por un programa sin
este tipo de instrucciones, tal como lo veremos mas adelante (Lema
\ref{simulacion}).

Para poder hacer concretamente las maquinas simuladoras de instrucciones
deberemos dise\~{n}ar antes algunas maquinas auxiliares. Todas las maquinas
descriptas a continuacion tendran a $\shortmid$ como unit y a $B$ como blanco,
tendran a $\Sigma$ como su alfabeto terminal y su alfabeto mayor sera
$\Gamma=\Sigma\cup\{B,\shortmid\}\cup\{\tilde{a}:a\in\Sigma\cup\{\shortmid
\}\}$. Ademas tendran uno o dos estados finales con la propiedad de que si $q$
es un estado final, entonces $(q,\sigma)\notin D_{\delta}$, para cada
$\sigma\in\Gamma$.

Para cada $j\geq1$, sea $D_{j}$ la siguiente maquina:

@@figura:figure1.png@@

\noindent Notese que%
\[%
\begin{array}
[c]{lcr}%
\alpha B\beta_{1}B\beta_{2}B...B\beta_{j}B\gamma & \overset{\ast}{\vdash} &
\alpha B\beta_{1}B\beta_{2}B...B\beta_{j}B\gamma\\
\ \ \uparrow &  & \uparrow\ \ \\
\ \ q_{0} &  & q_{f}\ \
\end{array}
\]
siempre que $\alpha,\gamma\in\Gamma^{\ast}$, $\beta_{1},...,\beta_{j}%
\in(\Gamma-\{B\})^{\ast}$. Es decir la maquina $D_{j}$ lo unico que hace es
mover el cabezal desde el blanco de la izquierda de un bloque determinado,
exactamente $j$ bloques a la derecha

Analogamente $I_{j}$ sera una maquina que desplaza el cabezal $j$ bloques a la
izquierda del blanco que esta escaneando. Es decir $I_{j}$ cumplira que%
\[%
\begin{array}
[c]{lcr}%
\alpha B\beta_{j}B...B\beta_{2}B\beta_{1}B\gamma & \overset{\ast}{\vdash} &
\alpha B\beta_{j}B...B\beta_{2}B\beta_{1}B\gamma\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \uparrow &  &
\uparrow\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ q_{0} &  &
q_{f}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\end{array}
\]
siempre que $\alpha,\gamma\in\Gamma^{\ast}$, $\beta_{1},...,\beta_{j}%
\in(\Gamma-\{B\})^{\ast}$. Dejamos al lector la manufactura de esta maquina.

Para $j\geq1$, sea $TD_{j}$ una maquina con un solo estado final $q_{f}$ y tal
que%
\[%
\begin{array}
[c]{ccc}%
\alpha B\gamma & \overset{\ast}{\vdash} & \alpha BB\gamma\\
\uparrow &  & \uparrow\ \ \\
q_{0} &  & q_{f}\ \
\end{array}
\]
cada vez que $\alpha,\gamma\in\Gamma^{\ast}$ y $\gamma$ tiene exactamente $j$
ocurrencias de $B$. Es decir la maquina $TD_{j}$ corre un espacio a la derecha
todo el segmento $\gamma$ y agrega un blanco en el espacio que se genera a la
izquierda. Por ejemplo, para el caso de $\Sigma=\{a\}$ podemos tomar $TD_{3}$
igual a la siguiente maquina:

@@figura:figure2.png@@

Analogamente, para $j\geq1$, sea $TI_{j}$ una maquina tal que%
\[%
\begin{array}
[c]{ccc}%
\alpha B\sigma\gamma & \overset{\ast}{\vdash} & \alpha B\gamma\\
\uparrow\  &  & \uparrow\\
q_{0}\ \  &  & q_{f}%
\end{array}
\]
cada vez que $\alpha\in\Gamma^{\ast}$, $\sigma\in\Gamma$ y $\gamma$ tiene
exactamente $j$ ocurrencias de $B$. Es decir la maquina $TI_{j}$ corre un
espacio a la izquierda todo el segmaneto $\gamma$ (por lo cual en el lugar de
$\sigma$ queda el primer simbolo de $\gamma$). Dejamos al lector la
construccion de por ejemplo $TI_{3}$ para $\Sigma=\{a\}$.

A continuacion describiremos las distintas maquinas simuladoras de
instrucciones (y para algunos casos mostraremos concretamente como pueden ser
hechas usando las maquinas anteriores).

Para $1\leq i\leq k$, sea $M_{i,k}^{+}$ una maquina tal que cualesquiera sean
$x_{1},...,x_{k}\in\omega$ y $\alpha_{1},...,\alpha_{k}\in\Sigma^{\ast}$:%
\[%
\begin{array}
[c]{lcl}%
B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha_{1}...B\alpha_{k} &
\overset{\ast}{\vdash} & B\shortmid^{x_{1}}...B\shortmid^{x_{i-1}}%
B\shortmid^{x_{i}+1}B\shortmid^{x_{i+1}}...B\shortmid^{x_{k}}B\alpha
_{1}...B\alpha_{k}\\
\uparrow &  & \uparrow\\
q_{0} &  & q_{f}%
\end{array}
\]
donde $q_{0}$ es el estado inicial y $q_{f}$ es el unico estado final de
$M_{i,k}^{+}$. Es claro que la maquina $M_{i,k}^{+}$ simula la instruccion
$\mathrm{N}\bar{\imath}\leftarrow\mathrm{N}\bar{\imath}+1$, via la
representacion de estados en la cinta con respecto a $k$.

Para $1\leq i\leq k$, sea $M_{i,k}^{\dot{-}}$ una maquina tal que cualesquiera
sean $x_{1},...,x_{k}\in\omega$ y $\alpha_{1},...,\alpha_{k}\in\Sigma^{\ast}$:%
\[%
\begin{array}
[c]{lcl}%
B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha_{1}...B\alpha_{k} &
\overset{\ast}{\vdash} & B\shortmid^{x_{1}}...B\shortmid^{x_{i-1}}%
B\shortmid^{x_{i}\dot{-}1}B\shortmid^{x_{i+1}}...B\shortmid^{x_{k}}B\alpha
_{1}...B\alpha_{k}\\
\uparrow &  & \uparrow\\
q_{0} &  & q_{f}%
\end{array}
\]
donde $q_{0}$ es el estado inicial y $q_{f}$ es el unico estado final de
$M_{i,k}^{\dot{-}}$. Es claro que la maquina $M_{i,k}^{\dot{-}}$ simula la
instruccion $\mathrm{P}\bar{\imath}\leftarrow\mathrm{P}\bar{\imath}\dot{-}1$,
via la representacion de estados en la cinta con respecto a $k$.

Para $1\leq i\leq k$ y $a\in\Sigma$, sea $M_{i,k}^{a}$ una maquina tal que
cualesquiera sean $x_{1},...,x_{k}\in\omega$ y $\alpha_{1},...,\alpha_{k}%
\in\Sigma^{\ast}$:%
\[%
\begin{array}
[c]{lcl}%
B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha_{1}...B\alpha_{k} &
\overset{\ast}{\vdash} & B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha
_{1}...B\alpha_{i-1}B\alpha_{i}aB\alpha_{i+1}...B\alpha_{k}\\
\uparrow &  & \uparrow\\
q_{0} &  & q_{f}%
\end{array}
\]
donde $q_{0}$ es el estado inicial y $q_{f}$ es el unico estado final de
$M_{i,k}^{a}$. Es claro que la maquina $M_{i,k}^{a}$ simula la instruccion
$\mathrm{P}\bar{\imath}\leftarrow\mathrm{P}\bar{\imath}.a$, via la
representacion de estados en la cinta con respecto a $k$. La maquina
$M_{i,k}^{a}$.puede hacerse de la siguiente manera:

@@figura:figure3.png@@

Para $1\leq i\leq k$, sea $M_{i,k}^{\curvearrowright}$ una maquina tal que
cualesquiera sean $x_{1},...,x_{k}\in\omega$ y $\alpha_{1},...,\alpha_{k}%
\in\Sigma^{\ast}$:%
\[%
\begin{array}
[c]{lcl}%
B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha_{1}...B\alpha_{k} &
\overset{\ast}{\vdash} & B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha
_{1}...B\alpha_{i-1}B^{\curvearrowright}\alpha_{i}B\alpha_{i+1}...B\alpha
_{k}\\
\uparrow &  & \uparrow\\
q_{0} &  & q_{f}%
\end{array}
\]
donde $q_{0}$ es el estado inicial y $q_{f}$ es el unico estado final de
$M_{i,k}^{\curvearrowright}$. Es claro que la maquina $M_{i,k}%
^{\curvearrowright}$ simula la instruccion $\mathrm{P}\bar{\imath}%
\leftarrow\ ^{\curvearrowright}\mathrm{P}\bar{\imath}$, via la representacion
de estados en la cinta con respecto a $k$.

Para $1\leq i,j\leq k$, sea $M_{i\leftarrow j}^{\#,k}$ una maquina tal que
cualesquiera sean $x_{1},...,x_{k}\in\omega$ y $\alpha_{1},...,\alpha_{k}%
\in\Sigma^{\ast}$:%
\[%
\begin{array}
[c]{lcl}%
B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha_{1}...B\alpha_{k} &
\overset{\ast}{\vdash} & B\shortmid^{x_{1}}...B\shortmid^{x_{i-1}}%
B\shortmid^{x_{j}}B\shortmid^{x_{i+1}}...B\shortmid^{x_{k}}B\alpha
_{1}...B\alpha_{k}\\
\uparrow &  & \uparrow\\
q_{0} &  & q_{f}%
\end{array}
\]
donde $q_{0}$ es el estado inicial y $q_{f}$ es el unico estado final de
$M_{i\leftarrow j}^{\#,k}$. Es claro que la maquina $M_{i\leftarrow j}^{\#,k}$
simula la instruccion $\mathrm{N}\bar{\imath}\leftarrow\mathrm{N}\bar{j}$, via
la representacion de estados en la cinta con respecto a $k$.

Para $1\leq i,j\leq k$, sea $M_{i\leftarrow j}^{\ast,k}$ una maquina tal que
cualesquiera sean $x_{1},...,x_{k}\in\omega$ y $\alpha_{1},...,\alpha_{k}%
\in\Sigma^{\ast}$:%
\[%
\begin{array}
[c]{lcl}%
B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha_{1}...B\alpha_{k} &
\overset{\ast}{\vdash} & B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha
_{1}...B\alpha_{i-1}B\alpha_{j}B\alpha_{i+1}...B\alpha_{k}\\
\uparrow &  & \uparrow\\
q_{0} &  & q_{f}%
\end{array}
\]
donde $q_{0}$ es el estado inicial y $q_{f}$ es el unico estado final de
$M_{i\leftarrow j}^{\ast,k}$. Es claro que la maquina $M_{i\leftarrow j}%
^{\ast,k}$ simula la instruccion $\mathrm{P}\bar{\imath}\leftarrow
\mathrm{P}\bar{j}$, via la representacion de estados en la cinta con respecto
a $k$. La maquina $M_{i\leftarrow j}^{\ast,k}$, para el caso $\Sigma=\{a,b\}$
y $i<j$ puede hacerse de la siguiente manera:

@@figura:figure4.png@@

Para $1\leq i\leq k$, sea $M_{i\leftarrow0}^{k}$ una maquina tal que
cualesquiera sean $x_{1},...,x_{k}\in\omega$ y $\alpha_{1},...,\alpha_{k}%
\in\Sigma^{\ast}$:%
\[%
\begin{array}
[c]{lcl}%
B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha_{1}...B\alpha_{k} &
\overset{\ast}{\vdash} & B\shortmid^{x_{1}}...B\shortmid^{x_{i-1}}%
BB\shortmid^{x_{i+1}}...B\shortmid^{x_{k}}B\alpha_{1}...B\alpha_{k}\\
\uparrow &  & \uparrow\\
q_{0} &  & q_{f}%
\end{array}
\]
donde $q_{0}$ es el estado inicial y $q_{f}$ es el unico estado final de
$M_{i\leftarrow0}^{k}$. Es claro que la maquina $M_{i\leftarrow0}^{k}$ simula
la instruccion $\mathrm{N}\bar{\imath}\leftarrow0$, via la representacion de
estados en la cinta con respecto a $k$.

Para $1\leq i\leq k$, sea $M_{i\leftarrow\varepsilon}^{k}$ una maquina tal que
cualesquiera sean $x_{1},...,x_{k}\in\omega$ y $\alpha_{1},...,\alpha_{k}%
\in\Sigma^{\ast}$:%
\[%
\begin{array}
[c]{lcl}%
B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha_{1}...B\alpha_{k} &
\overset{\ast}{\vdash} & B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha
_{1}...B\alpha_{i-1}BB\alpha_{i+1}...B\alpha_{k}\\
\uparrow &  & \uparrow\\
q_{0} &  & q_{f}%
\end{array}
\]
donde $q_{0}$ es el estado inicial y $q_{f}$ es el unico estado final de
$M_{i\leftarrow\varepsilon}^{k}$. Es claro que la maquina $M_{i\leftarrow
\varepsilon}^{k}$ simula la instruccion $\mathrm{P}\bar{\imath}\leftarrow
\varepsilon$, via la representacion de estados en la cinta con respecto a $k $.

Sea%
\[
M_{\mathrm{SKIP}}=\left(  \{q_{0},q_{f}\},\Gamma,\Sigma,\delta,q_{0}%
,B,\shortmid,\{q_{f}\}\right)  ,
\]
con $D_{\delta}=\{(q_{0},B)\}$ y $\delta(q_{0},B)=(q_{f},B,K)$. Es claro que
la maquina $M_{\mathrm{SKIP}}$ simula la instruccion $\mathrm{SKIP}$, via la
representacion de estados en la cinta con respecto a $k$ (cualquiera sea el
$k$).

Para $1\leq j\leq k$, sea $IF_{j,k}$ una maquina con estado inicial $q_{0}$ y
dos estados finales $q_{si}$ y $q_{no}$ tal que cualesquiera sean
$x_{1},...,x_{k}\in\omega$ y $\alpha_{1},...,\alpha_{k}\in\Sigma^{\ast} $, si
$x_{j}\neq0$, entonces%
\[%
\begin{array}
[c]{lcl}%
B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha_{1}...B\alpha_{k} &
\overset{\ast}{\vdash} & B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha
_{1}...B\alpha_{k}\\
\uparrow &  & \uparrow\\
q_{0} &  & q_{si}%
\end{array}
\]
y si $x_{j}=0$, entonces%
\[%
\begin{array}
[c]{lcl}%
B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha_{1}...B\alpha_{k} &
\overset{\ast}{\vdash} & B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha
_{1}...B\alpha_{k}\\
\uparrow &  & \uparrow\\
q_{0} &  & q_{no}%
\end{array}
\]
\bigskip

Para $1\leq i\leq k$ y $a\in\Sigma$, sea $IF_{j,k}^{a}$ una maquina con estado
inicial $q_{0}$ y dos estados finales $q_{si}$ y $q_{no}$ tal que cualesquiera
sean $x_{1},...,x_{k}\in\omega$ y $\alpha_{1},...,\alpha_{k}\in\Sigma^{\ast}$,
si $\alpha_{j}$ comienza con $a$, entonces%
\[%
\begin{array}
[c]{lcl}%
B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha_{1}...B\alpha_{k} &
\overset{\ast}{\vdash} & B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha
_{1}...B\alpha_{k}\\
\uparrow &  & \uparrow\\
q_{0} &  & q_{si}%
\end{array}
\]
y en caso contrario%
\[%
\begin{array}
[c]{lcl}%
B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha_{1}...B\alpha_{k} &
\overset{\ast}{\vdash} & B\shortmid^{x_{1}}...B\shortmid^{x_{k}}B\alpha
_{1}...B\alpha_{k}\\
\uparrow &  & \uparrow\\
q_{0} &  & q_{no}%
\end{array}
\]
La maquina $IF_{j,k}^{a}$ puede hacerse de la siguinete manera:

@@figura:figure5.png@@

\bigskip

\subparagraph{Ejemplo de maquina simuladora de un programa}

A continuacion veremos un ejemplo de como se arma la maquina simuladora de un
programa dado. Sea $\Sigma=\{\blacktriangle,\#\}$ y sea $\mathcal{P}$ el
siguiente programa%
\[%
\begin{array}
[c]{ll}%
\mathrm{L}3 & \mathrm{N}4\leftarrow\mathrm{N}4+1\\
& \mathrm{P}1\leftarrow\ ^{\curvearrowright}\mathrm{P}1\\
& \mathrm{IF\ P}1\ \mathrm{BEGINS\ }\blacktriangle\ \mathrm{GOTO}%
\;\mathrm{L}3\\
& \mathrm{P}3\leftarrow\mathrm{P}3.\#
\end{array}
\]
Tomemos $k=5$. Es claro que $k\geq N(\mathcal{P})=4$. A la maquina que
simulara a $\mathcal{P}$ respecto de $k$, la llamaremos $M_{sim}$ y sera la
siguiente maquina:

@@figura:figure6.png@@

\noindent Veamos con un ejemplo como $M_{sim}$ simula a $\mathcal{P}$.
Supongamos que corremos $\mathcal{P}$ desde el estado%
\[
\left\Vert 2,1,0,5,3,\#\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#\right\Vert
\]
Tendremos entonces la siguiente sucesion de descripciones instantaneas:%
\begin{align*}
& (1,\left\Vert 2,1,0,5,3,\#\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#\right\Vert )\\
& \\
& (2,\left\Vert 2,1,0,6,3,\#\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#\right\Vert )\\
& \\
& (3,\left\Vert 2,1,0,6,3,\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#\right\Vert )\\
& \\
& (1,\left\Vert 2,1,0,6,3,\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#\right\Vert )\\
& \\
& (2,\left\Vert 2,1,0,7,3,\blacktriangle\#\#,\varepsilon,\blacktriangle
\blacktriangle,\#\blacktriangle,\#\right\Vert )\\
& \\
& (3,\left\Vert 2,1,0,7,3,\#\#,\varepsilon,\blacktriangle\blacktriangle
,\#\blacktriangle,\#\right\Vert )\\
& \\
& (4,\left\Vert 2,1,0,7,3,\#\#,\varepsilon,\blacktriangle\blacktriangle
,\#\blacktriangle,\#\right\Vert )\\
& \\
& (5,\left\Vert 2,1,0,7,3,\#\#,\varepsilon,\blacktriangle\blacktriangle
\#,\#\blacktriangle,\#\right\Vert )
\end{align*}
Si hacemos funcionar a $M_{sim}$ desde $q_{0}B\shortmid^{2}B\shortmid
BB\shortmid^{5}B\shortmid^{3}B\#\blacktriangle\#\#BB\blacktriangle
\blacktriangle B\#\blacktriangle B\#B$ obtendremos una sucesion de
descripciones instantaneas dentro de la cual estara la siguiente subsucesion
que se corresponde con las descripciones instantaneas de la computacion anterior.%

\begin{align*}
q_{0}B  & \shortmid^{2}B\shortmid BB\shortmid^{5}B\shortmid^{3}%
B\#\blacktriangle\#\#BB\blacktriangle\blacktriangle B\#\blacktriangle B\#B\\
& \\
q_{1}B  & \shortmid^{2}B\shortmid BB\shortmid^{6}B\shortmid^{3}%
B\#\blacktriangle\#\#BB\blacktriangle\blacktriangle B\#\blacktriangle B\#B\\
q_{2}B  & \shortmid^{2}B\shortmid BB\shortmid^{6}B\shortmid^{3}%
B\#\blacktriangle\#\#BB\blacktriangle\blacktriangle B\#\blacktriangle B\#B\\
& \\
q_{3}B  & \shortmid^{2}B\shortmid BB\shortmid^{6}B\shortmid^{3}B\blacktriangle
\#\#BB\blacktriangle\blacktriangle B\#\blacktriangle B\#B\\
q_{4}B  & \shortmid^{2}B\shortmid BB\shortmid^{6}B\shortmid^{3}B\blacktriangle
\#\#BB\blacktriangle\blacktriangle B\#\blacktriangle B\#B\\
& \\
q_{si}B  & \shortmid^{2}B\shortmid BB\shortmid^{6}B\shortmid^{3}%
B\blacktriangle\#\#BB\blacktriangle\blacktriangle B\#\blacktriangle B\#B\\
q_{0}B  & \shortmid^{2}B\shortmid BB\shortmid^{6}B\shortmid^{3}B\blacktriangle
\#\#BB\blacktriangle\blacktriangle B\#\blacktriangle B\#B\\
& \\
q_{1}B  & \shortmid^{2}B\shortmid BB\shortmid^{7}B\shortmid^{3}B\blacktriangle
\#\#BB\blacktriangle\blacktriangle B\#\blacktriangle B\#B\\
q_{2}B  & \shortmid^{2}B\shortmid BB\shortmid^{7}B\shortmid^{3}B\blacktriangle
\#\#BB\blacktriangle\blacktriangle B\#\blacktriangle B\#B\\
& \\
q_{3}B  & \shortmid^{2}B\shortmid BB\shortmid^{7}B\shortmid^{3}%
B\#\#BB\blacktriangle\blacktriangle B\#\blacktriangle B\#B\\
q_{4}B  & \shortmid^{2}B\shortmid BB\shortmid^{7}B\shortmid^{3}%
B\#\#BB\blacktriangle\blacktriangle B\#\blacktriangle B\#B\\
& \\
q_{no}B  & \shortmid^{2}B\shortmid BB\shortmid^{7}B\shortmid^{3}%
B\#\#BB\blacktriangle\blacktriangle B\#\blacktriangle B\#B\\
q_{5}B  & \shortmid^{2}B\shortmid BB\shortmid^{7}B\shortmid^{3}%
B\#\#BB\blacktriangle\blacktriangle B\#\blacktriangle B\#B\\
& \\
q_{6}B  & \shortmid^{2}B\shortmid BB\shortmid^{7}B\shortmid^{3}%
B\#\#BB\blacktriangle\blacktriangle\#B\#\blacktriangle B\#B
\end{align*}
Dejamos al lector ver en detalle el paralelismo que hay entre las dos
sucesiones de descripciones instantaneas arriba expuestas.

\bigskip

\subparagraph{La contruccion de la maquina simuladora}

A continuacion describiremos en general como hacer la maquina simuladora de
$\mathcal{P}$, respecto de $k$. Supongamos que $\mathcal{P}=I_{1}...I_{n}$.
Para cada $i=1,...,n$, llamaremos $M_{i}$ a la maquina que simulara el efecto
que produce la instruccion $I_{i}$, es decir tomemos

\begin{enumerate}
\item[-] $M_{i}=M_{j,k}^{+}$, si $Bas(I_{i})=\mathrm{N}\bar{j}\leftarrow
\mathrm{N}\bar{j}+1$

\item[-] $M_{i}=M_{j,k}^{\dot{-}}$, si $Bas(I_{i})=\mathrm{N}\bar{j}%
\leftarrow\mathrm{N}\bar{j}\dot{-}1$

\item[-] $M_{i}=M_{j,k}^{a}$, si $Bas(I_{i})=\mathrm{P}\bar{j}\leftarrow
\mathrm{P}\bar{j}.a$

\item[-] $M_{i}=M_{j,k}^{\curvearrowright}$, si $Bas(I_{i})=\mathrm{P}\bar
{j}\leftarrow\ ^{\curvearrowright}\mathrm{P}\bar{j}$

\item[-] $M_{i}=M_{j\leftarrow m}^{\#,k}$, si $Bas(I_{i})=\mathrm{N}\bar
{j}\leftarrow\mathrm{N}\bar{m}$

\item[-] $M_{i}=M_{j\leftarrow m}^{\ast,k}$, si $Bas(I_{i})=\mathrm{P}\bar
{j}\leftarrow\mathrm{P}\bar{m}$

\item[-] $M_{i}=M_{j\leftarrow0}^{k}$, si $Bas(I_{i})=\mathrm{N}\bar
{j}\leftarrow0$

\item[-] $M_{i}=M_{j\leftarrow\varepsilon}^{k}$, si $Bas(I_{i})=\mathrm{P}%
\bar{j}\leftarrow\varepsilon$

\item[-] $M_{i}=M_{\mathrm{SKIP}}$, si $Bas(I_{i})=\mathrm{SKIP}$

\item[-] $M_{i}=IF_{j,k}$, si $Bas(I_{i})=\mathrm{IF}\;\mathrm{N}\bar{j}%
\not =0$\ $\mathrm{GOTO}\;\mathrm{L}\bar{m}$, para algun $m$

\item[-] $M_{i}=IF_{j,k}^{a}$, si $Bas(I_{i})=\mathrm{IF}\;\mathrm{P}\bar
{j}\;\mathrm{BEGINS}\;a\;\mathrm{GOTO}\;\mathrm{L}\bar{m}$, para algun $m$
\end{enumerate}

\noindent Ya que la maquina $M_{i}$ puede tener uno o dos estados finales, la
representaremos como se muestra a continuacion:

@@figura:figure7.png@@

\noindent entendiendo que en el caso en que $M_{i}$ tiene un solo estado
final, este esta representado por el circulo de abajo a la izquierda y en el
caso en que $M_{i}$ tiene dos estados finales, el circulo de abajo a la
izquierda corresponde al estado final $q_{no}$ y el circulo de abajo a la
derecha corresponde al estado $q_{si}$. Para armar la maquina que simulara a
$\mathcal{P}$ hacemos lo siguiente. Primero unimos las maquinas $M_{1}%
,...,M_{n}$ de la siguiente manera:

@@figura:figure8.png@@

\noindent Luego para cada $i$ tal que $Bas(I_{i})$ es de la forma
$\alpha\mathrm{GOTO}\;\mathrm{L}\bar{m}$, ligamos con una flecha de la forma%
\[
\underrightarrow{\;\;\;\;\;\;B,B,K\;\;\;\;\;\;}%
\]
el estado final $q_{si}$ de la $M_{i}$ con el estado inicial de la $M_{h}$,
donde $h$ es tal que $I_{h}$ es la primer instruccion que tiene label
$\mathrm{L}\bar{m}$.

\paragraph{El lema de la simulacion}

A continuacion enunciaremos en forma de lema la existencia de la maquina
simuladora y de las propiedades esenciales que usaremos luego para probar que
toda funcion $\Sigma$-computable es $\Sigma$-Turing computable.

\begin{lemma}
\label{simulacion}Sea $\mathcal{P}\in\mathrm{Pro}^{\Sigma}$ y sea $k\geq
N(\mathcal{P})$. Supongamos que en $\mathcal{P}$ no hay instrucciones de la
forma $\mathrm{GOTO}\;\mathrm{L}\bar{m}$ ni de la forma $\mathrm{L}\bar
{n}\ \mathrm{GOTO}\;\mathrm{L}\bar{m}$. Para cada $a\in\Sigma\cup\{\shortmid\}
$, sea $\tilde{a}$ un nuevo simbolo. Sea $\Gamma=\Sigma\cup\{B,\shortmid
\}\cup\{\tilde{a}:a\in\Sigma\cup\{\shortmid\}\}$. Entonces hay una maquina de
Turing deterministica con unit $M=\left(  Q,\Gamma,\Sigma,\delta
,q_{0},B,\shortmid,\{q_{f}\}\right)  $ la cual satisface

\begin{enumerate}
\item[(1)] $(q_{f},\sigma)\notin D_{\delta}$, para cada $\sigma\in\Gamma$.

\item[(2)] Cualesquiera sean $x_{1},...,x_{k}\in\omega$ y $\alpha
_{1},...,\alpha_{k}\in\Sigma^{\ast}$, el programa $\mathcal{P}$ se detiene
partiendo del estado%
\[
\left\Vert x_{1},...,x_{k},\alpha_{1},...,\alpha_{k}\right\Vert
\]
sii $M$ se detiene partiendo de la descripcion instantanea%
\[
\left\lfloor q_{0}B\shortmid^{x_{1}}B...B\shortmid^{x_{k}}B\alpha
_{1}B...B\alpha_{k}B\right\rfloor
\]


\item[(3)] Si $x_{1},...,x_{k}\in\omega$ y $\alpha_{1},...,\alpha_{k}\in
\Sigma^{\ast}$ son tales que $\mathcal{P}$ se detiene partiendo del estado%
\[
\left\Vert x_{1},...,x_{k},\alpha_{1},...,\alpha_{k}\right\Vert
\]
y llega al estado%
\[
\left\Vert y_{1},...,y_{k},\beta_{1},...,\beta_{k}\right\Vert
\]
entonces%
\[
\left\lfloor q_{0}B\shortmid^{x_{1}}B...B\shortmid^{x_{k}}B\alpha
_{1}B...B\alpha_{k}B\right\rfloor \overset{\ast}{\underset{M}{\vdash}%
}\left\lfloor q_{f}B\shortmid^{y_{1}}B...B\shortmid^{y_{k}}B\beta
_{1}B...B\beta_{k}B\right\rfloor
\]

\end{enumerate}
\end{lemma}

Cabe destacar que si bien la veracidad de este lema es sustentada en las
explicaciones anteriores, una prueba formal rigurosa del mismo resultaria
extremadamente larga y tediosa. La ventaja de que sea un resultado
intuitivamente claro nos permite aceptarlo y seguir adelante en nuestro analisis.

\bigskip

\paragraph{Turing vence a Neumann}

En lo que sigue usaremos la existencia de la maquina simuladora de un programa
para probar que toda funcion $\Sigma$-computable es $\Sigma$-Turing
computable. Antes un lema.

\begin{lemma}
\label{sinGOTO}Si $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast m}%
\rightarrow\Sigma^{\ast}$ es $\Sigma$-computable, entonces hay un programa
$\mathcal{Q}$ el cual computa a $f$ y el cual cumple con las siguientes propiedades

\begin{enumerate}
\item[(1)] En $\mathcal{Q}$ no hay instrucciones de la forma $\mathrm{GOTO}%
\;\mathrm{L}\bar{\imath}$ ni de la forma $\mathrm{L}\bar{j}\ \mathrm{GOTO}%
\;\mathrm{L}\bar{\imath}$

\item[(2)] Cuando $\mathcal{Q}$ termina partiendo de un estado cualquiera
dado, el estado alcansado es tal que las variables numericas tienen todas el
valor $0$ y las alfabeticas tienen todas exepto $\mathrm{P}1$ el valor
$\varepsilon$.
\end{enumerate}
\end{lemma}

\begin{proof}
Sea $\mathcal{P}$ un programa que compute a $f$. Sea $r\in\mathbf{N}$ tal que
$r>N(\mathcal{P}),n,m$. Sea $\mathcal{\tilde{P}}$ el resultado de reemplazar
en $\mathcal{P}$ cada instruccion de la forma%
\[
\alpha\mathrm{GOTO}\;\mathrm{L}\bar{\imath}%
\]
con $\alpha\in\{\varepsilon\}\cup\{\mathrm{L}\bar{j}:j\in\mathbf{N}\}$ por
$\alpha\mathrm{IF\ N}\bar{r}\neq0\ \mathrm{GOTO}\;\mathrm{L}\bar{\imath}$.
Ahora sea $\mathcal{Q}$ el siguiente programa%
\[%
\begin{array}
[c]{l}%
\mathrm{N}\bar{r}\leftarrow\mathrm{N}\bar{r}+1\\
\mathcal{\tilde{P}}\\
\mathrm{N}1\leftarrow0\\
\vdots\\
\mathrm{N}\bar{r}\leftarrow0\\
\mathrm{P}2\leftarrow\varepsilon\\
\vdots\\
\mathrm{P}\bar{r}\leftarrow\varepsilon
\end{array}
\]
Es facil ver que $\mathcal{Q}$ tiene las propiedades (1) y (2).
\end{proof}

\bigskip

Por supuesto, hay un lema analogo para el caso en que $f$ llega a $\omega$ en
lugar de llegar a $\Sigma^{\ast}$. Ahora si, el anunciado teorema:

\begin{theorem}
\label{ComputableImplicaTuringComputable}Si $f:D_{f}\subseteq\omega^{n}%
\times\Sigma^{\ast m}\rightarrow O$ es $\Sigma$-computable, entonces $f$ es
$\Sigma$-Turing computable\textit{.}
\end{theorem}

\begin{proof}
Supongamos $O=\Sigma^{\ast}$. Por el Lema \ref{sinGOTO} existe $\mathcal{P}%
\in\mathrm{Pro}^{\Sigma}$ el cual computa $f$ y tiene las propiedades (1) y
(2). Sea $k=\max\{n,m,N(\mathcal{P})\}$ y sea $M_{sim}$ la maquina de Turing
con unit que simula a $\mathcal{P}$ respecto de $k$. Como puede observarse, la
maquina $M_{sim}$, no necesariamente computara a $f$. Sea $M_{1}$ la siguiente maquina:

@@figura:figure9.png@@

\noindent(Cuando $n=0$ debemos interpretar que $D_{0}=\left(  \{q_{0}%
,q_{f}\},\Gamma,\Sigma,\delta,q_{0},B,\shortmid,\{q_{f}\}\right)  $, con
$D_{\delta}=\{(q_{0},B)\}$ y $\delta(q_{0},B)=(q_{f},B,K)$. Notese que $M_{1}$
cumple que para cada $(\vec{x},\vec{\alpha})\in\omega^{n}\times\Sigma^{\ast
m}$,%
\[
\left\lfloor q_{0}B\shortmid^{x_{1}}B...B\shortmid^{x_{n}}B\alpha
_{1}B...B\alpha_{m}B\right\rfloor \overset{\ast}{\vdash}\left\lfloor
q_{f}B\shortmid^{x_{1}}B...B\shortmid^{x_{n}}B^{k-n}B\alpha_{1}B...B\alpha
_{m}B\right\rfloor
\]
Sea $M_{2}$ la siguiente maquina

@@figura:figure10.png@@

\noindent Notese que $M_{2}$ cumple que para cada $\alpha\in\Sigma^{\ast} $,%
\[
\left\lfloor q_{0}B^{k+1}\alpha\right\rfloor \overset{\ast}{\vdash
}\left\lfloor q_{f}B\alpha\right\rfloor
\]
Sea $M$ la siguiente maquina:

@@figura:figure11.png@@

\noindent A continuacion veremos que $M$ computa a $f$. Supongamos que
$(\vec{x},\vec{\alpha})\in(\omega^{n}\times\Sigma^{\ast m})-D_{f}$. Deberemos
ver que $M$ no termina partiendo de

\begin{enumerate}
\item[(*)] $\left\lfloor q_{0}B\shortmid^{x_{1}}B...B\shortmid^{x_{n}}%
B\alpha_{1}B...B\alpha_{m}B\right\rfloor $
\end{enumerate}

Primero notemos que, ya que $\mathcal{P}$ computa a $f$, tenemos que
$\mathcal{P}$ no termina partiendo de $\left\Vert x_{1},...,x_{n},\alpha
_{1},...,\alpha_{m}\right\Vert $ por lo cual $\mathcal{P}$ no termina
partiendo de%
\[
\left\Vert x_{1},...,x_{n},\overset{k-n}{\overbrace{0,...,0}},\alpha
_{1},...,\alpha_{m},\overset{k-m}{\overbrace{\varepsilon,...,\varepsilon}%
}\right\Vert
\]
lo cual implica (Lema \ref{simulacion}) que

\begin{enumerate}
\item[(**)] $M_{sim}$ no termina partiendo de $\left\lfloor q_{0}%
B\shortmid^{x_{1}}B...B\shortmid^{x_{n}}B^{k-n}B\alpha_{1}B...B\alpha
_{m}B\right\rfloor $
\end{enumerate}

Ahora notese que si hacemos funcionar a $M$ desde la descripcion instantanea
dada en (*), llegaremos (via la copia de $M_{1}$ dentro de $M$)
indefectiblemente (ya que $M$ es deterministica) a la siguiente descripcion
instantanea%
\[
\left\lfloor q_{2}B\shortmid^{x_{1}}B...B\shortmid^{x_{n}}B^{k-n}B\alpha
_{1}B...B\alpha_{m}B\right\rfloor
\]
Luego entonces (**) nos dice que al seguir trabajando $M$ (ahora via la copia
de $M_{sim}$ dentro de $M$), la maquina $M$ nunca terminara.

Para terminar de ver que $M$ computa a $f$, tomemos $(\vec{x},\vec{\alpha})\in
D_{f}$ y veamos que%
\[
\left\lfloor q_{0}B\shortmid^{x_{1}}B...B\shortmid^{x_{n}}B\alpha
_{1}B...B\alpha_{m}B\right\rfloor \overset{\ast}{\underset{M}{\vdash}%
}\left\lfloor q_{5}Bf(\vec{x},\vec{\alpha})\right\rfloor
\]
y que la maquina $M$ se detiene en $\left\lfloor q_{5}Bf(\vec{x},\vec{\alpha
})\right\rfloor $. La maquina $M$ se detiene en $\left\lfloor q_{5}Bf(\vec
{x},\vec{\alpha})\right\rfloor $ ya que $q_{5}$ es el estado final de una
copia de $M_{2}$ y por lo tanto no sale ninguna flecha desde el. Ya que
$\mathcal{P}$ computa a $f$ y tiene la propiedad (2) del Lema \ref{sinGOTO},
tenemos que $\mathcal{P}$ termina partiendo de $\left\Vert x_{1}%
,...,x_{n},\alpha_{1},...,\alpha_{m}\right\Vert $ y llega al estado
$\left\Vert f(\vec{x},\vec{\alpha})\right\Vert $, o lo que es lo mismo,
$\mathcal{P}$ termina partiendo de%
\[
\left\Vert x_{1},...,x_{n},\overset{k-n}{\overbrace{0,...,0}},\alpha
_{1},...,\alpha_{m},\overset{k-m}{\overbrace{\varepsilon,...,\varepsilon}%
}\right\Vert
\]
y llega al estado%
\[
\left\Vert \overset{k}{\overbrace{0,...,0}},f(\vec{x},\vec{\alpha}%
),\overset{k-1}{\overbrace{\varepsilon,...,\varepsilon}}\right\Vert
\]
Pero entonces el Lema \ref{simulacion} nos dice que

\begin{enumerate}
\item[(***)] $\left\lfloor q_{0}B\shortmid^{x_{1}}B...B\shortmid^{x_{n}%
}B^{k-n}B\alpha_{1}B...B\alpha_{m}B\right\rfloor \overset{\ast}{\underset
{M_{sim}}{\vdash}}\left\lfloor q_{f}B^{k+1}f(\vec{x},\vec{\alpha
})\right\rfloor $
\end{enumerate}

Como ya lo vimos, si hacemos funcionar a $M$ desde $\left\lfloor
q_{0}B\shortmid^{x_{1}}B...B\shortmid^{x_{n}}B\alpha_{1}B...B\alpha
_{m}B\right\rfloor $, llegaremos (via la copia de $M_{1}$ dentro de $M$)
indefectiblemente a la siguiente descripcion instantanea%
\[
\left\lfloor q_{2}B\shortmid^{x_{1}}B...B\shortmid^{x_{n}}B^{k-n}B\alpha
_{1}B...B\alpha_{m}B\right\rfloor
\]
Luego (***) nos dice que, via la copia de $M_{sim}$ dentro de $M$, llegaremos
a $\left\lfloor q_{3}B^{k+1}f(\vec{x},\vec{\alpha})\right\rfloor $ e
inmediatamente a $\left\lfloor q_{4}B^{k+1}f(\vec{x},\vec{\alpha
})\right\rfloor $. Finalmente, via la copia de $M_{2}$ dentro de $M$,
llegaremos a $\left\lfloor q_{5}Bf(\vec{x},\vec{\alpha})\right\rfloor $, lo
cual termina de demostrar que $M$ computa a $f$
\end{proof}

\bigskip

\subsection{Conclusiones: La tesis de Church}

En virtud de los teoremas ya probados tenemos el siguiente teorema que asegura
que los tres paradigmas son equivalentes.

\begin{theorem}
Sea $\Sigma$ un alfabeto finito. Dada una funcion $f$, las siguientes son equivalentes:

\begin{enumerate}
\item[(1)] $f$ es $\Sigma$-Turing computable

\item[(2)] $f$ es $\Sigma$-recursiva

\item[(3)] $f$ es $\Sigma$-computable
\end{enumerate}
\end{theorem}

\begin{proof}
(1)$\Rightarrow$(2) es probado en el Teorema
\ref{TuringComputableImplicaRecursiva}. (2)$\Rightarrow$(3) es probado en el
Teorema \ref{RimplicaComp}. (3)$\Rightarrow$(1) es probado en el Teorema
\ref{ComputableImplicaTuringComputable}.
\end{proof}

\bigskip

Tambien los tres paradigmas son equivalentes con respecto a los dos tipos de
conjuntos estudiados, es decir:

\begin{theorem}
Sea $\Sigma$ un alfabeto finito y sea $S\subseteq\omega^{n}\times\Sigma^{\ast
m}$. Las siguientes son equivalentes:

\begin{enumerate}
\item[(1)] $S$ es $\Sigma$-Turing enumerable

\item[(2)] $S$ es $\Sigma$-recursivamente enumerable

\item[(3)] $S$ es $\Sigma$-enumerable
\end{enumerate}
\end{theorem}

\begin{proof}
Directo de las definiciones y el teorema anterior.
\end{proof}

\begin{theorem}
Sea $\Sigma$ un alfabeto finito y sea $S\subseteq\omega^{n}\times\Sigma^{\ast
m}$. Las siguientes son equivalentes:

\begin{enumerate}
\item[(1)] $S$ es $\Sigma$-Turing computable

\item[(2)] $S$ es $\Sigma$-recursivo

\item[(3)] $S$ es $\Sigma$-computable
\end{enumerate}
\end{theorem}

\begin{proof}
Directo de las definiciones y el teorema anterior.
\end{proof}

\bigskip

Otro modelo matematico de computabilidad efectiva es el llamado lamda
calculus, introducido por Church, el cual tambien resulta equivalente a los
estudiados por nosotros. El hecho de que tan distintos paradigmas
computacionales hayan resultado equivalentes hace pensar que en realidad los
mismos han tenido exito en capturar la totalidad de las funciones $\Sigma
$-efectivamente computables. Esta aseveracion es conocida como la

\bigskip

{\Large Tesis de Church:} \textit{Toda funcion }$\Sigma$\textit{-efectivamente
computable es }$\Sigma$\textit{-recursiva.}

\bigskip

\noindent Si bien no se ha podido dar una prueba estrictamente matematica de
la Tesis de Church, es un sentimiento comun de los investigadores del area que
la misma es verdadera.

\bigskip

\subsection{\label{survey recursivo}Resultados basicos presentados en
paradigma recursivo}

En esta seccion presentaremos varios de los resultados basicos de
computabilidad, expresados en el paradigma recursivo, ya que es el mas
habitual y comodo. Varios de estos resultados ya han sido establecidos dentro
del desarrollo de la computabilidad efectiva en el Capitulo
\ref{ParadigmaFilosofico}. A estos resultados los enunciaremos dentro del
paradigma de Godel y daremos pruebas rigurosas matematicas de ellos usando la
teoria desarrollada hasta ahora. Sin envargo, veremos que hay otros resultados
que son dependientes del desarrollo matematico hecho y aportan nueva
informacion al paradigma filosofico (la indecidibilidad del halting problem,
por ejemplo).

\subsubsection{Lema de division por casos para funciones $\Sigma$-recursivas}

\begin{lemma}
\label{dpc1}Supongamos $f_{i}:D_{f_{i}}\subseteq\omega^{n}\times\Sigma^{\ast
m}\rightarrow O$, $i=1,...,k$, son funciones $\Sigma$-recursivas tales que
$D_{f_{i}}\cap D_{f_{j}}=\emptyset$ para $i\neq j$. Entonces la funcion
$f_{1}\cup...\cup f_{k}$ es $\Sigma$-recursiva.
\end{lemma}

\begin{proof}
Probaremos el caso $k=2$ y $O=\Sigma^{\ast}$. Ademas supondremos que $n=m=1 $.
Sean $\mathcal{P}_{1}$ y $\mathcal{P}_{2}$ programas que computen las
funciones $f_{1}$ y $f_{2}$, respectivamente. Para $i=1,2$, definamos%
\[
H_{i}=\lambda tx_{1}\alpha_{1}\left[  Halt^{1,1}(t,x_{1},\alpha_{1}%
,\mathcal{P}_{i})\right]
\]
Notar que $D_{H_{i}}=\omega^{2}\times\Sigma^{\ast}$ y que $H_{i}$ es $\Sigma
$-mixta. Ademas sabemos que la funcion $Halt^{1,1}$ es $(\Sigma\cup\Sigma
_{p})$-p.r. por lo cual resulta facilmente que $H_{i}$ es $(\Sigma\cup
\Sigma_{p})$-p.r.. Por el Teorema de Independencia del Alfabeto tenemos que
$H_{i}$ es $\Sigma$-p.r.. Entonces $H_{i}$ es $\Sigma$-computable por lo cual
tenemos que hay un macro:%
\[
\left[  \mathrm{IF}\;H_{i}(\mathrm{V}1,\mathrm{V}2,\mathrm{W}1)\;\mathrm{GOTO}%
\;\mathrm{A}1\right]
\]
Para hacer mas intuitivo el uso de este macro lo escribiremos de la siguiente
manera%
\[
\left[  \mathrm{IF}\;Halt^{1,1}(\mathrm{V}1,\mathrm{V}2,\mathrm{W}%
1,\mathcal{P}_{i})\;\mathrm{GOTO}\;\mathrm{A}1\right]
\]
Ya que cada $f_{i}$ es $\Sigma$-computable, hay macros%
\begin{align*}
& \left[  \mathrm{W}2\leftarrow f_{1}(\mathrm{V}1,\mathrm{W}1)\right] \\
& \left[  \mathrm{W}2\leftarrow f_{2}(\mathrm{V}1,\mathrm{W}1)\right]
\end{align*}
Sea $\mathcal{P}$ el siguiente programa:%
\[%
\begin{array}
[c]{l}%
\mathrm{L}1\ \mathrm{N}20\leftarrow\mathrm{N}20+1\\
\left[  \mathrm{IF}\;Halt^{1,1}(\mathrm{N}20,\mathrm{N}1,\mathrm{P}%
1,\mathcal{P}_{1})\;\mathrm{GOTO}\;\mathrm{L}2\right] \\
\left[  \mathrm{IF}\;Halt^{1,1}(\mathrm{N}20,\mathrm{N}1,\mathrm{P}%
1,\mathcal{P}_{2})\;\mathrm{GOTO}\;\mathrm{L}3\right] \\
\mathrm{GOTO}\;\mathrm{L}1\\
\mathrm{L}2\ \left[  \mathrm{P}1\leftarrow f_{1}(\mathrm{N}1,\mathrm{P}%
1)\right] \\
\mathrm{GOTO}\;\mathrm{L}4\\
\mathrm{L}3\ \left[  \mathrm{P}1\leftarrow f_{2}(\mathrm{N}1,\mathrm{P}%
1)\right] \\
\mathrm{L}4\ \mathrm{SKIP}%
\end{array}
\]
Notese que $\mathcal{P}$ computa la funcion $f_{1}\cup f_{2}$
\end{proof}

\bigskip

La prueba del lema anterior es de naturaleza imperativa ya que da
explicitamente un programa (de todas maneras usa el paradigma recursivo o
Godeliano para justificar la existencia de los macros). A continuacion daremos
una prueba la cual es mas recursiva (aunque aun usa el paradigma imperativo en
la existencia de los programas $\mathcal{P}_{i}$).

\bigskip

\begin{proof}
Sean $\mathcal{P}_{1}$ y $\mathcal{P}_{2}$ programas que computen las
funciones $f_{1}$ y $f_{2}$, respectivamente. Sean%
\begin{align*}
P_{1}  & =\lambda t\vec{x}\vec{\alpha}\left[  Halt^{n,m}(t,\vec{x},\vec
{\alpha},\mathcal{P}_{1})\right] \\
P_{2}  & =\lambda t\vec{x}\vec{\alpha}\left[  Halt^{n,m}(t,\vec{x},\vec
{\alpha},\mathcal{P}_{2})\right]
\end{align*}
Notese que $D_{P_{1}}=D_{P_{2}}=\omega\times\omega^{n}\times\Sigma^{\ast m}$ y
que $P_{1}$ y $P_{2}$ son $(\Sigma\cup\Sigma_{p})$-p.r.. Ya que son $\Sigma
$-mixtos, el Teorema \ref{independencia} nos dice que son $\Sigma$-p.r..
Tambien notese que $D_{M((P_{1}\vee P_{2}))}=D_{f_{1}}\cup D_{f_{2}}$.
Definamos%
\begin{align*}
g_{1}  & =\lambda\vec{x}\vec{\alpha}\left[  E_{\ast1}^{n,m}(M\left(
(P_{1}\vee P_{2})\right)  (\vec{x},\vec{\alpha}),\vec{x},\vec{\alpha
},\mathcal{P}_{1})^{P_{1}(M\left(  (P_{1}\vee P_{2})\right)  (\vec{x}%
,\vec{\alpha}),\vec{x},\vec{\alpha})}\right] \\
g_{2}  & =\lambda\vec{x}\vec{\alpha}\left[  E_{\ast1}^{n,m}(M\left(
(P_{1}\vee P_{2})\right)  (\vec{x},\vec{\alpha}),\vec{x},\vec{\alpha
},\mathcal{P}_{2})^{P_{2}(M\left(  (P_{1}\vee P_{2})\right)  (\vec{x}%
,\vec{\alpha}),\vec{x},\vec{\alpha})}\right]
\end{align*}
Notese que $g_{1}$ y $g_{2}$ son $\Sigma$-recursivas y que $D_{g_{1}}%
=D_{g_{2}}=D_{f_{1}}\cup D_{f_{2}}$, Ademas notese que%
\[
g_{1}(\vec{x},\vec{\alpha})=\left\{
\begin{array}
[c]{lll}%
f_{1}(\vec{x},\vec{\alpha}) &  & \text{si }(\vec{x},\vec{\alpha})\in D_{f_{1}%
}\\
\varepsilon &  & \text{caso contrario}%
\end{array}
\right.
\]%
\[
g_{2}(\vec{x},\vec{\alpha})=\left\{
\begin{array}
[c]{lll}%
f_{2}(\vec{x},\vec{\alpha}) &  & \text{si }(\vec{x},\vec{\alpha})\in D_{f_{2}%
}\\
\varepsilon &  & \text{caso contrario}%
\end{array}
\right.
\]
O sea que $f_{1}\cup f_{2}=\lambda\alpha\beta\left[  \alpha\beta\right]
\circ\left[  g_{1},g_{2}\right]  $ es $\Sigma$-recursiva.
\end{proof}

\bigskip

\bigskip

\subsubsection{\label{basicas de conjuntos R y RE}Conjuntos $\Sigma
$-recursivos y $\Sigma$-recursivamente enumerables}

A continuacion probaremos los resultados basicos sobre conjuntos $\Sigma
$-efectivamente computables y $\Sigma$-efectivamente enumerables, dados en las
Secciones \ref{conjuntos sigma-efectivamente enumerables} y
\ref{conjuntos sigma-efectivamente computables}, pero enunciados dentro del
paradigma de Godel.

\begin{lemma}
Si $P:S\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow\omega$ y
$Q:S\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow\omega$ son predicados
$\Sigma$-r., entonces $(P\vee Q)$, $(P\wedge Q)$ y $\lnot P$ lo son tambien.
\end{lemma}

\begin{proof}
Note que%
\begin{align*}
\lnot P  & =\lambda xy\left[  x\dot{-}y\right]  \circ\left[  C_{1}%
^{n,m},P\right] \\
(P\wedge Q)  & =\lambda xy\left[  x.y\right]  \circ\lbrack P,Q]\\
(P\vee Q)  & =\lnot(\lnot P\wedge\lnot Q).
\end{align*}

\end{proof}

\begin{lemma}
Supongamos $S_{1},S_{2}\subseteq\omega^{n}\times\Sigma^{\ast m}$ son conjuntos
$\Sigma$-recursivos. Entonces $S_{1}\cup S_{2}$, $S_{1}\cap S_{2}$ y
$S_{1}-S_{2}$ son $\Sigma$-recursivos
\end{lemma}

\begin{proof}
Es directa del lema anterior.
\end{proof}

\begin{lemma}
\label{union e interseccion de r.e.}Supongamos $S_{1},S_{2}\subseteq\omega
^{n}\times\Sigma^{\ast m}$ son conjuntos $\Sigma$-r.e.. Entonces

\begin{enumerate}
\item[(1)] $S_{1}\cup S_{2}$ es $\Sigma$-r.e..

\item[(2)] $S_{1}\cap S_{2}$ es $\Sigma$-r.e..
\end{enumerate}
\end{lemma}

\begin{proof}
Podemos suponer que ni $S_{1}$ ni $S_{2}$ son vacios ya que de lo contrario
los resultados son triviales. Ademas supondremos que $n=2$ y $m=1$.

(1). La idea de la prueba es la misma que la que usamos para probar que la
union de conjuntos $\Sigma$-efectivamente enumerables es $\Sigma
$-efectivamente enumerable. Daremos usando macros un programa que enumera a
$S_{1}\cup S_{2}$ y luego aplicaremos la Proposicion \ref{P enumera a S}. Por
hipotesis hay funciones $F:\omega\rightarrow\omega\times\omega\times
\Sigma^{\ast}$ y $G:\omega\rightarrow\omega\times\omega\times\Sigma^{\ast}$
tales que $F_{(1)}$, $F_{(2)}$, $F_{(3)}$, $G_{(1)}$, $G_{(2)}$ y $G_{(3)}$
son $\Sigma$-recursivas, $\operatorname{Im}(F)=S_{1}$ y $\operatorname{Im}%
(G)=S_{2} $. Ya que estas funciones tambien son $\Sigma$-computables, hay
macros%
\begin{align*}
& \left[  \mathrm{V}2\leftarrow F_{(1)}(\mathrm{V}1)\right] \\
& \left[  \mathrm{V}2\leftarrow F_{(2)}(\mathrm{V}1)\right] \\
& \left[  \mathrm{W}1\leftarrow F_{(3)}(\mathrm{V}1)\right] \\
& \left[  \mathrm{V}2\leftarrow G_{(1)}(\mathrm{V}1)\right] \\
& \left[  \mathrm{V}2\leftarrow G_{(2)}(\mathrm{V}1)\right] \\
& \left[  \mathrm{W}1\leftarrow G_{(3)}(\mathrm{V}1)\right]
\end{align*}
Ya que el predicado $Par=\lambda x[x$ es par$]$ es $\Sigma$-p.r., tenemos que
$Par$ es $\Sigma$-computable. Es decir que hay un macro:%
\[
\lbrack\mathrm{IF\ }Par(\mathrm{V}1)\ \mathrm{GOTO\ A}1]
\]
el cual escribiremos de la siguiente manera mas intuitiva%
\[
\lbrack\mathrm{IF\ V}1\text{ es par }\mathrm{GOTO\ A}1]
\]
Ya que la funcion $D=\lambda x[\lfloor x/2\rfloor]$ es $\Sigma$-p.r., tenemos
que $D$ es $\Sigma$-computable. Es decir que hay un macro:%
\[
\lbrack\mathrm{V}2\leftarrow D(\mathrm{V}1)]
\]
el cual escribiremos de la siguiente manera mas intuitiva%
\[
\lbrack\mathrm{V}2\leftarrow\lfloor\mathrm{V}1/2\rfloor]
\]
Sea $\mathcal{P}$ el siguiente programa:%
\[%
\begin{array}
[c]{ll}
& [\mathrm{IF\ N}1\text{ es par }\mathrm{GOTO\ L}1\\
& \mathrm{N}1\leftarrow\mathrm{N}1\dot{-}1\\
& [\mathrm{N}1111\leftarrow\lfloor\mathrm{N}1/2\rfloor]\\
& \left[  \mathrm{N}1\leftarrow G_{(1)}(\mathrm{N}1111)\right] \\
& \left[  \mathrm{N}2\leftarrow G_{(2)}(\mathrm{N}1111)\right] \\
& \left[  \mathrm{P}1\leftarrow G_{(3)}(\mathrm{N}1111)\right] \\
& \mathrm{GOTO\ L}2\\
\mathrm{L}1 & [\mathrm{N}1111\leftarrow\lfloor\mathrm{N}1/2\rfloor]\\
& \left[  \mathrm{N}1\leftarrow F_{(1)}(\mathrm{N}1111)\right] \\
& \left[  \mathrm{N}2\leftarrow F_{(2)}(\mathrm{N}1111)\right] \\
& \left[  \mathrm{P}1\leftarrow F_{(3)}(\mathrm{N}1111)\right] \\
\mathrm{L}2 & \mathrm{SKIP}%
\end{array}
\]
Es facil ver que $\mathcal{P}$ cumple a y b de (3) de la Proposicion
\ref{P enumera a S} por lo cual $S_{1}\cup S_{2}$ es $\Sigma$-enumerable.

(2). Es dejada al lector
\end{proof}

\bigskip

Tal como veremos mas adelante hay conjuntos $\Sigma$-recursivamente
enumerables los cuales no son $\Sigma$-recursivos. Sin envargo tenemos el
siguiente interesante resultado.

\begin{theorem}
\label{carac recursivos}Sea $S\subseteq\omega^{n}\times\Sigma^{\ast m}$. Son equivalentes

\begin{enumerate}
\item[(a)] $S$ es $\Sigma$-recursivo

\item[(b)] $S$ y $(\omega^{n}\times\Sigma^{\ast m})-S$ son $\Sigma
$-recursivamente enumerables
\end{enumerate}
\end{theorem}

\begin{proof}
(a)$\Rightarrow$(b). Si $S=\emptyset$, por definicion $S$ es $\Sigma
$-recursivamente enumerable. Supongamos entonces $S\neq\emptyset$. Haremos el
caso en el que $n=m=1$ y $(0,\varepsilon)\in S$. Sea $\leq$ un orden total
sobre $\Sigma$. Por hipotesis tenemos que $\chi_{S}^{\omega\times\Sigma^{\ast
}}$ es $\Sigma$-recursiva por lo cual es $\Sigma$-computable. O sea que
tenemos un macro%
\[
\lbrack\mathrm{IF\ }\chi_{S}^{\omega\times\Sigma^{\ast}}(\mathrm{V}%
1,\mathrm{W}1)\ \mathrm{GOTO\ A}1]
\]
Ya que la funcion $f=\lambda x[(x)_{1}]$ es $\Sigma$-p.r., ella es $\Sigma
$-computable por lo cual hay un macro%
\[
\lbrack\mathrm{V}2\leftarrow f(\mathrm{V}1)]
\]
el cual escribiremos de la siguiente manera:%
\[
\lbrack\mathrm{V}2\leftarrow(\mathrm{V}1)_{1}]
\]
Ya que la funcion $g=\lambda x[\ast^{\leq}((x)_{2})]$ es $\Sigma$-p.r., ella
es $\Sigma$-computable por lo cual hay un macro%
\[
\lbrack\mathrm{W}1\leftarrow g(\mathrm{V}1)]
\]
el cual escribiremos de la siguiente manera:%
\[
\lbrack\mathrm{W}1\leftarrow\ast^{\leq}((\mathrm{V}1)_{2})]
\]
(Dejamos al lector entender bien el funcionamiento de estos macros.) Sea
$\mathcal{P}$ el siguiente programa:%
\[%
\begin{array}
[c]{l}%
\mathrm{N}1\leftarrow\mathrm{N}1+1\\
\lbrack\mathrm{N}2\leftarrow(\mathrm{N}1)_{1}]\\
\lbrack\mathrm{P}2\leftarrow\ast^{\leq}(\mathrm{N}1)_{2}]\\
\lbrack\mathrm{IF\ }\chi_{S}^{\omega\times\Sigma^{\ast}}(\mathrm{N}%
2,\mathrm{P}2)\ \mathrm{GOTO\ L}1\\
\mathrm{N}1\leftarrow0\\
\mathrm{P}1\leftarrow\varepsilon\\
\mathrm{GOTO\ L}2\\
\mathrm{L}1\ [\mathrm{N}1\leftarrow\mathrm{N}2]\\
\left[  \mathrm{P}1\leftarrow\mathrm{P}2)\right] \\
\mathrm{L}2\ \mathrm{SKIP}%
\end{array}
\]
Notese que $\mathrm{Dom}(\left[  \Psi_{\mathcal{P}}^{1,0,\#},\Psi
_{\mathcal{P}}^{1,0,\ast}\right]  )=\omega$ y que $\operatorname{Im}(\left[
\Psi_{\mathcal{P}}^{1,0,\#},\Psi_{\mathcal{P}}^{1,0,\ast}\right]  )=S$ por lo
cual $S$ es $\Sigma$-enumerable lo que nos dice que $S$ es $\Sigma
$-recursivamente enumerable.

(b)$\Rightarrow$(a). Haremos el caso en que los conjuntos $S$ y $(\omega
^{n}\times\Sigma^{\ast m})-S$ son no vacios. Tambien supondremos $n=m=1$. Por
hipotesis hay funciones $F:\omega\rightarrow\omega\times\Sigma^{\ast}$ y
$G:\omega\rightarrow\omega\times\Sigma^{\ast}$ tales que $F_{(1)}$, $F_{(2)}$,
$G_{(1)}$ y $G_{(2)}$ son $\Sigma$-recursivas, $\operatorname{Im}(F)=S$ y
$\operatorname{Im}(G)=(\omega\times\Sigma^{\ast})-S$. Ya que estas funciones
tambien son $\Sigma$-computables, hay macros%
\begin{align*}
& \left[  \mathrm{V}2\leftarrow F_{(1)}(\mathrm{V}1)\right] \\
& \left[  \mathrm{W}1\leftarrow F_{(2)}(\mathrm{V}1)\right] \\
& \left[  \mathrm{V}1\leftarrow G_{(1)}(\mathrm{V}1)\right] \\
& \left[  \mathrm{W}1\leftarrow G_{(2)}(\mathrm{V}1)\right]
\end{align*}
Ya que los predicados $D=\lambda xy[x\neq y]$ y $D^{\prime}=\lambda\alpha
\beta\lbrack\alpha\neq\beta]$ son $\Sigma$-computables, hay macros%
\begin{align*}
& \left[  \mathrm{IF}\;D(\mathrm{V}1,\mathrm{V}2)\;\mathrm{GOTO}%
\;\mathrm{A}1\right] \\
& \left[  \mathrm{IF}\;D^{\prime}(\mathrm{W}1,\mathrm{W}2)\;\mathrm{GOTO}%
\;\mathrm{A}1\right]
\end{align*}
los cuales para hacer mas amigable la lectura los escribieremos de la
siguiente manera%
\begin{align*}
& \left[  \mathrm{IF}\;\mathrm{V}1\neq\mathrm{V}2\;\mathrm{GOTO}%
\;\mathrm{A}1\right] \\
& \left[  \mathrm{IF}\;\mathrm{W}1\neq\mathrm{W}2\;\mathrm{GOTO}%
\;\mathrm{A}1\right]
\end{align*}
Tambien usaremos el macro%
\[
\lbrack\mathrm{V}1\leftarrow C_{1}^{0,0}(\Diamond)]
\]
(asociado a la funcion $\Sigma$-computable $C_{1}^{0,0}$), el cual
escribiremos de la siguiente manera%
\[
\lbrack\mathrm{V}1\leftarrow1]
\]
Sea $\mathcal{P}$ el siguiente programa:%
\[%
\begin{array}
[c]{l}%
\mathrm{L}1\ [\mathrm{N}2\leftarrow F_{(1)}(\mathrm{N}20)]\\
\lbrack\mathrm{P}2\leftarrow F_{(2)}(\mathrm{N}20)]\\
\left[  \mathrm{IF}\;\mathrm{N}2\neq\mathrm{N}1\;\mathrm{GOTO}\;\mathrm{L}%
2\right] \\
\left[  \mathrm{IF}\;\mathrm{P}2\neq\mathrm{P}1\;\mathrm{GOTO}\;\mathrm{L}%
2\right] \\
\lbrack\mathrm{N}1\leftarrow1]\\
\mathrm{GOTO}\;\mathrm{L}3\\
\mathrm{L}2\ [\mathrm{N}2\leftarrow G_{(1)}(\mathrm{N}20)]\\
\lbrack\mathrm{P}2\leftarrow G_{(2)}(\mathrm{N}20)]\\
\left[  \mathrm{IF}\;\mathrm{N}2\neq\mathrm{N}1\;\mathrm{GOTO}\;\mathrm{L}%
4\right] \\
\left[  \mathrm{IF}\;\mathrm{P}2\neq\mathrm{P}1\;\mathrm{GOTO}\;\mathrm{L}%
4\right] \\
\mathrm{N}1\leftarrow0\\
\mathrm{GOTO}\;\mathrm{L}3\\
\mathrm{L}4\ \mathrm{N}20\leftarrow\mathrm{N}20+1\\
\mathrm{GOTO}\;\mathrm{L}1\\
\mathrm{L}3\ \mathrm{SKIP}%
\end{array}
\]
Notese que $\mathcal{P}$ computa a la funcion $\chi_{S}^{\omega\times
\Sigma^{\ast}}$ por lo cual $\chi_{S}^{\omega\times\Sigma^{\ast}}$ es $\Sigma
$-computable lo que nos dice que es $\Sigma$-recursiva. Esto por definicion
nos dice que $S$ es $\Sigma$-recursivo
\end{proof}

\bigskip

\bigskip

\begin{lemma}
\label{restriccion1}Supongamos $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast
m}\rightarrow O$ es $\Sigma$-recursiva y $S\subseteq D_{f}$ es $\Sigma$-r.e.,
entonces $f|_{S}$ es $\Sigma$-recursiva.
\end{lemma}

\begin{proof}
Si $S=\emptyset$, entonces $f|_{S}=\emptyset$ y por lo tanto $f|_{S}$ es
$\Sigma$-recursiva. Supongamos $S\neq\emptyset$. Haremos el caso $n=m=1$ y
$O=\Sigma^{\ast}$. Tenemos que hay una $F:\omega\rightarrow\omega\times
\Sigma^{\ast}$ tal que $\operatorname{Im}F=S$ y $F_{(1)}$, $F_{(2)}$ son
$\Sigma$-recursivas. Ya que $f$, $F_{(1)}$ y $F_{(2)}$ son $\Sigma
$-computables, hay macros%
\begin{align*}
& \left[  \mathrm{W}2\leftarrow f(\mathrm{V}1,\mathrm{W}1)\right] \\
& \left[  \mathrm{V}2\leftarrow F_{(1)}(\mathrm{V}1)\right] \\
& \left[  \mathrm{W}1\leftarrow F_{(2)}(\mathrm{V}1)\right]
\end{align*}
Usaremos los macros%
\begin{align*}
& \left[  \mathrm{IF}\;\mathrm{V}1\neq\mathrm{V}2\;\mathrm{GOTO}%
\;\mathrm{A}1\right] \\
& \left[  \mathrm{IF}\;\mathrm{W}1\neq\mathrm{W}2\;\mathrm{GOTO}%
\;\mathrm{A}1\right]
\end{align*}
Sea $\mathcal{P}$ el siguiente programa%
\[%
\begin{array}
[c]{ll}%
\mathrm{L}2 & [\mathrm{N}2\leftarrow F_{(1)}(\mathrm{N}20)]\\
& [\mathrm{P}2\leftarrow F_{(2)}(\mathrm{N}20)]\\
& \left[  \mathrm{IF}\;\mathrm{N}1\neq\mathrm{N}2\;\mathrm{GOTO}%
\;\mathrm{L}1\right] \\
& \left[  \mathrm{IF}\;\mathrm{P}1\neq\mathrm{P}2\;\mathrm{GOTO}%
\;\mathrm{L}1\right] \\
& \left[  \mathrm{P}1\leftarrow f(\mathrm{N}1,\mathrm{P}1)\right] \\
& \mathrm{GOTO}\;\mathrm{L}3\\
\mathrm{L}1 & \mathrm{N}20\leftarrow\mathrm{N}20+1\\
& \mathrm{GOTO}\;\mathrm{L}2\\
\mathrm{L}3 & \mathrm{SKIP}%
\end{array}
\]
Es facil ver que $\mathcal{P}$ computa a $f|_{S}$
\end{proof}

\bigskip

\bigskip

Ahora probaremos el analogo recursivo del Teorema
\ref{equivalencias de efectivamente enumerable}.

\begin{theorem}
\label{equivalencias-r.e.}Dado $S\subseteq\omega^{n}\times\Sigma^{\ast m} $,
son equivalentes

\begin{enumerate}
\item[(1)] $S$ es $\Sigma$-recursivamente enumerable

\item[(2)] $S=I_{F}$, para alguna $F:D_{F}\subseteq\omega^{k}\times
\Sigma^{\ast l}\rightarrow\omega^{n}\times\Sigma^{\ast m}$ tal que cada
$F_{(i)}$ es $\Sigma$-recursiva.

\item[(3)] $S=D_{f}$, para alguna funcion $\Sigma$-recursiva $f$

\item[(4)] $S=\emptyset$ o $S=I_{F}$, para alguna $F:\omega\rightarrow
\omega^{n}\times\Sigma^{\ast m}$ tal que cada $F_{(i)}$ es $\Sigma$-p.r.
\end{enumerate}
\end{theorem}

\begin{proof}
El caso $n=m=0$ es facil y es dejado al lector. Supongamos entonces que
$n+m\geq1$.

(2)$\Rightarrow$(3). Haremos el caso $k=l=1$ y $n=m=2$. El caso general es
completamente analogo. Notese que entonces tenemos que $S\subseteq\omega
^{2}\times\Sigma^{\ast2}$ y $F:D_{F}\subseteq\omega\times\Sigma^{\ast
}\rightarrow\omega^{2}\times\Sigma^{\ast2}$ es tal que $\operatorname{Im}F=S$
y $F_{(1)}$, $F_{(2)}$, $F_{(3)}$, $F_{(4)}$ son $\Sigma$-recursivas. Para
cada $i\in\{1,2,3,4\}$, sea $\mathcal{P}_{i}$ un programa el cual computa a
$F_{(i)}$. Sea $\leq$ un orden total sobre $\Sigma$. Definamos%
\[
H_{i}=\lambda tx_{1}\alpha_{1}\left[  \lnot Halt^{1,1}(t,x_{1},\alpha
_{1},\mathcal{P}_{i})\right]
\]
Notar que $D_{H_{i}}=\omega^{2}\times\Sigma^{\ast}$ y que $H_{i}$ es $\Sigma
$-mixta. Ademas sabemos que la funcion $Halt^{1,1}$ es $(\Sigma\cup\Sigma
_{p})$-p.r. por lo cual resulta facilmente que $H_{i}$ es $(\Sigma\cup
\Sigma_{p})$-p.r.. Por la Proposicion de Independencia del Alfabeto tenemos
que $H_{i}$ es $\Sigma$-p.r.. Entonces $H_{i}$ es $\Sigma$-computable por lo
cual tenemos que hay un macro:%
\[
\left[  \mathrm{IF}\;H_{i}(\mathrm{V}2,\mathrm{V}1,\mathrm{W}1)\;\mathrm{GOTO}%
\;\mathrm{A}1\right]
\]
Para hacer mas intuitivo el uso de este macro lo escribiremos de la siguiente
manera%
\[
\left[  \mathrm{IF}\;\lnot Halt^{1,1}(\mathrm{V}2,\mathrm{V}1,\mathrm{W}%
1,\mathcal{P}_{i})\;\mathrm{GOTO}\;\mathrm{A}1\right]
\]
Para $i=1,2$, definamos%
\[
E_{i}=\lambda xtx_{1}\alpha_{1}\left[  x\neq E_{\#1}^{1,1}(t,x_{1},\alpha
_{1},\mathcal{P}_{i})\right]
\]
Para $i=3,4$, definamos%
\[
E_{i}=\lambda tx_{1}\alpha_{1}\alpha\left[  \alpha\neq E_{\ast1}^{1,1}%
(t,x_{1},\alpha_{1},\mathcal{P}_{i})\right]
\]
Dejamos al lector probar que las funciones $E_{i}$ son $\Sigma$-p.r.. O sea
que son $\Sigma$-computables por lo cual para cada $i\in\{1,2\}$ hay un macro%
\[
\left[  \mathrm{IF}\;E_{i}(\mathrm{V}2,\mathrm{V}3,\mathrm{V}1,\mathrm{W}%
1)\;\mathrm{GOTO}\;\mathrm{A}1\right]
\]
y para cada $i\in\{3,4\}$ hay un macro%
\[
\left[  \mathrm{IF}\;E_{i}(\mathrm{V}2,\mathrm{V}1,\mathrm{W}1,\mathrm{W}%
2)\;\mathrm{GOTO}\;\mathrm{A}1\right]
\]
Haremos mas intuitiva la forma de escribir estos macros, por ejemplo para
$i=1$, lo escribiremos de la siguiente manera%
\[
\left[  \mathrm{IF}\;\mathrm{V}2\neq E_{\#1}^{1,1}(\mathrm{V}3,\mathrm{V}%
1,\mathrm{W}1,\mathcal{P}_{1})\;\mathrm{GOTO}\;\mathrm{A}1\right]
\]
Ya que la funcion $f=\lambda x[(x)_{1}]$ es $\Sigma$-p.r., ella es $\Sigma
$-computable por lo cual hay un macro%
\[
\lbrack\mathrm{V}2\leftarrow f(\mathrm{V}1)]
\]
el cual escribiremos de la siguiente manera:%
\[
\lbrack\mathrm{V}2\leftarrow(\mathrm{V}1)_{1}]
\]
Similarmente hay macros:%
\[
\lbrack\mathrm{W}1\leftarrow\ast^{\leq}(\mathrm{V}1)_{3}]
\]%
\[
\lbrack\mathrm{V}2\leftarrow(\mathrm{V}1)_{2}]
\]
(dejamos al lector entender bien el funcionamiento de estos macros). Sea
$\mathcal{P}$ el siguiente programa de $\mathcal{S}^{\Sigma}$:%
\[%
\begin{array}
[c]{l}%
\mathrm{L}1\ \mathrm{N}20\leftarrow\mathrm{N}20+1\\
\lbrack\mathrm{N}10\leftarrow(\mathrm{N}20)_{1}]\\
\lbrack\mathrm{N}3\leftarrow(\mathrm{N}20)_{2}]\\
\lbrack\mathrm{P}3\leftarrow\ast^{\leq}(\mathrm{N}20)_{3}]\\
\left[  \mathrm{IF}\;\lnot Halt^{1,1}(\mathrm{N}10,\mathrm{N}3,\mathrm{P}%
3,\mathcal{P}_{1})\;\mathrm{GOTO}\;\mathrm{L}1\right] \\
\left[  \mathrm{IF}\;\lnot Halt^{1,1}(\mathrm{N}10,\mathrm{N}3,\mathrm{P}%
3,\mathcal{P}_{2})\;\mathrm{GOTO}\;\mathrm{L}1\right] \\
\left[  \mathrm{IF}\;\lnot Halt^{1,1}(\mathrm{N}10,\mathrm{N}3,\mathrm{P}%
3,\mathcal{P}_{3})\;\mathrm{GOTO}\;\mathrm{L}1\right] \\
\left[  \mathrm{IF}\;\lnot Halt^{1,1}(\mathrm{N}10,\mathrm{N}3,\mathrm{P}%
3,\mathcal{P}_{4})\;\mathrm{GOTO}\;\mathrm{L}1\right] \\
\left[  \mathrm{IF}\;\mathrm{N}1\neq E_{\#1}^{1,1}(\mathrm{N}10,\mathrm{N}%
3,\mathrm{P}3,\mathcal{P}_{1})\;\mathrm{GOTO}\;\mathrm{L}1\right] \\
\left[  \mathrm{IF}\;\mathrm{N}2\neq E_{\#1}^{1,1}(\mathrm{N}10,\mathrm{N}%
3,\mathrm{P}3,\mathcal{P}_{2})\;\mathrm{GOTO}\;\mathrm{L}1\right] \\
\left[  \mathrm{IF}\;\mathrm{P}1\neq E_{\ast1}^{1,1}(\mathrm{N}10,\mathrm{N}%
3,\mathrm{P}3,\mathcal{P}_{3})\;\mathrm{GOTO}\;\mathrm{L}1\right] \\
\left[  \mathrm{IF}\;\mathrm{P}2\neq E_{\ast1}^{1,1}(\mathrm{N}10,\mathrm{N}%
3,\mathrm{P}3,\mathcal{P}_{4})\;\mathrm{GOTO}\;\mathrm{L}1\right]
\end{array}
\]
Dejamos al lector la tarea de comprender el funcionamiento de este programa y
convenserse de que computa la funcion $p_{1}^{2,2}|_{S}$. Pero entonces
$p_{1}^{2,2}|_{S}$ es $\Sigma$-computable por lo cual es $\Sigma$-recursiva,
lo cual prueba (3) ya que $\mathrm{Dom}(p_{1}^{2,2}|_{S})=S$.

(3)$\Rightarrow$(4). Supongamos $S\neq\emptyset$. Sea $(z_{1},...,z_{n}%
,\gamma_{1},...,\gamma_{m})\in S$ fijo. Sea $\mathcal{P}$ un programa el cual
compute a $f$ y Sea $\leq$ un orden total sobre $\Sigma$. Sea $P:\mathbf{N}%
\rightarrow\omega$ dado por $P(x)=1$ sii%
\[
Halt^{n,m}\left(  (x)_{n+m+1},(x)_{1},...,(x)_{n},\ast^{\leq}((x)_{n+1}%
),...,\ast^{\leq}((x)_{n+m})),\mathcal{P}\right)  =1
\]
Es facil ver que $P$ es $(\Sigma\cup\Sigma_{p})$-p.r. por lo cual es $\Sigma
$-p.r.. Sea $\bar{P}=P\cup C_{0}^{1,0}|_{\{0\}}$. Para $i=1,...,n$, definamos
$F_{i}:\omega\rightarrow\omega$ de la siguiente manera%
\[
F_{i}(x)=\left\{
\begin{array}
[c]{ccc}%
(x)_{i} & \text{si} & \bar{P}(x)=1\\
z_{i} & \text{si} & \bar{P}(x)\neq1
\end{array}
\right.
\]
Para $i=n+1,...,n+m$, definamos $F_{i}:\omega\rightarrow\Sigma^{\ast}$ de la
siguiente manera%
\[
F_{i}(x)=\left\{
\begin{array}
[c]{lll}%
\ast^{\leq}((x)_{i}) & \text{si} & \bar{P}(x)=1\\
\gamma_{i-n} & \text{si} & \bar{P}(x)\neq1
\end{array}
\right.
\]
Por el lema de division por casos, cada $F_{i}$ es $\Sigma$-p.r.. Es facil ver
que $F=[F_{1},...,F_{n+m}]$ cumple (4).
\end{proof}

\bigskip

La prueba de (2)$\Rightarrow$(3) del teorema anterior es de naturaleza
imperativa ya que da explicitamente un programa (de todas maneras usa el
paradigma recursivo o Godeliano para justificar la existencia de los macros).
A continuacion daremos una prueba de (2)$\Rightarrow$(3) la cual es mas
recursiva (aunque aun usa el paradigma imperativo en la existencia de los
programas $\mathcal{P}_{i}$).

\bigskip

\begin{proof}
[(2)$\Rightarrow$(3)]Para $i=1,...,n+m$, sea $\mathcal{P}_{i}$ un programa el
cual computa a $F_{(i)}$ y Sea $\leq$ un orden total sobre $\Sigma$. Sea
$P:\mathbf{N}\times\omega^{n}\times\Sigma^{\ast m}\rightarrow\omega$ dado por
$P(t,\vec{x},\vec{\alpha})=1$ sii se cumplen las siguientes condiciones%
\begin{align*}
Halt^{k,l}(\left(  (t)_{k+l+1},(t)_{1},...,(t)_{k},\ast^{\leq}((t)_{k+1}%
),...,\ast^{\leq}((t)_{k+l})),\mathcal{P}_{1}\right)   & =1\\
& \vdots\\
Halt^{k,l}\left(  (t)_{k+l+1},(t)_{1}...(t)_{k},\ast^{\leq}((t)_{k+1}%
)...\ast^{\leq}((t)_{k+l})),\mathcal{P}_{n+m}\right)   & =1\\
E_{\#1}^{k,l}((t)_{k+l+1},(t)_{1},...,(t)_{k},\ast^{\leq}((t)_{k+1}%
),...,\ast^{\leq}((t)_{k+l})),\mathcal{P}_{1})  & =x_{1}\\
& \vdots\\
E_{\#1}^{k,l}((t)_{k+l+1},(t)_{1},...,(t)_{k},\ast^{\leq}((t)_{k+1}%
),...,\ast^{\leq}((t)_{k+l})),\mathcal{P}_{n})  & =x_{n}\\
E_{\ast1}^{k,l}((t)_{k+l+1},(t)_{1},...,(t)_{k},\ast^{\leq}((t)_{k+1}%
),...,\ast^{\leq}((t)_{k+l})),\mathcal{P}_{n+1})  & =\alpha_{1}\\
& \vdots\\
E_{\ast1}^{k,l}((t)_{k+l+1},(t)_{1},...,(t)_{k},\ast^{\leq}((t)_{k+1}%
),...,\ast^{\leq}((t)_{k+l})),\mathcal{P}_{n+m})  & =\alpha_{m}%
\end{align*}
Note que $P$ es $(\Sigma\cup\Sigma_{p})$-p.r. y por lo tanto $P$ es $\Sigma
$-p.r.. Pero entonces $M(P)$ es $\Sigma$-r. lo cual nos dice que se cumple (3)
ya que $D_{M(P)}=I_{F}=S$.
\end{proof}

\begin{corollary}
Supongamos $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow O$ es
$\Sigma$-recursiva y $S\subseteq I_{f}$ es $\Sigma$-r.e., entonces
$f^{-1}(S)=\{(\vec{x},\vec{\alpha}):f(\vec{x},\vec{\alpha})\in S\}$ es
$\Sigma$-r.e..
\end{corollary}

\begin{proof}
Por el teorema anterior $S=D_{g}$, para alguna funcion $\Sigma$-recursiva $g
$. Note que $f^{-1}(S)=D_{g\circ f}$, lo cual nuevamente por el teorema
anterior nos dice que $f^{-1}(S)$ es $\Sigma$-r.e..
\end{proof}

\bigskip

Dejamos como ejercicio dar una prueba imperativa del corolario anterior. Los
Lemas \ref{restriccion1}\ y \ref{union e interseccion de r.e.}\ pueden
obtenerse facilmente como corolarios del teorema anterior. Se gana en
elegancia y simplicidad pero cabe destacar que se pierde en intuicion

\begin{corollary}
Supongamos $f:D_{f}\subseteq\omega^{n}\times\Sigma^{\ast m}\rightarrow O$ es
$\Sigma$-recursiva y $S\subseteq D_{f}$ es $\Sigma$-r.e., entonces $f|_{S}$ es
$\Sigma$-recursiva.
\end{corollary}

\begin{proof}
Supongamos $O=\Sigma^{\ast}$. Por el teorema anterior $S=D_{g}$, para alguna
funcion $\Sigma$-recursiva $g$. Notese que componiendo adecuadamente podemos
suponer que $I_{g}=\{\varepsilon\}.$ O sea que tenemos $f|_{S}=\lambda
\alpha\beta\left[  \alpha\beta\right]  \circ\lbrack f,g]$.
\end{proof}

\bigskip

\begin{corollary}
Supongamos $S_{1},S_{2}\subseteq\omega^{n}\times\Sigma^{\ast m}$ son conjuntos
$\Sigma$-r.e.. Entonces $S_{1}\cap S_{2}$ es $\Sigma$-r.e..
\end{corollary}

\begin{proof}
Por el teorema anterior $S_{i}=D_{g_{i}}$, con $g_{1},g_{2}$ funciones
$\Sigma$-recursivas$.$ Notese que podemos suponer que $I_{g_{1}},I_{g_{2}%
}\subseteq\omega$ por lo que $S_{1}\cap S_{2}=D_{\lambda xy\left[  xy\right]
\circ\lbrack g_{1},g_{2}]}$ es $\Sigma$-r.e.$.$
\end{proof}

\bigskip

\begin{corollary}
Supongamos $S_{1},S_{2}\subseteq\omega^{n}\times\Sigma^{\ast m}$ son conjuntos
$\Sigma$-r.e.. Entonces $S_{1}\cup S_{2}$ es $\Sigma$-r.e.
\end{corollary}

\begin{proof}
Supongamos $S_{1}\neq\emptyset\neq S_{2}.$ Sean $F,G:\omega\rightarrow
\omega^{n}\times\Sigma^{\ast m}$ tales que $I_{F}=S_{1}$, $I_{G}=S_{2}$ y las
funciones $F_{(i)}%
%TCIMACRO{\U{b4}}%
%BeginExpansion
\acute{}%
%EndExpansion
s$ y $G_{(i)}%
%TCIMACRO{\U{b4}}%
%BeginExpansion
\acute{}%
%EndExpansion
s$ son $\Sigma$-recursivas. Sean $f=\lambda x\left[  Q(x,2)\right]  $ y
$g=\lambda x\left[  Q(x\dot{-}1,2)\right]  .$ Sea $H:\omega\rightarrow
\omega^{n}\times\Sigma^{\ast m}$ dada por%
\[
H_{(i)}=(F_{(i)}\circ f)\mathrm{|}_{\{x:x\mathrm{\ es\ par}\}}\cup
(G_{(i)}\circ g)\mathrm{|}_{\{x:x\mathrm{\ es\ impar}\}}%
\]
Por el Lema \ref{restriccion1} y el Lema \ref{dpc1}, cada $H_{i}$ es $\Sigma
$-recursiva. Ya que $I_{H}=S_{1}\cup S_{2}$.tenemos que $S_{1}\cup S_{2}$ es
$\Sigma$-r.e.
\end{proof}

\bigskip

A continuacion dejamos un sketch de una prueba alternativa del Teorema
\ref{carac recursivos}. Dejamos al lector completar los detalles.

\begin{proof}
(a)$\Rightarrow$(b)$.$ Note que $S=D_{Pred\circ\chi_{S}^{\omega^{n}%
\times\Sigma^{\ast m}}}.$

(b)$\Rightarrow$(a). Note que $\chi_{S}^{\omega^{n}\times\Sigma^{\ast m}%
}=C_{1}^{n,m}$\textrm{$|$}$_{S}\cup C_{0}^{n,m}$\textrm{$|$}$_{(\omega
^{n}\times\Sigma^{\ast m})-S}$.
\end{proof}

\bigskip

Los dos siguientes teoremas, nos agregan una equivalencia mas al Teorema
\ref{equivalencias-r.e.}, para el caso $n=0$, $m=1$.

\begin{theorem}
Si $L\subseteq\Sigma^{\ast}$ es $\Sigma$-r.e., entonces $L=L(M)=H(M)$ para
\ alguna maquina de Turing deterministica $M$.
\end{theorem}

\begin{proof}
La prueba es similar a la del Teorema \ref{ComputableImplicaTuringComputable}
asique solo daremos un skech de la misma. Por el Teorema
\ref{equivalencias-r.e.} $L=D_{f}$ para alguna funcion $f$ la cual es $\Sigma
$-recursiva. Notese que podemos suponer que $\operatorname{Im}f\subseteq
\Sigma^{\ast}$. Ya que $f$ es $\Sigma$-recursiva, tambien es $\Sigma
$-computable. Por el Lema \ref{sinGOTO} existe $\mathcal{P}\in\mathrm{Pro}%
^{\Sigma}$ el cual computa $f$ y tiene las propiedades (1) y (2). Sea
$k=N(\mathcal{P})$ y sea $M_{sim}$ la maquina de Turing con unit que simula a
$\mathcal{P}$ respecto de $k$. Sea $M_{1}$ una maquina de Turing
deterministica con un solo estado final $q_{f}$ (del cual no salen flechas) y
tal que para todo $\alpha\in\Sigma^{\ast}$,%
\[
\left\lfloor q_{0}B\alpha\right\rfloor \overset{\ast}{\vdash}\left\lfloor
q_{f}B^{k+1}\alpha\right\rfloor
\]
Note que la concatenacion de $M_{1}$ con $M$ produce una maquina de Turing
deterministica $M_{2}$ tal que $H(M_{2})=L(M_{2})=L$. Dejamos al lector los
detalles de la construccion de $M_{2}.$
\end{proof}

\bigskip

\begin{theorem}
Sea $M=\left(  Q,\Sigma,\Gamma,\delta,q_{0},B,F\right)  $ una maquina de
Turing. Entonces $L(M)$ y $H(M)$ son $\Sigma$-recursivamente enumerables.
\end{theorem}

\begin{proof}
Veamos que $L(M)$ es $\Sigma$-recursivamente enumerable. Sea $P$ el siguiente
predicado $(\Gamma\cup Q)$-mixto%
\[
\lambda n\alpha\left[  (\exists d\in Des)\;\left\lfloor q_{0}B\alpha
\right\rfloor \underset{M}{\overset{n}{\vdash}}d\wedge St(d)\in F\right]
\]
Notese que $D_{P}=\omega\times\Gamma^{\ast}$. Dejamos al lector probar que $P$
es $(\Gamma\cup Q)$-p.r.. Sea $P^{\prime}=P|_{\omega\times\Sigma^{\ast}}$.
Notese que $P^{\prime}(n,\alpha)=1$ sii $\alpha\in L(M)$ atestiguado por una
computacion de longitud $n$. Ya que $P^{\prime}$ es $(\Gamma\cup Q)$-p.r. (por
que?) y ademas es $\Sigma$-mixto, el Teorema \ref{independencia} nos dice que
$P^{\prime}$ es $\Sigma$-p.r.. Ya que $L(M)=D_{M(P^{\prime})}$, el Teorema
\ref{equivalencias-r.e.} nos dice que $L(M)$ es $\Sigma$-r.e..

Dejamos al lector la prueba parecida de que $H(M)$ es $\Sigma$-recursivamente enumerable.
\end{proof}

\bigskip

\subsubsection{El halting problem y los conjuntos $A$ y $N$}

Cuando $\Sigma\supseteq\Sigma_{p}$, podemos definir%
\[
AutoHalt^{\Sigma}=\lambda\mathcal{P}\left[  (\exists t\in\omega)\;Halt^{0,1}%
(t,\mathcal{P},\mathcal{P})\right]  \text{.}%
\]
Notar que el dominio de $AutoHalt^{\Sigma}$ es $\mathrm{Pro}^{\Sigma}$ y que
para cada $\mathcal{P}\in\mathrm{Pro}^{\Sigma}$ tenemos que

\begin{enumerate}
\item[(*)] $AutoHalt(\mathcal{P})=1$ sii $\mathcal{P}$ se detiene partiendo
del estado $\left\Vert \mathcal{P}\right\Vert $.
\end{enumerate}

\begin{lemma}
\label{autohalt}Supongamos $\Sigma\supseteq\Sigma_{p}$. Entonces
$AutoHalt^{\Sigma}$ no es $\Sigma$-recursivo.
\end{lemma}

\begin{proof}
Supongamos $AutoHalt^{\Sigma}$ es $\Sigma$-recursivo y por lo tanto $\Sigma
$-computable. Por la proposicion de existencia de macros tenemos que hay un
macro%
\[
\left[  \mathrm{IF}\;AutoHalt^{\Sigma}(\mathrm{W}1)\;\mathrm{GOTO}%
\;\mathrm{A}1\right]
\]
Sea $\mathcal{P}_{0}$ el siguiente programa de $\mathcal{S}^{\Sigma}$%
\[
\mathrm{L}1\;\left[  \mathrm{IF}\;AutoHalt^{\Sigma}(\mathrm{P}%
1)\;\mathrm{GOTO}\;\mathrm{L}1\right]
\]
Note que

\begin{enumerate}
\item[-] $\mathcal{P}_{0}$ termina partiendo desde $\left\Vert \mathcal{P}%
_{0}\right\Vert $ sii $AutoHalt^{\Sigma}(\mathcal{P}_{0})=0$,
\end{enumerate}

\noindent lo cual produce una contradiccion si tomamos en (*) $\mathcal{P}%
=\mathcal{P}_{0}$.
\end{proof}

\bigskip

Usando el lema anterior y la Tesis de Church podemos probar el siguiente
impactante resultado.

\begin{theorem}
\label{autohalt es no EC}Supongamos $\Sigma\supseteq\Sigma_{p}$. Entonces
$AutoHalt^{\Sigma}$ no es $\Sigma$-efectivamente computable. Es decir no hay
ningun procedimiento efectivo que decida si un programa de $\mathcal{S}%
^{\Sigma}$ termina partiendo de si mismo.
\end{theorem}

\begin{proof}
Si $AutoHalt^{\Sigma}$ fuera $\Sigma$-efectivamente computable, la Tesis de
Church nos diria que es $\Sigma$-recursivo, contradiciendo el lema anterior.
\end{proof}

\bigskip

Notese que $AutoHalt^{\Sigma}$ provee de un ejemplo natural en el cual la
cuantificacion (no acotada) de un predicado $\Sigma$-p.r. con dominio
rectangular no es $\Sigma$-efectivamente computable

Ahora estamos en condiciones de dar un ejemplo natural de un conjunto $A$ que
es $\Sigma$-recursivamente enumerable pero el cual no es $\Sigma$-recursivo.

\begin{lemma}
\label{A es RE y no R}Supongamos que $\Sigma\supseteq\Sigma_{p}.$ Entonces%
\[
A=\left\{  \mathcal{P}\in\mathrm{Pro}^{\Sigma}:AutoHalt^{\Sigma}%
(\mathcal{P})=1\right\}
\]
es $\Sigma$-r.e. y no es $\Sigma$-recursivo. Mas aun el conjunto%
\[
N=\left\{  \mathcal{P}\in\mathrm{Pro}^{\Sigma}:AutoHalt^{\Sigma}%
(\mathcal{P})=0\right\}
\]
no es $\Sigma$-r.e.
\end{lemma}

\begin{proof}
Para ver que $A$ es $\Sigma$-r.e. se lo puede hacer imperativamente dando un
programa (usando macros) que enumere a $A$. De esta forma probariamos que $A$
es $\Sigma$-enumerable y por lo tanto es $\Sigma$-r.e.. Daremos ahora una
prueba no imperativa de este hecho, es decir mas propia del paradigma
funcional. Sea $P=\lambda t\mathcal{P}\left[  Halt^{0,1}(t,\mathcal{P}%
,\mathcal{P})\right]  $. Note que $P$ es $\Sigma$-p.r. por lo que $M(P)$ es
$\Sigma$-r.. Ademas note que $D_{M(P)}=A$, lo cual implica que $A$ es $\Sigma$-r.e..

Supongamos ahora que $N$ es $\Sigma$-r.e.. Entonces la funcion $C_{0}%
^{0,1}|_{N}$ es $\Sigma$-recursiva ya que $C_{0}^{0,1}$ lo es. Ademas ya que
$A$ es $\Sigma$-r.e. tenemos que $C_{1}^{0,1}|_{A}$ es $\Sigma$-recursiva. Ya
que%
\[
AutoHalt^{\Sigma}=C_{1}^{0,1}|_{A}\cup C_{0}^{0,1}|_{N}%
\]
el Lema \ref{dpc1} nos dice que $AutoHalt^{\Sigma}$ es $\Sigma$-recursivo,
contradiciendo el Lema \ref{autohalt}. Esto prueba que $N$ no es $\Sigma$-r.e..

Finalmente supongamos $A$ es $\Sigma$-recursivo. Entonces el conjunto%
\[
N=\left(  \Sigma^{\ast}-A\right)  \cap\mathrm{Pro}^{\Sigma}%
\]
deberia serlo, lo cual es absurdo. Hemos probado entonces que $A$ no es
$\Sigma$-recursivo.
\end{proof}

\bigskip

Cabe destacar aqui que el dominio de una funcion $\Sigma$-recursiva no siempre
sera un conjunto $\Sigma$-recursivo. Por ejemplo si tomamos $\Sigma$ tal que
$\Sigma\supseteq\Sigma_{p}$, entonces $C_{1}^{0,1}|_{A}$ es una funcion
$\Sigma$-recursiva ya que es la restriccion de la funcion $\Sigma$-recursiva
$C_{1}^{0,1}$ al conjunto $\Sigma$-r.e. $A$, pero $\mathrm{Dom}(C_{1}%
^{0,1}|_{A})=A$ no es $\Sigma$-recursivo.

\bigskip

Usando la Tesis de Church obtenemos el siguiente resultado.

\begin{proposition}
\label{A es EE y no EC}Supongamos que $\Sigma\supseteq\Sigma_{p}.$ Entonces
$A$ es $\Sigma$-efectivamente enumerable y no es $\Sigma$-efectivamente
computable. El conjunto $N$ no es $\Sigma$-efectivamente enumerable. Es decir,
$A$ puede ser enumerado por un procedimiento efectivo pero no hay ningun
procedimiento efectivo que decida la pertenencia a $A$ y no hay ningun
procedimiento efectivo que enumere a $N$. Mas aun, si un procedimiento
efectivo da como salida siempre elementos de $N$, entonces hay una cantidad
infinita de elementos de $N$ los cuales nunca da como salida
\end{proposition}

\bigskip

Con los resultados anteriores estamos en condiciones de dar un ejemplo de un
predicado $\Sigma$-recursivo, cuya minimizacion no es $\Sigma$-efectivamente
computable (y por lo tanto es no $\Sigma$-recursiva).

\bigskip

\begin{proposition}
\label{P recursivo no implica M(P) recursiva}Supongamos que $\Sigma
\supseteq\Sigma_{p}$. Sea $P=C_{1}^{0,1}|_{A}\circ\lambda t\alpha\left[
\alpha^{1\dot{-}t}\mathrm{SKIP}^{t}\right]  |_{\omega\times\mathrm{Pro}%
^{\Sigma}}$. La funcion $M(P)$ no es $\Sigma$-efectivamente computable (y por
lo tanto es no $\Sigma$-recursiva)
\end{proposition}

\begin{proof}
Notese que $D_{M(P)}=\mathrm{Pro}^{\Sigma}$ y que para cada $\mathcal{P}%
\in\mathrm{Pro}^{\Sigma}$ se tiene que%
\[
M(P)(\mathcal{P})=0\text{ sii }\mathcal{P}\in A
\]
O sea que $AutoHalt^{\Sigma}=\lambda x[x=0]\circ M(P)$ lo cual nos dice que
$M(P)$ no es $\Sigma$-recursiva ya que si lo fuese lo seria tambien
$AutoHalt^{\Sigma}$. Por la Tesis de Church $M(P)$ tampoco es $\Sigma
$-efectivamente computable
\end{proof}

\bigskip

\bigskip

Supongamos $\Sigma\supseteq\Sigma_{p}$. Sea $f=\lambda\mathcal{P}\left[
T^{0,1}(\mathcal{P},\mathcal{P})\right]  $. Note que $D_{f}=A$ y
$f(\mathcal{P})$ es la cantidad de pasos en la que $\mathcal{P}$ se detiene
partiendo de $\left\Vert \mathcal{P}\right\Vert $.

\begin{lemma}
No hay ninguna funcion $F:\mathrm{Pro}^{\Sigma}\rightarrow\omega$ la cual sea
$\Sigma$-recursiva y extienda a $f$
\end{lemma}

\begin{proof}
Supongamos hay una tal $F$. Notese que $AutoHalt^{\Sigma}=\lambda
\mathcal{P}\left[  Halt^{0,1}(F(\mathcal{P}),\mathcal{P},\mathcal{P})\right]
$ lo cual nos dice que $AutoHalt^{\Sigma}$ es $\Sigma$-recursiva, llegando a
una contradiccion.
\end{proof}

\bigskip

\section{\label{Seccion estructuras algebraicas ordenadas}Estructuras
algebraicas ordenadas}

En esta seccion estudiaremos varias clases de estructuras algebraicas en las
cuales hay un orden parcial involucrado. Esto tendra una triple utilidad. Por
un lado algunos de los resultados probados sobre algebras de Boole (por
ejemplo el teorema de Rasiova y Sikorski) seran utilizados mas adelante para
la prueba del teorema de completitud de la logica de primer orden. Tambien,
esta seccion servira para volvernos algebristas maduros (lo mas que se pueda)
ya que esto nos sera util a la hora de hacer logica matematica. La logica
matematica es \textit{matematica aplicada} al estudio de los matematicos, su
lenguaje y sus metodos de demostracion, y que mas comodo para hacer logica
matematica que contar con un matematico dentro de uno mismo para estudiarlo!

Finalmente cabe destacar que los resultados cubiertos en esta seccion son
importantes en si mismos fuera de su vinculacion con la logica y tienen
fuertes aplicaciones en otras disciplinas y ramas de la matematica.

\bigskip

\subsection{Conjuntos parcialmente ordenados}

Recordamos que tal como se lo definio en la Seccion \ref{ordenes parciales},
una relacion binaria $\leq$ sobre un conjunto $P$ es llamada un \textit{orden
parcial sobre }$P$\textit{\ }si se cumplen las siguientes condiciones:

\begin{enumerate}
\item[(1)] $\leq$ es reflexiva, i. e. para todo $a\in P$, $a\leq a$

\item[(2)] $\leq$ es antisimetrica, i. e. para todo $a,b\in P$, si $a\leq b$ y
$b\leq a$, entonces $a=b.$

\item[(3)] $\leq$ es transitiva, i. e. para todo $a,b,c\in P$, si $a\leq b$ y
$b\leq c$, entonces $a\leq c$.
\end{enumerate}

\noindent(recomendamos antes de leer este tema, leer la Seccion
\ref{ordenes parciales}, para familiarizarse con la notacion y las propiedades
basicas de los ordenes parciales).

Un \textit{conjunto parcialmente ordenado} \textit{o poset} es un par
$(P,\leq)$ donde $P$ es un conjunto no vacio cualquiera y $\leq$ es un orden
parcial sobre $P$. Dado un poset $(P,\leq)$, el conjunto $P$ sera llamado el
\textit{universo} de $(P,\leq)$. Algunos ejemplos:

\begin{enumerate}
\item[(E1)] $(\mathbf{R},\leq)$ es un poset, donde $\leq$ es el orden usual de
los numeros relales

\item[(E2)] $(\{1,2,3\},\{(1,2),(1,3),(1,1),(2,2),(3,3)\})$ es un poset

\item[(E3)] $(\mathcal{P}(\omega),\leq)$ es un poset, donde $\mathrm{\leq
}=\{(S,T)\in\mathcal{P}(\omega)^{2}:S\subseteq T\}$

\item[(E4)] $(\{1\},\{(1,1)\}$ es un poset

\item[(E5)] $(\mathbf{N},\leq)$ es un poset, donde $\mathrm{\leq}%
=\{(n,m)\in\mathbf{N}^{2}:n\mid m\}$

\item[(E6)] $(A,\{(a,b):a=b\})$ es un poset, cualesquiera sea el conjunto no
vacio $A$
\end{enumerate}

Usaremos la siguiente

\begin{enumerate}
\item[Convencion notacional 1] Si hemos denotado con $\leq$ a cierto orden
parcial sobre un conjunto $A$, entonces

\begin{enumerate}
\item Denotaremos con $<$ a la relacion binaria $\{(a,b)\in A^{2}:a\leq b$ y
$a\neq b\}$. Es decir que $\mathrm{<}=\{(a,b)\in A^{2}:a\leq b$ y $a\neq b\}$.
Cuando se de $a<b$ diremos que $a$ \textit{es menor que }$b$ o que $b$
\textit{es mayor que }$a$ (\textit{respecto de }$\leq$)

\item Denotaremos con $\prec$ a la relacion binaria%
\[
\{(a,b)\in A^{2}:a<b\text{ y no existe }z\text{ tal que }a<z<b\}
\]
Cuando se de $a\prec b$ diremos que $a$ \textit{es cubierto por }$b$ o que $b
$ \textit{cubre a }$a$ (\textit{respecto de }$\leq$).
\end{enumerate}
\end{enumerate}

El mismo tipo de convencion notacional se hara cuando denotemos con
$\leq^{\prime}$ (o $\tilde{\leq}$, etc) a un orden parcial sobre $A$. Es decir
tendremos dos relaciones binarias nuevas tacitamente definidas, a saber:%
\begin{align*}
& \mathrm{<}^{\prime}=\{(a,b)\in A^{2}:a\leq^{\prime}b\text{ y }a\neq b\}\\
& \mathrm{\prec}^{\prime}=\{(a,b)\in A^{2}:a<^{\prime}b\text{ y no existe
}z\text{ tal que }a<^{\prime}z<^{\prime}b\}
\end{align*}


\subsubsection{Diagramas de Hasse}

Dado un poset $(P,\leq)$, con $P$ un conjunto finito, podemos realizar un
diagrama llamado \textit{diagrama de Hasse,} siguiendo las siguientes instrucciones:

\begin{enumerate}
\item[(1)] Asociar en forma inyectiva, a cada $a\in$ $P$ un punto $p_{a}$ del plano

\item[(2)] Trazar un segmento de recta uniendo los puntos $p_{a}$ y $p_{b}$,
cada vez que $a\prec b$

\item[(3)] Realizar lo indicado en los puntos (1) y (2) en tal forma que

\begin{enumerate}
\item[(i)] Si $a\prec b$, entonces $p_{a}$ esta por debajo de $p_{b}$

\item[(ii)] Si un punto $p_{a}$ ocurre en un segmento del diagrama entonces lo
hace en alguno de sus extremos.
\end{enumerate}
\end{enumerate}

\noindent La relacion de orden $\leq$ puede ser facilmente obtenida a partir
del diagrama, a saber, $a\leq b$ sucedera si y solo si $p_{a}=p_{b}$ o hay una
sucesion de segmentos ascendentes desde $p_{a}$ hasta $p_{b}$.

Algunos ejemplos:

\bigskip

\subsubsection{Elementos maximales, maximos, minimales y minimos}

Sea $(P,\leq)$ un poset. Diremos que $a\in P$ es un elemento \textit{maximal
de }$(P,\leq)$ si no existe un $b\in P$ tal que $a<b$. Diremos que $a\in P$ es
un elemento \textit{maximo de }$(P,\leq)$ si $b\leq a$, para todo $b\in P$. En
forma analoga se definen los conceptos de elemento \textit{minimal }y
\textit{minimo}. Algunos ejemplos:

\begin{enumerate}
\item[(E1)] Sea $\leq$ el orden usual de los numeros reales. El poset
$(\mathbf{R},\leq)$ no tiene elemento maximo ni minimo. Tampoco tiene
elementos maximales ni minimales.

\item[(E2)] El poset $\mathbf{P}%
=(\{1,2,3\},\{(1,2),(1,3),(1,1),(2,2),(3,3)\})$ no tiene elemento maximo. $1$
es un elemento minimo de $\mathbf{P}$. El unico elemento minimal de
$\mathbf{P}$ es $1$. Los elementos $2$ y $3$ son los unicos maximales de
$\mathbf{P}$.

\item[(E3)] $1$ es un elemento maximo de del poset $(\{1\},\{(1,1)\})$.
Tambien $1$ es un elemento minimo de $(\{1\},\{(1,1)\})$.
\end{enumerate}

Como lo muestra el ejemplo (E3), no siempre hay elementos maximales o maximos
en un poset. Ademas un poset tiene a lo sumo un maximo y un minimo (por que?),
los cuales en caso de existir algunas veces seran denotados con $1$ y $0$,
respectivamente. Tambien diremos que $(P,\leq)$ \textit{tiene un} $1$ (resp.
$0$) para expresar que $(P,\leq)$ tiene un elemento maximo (resp. minimo).
Notese tambien que todo elemento maximo (resp. minimo) de $(P,\leq)$ es un
elemento maximal (resp. minimal) de $(P,\leq)$ (por que?).

\bigskip

\subsubsection{Supremos}

Sea $(P,\leq)$ un poset. Dado $S\subseteq P$, diremos que un elemento $a\in P$
es \textit{cota superior de }$S$ \textit{en }$(P,\leq)$ cuando $b\leq a$, para
todo $b\in S$. Notese que todo elemento de $P$ es cota superior de $\emptyset$
en $(P,\leq)$. Un elemento $a\in P$ sera llamado \textit{supremo de }$S$
\textit{en }$(P,\leq)$ cuando se den las siguientes dos propiedades

\begin{enumerate}
\item[(1)] $a$ es a cota superior de $S$ en $(P,\leq)$

\item[(2)] Para cada $b\in P$, si $b$ es una cota superior de $S$ en
$(P,\leq)$, entonces $a\leq b$.
\end{enumerate}

\noindent Algunos ejemplos:

\begin{enumerate}
\item[(E1)] Consideremos el poset $(\mathbf{R},\leq)$, donde $\leq$ es el
orden usual de los numeros reales. Notese que ningun elemento de $\mathbf{R}$
es cota superior de $\omega$ en $(\mathbf{R},\leq)$. O sea que ningun elemento
de $\mathbf{R}$\ es supremo de $\omega$ en $(\mathbf{R},\leq)$. Sea%
\begin{align*}
S  & =\{-1/n:n\in\mathbf{N}\}\\
& =\{-1,-1/2,-1/3,...\}
\end{align*}
Es facil ver que $0$ es supremo de $S$ en $(\mathbf{R},\leq)$.

\item[(E2)] Consideremos el poset $(\mathcal{P}(\omega),\leq)$, donde
$\mathrm{\leq}=\{(A,B)\in\mathcal{P}(\omega)^{2}:A\subseteq B\}$. Sean
$A,B\in\mathcal{P}(\omega)$. Es facil ver que $A\cup B$ es supremo de
$\{A,B\}$ en $(\mathcal{P}(\omega),\leq)$.
\end{enumerate}

\bigskip

Como lo muestra el ejemplo (E1) no siempre existe un supremo de $S$ en
$(P,\leq)$. Ademas notese que en caso de existir es unico, es decir, si $a$ es
supremo de $S$ en $(P,\leq)$ y $a^{\prime}$ es supremo de $S$ en $(P,\leq)$,
entonces $a=a^{\prime}$ (por que?). Esto nos permite hablar de EL supremo de
$S$ en $(P,\leq)$, cuando exista. Denotaremos con $\sup(S)$ al supremo de $S$
en $(P,\leq)$, en caso de que exista. A veces para hacer mas dinamicos los
enunciados en lugar de escribir $z$ es supremo de $S$ en $(P,\leq)$
escribiremos $z=\sup(S)$ o $\sup(S)=z$.

Notese que (E1) nos muestra que no siempre el supremo de un conjunto pertenece
al conjunto. Notese ademas que, en caso de existir, el supremo del conjunto
$\emptyset$ en $(P,\leq)$ es un elemento minimo de $(P,\leq)$. Esto es porque
todo elemento de $P$ es cota superior de $\emptyset$ en $(P,\leq)$.

\bigskip

Daremos otro ejemplo muy importante pero antes un poco de matematica basica.
Recordemos que dados $x,y\in\mathbf{N}$ decimos que $x$ \textit{es multiplo de
}$y$ cuando $y$ divide a $x$. Ademas, por definicion, el \textit{minimo comun
multiplo de }$x$ \textit{e} $y$ es el menor elemento del conjunto
$\{z\in\mathbf{N}:z$ es multiplo de $x$ y de $y\}$. El minimo comun multiplo
de $x$ e $y$ se denota con $mcm(x,y)$. Una propiedad importante es la siguiente:

\begin{enumerate}
\item[(*)] Si $z$ es multiplo de $x$ y de $y$, entonces $mcm(x,y)|z$, es decir
no solo $mcm(x,y)$ es menor o igual a cada multiplo comun de $x$ e $y$, sino
que $mcm(x,y)$ divide a cada multiplo comun de $x$ e $y$
\end{enumerate}

Un ejemplo importante de existencia de supremos es el siguiente:

\bigskip

\begin{enumerate}
\item[(E3)] Consideremos el poset $(\mathbf{N},D)$, donde $D=\{(x,y)\in
\mathbf{N}^{2}:x|y\}$. Dados $x,y\in\mathbf{N}$, se tiene que $mcm(x,y)$ es el
supremo de $\{x,y\}$ en $(\mathbf{N},D)$. Es claro que $mcm(x,y)$ es cota
superior de $\{x,y\}$ en $(\mathbf{N},D)$. Ademas la propiedad (*) nos asegura
que la propiedad (2) de la definicion de supremo se cumple. No es obvio que se
cumple (2) de la definicion de supremo? Por que es necesario aplicar la
propiedad (*)?
\end{enumerate}

\bigskip

\subsubsection{Infimos}

Sea $(P,\leq)$ un poset. Dado $S\subseteq P$, diremos que un elemento $a\in P$
es \textit{\textit{cota }inferior} \textit{de }$S$ \textit{en }$(P,\leq)$,
cuando $a\leq b$, para cada $b\in S$. Notese que todo elemento de $P$ es cota
inferior de $\emptyset$ en $(P,\leq)$. Un elemento $a\in P$ sera llamado
\textit{infimo de }$S$ \textit{en }$(P,\leq)$ cuando se den las siguientes dos propiedades

\begin{enumerate}
\item[(1)] $a$ es a cota inferior de $S$ en $(P,\leq)$

\item[(2)] Para cada $b\in P$, si $b$ es una cota inferior de $S$ en
$(P,\leq)$, entonces $b\leq a$.
\end{enumerate}

\noindent Algunos ejemplos:

\begin{enumerate}
\item[(E1)] Consideremos el poset $(\mathbf{R},\leq)$, donde $\leq$ es el
orden usual de los numeros reales. Notese que ningun elemento de $\mathbf{R}$
es cota inferior de $\mathbf{Z}$ en $(\mathbf{R},\leq)$. O sea que ningun
elemento de $\mathbf{R}$\ es infimo de $\mathbf{Z}$ en $(\mathbf{R},\leq)$.
Sea%
\begin{align*}
S  & =\{1/n:n\in\mathbf{N}\}\\
& =\{1,1/2,1/3,...\}
\end{align*}
Es facil ver que $0$ es infimo de $S$ en $(\mathbf{R},\leq)$. Notar que $\inf
S\notin S$.

\item[(E2)] Consideremos el poset $(\mathcal{P}(\omega),\leq)$, donde
$\mathrm{\leq}=\{(A,B)\in\mathcal{P}(\omega)^{2}:A\subseteq B\}$. Sean
$A,B\in\mathcal{P}(\omega)$. Es facil ver que $A\cap B$ es infimo de $\{A,B\}$
en $(\mathcal{P}(\omega),\leq)$.
\end{enumerate}

\bigskip

Como lo muestra el ejemplo (E1) no siempre existe un infimo de $S$ en
$(P,\leq)$. Ademas notese que en caso de existir es unico, es decir, si $a$ es
infimo de $S$ en $(P,\leq)$ y $a^{\prime}$ es infimo de $S$ en $(P,\leq)$,
entonces $a=a^{\prime}$ (por que?). Esto nos permite hablar de EL infimo de
$S$ en $(P,\leq)$, cuando exista. Denotaremos con $\inf(S)$ al infimo de $S$
en $(P,\leq)$, en caso de que exista. A veces para hacer mas dinamicos los
enunciados en lugar de escribir $z$ es infimo de $S$ en $(P,\leq)$
escribiremos $z=\inf(S)$ o $\inf(S)=z$.

Notese que (E1) nos muestra que no siempre el infimo de un conjunto pertenece
al conjunto. Notese ademas que en caso de existir el infimo del conjunto
$\emptyset$ en $(P,\leq)$ es un elemento maximo de $(P,\leq)$.

\bigskip

Daremos otro ejemplo muy importante pero antes un poco de matematica basica.
Recordemos que dados $x,y\in\mathbf{N}$, por definicion, el \textit{maximo
comun divisor de }$x$ \textit{e} $y$ es el mayor elemento del conjunto
$\{z\in\mathbf{N}:z|x$ y $z|y\}$. El maximo comun divisor de $x$ e $y$ se
denota con $mcd(x,y)$. Una propiedad importante es la siguiente:

\begin{enumerate}
\item[(**)] Si $z|x$ y $z|y$, entonces $z|mcd(x,y)$, es decir no solo
$mcd(x,y)$ es mayor o igual a cada divisor comun de $x$ e $y$, sino que
$mcd(x,y)$ es divisible por cada divisor comun de $x$ e $y$
\end{enumerate}

Un ejemplo importante de existencia de infimos es el siguiente:

\bigskip

\begin{enumerate}
\item[(E3)] Consideremos el poset $(\mathbf{N},D)$, donde $D=\{(x,y)\in
\mathbf{N}^{2}:x|y\}$. Dados $x,y\in\mathbf{N}$, se tiene que $mcd(x,y)$ es el
infimo de $\{x,y\}$ en $(\mathbf{N},D)$. Es claro que $mcd(x,y)$ es cota
inferior de $\{x,y\}$ en $(\mathbf{N},D)$. Ademas la propiedad (**) nos
asegura que la propiedad (2) de la definicion de infimo se cumple. No es obvio
que se cumple (2) de la definicion de infimo? Por que es necesario aplicar la
propiedad (**)?
\end{enumerate}

\bigskip

\subsubsection{Homomorfismos de posets}

Sean $(P,\leq)$ y $(P^{\prime},\leq^{\prime})$ posets. Una funcion
$F:P\rightarrow P^{\prime}$ sera llamada un \textit{homomorfismo de }%
$(P,\leq)$ \textit{en} $(P^{\prime},\leq^{\prime})$ si para todo $x,y\in P $
se cumple que $x\leq y$ implica $F(x)\leq^{\prime}F(y)$. Escribiremos
$F:(P,\leq)\rightarrow(P^{\prime},\leq^{\prime})$\ para expresar que $F$ es un
homomorfismo de\textit{\ }$(P,\leq)$ en $(P^{\prime},\leq^{\prime}) $. Algunos ejemplos:

\begin{enumerate}
\item[(E1)] $F:\mathbf{R}\rightarrow\mathbf{R}$ dada por $F(r)=3.r$ es un
homomorfismo de $(\mathbf{R},\leq)$ en $(\mathbf{R},\leq)$

\item[(E2)] Sea $\mathrm{\leq}=\{(n,m)\in\omega:n=m\}$ y sea $(P^{\prime}%
,\leq^{\prime})$ un poset cualquiera. Entonces cualquier funcion
$F:\omega\rightarrow P^{\prime}$ es un homomorfismo de $(\omega,\leq)$ en
$(P^{\prime},\leq^{\prime})$ (glup!)

\item[(E3)] Consideremos el poset $(\mathcal{P}(\omega),\leq)$, donde
$\mathrm{\leq}=\{(A,B)\in\mathcal{P}(\omega)^{2}:A\subseteq B\}$ y el poset
$(\mathcal{P}(\{1,2,3,4\}),\leq^{\prime})$, donde $\mathrm{\leq}^{\prime
}=\{(A,B)\in\mathcal{P}(\{1,2,3,4\})^{2}:A\subseteq B\}$. Entonces
$F:\mathcal{P}(\omega)\rightarrow\mathcal{P}(1,2,3,4)$ dada por $F(A)=A\cap
\{1,2,3,4\}$ es un homomorfismo de $(\mathcal{P}(\omega),\leq)$ en
$(\mathcal{P}(\{1,2,3,4\}),\leq^{\prime})$
\end{enumerate}

Una funcion $F:P\rightarrow P^{\prime}$ sera llamada un \textit{isomorfismo de
}$(P,\leq)$ \textit{en} $(P^{\prime},\leq^{\prime})$\ si $F$ es biyectiva, $F$
es un homomorfismo de $(P,\leq)$ en $(P^{\prime},\leq^{\prime})$ y $F^{-1}$ es
un homomorfismo de $(P^{\prime},\leq^{\prime})$ en $(P,\leq)$. Escribiremos
$(P,\leq)\cong(P^{\prime},\leq^{\prime})$\ cuando exista un isomorfismo de
$(P,\leq)$ en $(P^{\prime},\leq^{\prime}) $ y en tal caso diremos que
$(P,\leq)$ y $(P^{\prime},\leq^{\prime})$\ son \textit{isomorfos}. Cabe
observar que un homomorfismo biyectivo no necesariamente es un isomorfismo
como lo muestra el siguiente ejemplo.

\begin{enumerate}
\item[-] Consideremos los posets $\mathbf{P}=(\{1,2\},\{(1,1),(2,2)\})$ y
$\mathbf{Q}=(\{1,2\},\{(1,1),(2,2),(1,2)\})$. Es facil ver que
$F:\{1,2\}\rightarrow\{1,2\}$, dada por $F(1)=1$ y $F(2)=2$ es un homomorfismo
de $\mathbf{P}$ en $\mathbf{Q}$. Dejamos al lector chequear que $F^{-1}$ no es
un homomorfismo de $\mathbf{Q}$ en $\mathbf{P}$.

\item[Notacion:] Dada una funcion $F:A\rightarrow B$ y $S\subseteq A$,
denotaremos con $F(S)$ al conjunto $\{F(a):a\in S\}$
\end{enumerate}

El siguiente lema aporta evidencia al hecho de que los isomorfismos preservan
todas las propiedades matematicas.

\begin{lemma}
\label{isoposets}Sean $(P,\leq)$ y $(P^{\prime},\leq^{\prime})$\ posets.
Supongamos $F$ es un isomorfismo de $(P,\leq)$ en $(P^{\prime},\leq^{\prime})$.

\begin{enumerate}
\item[(a)] Para $x,y\in P$, tenemos que $x<y$ si y solo si $F(x)<^{\prime
}F(y)$.

\item[(b)] Para cada $x\in P$, se tiene que $x$ es maximo (resp. minimo) de
$(P,\leq)$ si y solo si $F(x)$ es maximo (resp. minimo) de $(P^{\prime}%
,\leq^{\prime})$.

\item[(c)] Para cada $x\in P$, se tiene que $x$ es maximal (resp. minimal) en
$(P,\leq)$ si y solo si $F(x)$ es maximal (resp. minimal) en $(P^{\prime}%
,\leq^{\prime})$.

\item[(d)] Para $x,y\in P$, tenemos que $x\prec y$ si y solo si $F(x)\prec
^{\prime}F(y)$.

\item[(e)] Para cada $S\subseteq P$ y cada $a\in P$, se tiene que $a$ es cota
superior (resp. inferior) de $S$ si y solo si $F(a)$ es cota superior (resp.
inferior) de $F(S)$.

\item[(f)] Para cada $S\subseteq P$, se tiene que existe $\sup(S)$ si y solo
si existe $\sup(F(S))$ y en el caso de que existan tales elementos se tiene
que $F(\sup(S))=\sup(F(S))$.

\item[(g)] Para $x,y,z\in P$, tenemos que $z=\sup\{x,y\}$ si y solo si
$F(z)=\sup\{F(x),F(y)\}$

\item[(h)] Para $x,y,z\in P$, tenemos que $z=\inf\{x,y\}$ si y solo si
$F(z)=\inf\{F(x),F(y)\}$
\end{enumerate}
\end{lemma}

\begin{proof}
(b) Sea $a\in P$ un elemento fijo. Supongamos $a\in P$ es maximo de $(P,\leq
)$. Probaremos que $F(a)$ es maximo de $(P^{\prime},\leq^{\prime})$. Sea $b$
un elemento fijo pero arbitrario de $P^{\prime}$. Probaremos que
$b\leq^{\prime}F(a)$. Sea $d\in P$ tal que $F(d)=b$ (tal $d$ existe y que $F$
es sobreyectiva). Ya que $a$ es maximo de $(P,\leq)$ tenemos que $d\leq a$. Ya
que $F$ es un homomorfismo tenemos que $F(d)\leq^{\prime}F(a)$ por lo cual
$b\leq^{\prime}F(a)$ ya que $F(d)=b$. Ya que $b$ era arbitrario hemos probado
que $x\leq^{\prime}F(a)$ para cada $x\in P^{\prime}$, lo cual por definicion
nos dice que $F(a)$ es maximo de $(P^{\prime},\leq^{\prime})$.

Supongamos ahora que $F(a)$ es maximo de $(P^{\prime},\leq^{\prime})$.
Probaremos que $a$ es maximo de $(P,\leq)$. Sea $b$ un elemento fijo pero
arbitrario de $P$. Probaremos que $b\leq a$. Ya que $F(a)$ es maximo de
$(P^{\prime},\leq^{\prime})$ tenemos que $F(b)\leq^{\prime}F(a)$. Ya que
$F^{-1}$ es un homomorfismo tenemos que $F^{-1}(F(b))\leq F^{-1}(F(a))$, por
lo cual $b\leq a$. Ya que $b$ era arbitrario hemos probado que $x\leq a$ para
cada $x\in P$, lo cual por definicion nos dice que $a$ es maximo de $(P,\leq)$.

Ya que $a$ era fijo pero arbitrario hemos probado que cualquiera sea $x\in P$,
se tiene que $x$ es maximo de $(P,\leq)$ si y solo si $F(x)$ es maximo de
$(P^{\prime},\leq^{\prime})$.

(e) Supongamos que $a$ es cota superior de $S$. Veamos que entonces $F(a)$ es
cota superior de $F(S)$. Sea $x\in F(S)$. Sea $s\in S$ tal que $x=F(s)$. Ya
que $s\leq a$, tenemos que $x=F(s)\leq^{\prime}F(a)$. Supongamos ahora que
$F(a)$ es cota superior de $F(S)$ y veamos que entonces $a$ es cota superior
de $S$. Sea $s\in S$. Ya que $F(s)\leq^{\prime}F(a)$, tenemos que
$s=F^{-1}(F(s))\leq F^{-1}(F(a))=a$.

(f) Supongamos existe $\sup(S)$. Veamos entonces que $F(\sup(S))$ es el
supremo de $F(S)$. Por (e) $F(\sup(S))$ es cota superior de $F(S)$. Supongamos
$b$ es cota superior de $F(S)$. Entonces $F^{-1}(b)$ es cota superior de $S$,
por lo cual $\sup(S)\leq F^{-1}(b)$, produciendo $F(\sup(S))\leq^{\prime}b$.
En forma analoga se ve que si existe $\sup(F(S))$, entonces $F^{-1}%
(\sup(F(S)))$ es el supremo de $S$.

Las pruebas faltantes son dejadas como ejercicio.
\end{proof}

\bigskip

Notese que (d) nos garantiza que si dos posets finitos son isomorfos, entonces
pueden representarse con el mismo diagrama de Hasse.

En la prueba de (b) del lema anterior se uso tacitamente la siguiente
propiedad que es obvia pero clave en la demostracion:

\begin{enumerate}
\item[-] Si $F$ es una funcion y $b\in\operatorname{Im}(F)$, entonces
$b=F(d)$, para algun $d\in D_{F}$
\end{enumerate}

\bigskip

Esto da lugar a la siguiente regla la cual es muy util a la hora de hacer pruebas:

\bigskip

\textbf{Regla Pertenecer a la Imagen}

Si ud en el desarrollo de una prueba conoce que un elemento $b$ esta en la
imagen de una funcion $F$, entonces escriba al elemento $b$ en la forma $F(a)
$, donde $a$ denota algun elemento fijo del dominio de $F$

\bigskip

Muchas veces tener presente dicha regla es la diferencia a que a uno le salga
o no una prueba determinada.

\bigskip

\subsection{Version geometrica del concepto de reticulado}

El concepto de reticulado puede ser abordado en dos formas distintas, una
geometrica (via posets) y la otra algebraica (via estructuras algebraicas
definidas ecuacionalmente). Como veremos mas adelante ambas definiciones son equivalentes.

Diremos que un poset $(P,\leq)$ es un \textit{reticulado} si para todo $a,b\in
P$, existen (en $(P,\leq)$) $\sup(\{a,b\})$ e $\inf(\{a,b\})$. algunos ejemplos:

\begin{enumerate}
\item[(E1)] El poset $(\mathbf{N},D)$ es un reticulado ($D=\{(x,y)\in
\mathbf{N}^{2}:x|y\}$) ya que dados $x,y\in\mathbf{N}$, hemos visto que
$mcd(x,y)$ y $mcm(x,y)$ son infimo y supremo del conjunto $\{x,y\}$ en
$(\mathbf{N},D)$

\item[(E2)] El poset $(\mathcal{P}(\omega),\leq)$ es un reticulado
($\mathrm{\leq}=\{(A,B)\in\mathcal{P}(\omega)^{2}:A\subseteq B\}$) ya que
dados $A,B\in\mathcal{P}(\omega)$, hemos visto que $A\cap B$ y $A\cup B$ son
infimo y supremo del conjunto $\{A,B\}$ en $(\mathcal{P}(\omega),\leq)$
\end{enumerate}

\bigskip

Recordemos que dado un conjunto $A$, por una \textit{operacion binaria sobre
}$A$ entenderemos una funcion cuyo dominio es $A^{2}$ y cuya imagen esta
contenida en $A$. En un reticulado $(P,\leq)$ tenemos dos operaciones binarias
naturalmente definidas:%
\[%
\begin{array}
[c]{rcl}%
\mathsf{s}:P^{2} & \rightarrow & P\\
(a,b) & \rightarrow & \sup(\{a,b\})
\end{array}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\begin{array}
[c]{rcl}%
\mathsf{i}:P^{2} & \rightarrow & P\\
(a,b) & \rightarrow & \inf(\{a,b\})
\end{array}
\]
Escribiremos $a\mathsf{\;s\;}b$ en lugar de $\mathsf{s}(a,b)$ y
$a\mathsf{\;i\;}b$ en lugar de $\mathsf{i}(a,b)$.

A continuacion nos dedicaremos a probar varias propiedades agradables que se
cumplen en un reticulado $(L,\leq)$. Recomendamos al lector que en algunos
casos practique encontrando pruebas perfectas. Esto lo entrenara en su
capacidad de ser un matematico maduro, la cual sera crucial a la hora de hacer
logica ya que la logica estudia (modeliza) matematicamente el funcionar de un
matematico y sera muy practico que cada uno cuente con un matematico maduro en
su propia mente

\bigskip

\begin{lemma}
\label{basicasRet1}Dado un reticulado $(L,\leq)$ se cumplen las siguientes.

\begin{enumerate}
\item[(1)] $x\leq x$ $\mathsf{s}$ $y$, cualesquiera sean $x,y\in L$

\item[(2)] $x\;\mathsf{i\;}y\leq x$, cualesquiera sean $x,y\in L$

\item[(3)] $x\;\mathsf{s}\;x=x$, cualesquiera sean $x\in L$

\item[(4)] $x\mathsf{\;i\;}x=x$, cualesquiera sean $x\in L$

\item[(5)] $x\;\mathsf{s}\;y=y\;\mathsf{s}\;x$, cualesquiera sean $x,y\in L$

\item[(6)] $x\mathsf{\;i\;}y=y\mathsf{\;i\;}x$, cualesquiera sean $x,y\in L$
\end{enumerate}
\end{lemma}

\begin{proof}
Prueba perfecta de (1). Sean $a,b\in L$, fijos. Por definicion de $\mathsf{s}
$ tenemos que $a$ $\mathsf{s}$ $b=\sup(\{a,b\})$. O sea que por definicion de
supremo de un conjunto tenemos que $a$ $\mathsf{s}$ $b$ es cota superior del
conjunto $\{a,b\}$ en $(L\leq)$. O sea que $a\leq a$ $\mathsf{s}$ $b$. Ya que
$a,b$ eran arbitrarios, hemos probado que vale (1).

Prueba perfecta de (3). Sea $a\in L$, fijo. Por definicion de $\mathsf{s}$
tenemos que $a$ $\mathsf{s}$ $a=\sup(\{a,a\})=\sup(\{a\})$. O sea que debemos
probar que $a=\sup(\{a\})$. Es claro que $a$ es cota superior de $\{a\}$.
Ademas es claro que si $z$ es cota superior de $\{a\}$, entonces $z\geq a$. O
sea que por definicion de supremo de un conjunto tenemos que $a=\sup(\{a\})$.
O sea que hemos probado que $a\mathsf{\;s\;}a=a$. Ya que $a$ era arbitrario,
hemos probado que vale (3).

Dejamos al lector completar la prueba.
\end{proof}

\bigskip

\begin{lemma}
\label{basicasRet2}Dado un reticulado $(L,\leq)$ se tiene que:

\begin{enumerate}
\item[(1)] $x\leq y$ si y solo si $x\;\mathsf{s}\;y=y$, cualesquiera sean
$x,y\in L$

\item[(2)] $x\leq y$ si y solo si $x\;\mathsf{i}\;y=x$, cualesquiera sean
$x,y\in L$
\end{enumerate}
\end{lemma}

\begin{proof}
Ejercicio
\end{proof}

\bigskip

Las siguientes dos propiedades son conocidas como leyes de absorcion (por que?)

\begin{lemma}
\label{basicasRet3}Dado un reticulado $(L,\leq)$, se tiene que:

\begin{enumerate}
\item[(1)] $x\;\mathsf{s}\;(x\mathsf{\;i\;}y)=x$, cualesquiera sean $x,y\in L
$

\item[(2)] $x\mathsf{\;i\;}(x\;\mathsf{s}\;y)=x$, cualesquiera sean $x,y\in L
$
\end{enumerate}
\end{lemma}

\begin{proof}
(1). Sean $a,b\in L$, fijos. Por definicion de $\mathsf{i}$ debemos probar que
$\sup(\{a,a\mathsf{\;i\;}b\})=a$. O sea debemos probar que $a$ es la menor
cota superior de $\{a,a\mathsf{\;i\;}b\}$. Por un lema anterior tenemos que
$a\mathsf{\;i\;}b\leq a$ y obviamente se da $a\leq a$, lo cual nos dice que
$a$ es cota superior de $\{a,a\mathsf{\;i\;}b\}$. Es claro que es menor o
igual que cualquier otra cota superior. O sea que hemos probado que
$a\;\mathsf{s}\;(a\mathsf{\;i\;}b)=a$, lo cual ya que $a,b$ eran elementos
arbitrarios nos dice que vale (1).

(2) es dejada al lector.
\end{proof}

\bigskip

Antes de seguir dando propiedades basicas de los reticulados daremos tres
reglas que seran de suma utilidad para encontrar pruebas. Dejamos al lector
justificar matematicamente la validez de dichas reglas.

\bigskip

\textbf{Regla Igualdad en Posets}

Si ud esta intentando probar que en un poset $(P,\leq)$ dos elementos $x,y$
son iguales, desdoble su tarea en las dos tareas siguientes:

\begin{enumerate}
\item[-] Probar que $x\leq y$

\item[-] Probar que $y\leq x$
\end{enumerate}

\bigskip

\textbf{Regla Superar un Supremo}

Si ud esta intentando probar que en un reticulado $(L,\leq)$ se da que $z\geq
x\;\mathsf{s}\;y$, desdoble su tarea en las dos tareas siguientes:

\begin{enumerate}
\item[-] Probar que $z\geq x$

\item[-] Probar que $z\geq y$
\end{enumerate}

\bigskip

\textbf{Regla Ser Menor o Igual que un Infimo}

Si ud esta intentando probar que en un reticulado $(L,\leq)$ se da que $z\leq
x\;\mathsf{i}\;y$, desdoble su tarea en las dos tareas siguientes:

\begin{enumerate}
\item[-] Probar que $z\leq x$

\item[-] Probar que $z\leq y$
\end{enumerate}

\bigskip

Ambas operaciones $\mathsf{s}$\ e $\mathsf{i}$\ son asociativas, es decir:

\begin{lemma}
\label{basicasRet4}Dado un reticulado $(L,\leq)$, se tiene que:

\begin{enumerate}
\item[(1)] $(x\;\mathsf{s}\;y)\;\mathsf{s}\;z=x\;\mathsf{s}\;(y\;\mathsf{s}%
\;z)$, cualesquiera sean $x,y,z\in L$

\item[(2)] $(x\mathsf{\;i\;}y)\mathsf{\;i\;}z=x\mathsf{\;i\;}(y\mathsf{\;i\;}%
z)$, cualesquiera sean $x,y,z\in L$
\end{enumerate}
\end{lemma}

\begin{proof}
(1) Sean $a,b,c\in L$, fijos. Usaremos la regla Igualdad en Posets. Primero
probaremos $(a\;\mathsf{s}\;b)\;\mathsf{s}\;c\leq a\;\mathsf{s}%
\;(b\;\mathsf{s}\;c)$. Para esto usaremos la regla Superar un Supremo. Es
decir que debemos probar que%
\begin{align*}
(a\;\mathsf{s}\;b)  & \leq a\;\mathsf{s}\;(b\;\mathsf{s}\;c)\\
c  & \leq a\;\mathsf{s}\;(b\;\mathsf{s}\;c)
\end{align*}
Para la primer desigualdad usaremos tambien la regla Superar un Supremo, por
lo cual deberemos probar%
\begin{align*}
a  & \leq a\;\mathsf{s}\;(b\;\mathsf{s}\;c)\\
b  & \leq a\;\mathsf{s}\;(b\;\mathsf{s}\;c)
\end{align*}
O sea que en suma debemos probar las siguientes desigualdades%
\begin{align*}
a  & \leq a\;\mathsf{s}\;(b\;\mathsf{s}\;c)\\
b  & \leq a\;\mathsf{s}\;(b\;\mathsf{s}\;c)\\
c  & \leq a\;\mathsf{s}\;(b\;\mathsf{s}\;c)
\end{align*}
La primera es directa de un lema anterior, y para la segunda notese que el
mismo lema nos dice que%
\[
b\leq(b\;\mathsf{s}\;c)\text{ y }(b\;\mathsf{s}\;c)\leq a\;\mathsf{s}%
\;(b\;\mathsf{s}\;c)
\]
por lo cual solo resta usar que $\leq$ es transitiva. La tercera es
completamente analoga a la segunda.

En forma similar se prueba que $a\;\mathsf{s}\;(b\;\mathsf{s}\;c)\leq
(a\;\mathsf{s}\;b)\;\mathsf{s}\;c$. Es decir que por la regla Igualdad en
Posets tenemos que $a\;\mathsf{s}\;(b\;\mathsf{s}\;c)=(a\;\mathsf{s}%
\;b)\;\mathsf{s}\;c$. Ya que $a,b,c$ eran elementos arbitrarios hemos probado
que vale (1).

(2) es dejada como ejercicio.
\end{proof}

\bigskip

El siguiente lema prueba que en un reticulado las operaciones $\mathsf{s}$\ e
$\mathsf{i}$\ preservan el orden.

\begin{lemma}
\label{basicasRet5}Dado un reticulado $(L,\leq)$, se tiene que:

\begin{enumerate}
\item[(1)] $x\leq z$ e $y\leq w$ implica $x\;\mathsf{s}\ y\leq z\;\mathsf{s}%
\ w$, cualesquiera sean $x,y,z,w\in L$

\item[(2)] $x\leq z$ e $y\leq w$ implica $x\mathsf{\;i\;}y\leq z\mathsf{\;i\;}%
w$, cualesquiera sean $x,y,z,w\in L$
\end{enumerate}
\end{lemma}

\begin{proof}
(1) Sean $a,b,c,d\in L$, elementos fijos. Supongamos que $a\leq c$ e $b\leq d
$. Probaremos que entonces $a\;\mathsf{s}\ b\leq c\;\mathsf{s}\ d$. Por la
regla Superar un Supremo basta con probar que%
\begin{align*}
a  & \leq c\;\mathsf{s}\;d\\
b  & \leq c\;\mathsf{s}\;d
\end{align*}
Para ver que $a\leq c\;\mathsf{s}\;d$ notese que $a\leq c$ (por hipotesis) y
$c\leq c\;\mathsf{s}\;d$, por lo cual podemos usar que $\leq$ es transitiva.
La desigualdad $b\leq c\;\mathsf{s}\;d$ se prueba en forma similar. O sea que
hemos probado que%
\[
a\leq c\text{ y }b\leq d\text{ implica }a\;\mathsf{s}\ b\leq c\;\mathsf{s}\ d
\]
Ya que $a,b,c,d$ eran elementos arbitrarios hemos probado que vale (1).

(2) es dejada al lector
\end{proof}

\bigskip

\begin{lemma}
\label{basicasRet6}Dado un reticulado $(L,\leq)$, se tiene que:

\begin{enumerate}
\item[-] $(x\mathsf{\;i\;}y)\;\mathsf{s}\;(x\mathsf{\;i\;}z)\leq
x\mathsf{\;i\;}(y\;\mathsf{s}\;z)$, cualesquiera sean $x,y,z\in L$
\end{enumerate}
\end{lemma}

\begin{proof}
Sean $a,b,c\in L$, elementos fijos. Por la regla Superar un Supremo, basta con
probar que%
\begin{align*}
a\mathsf{\;i\;}b  & \leq a\mathsf{\;i\;}(b\;\mathsf{s}\;c)\\
a\mathsf{\;i\;}c  & \leq a\mathsf{\;i\;}(b\;\mathsf{s}\;c)
\end{align*}
Pero estas dos desigualdades pueden ser facilmente probadas aplicando (2) del
lema anterior. O sea que $(a\mathsf{\;i\;}b)\;\mathsf{s}\;(a\mathsf{\;i\;}%
c)\leq a\mathsf{\;i\;}(b\;\mathsf{s}\;c)$, de lo cual se deduce nuestro lema
ya que $a,b,c$ eran elementos arbitrarios.
\end{proof}

\bigskip

Iterar supremos (resp. infimos) da supremos (resp. infimos), es decir:

\begin{lemma}
Sea $(L,\leq)$ un reticulado. Se tiene que%
\begin{align*}
(...(x_{1}\;\mathsf{s\;}x_{2})\;\mathsf{s\;}...)\;\mathsf{s\;}x_{n}  &
=\sup(\{x_{1},...,x_{n}\})\\
(...(x_{1}\mathsf{\;i\;}x_{2})\mathsf{\;i\;}...)\mathsf{\;i\;}x_{n}  &
=\inf(\{x_{1},...,x_{n}\})
\end{align*}
cualesquiera sean los elementos $x_{1},...,x_{n}\in L$, con $n\geq2$.
\end{lemma}

\begin{proof}
Por induccion en $n$. Claramente el resultado vale para $n=2$. Supongamos vale
para $n$ y veamos entonces que vale para $n+1$. Sean $a_{1},...,a_{n+1}\in L$,
fijos. Por hipotesis inductiva tenemos que

\begin{enumerate}
\item[(1)] $(...(a_{1}\;\mathsf{s}\;a_{2})\;\mathsf{s\;}...)\;\mathsf{s\;}%
a_{n}=\sup(\{a_{1},...,a_{n}\})$
\end{enumerate}

\noindent Veamos entonces que

\begin{enumerate}
\item[(2)] $((...(a_{1}\;\mathsf{s\;}a_{2})\;\mathsf{s\;}...)\;\mathsf{s\;}%
a_{n})\;\mathsf{s\;}a_{n+1}=\sup(\{a_{1},...,a_{n+1}\})$
\end{enumerate}

\noindent Usando (1), es facil ver que $((...(a_{1}\;\mathsf{s\;}%
a_{2})\;\mathsf{s\;}...)\;\mathsf{s\;}a_{n})\;\mathsf{s\;}a_{n+1}$ es cota
superior de $\{a_{1},...,a_{n+1}\}$. Supongamos que $z$ es otra cota superior.
Ya que $z$ es tambien cota superior del conjunto $\{a_{1},...,a_{n}\}$, por
(1) tenemos que%
\[
(...(a_{1}\;\mathsf{s\;}a_{2})\;\mathsf{s}\;...)\;\mathsf{s\;}a_{n}\leq z
\]
Pero entonces ya que $a_{n+1}\leq z$, tenemos que%
\[
((...(a_{1}\;\mathsf{s\;}a_{2})\;\mathsf{s\;}...)\;\mathsf{s\;}a_{n}%
)\;\mathsf{s\;}a_{n+1}\leq z
\]
con lo cual hemos probado (2). Ya que $a_{1},...,a_{n+1}\in L$ eran elementos
arbitrarios, hemos probado que vale el enunciado del lema para $n+1 $.
\end{proof}

\bigskip

Dado que la distribucion de parentesis en una expresion de la forma%
\[
(...(x_{1}\;\mathsf{s\;}x_{2})\;\mathsf{s\;}...)\;\mathsf{s\;}x_{n}%
\]
es irrelevante (ya que$\;\mathsf{s\;}$es asociativa), en general suprimiremos
los parentesis.

\bigskip

Concluimos esta subseccion enunciando una regla que hemos usado constantemente:

\bigskip

\textbf{Regla Igualar un Supremo}

\begin{enumerate}
\item Si ud esta intentando probar que en un poset $(P,\leq)$ se da que
$x=\sup(S)$, desdoble su tarea en las dos tareas siguientes:

\begin{enumerate}
\item[-] Probar que $x$ es cota superior de $S$

\item[-] Probar que si $z$ es una cota superior de $S$, entonces $x\leq z$
\end{enumerate}
\end{enumerate}

\bigskip

Las cinco reglas consideradas estan muy vinculadas al concepto de inteligencia
artificial ya que si quisieramos hacer un probador automatico del tipo de
teoremas hechos en esta subseccion, claramente estas reglas le darian una
alternativa de busqueda que podria (y de hecho lo hace) dar el camino adecuado
para obtener la prueba de un enunciado dado.

\bigskip

\subsection{Version algebraica del concepto de reticulado}

De la diversas propiedades de las operaciones s e i de un reticulado
$(L,\leq)$ distinguiremos las siguientes:

\begin{enumerate}
\item[(I1)] $x\;\mathsf{s}\;x=x\mathsf{\;i\;}x=x$, cualesquiera sea $x\in L$

\item[(I2)] $x\mathsf{\;s\;}y=y\;\mathsf{s}\;x$, cualesquiera sean $x,y\in L$

\item[(I3)] $x\mathsf{\;i\;}y=y\mathsf{\;i\;}x$, cualesquiera sean $x,y\in L$

\item[(I4)] $(x\mathsf{\;s\;}y)\;\mathsf{s}\;z=x\;\mathsf{s}\;(y\;\mathsf{s}%
\;z)$, cualesquiera sean $x,y,z\in L$

\item[(I5)] $(x\mathsf{\;i\;}y)\mathsf{\;i\;}z=x\mathsf{\;i\;}(y\mathsf{\;i\;}%
z)$, cualesquiera sean $x,y,z\in L$

\item[(I6)] $x\;\mathsf{s}\;(x\mathsf{\;i\;}y)=x$, cualesquiera sean $x,y\in
L$

\item[(I7)] $x\mathsf{\;i\;}(x\;\mathsf{s}\;y)=x$, cualesquiera sean $x,y\in
L$
\end{enumerate}

Podemos abstraernos y pensar que s e i son dos operaciones cualesquiera sobre
un conjunto $L$ arbitrario y estudiar cuando se satisfacen y cuando no dichas
propiedades. Por ejemplo si tomamos $L=\mathbf{R}$ y%
\[%
\begin{array}
[c]{rcl}%
\mathsf{s}:\mathbf{R}^{2} & \rightarrow & \mathbf{R}\\
(a,b) & \rightarrow & a+b
\end{array}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\begin{array}
[c]{rcl}%
\mathsf{i}:\mathbf{R}^{2} & \rightarrow & \mathbf{R}\\
(a,b) & \rightarrow & a.b
\end{array}
\]
entonces se cumplen (I2), (I3), (I4) e (I5), pero (I1), (I6) e (I7)\ no se
cumplen. Otro ejemplo, si tomamos $L=\{1,2\}$ y%
\[%
\begin{array}
[c]{rcl}%
\mathsf{s}:\{1,2\}^{2} & \rightarrow & \{1,2\}\\
(1,1) & \rightarrow & 1\\
(1,2) & \rightarrow & 2\\
(2,1) & \rightarrow & 1\\
(2,2) & \rightarrow & 2
\end{array}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\begin{array}
[c]{rcl}%
\mathsf{i}:\{1,2\}^{2} & \rightarrow & \{1,2\}\\
(1,1) & \rightarrow & 1\\
(1,2) & \rightarrow & 1\\
(2,1) & \rightarrow & 1\\
(2,2) & \rightarrow & 1
\end{array}
\]
entonces se cumplen (I3), (I4) e (I5), pero (I1), (I2), (I6) e (I7)\ no se
cumplen. Un tercer ejemplo, si tomamos $L=\mathbf{N}$ y%
\[%
\begin{array}
[c]{rcl}%
\mathsf{s}:\mathbf{N}^{2} & \rightarrow & \mathbf{N}\\
(a,b) & \rightarrow & \max\{a,b\}
\end{array}
\ \ \ \ \ \
\begin{array}
[c]{rcl}%
\mathsf{i}:\mathbf{N}^{2} & \rightarrow & \mathbf{N}\\
(a,b) & \rightarrow & \text{maximo comun divisor de }a\text{ y }b
\end{array}
\]
entonces se cumplen (I1), (I2), (I3), (I4), (I5) e (I6), pero (I7)\ no se
cumple. Por supuesto si s e i son las operaciones supremo e infimo dadas por
algun orden parcial $\leq$ sobre $L$ el cual hace de $(L,\leq)$ un reticulado,
entonces las propedades (I1),...,(I7) se cumplen y esto es justamente lo
probado en la ultima serie de lemas. El ultimo ejemplo nos permite ver una
sutileza. Notese que en este ejemplo s es la operacion supremo del reticulado
$(\mathbf{N},\leq)$, donde $\leq$ es el orden usual de los naturales, e i es
la operacion infimo del reticulado $(\mathbf{N},|)$, donde $|$ es el orden de
la divisibilidad de los naturales. Sin envargo la ultima propiedad falla y
esto se debe a que s e i son supremo e infimo pero respecto de distintos
ordenes parciales.

Lo anterior motiva la siguiente definicion: Una terna $(L,\mathsf{s}%
,\mathsf{i})$, donde $L$ es un conjunto no vacio y $\mathsf{s}$ e $\mathsf{i}$
son dos operaciones binarias sobre $L$ sera llamada \textit{reticulado} cuando
cumpla (I1),...,(I7). Si $(L,\mathsf{s},\mathsf{i})$ es un reticulado,
llamaremos a $L$ el \textit{universo }de $(L,\mathsf{s},\mathsf{i})$.

\bigskip

\textbf{Observacion importante:} Notese que hemos llamado a ciertos posets
reticulados y a ciertas ternas reticulados pero deberia quedar claro que nunca
un poset que es un reticulado puede ser igual a una terna que es un reticulado
ya que son objetos matematicos de distinta naturaleza.

\bigskip

Tal como lo vimos recien, las ternas dadas por los tres ejemplos anteriores no
son reticulados ya que fallan alguna de las identidades (I1),...,(I7), y si
tomamos un poset $(L,\leq)$ el cual sea un reticulado, entonces la terna
$(L,\mathsf{s},\mathsf{i})$, con $\mathsf{s}$ e $\mathsf{i}$ definidas como
supremo e infimo, es un reticulado. El siguiente teorema muestra que todo
reticulado $(L,\mathsf{s},\mathsf{i})$ se obtiene de esta forma.

\begin{theorem}
[Dedekind]\label{equivalencia}Sea $(L,\mathsf{s},\mathsf{i})$ un reticulado.
La relacion binaria definida por:%
\[
x\leq y\text{ si y solo si }x\;\mathsf{s}\;y=y
\]
es un orden parcial sobre $L$ para el cual se cumple que:%
\begin{align*}
\sup(\{x,y\})  & =x\;\mathsf{s}\;y\\
\inf(\{x,y\})  & =x\mathsf{\;i\;}y
\end{align*}
cualesquiera sean $x,y\in L$
\end{theorem}

\begin{proof}
Dejamos como ejercicio para el lector probar que $\leq$ es reflexiva y
antisimetrica con respecto a $L$. Veamos que $\leq$ es transitiva con respecto
a $L$. Supongamos que $x\leq y$ e $y\leq z$. Es decir que por definicion de
$\leq$ tenemos que%
\begin{align*}
x\;\mathsf{s}\;y  & =y\\
y\;\mathsf{s}\;z  & =z
\end{align*}
Entonces%
\[
x\;\mathsf{s\;}z=x\;\mathsf{s\;}(y\;\mathsf{s\;}z)=(x\;\mathsf{s\;}%
y)\;\mathsf{s\;}z=y\;\mathsf{s\;}z=z
\]
por lo cual $x\leq z$. O sea que ya sabemos que $(L,\leq)$ es un poset. Veamos
ahora que $\sup(\{x,y\})=x\;\mathsf{s\;}y$. Primero debemos ver que
$x\;\mathsf{s\;}y$ es una cota superior del conjunto $\{x,y\}$, es decir%
\begin{align*}
x  & \leq x\;\mathsf{s}\;y\\
y  & \leq x\;\mathsf{s}\;y
\end{align*}
Por la definicion de $\leq$ debemos probar que%
\begin{align*}
x\ \mathsf{s}\;(x\;\mathsf{s}\;y)  & =x\;\mathsf{s}\;y\\
y\ \mathsf{s}\;(x\;\mathsf{s}\;y)  & =x\;\mathsf{s}\;y
\end{align*}
Estas igualdades se pueden probar usando (I1), (I2) y (I4). Dejamos al lector
hacerlo como ejercicio.

Nos falta ver entonces que $x\;\mathsf{s\;}y$ es menor o igual que cualquier
cota superior de $\{x,y\}$. Supongamos $x,y\leq z$. Es decir que por
definicion de $\leq$ tenemos que%
\begin{align*}
x\;\mathsf{s}\;z  & =z\\
y\;\mathsf{s}\;z  & =z
\end{align*}
Pero entonces%
\[
(x\;\mathsf{s\;}y)\;\mathsf{s\;}z=x\;\mathsf{s\;}(y\;\mathsf{s\;}%
z)=x\;\mathsf{s\;}z=z
\]
por lo que $x\;\mathsf{s\;}y\leq z$. Es decir que $x\;\mathsf{s\;}y$ es la
menor cota superior.

Para probar que $\inf(\{x,y\})=x\mathsf{\;i\;}y$, probaremos que para todo
$u,v\in L$,%
\[
u\leq v\text{ si y solo si }u\mathsf{\;i\;}v=u
\]
lo cual le permitira al lector aplicar un razonamiento similar al usado en la
prueba de que $\sup(\{x,y\})=x\;\mathsf{s\;}y$. Supongamos que $u\leq v$. Por
definicion tenemos que $u\;\mathsf{s}\;v=v$. Entonces%
\[
u\mathsf{\;i\;}v=u\mathsf{\;i\;}(u\;\mathsf{s}\;v)
\]
Pero por (I7) tenemos que $u\mathsf{\;i\;}(u\;\mathsf{s}\;v)=u$, lo cual
implica $u\mathsf{\;i\;}v=u$. Reciprocamente si $u\mathsf{\;i\;}v=u$, entonces%
\begin{align*}
u\;\mathsf{s}\;v  & =(u\mathsf{\;i\;}v)\;\mathsf{s}\;v\\
& =v\;\mathsf{s}\;(u\mathsf{\;i\;}v)\text{ (por (I2))}\\
& =v\;\mathsf{s}\;(v\mathsf{\;i\;}u)\text{ (por (I3))}\\
& =v\text{ (por (I6))}%
\end{align*}
lo cual nos dice que $u\leq v$.
\end{proof}

\bigskip

\begin{enumerate}
\item[Ejercicio:] Use los resultados anteriores para definir una funcion
$\mathcal{F}$ de $\{(L,\leq):(L,\leq)$ es un reticulado$\}$ en
$\{(L,\mathsf{s},\mathsf{i}):(L,\mathsf{s},\mathsf{i})$ es un reticulado$\}$
la cual sea biyectiva
\end{enumerate}

\bigskip

\textbf{Refleccion Informatica}

Como vimos recien a nivel de informacion es lo mismo tener un poset que es un
reticulado que una terna que es un reticulado. Es decir, los dos conceptos
pueden considerarse dos formas distintas de presentar la misma informacion.
Muchas veces es mas facil especificar un reticulado dando el poset ya que
simplemente podemos dar su diagrama de Hasse y esto en general es una forma
economica de dar las operaciones s e i.

Recordemos que algo similar sucedia con los conceptos equivalentes de relacion
de equivalencia y particion.

\bigskip

\subsubsection{El orden asociado a un reticulado}

Como vimos el teorema de Dedekind nos dice que un reticulado $(L,\mathsf{s}%
,\mathsf{i})$ es un objeto geometrico ya que si definimos%
\[
\mathrm{\leq}=\{(x,y):x\;\mathsf{s}\;y=y\}
\]
entonces $\leq$ es un orden parcial sobre $L$ y las operaciones $\mathsf{s}$ e
$\mathsf{i}$ resultan ser supremo e infimo. Llamaremos a $\mathrm{\leq
}=\{(x,y):x\;\mathsf{s}\;y=y\}$ el \textit{orden parcial asociado a
}$(L,\mathsf{s},\mathsf{i})$ y a $(L,\leq)$ el \textit{poset asociado a
}$(L,\mathsf{s},\mathsf{i})$. Notese que tambien tenemos que $\mathrm{\leq
}=\{(x,y):x\;\mathsf{i}\;y=x\}$ (\textquestiondown por que?). Muchos conceptos
definidos para posets ahora pueden aplicarse cuando tenemos un reticulado
$(L,\mathsf{s},\mathsf{i})$. Por ejemplo, si decimos que $(L,\mathsf{s}%
,\mathsf{i})$ tiene elemento maximo, esto significara que el poset $(L,\leq)$
tiene elemento maximo. Otro ejemplo, si decimos que en $(L,\mathsf{s}%
,\mathsf{i})$ se da que el supremo de un conjunto $S$ es $a$, nos estaremos
refiriendo a que en su poset asociado $(L,\leq)$ se da que el supremo de $S$
es $a$.

\bigskip

\subsubsection{Notacion}

Usaremos las siguientes practicas convenciones notacionales

\begin{enumerate}
\item[Convencion notacional 1:] Si $L$ es un conjunto no vacio cuyos elementos
son conjuntos y $L$ cumple la siguiente condicion

\begin{enumerate}
\item[-] Si $A,B\in L$, entonces $A\cup B,A\cap B\in L$
\end{enumerate}

entonces ciertas veces usaremos $\cup$ (resp. $\cap$) para denotar la
operacion binaria sobre $L$ dada por la union (resp. la interceccion). Es
decir $\cup$ e $\cap$ denotaran las funciones%
\[%
\begin{array}
[c]{rcl}%
L^{2} & \rightarrow & L\\
(A,B) & \rightarrow & A\cup B
\end{array}
\ \ \ \ \ \ \ \ \ \ \ \ \ \
\begin{array}
[c]{rcl}%
L^{2} & \rightarrow & L\\
(A,B) & \rightarrow & A\cap B
\end{array}
\]


\item[Convencion notacional 2:] Si $L$ es un conjunto no vacio cuyos elementos
son numeros reales entonces ciertas veces usaremos $\max$ y $\min$ para
denotar las operaciones binarias sobre $L$ dadas por%
\[%
\begin{array}
[c]{rcl}%
L^{2} & \rightarrow & L\\
(a,b) & \rightarrow & \max(a,b)
\end{array}
\ \ \ \ \ \ \ \ \ \ \ \ \ \
\begin{array}
[c]{rcl}%
L^{2} & \rightarrow & L\\
(a,b) & \rightarrow & \min(a,b)
\end{array}
\]


\item[Convencion notacional 3:] Si $L$ es un conjunto no vacio cuyos elementos
son numeros naturales y $L$ cumple la siguiente condicion

\begin{enumerate}
\item[-] Si $a,b\in L$, entonces $mcm(a,b),mcd(a,b)\in L$
\end{enumerate}

entonces ciertas veces usaremos $mcm$ y $mcd$ para denotar las operaciones
binarias sobre $L$ dadas por%
\[%
\begin{array}
[c]{rcl}%
L^{2} & \rightarrow & L\\
(a,b) & \rightarrow & mcm(a,b)
\end{array}
\ \ \ \ \ \ \ \ \ \ \ \ \ \
\begin{array}
[c]{rcl}%
L^{2} & \rightarrow & L\\
(a,b) & \rightarrow & mcd(a,b)
\end{array}
\]


\item[Convencion notacional 4:] Si $P$ es un conjunto no vacio contenido en
$\mathbf{N}$, entonces escribiremos $(P,|)$ para denotar al poset
$(P,\{(x,y)\in P^{2}:x|y\})$. Similarmente si $P$ es un conjunto cuyos
elementos son conjuntos, entonces escribiremos $(P,\subseteq)$ para denotar al
poset $(P,\{(A,B)\in P^{2}:A\subseteq B\})$
\end{enumerate}

\bigskip

En virtud de las convenciones notacionales anteriores notese que por ejemplo

\begin{enumerate}
\item $(\mathbf{R,}\max,\min)$

\item $([0,1]\mathbf{,}\max,\min)$

\item $(\mathcal{P}(\mathbf{N}),\cup,\cap)$

\item $(\{A\subseteq\mathbf{N}:A$ es finito$\},\cup,\cap)$

\item $(\mathbf{N},mcm,mcd)$

\item $(\{1,2,3,6,12\},mcm,mcd)$
\end{enumerate}

denotan reticulados pero deberia quedar claro que en los primeros dos ejemplos
$\max$ denota dos distintas operaciones. Analogamente sucede con $\min$,
$\cup$, $\cap$, $mcm$ y $mcd$.

Similarmente

\begin{enumerate}
\item $(\mathbf{N,}|)$

\item $(\{1,2,3,6,7\},|)$

\item $(\{\{1\},\{1,7\},\{1,2,3\},\{16,99,65\}\},\subseteq)$

\item $(\{A\subseteq\mathbf{N}:A$ es finito$\},\subseteq)$
\end{enumerate}

denotan posets pero deberia quedar claro que en los primeros dos ejemplos $|$
denota dos distintos ordenes parciales. Analogamente sucede con $\subseteq$

Estas ambiguedades no nos traeran problemas si estamos atentos al contexto.

\bigskip

\subsubsection{Subreticulados}

Si $f$ es una operacion $n$-aria sobre $A$ y $S\subseteq A$, entonces diremos
que $S$ es \textit{cerrado bajo} $f$\bigskip\ cuando se de que $f(a_{1}%
,...,a_{n})\in S$, cada ves que $a_{1},...,a_{n}\in S$. Notese que si $n=0$,
entonces $S$ es cerrado bajo $f$ si y solo si $f(\Diamond)\in S$.

Dados reticulados $(L,\mathsf{s},\mathsf{i})$ y $(L^{\prime},\mathsf{s}%
^{\prime},\mathsf{i}^{\prime})$ diremos que $(L,\mathsf{s},\mathsf{i})$
\textit{es un subreticulado de }$(L^{\prime},\mathsf{s}^{\prime}%
,\mathsf{i}^{\prime})$ si se dan las siguientes condiciones

\begin{enumerate}
\item[(1)] $L\subseteq L^{\prime}$

\item[(2)] $L$ es cerrado bajo las operaciones $\mathsf{s}^{\prime}$ e
$\mathsf{i}^{\prime}$

\item[(3)] $\mathsf{s}=\mathsf{s}^{\prime}$\textrm{$|$}$_{L\times L}$ y
$\mathsf{i}=\mathsf{i}^{\prime}$\textrm{$|$}$_{L\times L}$
\end{enumerate}

Sea $(L,\mathsf{s},\mathsf{i})$ un reticulado. Un conjunto $S\subseteq L$ es
llamado \textit{subuniverso }de $(L,\mathsf{s},\mathsf{i})$ si es no vacio y
cerrado bajo las operaciones $\mathsf{s}$ e $\mathsf{i}$. Es importante notar
que si bien los conceptos de subreticulado y subuniverso estan muy
relacionados, se trata de conceptos diferentes ya que los subreticulados de
$(L,\mathsf{s},\mathsf{i})$ son reticulados, es decir ternas y los
subuniversos de $(L,\mathsf{s},\mathsf{i})$ son conjuntos, por lo cual no son ternas.

Es facil de chequear que si $S$ es un subuniverso de $(L,\mathsf{s}%
,\mathsf{i})$, entonces $(S,\mathsf{s}\mathrm{\mid}_{S\times S},\mathsf{i}%
\mathrm{\mid}_{S\times S})$ es un subreticulado de $(L,\mathsf{s},\mathsf{i})$
y que todo subreticulado de $(L,\mathsf{s},\mathsf{i})$ se obtiene en esta
forma. Es decir, hay una biyeccion entre el conjunto de los subreticulados de
$(L,\mathsf{s},\mathsf{i})$ y el conjunto de los subuniversos de
$(L,\mathsf{s},\mathsf{i})$ (cual es?). Dicho de manera mas rapida: los
subuniversos de $(L,\mathsf{s},\mathsf{i})$ son ni mas ni menos que los
universos de los subreticulados de $(L,\mathsf{s},\mathsf{i})$.

\subsubsection{Homomorfismos de reticulados}

Sean $(L,\mathsf{s},\mathsf{i})$ y $(L^{\prime},\mathsf{s}^{\prime}%
,\mathsf{i}^{\prime})$ reticulados. Una funcion $F:L\rightarrow L^{\prime}$
sera llamada un \textit{homomorfismo de }$(L,\mathsf{s},\mathsf{i})$
\textit{en} $(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime})$ si para
todo $x,y\in L$ se cumple que%
\begin{align*}
F(x\mathsf{\;s\;}y)  & =F(x)\;\mathsf{s}^{\prime}\ F(y)\\
F(x\mathsf{\;i\;}y)  & =F(x)\;\mathsf{i}^{\prime}\ F(y).
\end{align*}
Un homomorfismo de\textit{\ }$(L,\mathsf{s},\mathsf{i})$ en $(L^{\prime
},\mathsf{s}^{\prime},\mathsf{i}^{\prime})$ sera llamado \textit{isomorfismo
de }$(L,\mathsf{s},\mathsf{i})$ \textit{en} $(L^{\prime},\mathsf{s}^{\prime}$,
$\mathsf{i}^{\prime}$ $)$ cuando sea biyectivo y su inversa sea tambien un
homomorfismo. Escribiremos $(L,\mathsf{s},\mathsf{i})\cong(L^{\prime
},\mathsf{s}^{\prime},\mathsf{i}^{\prime})$ cuando exista un isomorfismo de
$(L,\mathsf{s},\mathsf{i})$ en $(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}%
^{\prime})$. Escribiremos "Sea $F:(L,\mathsf{s},\mathsf{i})\rightarrow
(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime})$ un homomorfismo" para
expresar que $F$ es un homomorfismo de\textit{\ }$(L,\mathsf{s},\mathsf{i})$
en $(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime})$. No hay que
confundirse al leer esta notacion y pensar que $F$ es una funcion cuyo dominio
es $(L,\mathsf{s},\mathsf{i})$, lo cual por otra parte no tiene sentido ya que
el dominio de una funcion nunca puede ser una $3$-upla!

\begin{lemma}
\label{hom biyect implica iso}Si $F:(L,\mathsf{s},\mathsf{i})\rightarrow
(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime})$ es un homomorfismo
biyectivo, entonces $F$ es un isomorfismo
\end{lemma}

\begin{proof}
Solo falta ver que $F^{-1}$ es un homomorfismo. Sean $F(x),F(y)$ dos elementos
cualesquiera de $L^{\prime}$. Tenemos que%
\[
F^{-1}(F(x)\;\mathsf{s}^{\prime}\ F(y))=F^{-1}(F(x\mathsf{\;s\;}%
y))=x\mathsf{\;s\;}y=F^{-1}(F(x))\;\mathsf{s}\ F^{-1}(F(y))
\]

\end{proof}

\bigskip

\begin{lemma}
Sean $(L,\mathsf{s},\mathsf{i})$ y $(L^{\prime},\mathsf{s}^{\prime}%
,\mathsf{i}^{\prime})$ reticulados y sea $F:(L,\mathsf{s},\mathsf{i}%
)\rightarrow(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime})$ un
homomorfismo. Entonces $I_{F}$ es un subuniverso de $(L^{\prime}%
,\mathsf{s}^{\prime},\mathsf{i}^{\prime})$. Es decir que $F$ es tambien un
homomorfismo de $(L,\mathsf{s},\mathsf{i})$ en $(I_{F},\mathsf{s}^{\prime}%
$\textrm{$|$}$_{I_{F}\times I_{F}},\mathsf{i}^{\prime}$\textrm{$|$}%
$_{I_{F}\times I_{F}})$
\end{lemma}

\begin{proof}
Ya que $L$ es no vacio tenemos que $I_{F}$ tambien es no vacio. Sean $a,b\in
I_{F}$. Sean $x,y\in L$ tales que $F(x)=a$ y $F(y)=b$. Se tiene que%
\begin{align*}
a\;\mathsf{s}^{\prime}\ b  & =F(x)\;\mathsf{s}^{\prime}%
\ F(y)=F(x\mathsf{\;s\;}y)\in I_{F}\\
a\;\mathsf{i}^{\prime}\ b  & =F(x)\;\mathsf{i}^{\prime}%
\ F(y)=F(x\mathsf{\;i\;}y)\in I_{F}%
\end{align*}
por lo cual $I_{F}$ es cerrada bajo $\mathsf{s}^{\prime}$ e $\mathsf{i}%
^{\prime}$.
\end{proof}

\bigskip

\begin{lemma}
Sean $(L,\mathsf{s},\mathsf{i})$ y $(L^{\prime},\mathsf{s}^{\prime}%
,\mathsf{i}^{\prime})$ reticulados y sean $(L\leq)$ y $(L^{\prime}%
,\leq^{\prime})$ los posets asociados. Sea $F:L\rightarrow L^{\prime}$ una
funcion. Entonces $F$ es un isomorfismo de $(L,\mathsf{s},\mathsf{i})$ en
$(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime})$ si y solo si $F$ es un
isomorfismo de $(L,\leq)$ en $(L^{\prime},\leq^{\prime})$.
\end{lemma}

\begin{proof}
Supongamos $F$ es un isomorfismo de $(L,\mathsf{s},\mathsf{i})$ en
$(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime})$. Sean $x,y\in L$, tales
que $x\leq y$. Tenemos que $y=x\mathsf{\;s\;}y$ por lo cual
$F(y)=F(x\mathsf{\;s\;}y)=F(x)\mathsf{\;s^{\prime}\;}F(y)$, produciendo
$F(x)\leq^{\prime}F(y)$. En forma similar se puede ver que $F^{-1}$ es tambien
un homomorfismo de $(L^{\prime},\leq^{\prime})$ en $(L,\leq)$. Si $F$ es un
isomorfismo de $(L,\leq)$ en $(L^{\prime},\leq^{\prime})$, entonces (g) y (h)
del Lema \ref{isoposets} nos dicen que $F$ y $F^{-1}$ son homomorfismos (de
reticulados terna) por lo cual $F$ es un isomorfismo de $(L,\mathsf{s}%
,\mathsf{i})$ en $(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime})$.
\end{proof}

\bigskip

\begin{enumerate}
\item[Ejercicio:] Encontrar dos reticulados, $(L,\mathsf{s},\mathsf{i})$ y
$(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime})$, tales que haya una
funci\'{o}n biyectiva de $L$ en $L^{\prime}$ que preserve orden pero no sea
homomorfismo de reticulados.
\end{enumerate}

\bigskip

\subsubsection{Congruencias de reticulados}

Sea $(L,\mathsf{s},\mathsf{i})$ un reticulado. Una \textit{congruencia sobre}
$(L,\mathsf{s},\mathsf{i})$ sera una relacion de equivalencia $\theta$ sobre
$L$ la cual cumpla:

\begin{enumerate}
\item[(1)] $x\theta x^{\prime}$ y $y\theta y^{\prime}$ implica
$(x\mathsf{\;s\;}y)\theta(x^{\prime}\mathsf{\;s\;}y^{\prime})$ y
$(x\mathsf{\;i\;}y)\theta(x^{\prime}\mathsf{\;i\;}y^{\prime})$
\end{enumerate}

\noindent Gracias a esta condicion podemos definir sobre $L/\theta$ dos
operaciones binarias $\mathsf{\tilde{s}}$ e $\mathsf{\tilde{\imath}}$, de la
siguiente manera:%
\begin{align*}
x/\theta\mathsf{\;\tilde{s}\;}y/\theta & =(x\mathsf{\;s\;}y)/\theta\\
x/\theta\mathsf{\;\tilde{\imath}\;}y/\theta & =(x\mathsf{\;i\;}y)/\theta
\end{align*}


\bigskip Veamos algunos ejemplos:

\begin{enumerate}
\item[(E1)] Consideremos el reticulado $(\{1,2,3,4,5,6\},\max,\min)$. O sea
que aqui $L=\{1,2,3,4,5,6\}$, $\mathsf{s}$ es la operacion $\max$ sobre $L$ y
$\mathsf{i}$ es la operacion $\min$ sobre $L$. Sea $\theta$ la relacion de
equivalencia sobre $\{1,2,3,4,5,6\}$ dada por la particion
$\{\{1,2\},\{3\},\{4,5\}\}$. Se puede chequear que $\theta$ es una
congruencia, es decir satisface (1) de arriba. Notese que%
\begin{align*}
L/\theta & =\{\{1,2\},\{3\},\{4,5\}\}\\
\mathsf{\tilde{s}\;}  & =\widetilde{\max}:L/\theta\times L/\theta\rightarrow
L/\theta\\
\mathsf{\tilde{\imath}\;}  & =\widetilde{\min}:L/\theta\times L/\theta
\rightarrow L/\theta
\end{align*}
Por ejemplo tenemos que%
\[
\{1,2\}\ \widetilde{\max}\ \{3\}=\{3\}
\]
ya que $\{1,2\}\ \widetilde{\max}\ \{3\}=1/\theta\ \widetilde{\max}%
\ 3/\theta=(1\max3)/\theta=3/\theta=\{3\}$ (escribimos $1\max3$ en lugar de
$\max(1,3)$). Similarmente tenemos que%
\begin{align*}
\{4,5\}\ \widetilde{\max}\ \{3\}  & =\{4,5\}\\
\{1,2\}\ \widetilde{\min}\ \{4,5\}  & =\{1,2\}
\end{align*}


\item[(E2)] Consideremos el reticulado $(\{1,2,3,6\},mcm,mcd)$ (o sea el
rombo) y sea $\theta$ la relacion de equivalencia dada por la particion
$\{\{1,2\},\{3\},\{6\}\}$ (haga un dibujo). Entonces $\theta$ no es una
congruencia sobre $(\{1,2,3,6\},mcm,mcd)$. Esto es ya que si tomamos
\begin{align*}
x  & =1\\
x^{\prime}  & =2\\
y  & =3\\
y^{\prime}  & =3
\end{align*}
no se cumple la implicacion de (1) de la definicion de congruencia.
\end{enumerate}

La terna $(L/\theta,\mathsf{\tilde{s}},\mathsf{\tilde{\imath}})$ es llamada el
\textit{cociente de} $(L,\mathsf{s},\mathsf{i})$ \textit{sobre} $\theta$ y la
denotaremos con $(L,\mathsf{s},\mathsf{i})/\theta$.

\begin{lemma}
Sea $(L,\mathsf{s},\mathsf{i})$ un reticulado y sea $\theta$ una congruencia
de $(L,\mathsf{s},\mathsf{i})$. Entonces $(L/\theta,\mathsf{\tilde{s}%
},\mathsf{\tilde{\imath}})$ es un reticulado.
\end{lemma}

\begin{proof}
Veamos que la estructura $(L/\theta,\mathsf{\tilde{s}},\mathsf{\tilde{\imath}%
})$ cumple (I4). Sean $x/\theta$, $y/\theta$, $z/\theta$ elementos
cualesquiera de $L/\theta$. Tenemos que%
\[%
\begin{array}
[c]{ccl}%
(x/\theta\mathsf{\;\tilde{s}\;}y/\theta)\;\mathsf{\tilde{s}}\;z/\theta & = &
(x\mathsf{\;s\;}y)/\theta\;\mathsf{\tilde{s}}\;z/\theta\\
& = & ((x\mathsf{\;s\;}y)\;\mathsf{s}\;z)/\theta\\
& = & (x\mathsf{\;s\;}(y\;\mathsf{s}\;z))/\theta\\
& = & x/\theta\;\mathsf{\tilde{s}}\;(y\;\mathsf{s}\;z)/\theta\\
& = & x/\theta\mathsf{\;\tilde{s}\;}(y/\theta\;\mathsf{\tilde{s}}\;z/\theta)
\end{array}
\]
En forma similar se puede ver que la estructura $(L/\theta,\mathsf{\tilde{s}%
},\mathsf{\tilde{\imath}})$ cumple el resto de las identidades que definen reticulado.
\end{proof}

\bigskip

Denotaremos con $\tilde{\leq}$ al orden parcial asociado al reticulado
$(L/\theta,\mathsf{\tilde{s}},\mathsf{\tilde{\imath}})$.

\bigskip

\begin{lemma}
Sea $(L,\mathsf{s},\mathsf{i})$ un reticulado y sea $\theta$ una congruencia
de $(L,\mathsf{s},\mathsf{i})$. Entonces:%
\[
x/\theta\tilde{\leq}y/\theta\text{ sii }y\theta(x\mathsf{\;s\;}y)
\]
cualesquiera sean $x,y\in L$.
\end{lemma}

\begin{proof}
Por definicion de $\tilde{\leq}$ tenemos que $x/\theta\tilde{\leq}y/\theta$
sii $y/\theta=x/\theta\mathsf{\;\tilde{s}\;}y/\theta$. Pero $x/\theta
\mathsf{\;\tilde{s}\;}y/\theta=(x\mathsf{\;s\;}y)/\theta$ (por definicion de
$\mathsf{\tilde{s}}$) por lo cual tenemos que $x/\theta\tilde{\leq}y/\theta$
sii $y/\theta=(x\mathsf{\;s\;}y)/\theta$.
\end{proof}

\begin{corollary}
\label{1/tita es un maximo de L/tita}Sea $(L,\mathsf{s},\mathsf{i})$ un
reticulado en el cual hay un elemento maximo $1$ (resp. minimo $0$). Entonces
si $\theta$ es una congruencia sobre $(L,\mathsf{s},\mathsf{i})$, $1/\theta$
(resp. $0/\theta$) es un elemento maximo (resp. minimo) de $(L/\theta
,\mathsf{\tilde{s}},\mathsf{\tilde{\imath}})$.
\end{corollary}

\begin{proof}
Ya que $1=x\mathsf{\;s\;}1$, para cada $x\in L$, tenemos que $1/\theta
=(x\mathsf{\;s\;}1)/\theta$, para cada $x\in L$, lo cual por el lema anterior
nos dice que $x/\theta\tilde{\leq}1/\theta$, para cada $x\in L$.
\end{proof}

\bigskip

El siguiente lema nos da una forma natural de encontrar congruencias

\begin{lemma}
Si $F:(L,\mathsf{s},\mathsf{i})\rightarrow(L^{\prime},\mathsf{s}^{\prime
},\mathsf{i}^{\prime})$ es un homomorfismo, entonces $\ker F$ es una
congruencia sobre $(L,\mathsf{s},\mathsf{i})$.
\end{lemma}

\begin{proof}
Dejamos al lector ver que $\ker F$ es una relacion de equivalencia. Supongamos
$x\ker Fx^{\prime}$ y $y\ker Fy^{\prime}$. Entonces%
\[
F(x\mathsf{\;s\;}y)=F(x)\mathsf{\;s^{\prime}\;}F(y)=F(x^{\prime}%
)\mathsf{\;s^{\prime}\;}F(y^{\prime})=F(x^{\prime}\mathsf{\;s\;}y^{\prime})
\]
lo cual nos dice que $(x\mathsf{\;s\;}y)\ker F(x^{\prime}\mathsf{\;s\;}%
y^{\prime})$. En forma similar tenemos que $(x\mathsf{\;i\;}y)\ker
F(x^{\prime}\mathsf{\;i\;}y^{\prime})$.
\end{proof}

\bigskip

Ya vimos que el nucleo de un homomorfismo es una congruencia. El siguiente
lema muestra que toda congruencia es el nucleo de un homomorfismo.

\begin{lemma}
\label{pi sub tita es homomorfismo}Sea $(L,\mathsf{s},\mathsf{i})$ un
reticulado y sea $\theta$ una congruencia sobre $(L,\mathsf{s},\mathsf{i})$.
Entonces $\pi_{\theta}$ es un homomorfismo de $(L,\mathsf{s},\mathsf{i})$ en
$(L/\theta,\mathsf{\tilde{s}},\mathsf{\tilde{\imath}})$. Ademas $\ker
\pi_{\theta}=\theta$.
\end{lemma}

\begin{proof}
Sean $x,y\in L$. Tenemos que%
\[
\pi_{\theta}(x\mathsf{\;s\;}y)=(x\mathsf{\;s\;}y)/\theta=x/\theta
\mathsf{\;\tilde{s}\;}y/\theta=\pi_{\theta}(x)\mathsf{\;\tilde{s}\;}%
\pi_{\theta}(y)
\]
por lo cual $\pi_{\theta}$ preserva la operacion supremo. Para la operacion
infimo es similar.
\end{proof}

\bigskip

\subsection{Reticulados acotados}

\noindent Por un \textit{reticulado acotado} entenderemos una $5$-upla
$(L,\mathsf{s},\mathsf{i},0,1)$, tal que $(L,\mathsf{s},\mathsf{i})$ es un
reticulado, $0,1\in L$, y ademas se cumplen las siguientes identidades

\begin{enumerate}
\item[(I8)] $0\mathsf{\;s\;}x=x$, para cada $x\in L$

\item[(I9)] $x\mathsf{\;s\;}1=1$, para cada $x\in L$.
\end{enumerate}

\noindent Por ejemplo $(\{4,56,449\},\max,\min,4,449)$ es un reticulado
acotado pero es facil ver que $(\{4,56,449\},\max,\min,449,56)$ no lo es.

\bigskip

\textbf{Reflexion Informatica}

Por supuesto, en virtud de lo desarrollado en la subseccion anterior se tiene
que si $(L,\leq)$ es un poset el cual es un reticulado y en el cual hay un
maximo $1$ y un minimo $0$, entonces si tomamos:%
\[%
\begin{array}
[c]{rcl}%
\mathsf{s}:L^{2} & \rightarrow & L\\
(a,b) & \rightarrow & \sup(\{a,b\})
\end{array}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\begin{array}
[c]{rcl}%
\mathsf{i}:L^{2} & \rightarrow & L\\
(a,b) & \rightarrow & \inf(\{a,b\})
\end{array}
\]
tenemos que $(L,\mathsf{s},\mathsf{i},0,1)$ es un reticulado acotado. Ademas
en virtud del Teorema de Dedekind todo reticulado acotado se obtiene de esta
forma. O sea que a nivel de informacion, un poset que es un reticulado y tiene
$0$ y $1$ es exactamente lo mismo que un reticulado acotado

\bigskip

\subsubsection{Subreticulados acotados}

Dados reticulados acotados $(L,\mathsf{s},\mathsf{i},0,1)$ y $(L^{\prime
},\mathsf{s}^{\prime},\mathsf{i}^{\prime},0^{\prime},1^{\prime})$ diremos que
$(L,\mathsf{s},\mathsf{i},0,1)$ \textit{es un subreticulado acotado de
}$(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime},0^{\prime},1^{\prime})$
si se dan las siguientes condiciones

\begin{enumerate}
\item[(1)] $L\subseteq L^{\prime}$

\item[(2)] $L$ es cerrado bajo las operaciones $\mathsf{s}^{\prime}$ e
$\mathsf{i}^{\prime}$

\item[(3)] $0=0^{\prime}$ y $1=1^{\prime}$

\item[(4)] $\mathsf{s}=\mathsf{s}^{\prime}$\textrm{$|$}$_{L\times L}$ y
$\mathsf{i}=\mathsf{i}^{\prime}$\textrm{$|$}$_{L\times L}$
\end{enumerate}

Sea $(L,\mathsf{s},\mathsf{i},0,1)$ un reticulado acotado. Un conjunto
$S\subseteq L$ es llamado \textit{subuniverso }de $(L,\mathsf{s}%
,\mathsf{i},0,1)$ si $0,1\in S$ y ademas $S$ es cerrado bajo las operaciones
$\mathsf{s} $ e $\mathsf{i}$. Es importante notar que si bien los conceptos de
subreticulado acotado y subuniverso estan muy relacionados, se trata de
conceptos diferentes ya que los subreticulados acotados de $(L,\mathsf{s}%
,\mathsf{i},0,1)$ son reticulados acotados, es decir $5$-uplas y los
subuniversos de $(L,\mathsf{s},\mathsf{i},0,1)$ son conjuntos, por lo cual no
son $5$-uplas.

Es facil de chequear que si $S$ es un subuniverso de $(L,\mathsf{s}%
,\mathsf{i},0,1)$, entonces $(S,\mathsf{s}\mathrm{\mid}_{S\times S}%
,\mathsf{i}\mathrm{\mid}_{S\times S},0,1)$ es un subreticulado acotado de
$(L,\mathsf{s},\mathsf{i},0,1)$ y que todo subreticulado acotado de
$(L,\mathsf{s},\mathsf{i},0,1)$ se obtiene en esta forma. Es decir, hay una
biyeccion entre el conjunto de los subreticulados acotados de $(L,\mathsf{s}%
,\mathsf{i},0,1)$ y el conjunto de los subuniversos de $(L,\mathsf{s}%
,\mathsf{i},0,1)$ (cual es?). Dicho de manera mas rapida: los subuniversos de
$(L,\mathsf{s},\mathsf{i},0,1)$ son ni mas ni menos que los universos de los
subreticulados acotados de $(L,\mathsf{s},\mathsf{i},0,1)$.

\subsubsection{Homomorfismos de reticulados acotados}

Sean $(L,\mathsf{s},\mathsf{i},0,1)$ y $(L^{\prime},\mathsf{s}^{\prime
},\mathsf{i}^{\prime},0^{\prime},1^{\prime})$ reticulados acotados. Una
funcion $F:L\rightarrow L^{\prime}$ sera llamada un \textit{homomorfismo de
}$(L,\mathsf{s},\mathsf{i},0,1)$ \textit{en} $(L^{\prime},\mathsf{s}^{\prime
},\mathsf{i}^{\prime},0^{\prime},1^{\prime})$ si para todo $x,y\in L$ se
cumple que%
\begin{align*}
F(x\mathsf{\;s\;}y)  & =F(x)\;\mathsf{s}^{\prime}\ F(y)\\
F(x\mathsf{\;i\;}y)  & =F(x)\;\mathsf{i}^{\prime}\ F(y)\\
F(0)  & =0^{\prime}\\
F(1)  & =1^{\prime}%
\end{align*}
Un homomorfismo de\textit{\ }$(L,\mathsf{s},\mathsf{i},0,1)$ en $(L^{\prime
},\mathsf{s}^{\prime},\mathsf{i}^{\prime},0^{\prime},1^{\prime})$ sera llamado
\textit{isomorfismo }cuando sea biyectivo y su inversa sea tambien un
homomorfismo. Escribiremos $(L,\mathsf{s},\mathsf{i},0,1)\cong(L^{\prime
},\mathsf{s}^{\prime},\mathsf{i}^{\prime},0^{\prime},1^{\prime})$\ cuando
exista un isomorfismo de $(L,\mathsf{s},\mathsf{i},0,1)$ en $(L^{\prime
},\mathsf{s}^{\prime},\mathsf{i}^{\prime},0^{\prime},1^{\prime})$.
Escribiremos "Sea $F:(L,\mathsf{s},\mathsf{i},0,1)\rightarrow(L^{\prime
},\mathsf{s}^{\prime},\mathsf{i}^{\prime},0^{\prime},1^{\prime})$ un
homomorfismo" para expresar que $F$ es un homomorfismo de\textit{\ }%
$(L,\mathsf{s},\mathsf{i},0,1)$ en $(L^{\prime},\mathsf{s}^{\prime}%
,\mathsf{i}^{\prime},0^{\prime},1^{\prime})$. No hay que confundirse al leer
esta notacion y pensar que $F$ es una funcion cuyo dominio es $(L,\mathsf{s}%
,\mathsf{i},0,1)$, lo cual por otra parte no tiene sentido ya que el dominio
de una funcion nunca puede ser una $5$-upla!

\begin{lemma}
Si $F:(L,\mathsf{s},\mathsf{i},0,1)\rightarrow(L^{\prime},\mathsf{s}^{\prime
},\mathsf{i}^{\prime},0^{\prime},1^{\prime})$ un homomorfismo biyectivo,
entonces $F$ es un isomorfismo
\end{lemma}

\begin{proof}
Similar a la prueba del Lemma \ref{hom biyect implica iso}.
\end{proof}

\begin{lemma}
Si $F:(L,\mathsf{s},\mathsf{i},0,1)\rightarrow(L^{\prime},\mathsf{s}^{\prime
},\mathsf{i}^{\prime},0^{\prime},1^{\prime})$ es un homomorfismo, entonces
$I_{F}$ es un subuniverso de $(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}%
^{\prime},0^{\prime},1^{\prime})$. Es decir que $F$ es tambien un homomorfismo
de $(L,\mathsf{s},\mathsf{i},0,1)$ en $(I_{F},\mathsf{s}^{\prime}$\textrm{$|$%
}$_{I_{F}\times I_{F}},\mathsf{i}^{\prime}$\textrm{$|$}$_{I_{F}\times I_{F}%
},0^{\prime},1^{\prime})$
\end{lemma}

\begin{proof}
Ya que $F$ es un homomorfismo de\textit{\ }$(L,\mathsf{s},\mathsf{i})$ en
$(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime})$ tenemos que $I_{F}$ es
subuniverso de $(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime})$ lo cual
ya que $0^{\prime},1^{\prime}\in I_{F}$ implica que $I_{F}$ es un subuniverso
de $(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime},0^{\prime},1^{\prime
})$.
\end{proof}

\subsubsection{Congruencias de reticulados acotados}

Sea $(L,\mathsf{s},\mathsf{i},0,1)$ un reticulado acotado. Una
\textit{congruencia sobre} $(L,\mathsf{s},\mathsf{i},0,1)$ sera una relacion
de equivalencia $\theta$ la cual sea una congruencia sobre $(L,\mathsf{s}%
,\mathsf{i})$. Tenemos definidas sobre $L/\theta$ dos operaciones binarias
$\mathsf{\tilde{s}}$ e $\mathsf{\tilde{\imath}}$, de la siguiente manera:%
\begin{align*}
x/\theta\mathsf{\tilde{s}}y/\theta & =(x\mathsf{\;s\;}y)/\theta\\
x/\theta\mathsf{\tilde{\imath}}y/\theta & =(x\mathsf{\;i\;}y)/\theta
\end{align*}
La $5$-upla $(L/\theta,\mathsf{\tilde{s}},\mathsf{\tilde{\imath}}%
,0/\theta,1/\theta)$ es llamada el \textit{cociente de} $(L,\mathsf{s}%
,\mathsf{i},0,1)$ \textit{sobre} $\theta$ y la denotaremos con $(L,\mathsf{s}%
,\mathsf{i},0,1)/\theta$.

\begin{lemma}
\label{cociente de reticulados acotados}Sea $(L,\mathsf{s},\mathsf{i},0,1)$ un
reticulado acotado y $\theta$ una congruencia sobre $(L,\mathsf{s}%
,\mathsf{i},0,1)$.

\begin{enumerate}
\item[(a)] $(L/\theta,\mathsf{\tilde{s}},\mathsf{\tilde{\imath}}%
,0/\theta,1/\theta)$ es un reticulado acotado.

\item[(b)] $\pi_{\theta}$ es un homomorfismo de $(L,\mathsf{s},\mathsf{i}%
,0,1)$ en $(L/\theta,\mathsf{\tilde{s}},\mathsf{\tilde{\imath}},0/\theta
,1/\theta)$ cuyo nucleo es $\theta$.
\end{enumerate}
\end{lemma}

\begin{proof}
(a) Es facil ver que $(L/\theta,\mathsf{\tilde{s}},\mathsf{\tilde{\imath}%
},0/\theta,1/\theta)$ cumple (I1), (I2),...,(I9) dado que $(L,\mathsf{s}%
,\mathsf{i},0,1)$ las cumple.

(b) Sigue directamente del Lema \ref{pi sub tita es homomorfismo}
\end{proof}

\bigskip

\begin{lemma}
Si $F:(L,\mathsf{s},\mathsf{i},0,1)\rightarrow(L^{\prime},\mathsf{s}^{\prime
},\mathsf{i}^{\prime},0^{\prime},1^{\prime})$ es un homomorfismo de
reticulados acotados, entonces $\ker F$ es una congruencia sobre
$(L,\mathsf{s},\mathsf{i},0,1)$.
\end{lemma}

\begin{proof}
Ya que $F$ es un homomorfismo de $(L,\mathsf{s},\mathsf{i})\ $en $(L^{\prime
},\mathsf{s}^{\prime},\mathsf{i}^{\prime})$ tenemos que por un lema anterior
$\ker F$ es una congruencia sobre $(L,\mathsf{s},\mathsf{i})$ lo cual por
definicion nos dice que $\ker F$ es una congruencia sobre $(L,\mathsf{s}%
,\mathsf{i},0,1)$.
\end{proof}

\subsection{Reticulados complementados}

\noindent Sea $(L,\mathsf{s},\mathsf{i},0,1)$ un reticulado acotado. Dado
$a\in L$, diremos que $a$ es \textit{complementado} cuando exista un elemento
$b\in L$ (llamado \textit{complemento de a}) tal que:%
\begin{align*}
a\;\mathsf{s\;}b  & =1\\
a\;\mathsf{i\;}b  & =0
\end{align*}
Notese que dicho elemento $b$ puede no ser unico, es decir $a$ puede tener
varios complementos. Recordemos que una operacion unaria sobre un conjunto $L
$ es por definicion una funcion de $L$ en $L$. Muchas veces si $s$ denota una
operacion unaria, entonces escribiremos $x^{s}$ en lugar de $s(x)$. Por un
\textit{reticulado complementado} entederemos una $6$-upla $(L,\mathsf{s}%
,\mathsf{i},^{c},0,1)$ tal que $(L,\mathsf{s},\mathsf{i},0,1)$ es un
reticulado acotado y $^{c}$ es una operacion unaria sobre $L$ tal que

\begin{enumerate}
\item[(I10)] $x\mathsf{\;s\;}x^{c}=1$, para cada $x\in L$

\item[(I11)] $x\mathsf{\;i\;}x^{c}=0$, para cada $x\in L$
\end{enumerate}

\noindent Dado un reticulado acotado $(L,\mathsf{s},\mathsf{i},0,1)$ puede
haber mas de una operacion unaria $g$ tal que $(L,\mathsf{s},\mathsf{i}%
,g,0,1)$ resulte un reticulado complementado. Intente dar un ejemplo en el
cual $L$ tenga 5 elementos.

\bigskip

\textbf{Reflexion Informatica}

Notese que si tenemos un poset $(L,\leq)$ el cual es un reticulado en el cual
hay un maximo $1$ y un minimo $0$ y ademas tenemos una funcion $g:L\rightarrow
L$ tal que%

\begin{align*}
\sup\{x,g(x)\}  & =1\\
\inf\{x,g(x)\}  & =0
\end{align*}
para cada $x\in L$, entonces podemos definir%
\[%
\begin{array}
[c]{rcl}%
\mathsf{s}:L^{2} & \rightarrow & L\\
(a,b) & \rightarrow & \sup(\{a,b\})
\end{array}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\begin{array}
[c]{rcl}%
\mathsf{i}:L^{2} & \rightarrow & L\\
(a,b) & \rightarrow & \inf(\{a,b\})
\end{array}
\]
y se obtiene que $(L,\mathsf{s},\mathsf{i},g,0,1)$ es un reticulado
complementado. Ademas en virtud del Teorema de Dedekind todo reticulado
complementado se obtiene de esta forma. O sea que a nivel de informacion,
tener un poset que es un reticulado con $0$ y $1$ junto con una operacion
unaria que da complementos, es exactamente lo mismo que tener un reticulado acotado

\bigskip

\subsubsection{Subreticulados complementados}

Dados reticulados complementados $(L,\mathsf{s},\mathsf{i},^{c},0,1)$ y
$(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime},^{c^{\prime}},0^{\prime
},1^{\prime})$ diremos que $(L,\mathsf{s},\mathsf{i},^{c},0,1)$ \textit{es un
subreticulado complementado de }$(L^{\prime},\mathsf{s}^{\prime}%
,\mathsf{i}^{\prime},^{c^{\prime}},0^{\prime},1^{\prime})$ si se dan las
siguientes condiciones

\begin{enumerate}
\item[(1)] $L\subseteq L^{\prime}$

\item[(2)] $L$ es cerrado bajo las operaciones $\mathsf{s}^{\prime}$,
$\mathsf{i}^{\prime}$ y $^{c^{\prime}}$

\item[(3)] $0=0^{\prime}$ y $1=1^{\prime}$

\item[(4)] $\mathsf{s}=\mathsf{s}^{\prime}$\textrm{$|$}$_{L\times L}$,
$\mathsf{i}=\mathsf{i}^{\prime}$\textrm{$|$}$_{L\times L}$ y $^{c}%
=\ ^{c^{\prime}}$\textrm{$|$}$_{L}$
\end{enumerate}

Sea $(L,\mathsf{s},\mathsf{i},^{c},0,1)$ un reticulado complementado. Un
conjunto $S\subseteq L$ es llamado \textit{subuniverso }de $(L,\mathsf{s}%
,\mathsf{i},^{c},0,1)$ si $0,1\in S$ y ademas $S$ es cerrado bajo las
operaciones $\mathsf{s}$, $\mathsf{i}$ y $^{c}$. Es importante notar que si
bien los conceptos de subreticulado complementado y subuniverso estan muy
relacionados, se trata de conceptos diferentes ya que los subreticulados
complementados de $(L,\mathsf{s},\mathsf{i},^{c},0,1)$ son reticulados
complementados, es decir $6$-uplas y los subuniversos de $(L,\mathsf{s}%
,\mathsf{i},^{c},0,1)$ son conjuntos, por lo cual no son $6$-uplas.

Es facil de chequear que si $S$ es un subuniverso de $(L,\mathsf{s}%
,\mathsf{i},^{c},0,1)$, entonces $(S,\mathsf{s}\mathrm{\mid}_{S\times
S},\mathsf{i}\mathrm{\mid}_{S\times S},,^{c}\mathrm{\mid}_{S},0,1)$ es un
subreticulado complementado de $(L,\mathsf{s},\mathsf{i},^{c},0,1)$ y que todo
subreticulado complementado de $(L,\mathsf{s},\mathsf{i},^{c},0,1)$ se obtiene
en esta forma. Es decir, hay una biyeccion entre el conjunto de los
subreticulados complementados de $(L,\mathsf{s},\mathsf{i},^{c},0,1)$ y el
conjunto de los subuniversos de $(L,\mathsf{s},\mathsf{i},^{c},0,1)$ (cual
es?). Dicho de manera mas rapida: los subuniversos de $(L,\mathsf{s}%
,\mathsf{i},^{c},0,1)$ son ni mas ni menos que los universos de los
subreticulados complementados de $(L,\mathsf{s},\mathsf{i},^{c},0,1)$.

\subsubsection{Homomorfismos de reticulados complementados}

Sean $(L,\mathsf{s},\mathsf{i},^{c},0,1)$ y $(L^{\prime},\mathsf{s}^{\prime
},\mathsf{i}^{\prime},^{c^{\prime}},0^{\prime},1^{\prime})$ reticulados
complementados. Una funcion $F:L\rightarrow L^{\prime}$ sera llamada un
\textit{homomorfismo de }$(L,\mathsf{s},\mathsf{i},^{c},0,1)$ \textit{en}
$(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime},^{c^{\prime}},0^{\prime
},1^{\prime})$ si para todo $x,y\in L$ se cumple que%
\begin{align*}
F(x\mathsf{\;s\;}y)  & =F(x)\;\mathsf{s}^{\prime}\ F(y)\\
F(x\mathsf{\;i\;}y)  & =F(x)\;\mathsf{i}^{\prime}\ F(y)\\
F(x^{c})  & =F(x)^{c^{\prime}}\\
F(0)  & =0^{\prime}\\
F(1)  & =1^{\prime}%
\end{align*}
Un homomorfismo de\textit{\ }$(L,\mathsf{s},\mathsf{i},^{c},0,1)$ en
$(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime},^{c^{\prime}},0^{\prime
},1^{\prime})$ sera llamado \textit{isomorfismo }cuando sea biyectivo y su
inversa sea un homomorfismo. Como es usual usaremos el simbolo $\cong$ para
denotar la relacion de isomorfismo. Escribiremos "Sea $F:(L,\mathsf{s}%
,\mathsf{i},^{c},0,1)\rightarrow(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}%
^{\prime},^{c^{\prime}},0^{\prime},1^{\prime})$ un homomorfismo" para expresar
que $F$ es un homomorfismo de\textit{\ }$(L,\mathsf{s},\mathsf{i},^{c},0,1)$
en $(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime},^{c^{\prime}%
},0^{\prime},1^{\prime})$. No hay que confundirse al leer esta notacion y
pensar que $F$ es una funcion cuyo dominio es $(L,\mathsf{s},\mathsf{i}%
,^{c},0,1)$, lo cual por otra parte no tiene sentido ya que el dominio de una
funcion nunca puede ser una $6$-upla!

\begin{lemma}
Si $F:(L,\mathsf{s},\mathsf{i},^{c},0,1)\rightarrow(L^{\prime},\mathsf{s}%
^{\prime},\mathsf{i}^{\prime},^{c^{\prime}},0^{\prime},1^{\prime})$ un
homomorfismo biyectivo, entonces $F$ es un isomorfismo
\end{lemma}

\begin{proof}
Es dejada al lector.
\end{proof}

\begin{lemma}
Si $F:(L,\mathsf{s},\mathsf{i},^{c},0,1)\rightarrow(L^{\prime},\mathsf{s}%
^{\prime}$,$\mathsf{i}^{\prime},^{c^{\prime}},0^{\prime},1^{\prime})$\ es un
homomorfismo, entonces $I_{F}$ es un subuniverso de $(L^{\prime}%
,\mathsf{s}^{\prime}$,$\mathsf{i}^{\prime},^{c^{\prime}},0^{\prime},1^{\prime
})$. Es decir que $F$ es tambien un homomorfismo de $(L,\mathsf{s}%
,\mathsf{i},^{c},0,1)$ en $(I_{F},\mathsf{s}^{\prime}$\textrm{$|$}%
$_{I_{F}\times I_{F}},\mathsf{i}^{\prime}$\textrm{$|$}$_{I_{F}\times I_{F}%
},^{c^{\prime}},0^{\prime},1^{\prime})$
\end{lemma}

\begin{proof}
Es dejada al lector.
\end{proof}

\subsubsection{Congruencias de reticulados complementados}

Sea $(L,\mathsf{s},\mathsf{i},^{c},0,1)$ un reticulado complementado. Una
\textit{congruencia sobre} $(L,\mathsf{s},\mathsf{i},^{c},0,1)$ sera una
relacion de equivalencia sobre $L$ la cual cumpla:

\begin{enumerate}
\item[(1)] $\theta$ es una congruencia sobre $(L,\mathsf{s},\mathsf{i},0,1)$

\item[(2)] $x/\theta=y/\theta$ implica $x^{c}/\theta=y^{c}/\theta$
\end{enumerate}

\noindent Las condiciones anteriores nos permiten definir sobre $L/\theta$ dos
operaciones binarias $\mathsf{\tilde{s}}$ e $\mathsf{\tilde{\imath}}$, y una
operacion unaria $^{\tilde{c}}$ de la siguiente manera:%
\begin{align*}
x/\theta\mathsf{\;\tilde{s}\;}y/\theta & =(x\mathsf{\;s\;}y)/\theta\\
x/\theta\mathsf{\;\tilde{\imath}\;}y/\theta & =(x\mathsf{\;i\;}y)/\theta\\
(x/\theta)^{\tilde{c}}  & =x^{c}/\theta
\end{align*}
La $6$-upla $(L/\theta,\mathsf{\tilde{s}},\mathsf{\tilde{\imath}},^{\tilde{c}%
},0/\theta,1/\theta)$ es llamada el \textit{cociente de} $(L,\mathsf{s}%
,\mathsf{i},^{c},0,1)$ \textit{sobre} $\theta$ y la denotaremos con
$(L,\mathsf{s},\mathsf{i},^{c},0,1)/\theta$. Tal como era de esperar tenemos entonces

\begin{lemma}
Sea $(L,\mathsf{s},\mathsf{i},^{c},0,1)$ un reticulado complementado y sea
$\theta$ una congruencia sobre $(L,\mathsf{s},\mathsf{i},^{c},0,1)$.

\begin{enumerate}
\item[(a)] $(L/\theta,\mathsf{\tilde{s}},\mathsf{\tilde{\imath}},^{\tilde{c}%
},0/\theta,1/\theta)$ es un reticulado complementado.

\item[(b)] $\pi_{\theta}$ es un homomorfismo de $(L,\mathsf{s},\mathsf{i}%
,^{c},0,1)$ en $(L/\theta,\mathsf{\tilde{s}},\mathsf{\tilde{\imath}}%
,^{\tilde{c}},0/\theta,1/\theta)$ cuyo nucleo es $\theta$.
\end{enumerate}
\end{lemma}

\begin{proof}
(a) Por un lema anterior ya sabemos que $(L/\theta,\mathsf{\tilde{s}%
},\mathsf{\tilde{\imath}},0/\theta,1/\theta)$ es un reticulado acotado. Es
decir que solo nos falta ver que $(L/\theta,\mathsf{\tilde{s}},\mathsf{\tilde
{\imath}},^{\tilde{c}},0/\theta,1/\theta)$ sarisface las identidades (I10) y
(I11). Veamos por ejemplo que satisface la (I10). Sea $x/\theta$ un elemento
cualquiera de $L/\theta$. Ya que $(L,\mathsf{s},\mathsf{i},^{c},0,1)$
satisface la (I10), tenemos que $x\mathsf{\;s\;}x^{c}=1$. O sea que
$(x\mathsf{\;s\;}x^{c})/\theta=1/\theta$ y por lo tanto $x/\theta
\mathsf{\;\tilde{s}\;}x^{c}/\theta=1/\theta$. Pero por definicion de
$^{\tilde{c}}$ tenemos que $(x/\theta)^{\tilde{c}}=x^{c}/\theta$, lo cual nos
dice que $x/\theta\mathsf{\;\tilde{s}\;}(x/\theta)^{\tilde{c}}=1/\theta$.
Dejamos al lector ver que $(L/\theta,\mathsf{\tilde{s}},\mathsf{\tilde{\imath
}},^{\tilde{c}},0/\theta,1/\theta)$ sarisface la identidad (I11)

(b) Por el Lema \ref{cociente de reticulados acotados} tenemos que
$\pi_{\theta}$ es un homomorfismo de $(L,\mathsf{s},\mathsf{i},0,1)$ en
$(L/\theta,\mathsf{\tilde{s}},\mathsf{\tilde{\imath}},0/\theta,1/\theta)$ cuyo
nucleo es $\theta$. Notese que por definicion de $^{\tilde{c}}$ tenemos que
$x^{c}/\theta=(x/\theta)^{\tilde{c}}$, es decir $\pi_{\theta}(x^{c}%
)=(\pi_{\theta}(x))^{\tilde{c}}$, cualquiera sea $x\in L$
\end{proof}

\bigskip

\begin{lemma}
Si $F:(L,\mathsf{s},\mathsf{i},^{c},0,1)\rightarrow(L^{\prime},\mathsf{s}%
^{\prime},\mathsf{i}^{\prime},^{c^{\prime}},0^{\prime},1^{\prime})$ es un
homomorfismo de reticulados complementados, entonces $\ker F$ es una
congruencia sobre $(L,\mathsf{s},\mathsf{i},^{c},0,1)$

\begin{proof}
Ya que $F$ es un homomorfismo de $(L,\mathsf{s},\mathsf{i},0,1)$ en
$(L^{\prime},\mathsf{s}^{\prime},\mathsf{i}^{\prime},0^{\prime},1^{\prime})$
tenemos que por un lema anterior $\ker F$ es una congruencia sobre
$(L,\mathsf{s},\mathsf{i},0,1)$. Es decir que solo falta probar que para todos
$x,y\in L$, se tiene que $x/\ker F=y/\ker F$ implica $x^{c}/\ker F=y^{c}/\ker
F$, lo cual es dejado al lector
\end{proof}
\end{lemma}

\bigskip

\subsection{Algebras de Boole}

Un reticulado $(L,\mathsf{s},\mathsf{i})$ se llamara \textit{distributivo
}cuando cumpla la siguiente identidad

\begin{enumerate}
\item[Dis$_{1}$] $x\mathsf{\;i\;}(y\;\mathsf{s}\;z)=(x\mathsf{\;i\;}%
y)\;\mathsf{s}\;(x\mathsf{\;i\;}z)$, cualesquiera sean $x,y,z\in L$
\end{enumerate}

Diremos que un reticulado acotado $(L,\mathsf{s},\mathsf{i},0,1)$ (resp.
complementado $(L,\mathsf{s},\mathsf{i},^{c},0,1)$) es \textit{distributivo}
cuando $(L,\mathsf{s},\mathsf{i})$ lo sea. Consideremos la distributividad
dual a Dis$_{1}$, es decir

\begin{enumerate}
\item[Dis$_{2}$] $x\;\mathsf{s}\;(y\mathsf{\;i\;}z)=(x\mathsf{\;s\;}%
y)\mathsf{\;i\;}(x\;\mathsf{s}\;z)$, cualesquiera sean $x,y,z\in L$
\end{enumerate}

\begin{lemma}
Sea $(L,\mathsf{s},\mathsf{i})$ un reticulado. Entonces $(L,\mathsf{s}%
,\mathsf{i})$ satisface Dis$_{1}$ sii $(L,\mathsf{s},\mathsf{i})$ satisface
Dis$_{2}$
\end{lemma}

\begin{proof}
Supongamos $(L,\mathsf{s},\mathsf{i})$ satisface Dis$_{1}$. Sean $a,b,c\in L$
elementos fijos. Por Dis$_{1}$ tenemos que%
\[
(a\mathsf{\;s\;}b)\mathsf{\;i\;}(a\;\mathsf{s}\;c)=((a\mathsf{\;s\;}%
b)\mathsf{\;i\;}a)\;\mathsf{s}\;((a\mathsf{\;s\;}b)\mathsf{\;i\;}c)
\]
Pero por conmutatividad tenemos que%
\[
((a\mathsf{\;s\;}b)\mathsf{\;i\;}a)\;\mathsf{s}\;((a\mathsf{\;s\;}%
b)\mathsf{\;i\;}c)=(a\mathsf{\;i\;}(a\mathsf{\;s\;}b))\;\mathsf{s}%
\;(c\mathsf{\;i\;}(a\mathsf{\;s\;}b))
\]
Por (I7) tenemos que $a\mathsf{\;i\;}(a\mathsf{\;s\;}b)=a$ y por $Dis_{1}$
tenemos que $c\mathsf{\;i\;}(a\mathsf{\;s\;}b)=(c\mathsf{\;i\;}%
a)\mathsf{\;s\;}(c\mathsf{\;i\;}b)$ por lo cual%
\[
(a\mathsf{\;i\;}(a\mathsf{\;s\;}b))\;\mathsf{s}\;(c\mathsf{\;i\;}%
(a\mathsf{\;s\;}b))=a\;\mathsf{s}\;((c\mathsf{\;i\;}a)\mathsf{\;s\;}%
(c\mathsf{\;i\;}b))
\]
Por asociatividad tenemos que%
\[
a\;\mathsf{s}\;((c\mathsf{\;i\;}a)\mathsf{\;s\;}(c\mathsf{\;i\;}%
b))=(a\;\mathsf{s}\;(c\mathsf{\;i\;}a))\mathsf{\;s\;}(c\mathsf{\;i\;}b)
\]
Pero por conmutatividad tenemos que%
\[
(a\;\mathsf{s}\;(c\mathsf{\;i\;}a))\mathsf{\;s\;}(c\mathsf{\;i\;}%
b)=(a\;\mathsf{s}\;(a\mathsf{\;i\;}c))\mathsf{\;s\;}(b\mathsf{\;i\;}c)
\]
Lo cual por (I6) nos dice que%
\[
(a\;\mathsf{s}\;(a\mathsf{\;i\;}c))\mathsf{\;s\;}(b\mathsf{\;i\;}%
c)=a\mathsf{\;s\;}(b\mathsf{\;i\;}c)
\]
Por transitividad de la igualdad, las igualdades anteriores nos dicen que%
\[
a\mathsf{\;s\;}(b\mathsf{\;i\;}c)=(a\mathsf{\;s\;}b)\mathsf{\;i\;}%
(a\;\mathsf{s}\;c)
\]
Pero $a,b,c$ eran elementos arbitrarios por lo que hemos probado que vale
$Dis_{2}$.
\end{proof}

\bigskip

\begin{enumerate}
\item[Ejercicio:] Use la prueba del lema anterior para hacer un algoritmo el
cual tome de entrada un reticulado acotado $(L,\mathsf{s},\mathsf{i},0,1)$ y
elementos $x,y,z\in L$ tales que $y\neq z$ son complementos de $x$, y de como
salida elementos $a,b,c$ tales que $a\mathsf{\;i\;}(b\;\mathsf{s}%
\;c)\neq(a\mathsf{\;i\;}b)\;\mathsf{s}\;(a\mathsf{\;i\;}c)$
\end{enumerate}

\bigskip

Por un \textit{Algebra de Boole }entenderemos un reticulado complementado que
es distributivo. Algunos ejemplos:

\begin{enumerate}
\item[E1] Dado un conjunto no vacio $X$, la $6$-upla $(X,\cup,\cap
,^{c},\emptyset,X)$ es un algebra de Boole
\end{enumerate}

\bigskip

\bigskip

Para probar algunas propiedades fundamentales de un algebra de Boole
necesitaremos el siguiente

\begin{lemma}
\label{complementos unicos}Si $(L,\mathsf{s},\mathsf{i},0,1)$ un reticulado
acotado y distributivo, entonces todo elemento tiene a lo sumo un complemento.
Es decir, si $x\;\mathsf{s\;}u=x\;\mathsf{s\;}v=1$ y $x\;\mathsf{i\;}%
u=x\;\mathsf{i\;}v=0$, entonces $u=v$, cualesquiera sean $x,u,v\in L$.
\end{lemma}

\begin{proof}
Sean $a,b,c\in L$ elementos fijos. Supongamos que%
\begin{align*}
a\;\mathsf{s\;}b  & =a\;\mathsf{s\;}c=1\\
a\;\mathsf{i\;}b  & =a\;\mathsf{i\;}c=0
\end{align*}
(es decir $b$ y $c$ son ambos complementos de $a$). Veremos que entonces $b=c
$. Notese que%
\[
b=b\;\mathsf{i\;}1=b\;\mathsf{i\;}(a\;\mathsf{s\;}c)=(b\;\mathsf{i\;}%
a)\;\mathsf{s\;}(b\;\mathsf{i\;}c)=0\;\mathsf{s\;}(b\;\mathsf{i\;}%
c)=b\;\mathsf{i\;}c
\]
por lo cual $b\leq c$. Analogamente se puede probar que $c\leq b$ por lo cual
$b=c$. Ya que $a,b,c$ eran elementos cualesquiera de $L$, hemos probado el lema.
\end{proof}

\bigskip

Una propiedad muy importante que se da en las algebras de Boole es

\begin{lemma}
Sea $(B,\mathsf{s},\mathsf{i},^{\mathbf{c}},0,1)$ un \'{a}lgebra de Boole.
Cualesquiera sean $x,y\in B$, se tiene que $y=(y\;\mathsf{i\;}x)\;\mathsf{s\;}%
(y\mathsf{\;i\mathsf{\;}}x^{c})$.
\end{lemma}

\begin{proof}
Sean $a,b\in B$, fijos. Se tiene que%
\[
b=b\;\mathsf{i\;}1=b\mathsf{\;i\mathsf{\;}}(a\mathsf{\;s\mathsf{\;}}%
a^{c})=(b\;\mathsf{i\;}a)\;\mathsf{s\;}(b\mathsf{\;i\mathsf{\;}}a^{c})
\]
Ya que $a$ y $b$ eran elementos cualesquiera de $B$, hemos probado el lema.
\end{proof}

\bigskip

\begin{theorem}
Sea $(L,\mathsf{s},\mathsf{i},^{\mathbf{c}},0,1)$ un \'{a}lgebra de Boole y
sean $a,b\in B$. Se tiene que:

\begin{enumerate}
\item[(1)] $(a\,\mathsf{i\,}b)^{c}=a^{c}\,\mathsf{s\,}b^{c}$

\item[(2)] $(a\,\mathsf{s\,}b)^{c}=a^{c}\,\mathsf{i\,}b^{c}$

\item[(3)] $a^{cc}=a$

\item[(4)] $a\,\mathsf{i\,}b=0$ si y solo si $b\leq a^{c}$

\item[(5)] $a\leq b$ si y solo si $b^{c}\leq a^{c}$
\end{enumerate}
\end{theorem}

\begin{proof}
(1) Es facil ver que $a^{c}\,\mathsf{s\,}b^{c}$ es un complemento de
$a\,\mathsf{i\,}b$ (hacer!). Pero ya que $(L,\mathsf{s},\mathsf{i}%
,^{\mathbf{c}},0,1)$ es un reticulado complementado, tenemos que
$(a\,\mathsf{i\,}b)^{c}$ es un complemento de $a\,\mathsf{i\,}b$. El Lema
\ref{complementos unicos} nos dice que $(a\,\mathsf{i\,}b)^{c}$ y
$a^{c}\,\mathsf{s\,}b^{c}$ deben ser iguales.

(2) y (3) se prueban en forma similar (hacer!)

(4) Supongamos $a\,\mathsf{i\,}b=0$. Se tiene%
\begin{align*}
b  & =(b\;\mathsf{i\;}a)\;\mathsf{s\;}(b\mathsf{\;i\mathsf{\;}}a^{c}%
)\mathsf{\,}\\
& =(a\;\mathsf{i\;}b)\;\mathsf{s\;}(b\mathsf{\;i\mathsf{\;}}a^{c})\\
& =0\;\mathsf{s\;}(b\mathsf{\;i\mathsf{\;}}a^{c})\\
& =(b\mathsf{\;i\mathsf{\;}}a^{c})
\end{align*}
lo cual dice que $b\leq a^{c}$. Supongamos $b\leq a^{c}$. Entonces
$a\,\mathsf{i\,}b\leq a\,\mathsf{i\,}a^{c}=0$ por lo cual $a\,\mathsf{i\,}b=0$.

(5) Supongamos $a\leq b$. Entonces $a\,\mathsf{i\,}b=a$, lo cual por (1) nos
dice que $a^{c}\,\mathsf{s\,}b^{c}=a^{c}$ obteniendo que $b^{c}\leq a^{c}$. La
resiproca es dejada al lector (hint: use (3))
\end{proof}

\bigskip

\subsection{Teoremas del filtro primo y de Rasiowa Sikorski}

Un \textit{filtro }de un reticulado $(L,\mathsf{s},\mathsf{i})$ sera un
subconjunto $F\subseteq L$ tal que:

\begin{enumerate}
\item[(1)] $F\neq\emptyset$

\item[(2)] $x,y\in F\Rightarrow x\;\mathsf{i\;}y\in F$

\item[(3)] $x\in F$ y $x\leq y\Rightarrow y\in F$
\end{enumerate}

\noindent El nombre "filtro" es inspirado por la propiedad (3) ya que si un
filtro o colador atrapa a cierto objeto $x$, entonces claramente atrapara a
todos los objetos mas grandes que $x$.

Dado un conjunto $S\subseteq L$, denotemos con $[S)$ el siguiente conjunto%
\[
\{y\in L:y\geq s_{1}\;\mathsf{i\;}...\;\mathsf{i\;}s_{n}\text{, para algunos
}s_{1},...,s_{n}\in S\text{, }n\geq1\}
\]


\begin{lemma}
Supongamos $S$ es no vacio. Entonces $[S)$ es un filtro. Mas aun si $F$ es un
filtro y $F\supseteq S$, entonces $F\supseteq\lbrack S)$. Es decir, $[S)$ es
el menor filtro que contiene a $S$.
\end{lemma}

\begin{proof}
Ya que $S\subseteq\lbrack S)$, tenemos que $[S)\neq\emptyset$. Claramente
$[S)$ cumple la propiedad (3). Veamos cumple la (2). Si $y\geq s_{1}%
\;\mathsf{i\;}s_{2}\;\mathsf{i\;}...\;\mathsf{i\;}s_{n}$ y $z\geq
t_{1}\;\mathsf{i\;}t_{2}\;\mathsf{i\;}$...$\;\mathsf{i\;}t_{m}$, con
$s_{1},s_{2},...,s_{n}$, $t_{1},t_{2},...,t_{m}\in S$, entonces%
\[
y\;\mathsf{i\;}z\geq s_{1}\;\mathsf{i\;}s_{2}\;\mathsf{i\;}...\;\mathsf{i\;}%
s_{n}\;\mathsf{i\;}t_{1}\;\mathsf{i\;}t_{2}\;\mathsf{i\;}...\;\mathsf{i\;}%
t_{m}%
\]
lo cual prueba (2).
\end{proof}

\bigskip

Llamaremos a $[S)$ el \textit{filtro generado} \textit{por} $S$. Cuando $S$ es
finito, ya que existe $\inf S$, es claro que $[S)=\{y\in L:y\geq\inf S\}$.
Cuando $S$ es infinito y existe $\inf S$, en muchos casos se dara que
$[S)=\{y\in L:y\geq\inf S\}$ o que $[S)=\{y\in L:y>\inf S\}$, pero no
necesariamente esto sucedera siempre. Por ejemplo:

\begin{enumerate}
\item[-] Sea $\mathbf{L}=(\mathcal{P}(\mathbf{N}),\cup,\cap)$ y sea
$S=\{\mathbf{N}-\{n\}:n\in\mathbf{N}\}$. Es facil ver que $\inf S=\emptyset$ y
que $[S)=\{A\in\mathcal{P}(\mathbf{N}):\mathbf{N}-A$ es finito$\}$ por lo cual
no se da que $[S)=\{y\in L:y\geq\inf S\}$ o que $[S)=\{y\in L:y>\inf S\}$
\end{enumerate}

\bigskip

En general, si $(L,\mathsf{s},\mathsf{i},0,1)$ es un reticulado acotado,
diremos que $F$ es un \textit{filtro }de $(L,\mathsf{s},\mathsf{i},0,1)$
cuando $F$ sea un filtro de $(L,\mathsf{s},\mathsf{i})$. Lo mismo sucedera con
el concepto de filtro de un reticulado complementado $(L,\mathsf{s}%
,\mathsf{i},^{c},0,1)$

Sea $(P,\leq)$ un poset. Un subconjunto $C\subseteq P$ sera llamado una
\textit{cadena} si para cada $x,y\in C$, se tiene que $x\leq y$ o $y\leq x$.
Por ejemplo

\begin{enumerate}
\item[(E1)] $\{1,10,40,600\}$ y $\{2^{n}:n\in\mathbf{N}\}$ son cadenas del
poset $(\mathbf{N},|)$

\item[(E2)] $\{-3,5,2\}$ y el intervalo $[2,3]$ son cadenas del poset
$(\mathbf{R},\leq)$. De hecho todo subconjunto de $\mathbf{R}$ es una cadena
de $(\mathbf{R},\leq)$

\item[(E3)] $C=\{[0,n]:n\in\mathbf{N}\}$ es una cadena del poset
$(\mathcal{P}(\mathbf{R}),\subseteq)$. Notese que cada elemento de $C$ es un
conjunto (i.e. un intervalo).
\end{enumerate}

Es importante notar que las cadenas pueden ser infinitas y que dada una cadena
infinita $C$ puede no existir una infinitupla $(c_{1},c_{2},...)$ tal que
$C=\{c_{n}:n\in\mathbf{N}\}$. Este es el caso de la cadena $[0,1]$ del poset
$(\mathbf{R},\leq)$, ya que el bien conocido argumento diagonal de Cantor nos
dice que no existe una manera de enumerar los elementos del intervalo $[0,1]$.
Esto nos obliga a pensar con cierta madurez a las cadenas y no caer en la
falacia de pensar que sus elementos forman necesariamente una "filita discreta".

Tambien es importante para entender la prueba del Teorema del Filtro Primo que
viene a continuacion, imaginar las cadenas de posets que sus elementos son
conjuntos y su orden es la inclusion, es decir dichas cadenas seran un
conjunto de conjuntos $C$ con la propiedad que dados dos cualesquiera
elementos de $C$ siempre alguno contiene al otro. Un ejemplo de este tipo de
cadenas es dado en (E3). Otro ejemplo:

\begin{enumerate}
\item[(E4)] Sea $\mathcal{F}=\{F:F$ es un filtro del reticulado $(\mathbf{N}%
,mcm,mcd)\}$. Notar que dado $n\in\mathbf{N}$, el conjunto $\{x\in
\mathbf{N}:n|x\}$ es un filtro de $(\mathbf{N},mcm,mcd)\}$. Ya que
$\mathcal{F}$ es no vacio tenemos que $(\mathcal{F},\subseteq)$ es un poset.
Entonces%
\[
C=\{\{x\in\mathbf{N}:n|x\}:n\text{ es potencia de }2\}
\]
es una cadena de $(\mathcal{F},\subseteq)$.
\end{enumerate}

\bigskip

El siguiente resultado es una herramienta fundamental en el algebra moderna.

\begin{lemma}
[Lema de Zorn]Sea $(P,\leq)$ un poset y supongamos cada cadena de $(P,\leq)$
tiene al menos una cota superior. Entonces hay un elemento maximal en
$(P,\leq)$.
\end{lemma}

\bigskip

Obviamente en cada poset con universo finito hay al menos un elemento maximal.
O sea que el Lema de Zorn es interesante para el caso en que $P$ es un
conjunto infinito. Un argumento para creer en la veracidad del lema podria ser
el siguiente razonamiento por el absurdo. Supongamos que $(P,\leq)$ es un
poset en el cual cada cadena tiene al menos una cota superior y supongamos
ademas que no hay elementos maximales en $(P,\leq)$. Tomemos $x_{0}\in P$ un
elemento cualquiera. Ya que $x_{0}$ no es maximal, hay un $x_{1}\in P$ tal que
$x_{0}<x_{1}$. Iterando esta idea vemos que debe haber elementos $x_{2}%
,x_{3},...$ tales que:%
\[
x_{0}<x_{1}<x_{2}<x_{3}<\cdots
\]
Pero $\{x_{0},x_{1},x_{2},x_{3},...\}$ es una cadena por lo cual hay al menos
una cota superior de ella en $(P,\leq)$. Sea $x_{\omega}$ una de tales cotas.

\bigskip

\bigskip

\bigskip

\bigskip

Un filtro $F$ de un reticulado $(L,\mathsf{s},\mathsf{i})$ sera llamado
\textit{primo} cuando se cumplan:

\begin{enumerate}
\item[(1)] $F\neq L$

\item[(2)] $x\;\mathsf{s\;}y\in F\Rightarrow x\in F$ o $y\in F$.
\end{enumerate}

\bigskip

Algunos ejemplos:

\begin{enumerate}
\item[E1] Todo filtro de $(\mathbf{R},\max,\min)$, distinto de $\mathbf{R}$,
es primo (justificar)

\item[E2] Sea $B=\{X\subseteq\omega:X$ es finito o $\omega-X$ es finito$\} $.
Como vimos anteriormente $B$ es cerrado bajo las operaciones $\cup$ y $\cap$.
Sea $P=\{X\subseteq\omega:\omega-X$ es finito$\}$. Entonces $P$ es un filtro
primo de $(B,\cup,\cap)$.
\end{enumerate}

\bigskip

\begin{theorem}
[Teorema del Filtro Primo]Sea $(L,\mathsf{s},\mathsf{i})$ un reticulado
distributivo y $F$ un filtro. Supongamos $x_{0}\in L-F$. Entonces hay un
filtro primo $P$ tal que $x_{0}\notin P$ y $F\subseteq P$.
\end{theorem}

\begin{proof}
Sea%
\[
\mathcal{F}=\{F_{1}:F_{1}\text{ es un filtro, }x_{0}\notin F_{1}\text{ y
}F\subseteq F_{1}\}.
\]
Notese que $\mathcal{F}\neq\emptyset$, por lo cual $(\mathcal{F},\subseteq)$
es un poset. Veamos que cada cadena en $(\mathcal{F},\subseteq)$ tiene una
cota superior. Sea $C$ una cadena. Si $C=\emptyset$, entonces cualquier
elemento de $\mathcal{F}$ es cota de $C$. Supongamos entonces $C\neq\emptyset
$. Sea%
\[
G=\{x:x\in F_{1}\text{, para algun }F_{1}\in C\}.
\]
Veamos que $G$ es un filtro. Es claro que $G$ es no vacio. Supongamos que
$x,y\in G$. Sean $F_{1},F_{2}\in\mathcal{F}$ tales que $x\in F_{1}$ y $y\in
F_{2}$. Si $F_{1}\subseteq F_{2}$, entonces ya que $F_{2}$ es un filtro
tenemos que $x\;\mathsf{i\;}y\in F_{2}\subseteq G$. Si $F_{2}\subseteq F_{1}$,
entonces tenemos que $x\;\mathsf{i\;}y\in F_{1}\subseteq G$. Ya que $C$ es una
cadena, tenemos que siempre $x\;\mathsf{i\;}y\in G$. En forma analoga se
prueba la propiedad restante por lo cual tenemos que $G$ es un filtro. Ademas
$x_{0}\notin G$, por lo que $G\in\mathcal{F}$ es cota superior de $C$. Por el
lema de Zorn, $(\mathcal{F},\subseteq)$ tiene un elemento maximal $P$. Veamos
que $P$ es un filtro primo. Supongamos $x\;\mathsf{s\;}y\in P$ y $x,y\notin
P$. Notese que $[P\cup\{x\})$ es un filtro el cual contiene propiamente a $P$.
Entonces ya que $P$ es un elemento maximal de $(\mathcal{F},\subseteq)$,
tenemos que $x_{0}\in\lbrack P\cup\{x\})$. Analogamente tenemos que $x_{0}%
\in\lbrack P\cup\{y\})$. Ya que $x_{0}\in\lbrack P\cup\{x\})$, tenemos que hay
elementos $p_{1},...,p_{n}\in P$, tales que%
\[
x_{0}\geq p_{1}\;\mathsf{i\;}...\;\mathsf{i\;}p_{n}\;\mathsf{i\;}x
\]
(se deja como ejercicio justificar esto). Ya que $x_{0}\in\lbrack P\cup
\{y\})$, tenemos que hay elementos $q_{1},...,q_{m}\in P$, tales que%
\[
x_{0}\geq q_{1}\;\mathsf{i\;}...\;\mathsf{i\;}q_{m}\;\mathsf{i\;}y
\]
Si\ llamamos $p$ al siguiente elemento de $P$%
\[
p_{1}\;\mathsf{i\;}...\;\mathsf{i\;}p_{n}\;\mathsf{i\;}q_{1}\;\mathsf{i\;}%
...\;\mathsf{i\;}q_{m}%
\]
tenemos que%
\begin{align*}
x_{0}  & \geq p\;\mathsf{i\;}x\\
x_{0}  & \geq p\;\mathsf{i\;}y
\end{align*}
Se tiene entonces que $x_{0}\geq(p\;\mathsf{i\;}x)\;\mathsf{s\;}%
(p\;\mathsf{i\;}y)=p\;\mathsf{i\;}(x\;\mathsf{s\;}y)\in P$, lo cual es absurdo
ya que $x_{0}\notin P$.
\end{proof}

\begin{corollary}
Sea $(L,\mathsf{s},\mathsf{i},0,1)$ un reticulado acotado distributivo. Si
$\emptyset\neq S\subseteq L$ es tal que $s_{1}\;\mathsf{i\;}s_{2}%
\;\mathsf{i\;}...\;\mathsf{i\;}s_{n}\neq0$, para cada $s_{1},...,s_{n}\in S$,
entonces hay un filtro primo que contiene a $S$.
\end{corollary}

\begin{proof}
Notese que $[S)\neq L$ por lo cual se puede aplicar el Teorema del filtro primo.
\end{proof}

\bigskip

\begin{lemma}
Sea $(B,\mathsf{s},\mathsf{i},^{c},0,1)$ un algebra de Boole. Entonces para un
filtro $F\subsetneq B$ las siguientes son equivalentes:

\begin{enumerate}
\item[(1)] $F$ es primo

\item[(2)] $x\in F$ o $x^{c}\in F$, para cada $x\in B$.
\end{enumerate}
\end{lemma}

\begin{proof}
(1)$\Rightarrow$(2). Ya que $x\;\mathsf{s\;}x^{c}=1\in F$, (2) se cumple si
$F$ es primo.

(2)$\Rightarrow$(1). Ya sabemos por hipotesis que $F$ es un filtro y que
$F\neq B$. Supongamos que $x\;\mathsf{s\;}y\in F$ y que $x\not \in F$.
Entonces por (2), $x^{c}\in F$ y por lo tanto tenemos que%
\[
y\geq x^{c}\;\mathsf{i\;}y=(x^{c}\;\mathsf{i\;}x)\;\mathsf{s\;}(x^{c}%
\;\mathsf{i\;}y)=x^{c}\;\mathsf{i\;}(x\;\mathsf{s\;}y)\in F,
\]
lo cual dice que $y\in F$.
\end{proof}

\bigskip

Necesitaremos el siguiente lema.

\begin{lemma}
Sea $(B,\mathsf{s},\mathsf{i},^{c},0,1)$ un algebra de Boole. Supongamos que
$b\neq0$ y $a=\inf A$, con $A\subseteq B$. Entonces si $b\;\mathsf{i\;}a=0$,
existe un $e\in A$ tal que $b\;\mathsf{i\;}e^{c}\neq0$.
\end{lemma}

\begin{proof}
Supongamos que para cada $e\in A$, tengamos que $b\;\mathsf{i\;}e^{c}=0$.
Entonces tenemos que para cada $e\in A$,%
\[
b=b\;\mathsf{i\;}(e\;\mathsf{s\;}e^{c})=(b\;\mathsf{i\;}e)\;\mathsf{s\;}%
(b\;\mathsf{i\;}e^{c})=b\;\mathsf{i\;}e,
\]
lo cual nos dice que $b$ es cota inferior de $A$. Pero entonces $b\leq a$, por
lo cual $b=b\;\mathsf{i\;}a=0$, contradiciendo la hipotesis.
\end{proof}

\bigskip

Es claro que si $P$ es un filtro primo de un algebra de Boole $(B,\mathsf{s}%
,\mathsf{i},^{c},0,1)$, entonces cualquiera sea el conjunto finito $S$
contenido en $P$, se tiene que $\inf S\in P$. Cuando tomamos un subconjunto
$S\subseteq P$ el cual es infinito, la cosa cambia sustancialmente. Primero
cabe destacar que puede suceder que $S$ no tenga infimo en $(B,\mathsf{s}%
,\mathsf{i},^{c},0,1)$. Pero tambien puede pasar que $S$ tenga infimo pero que
$\inf S$ no pertenesca a $P$. Por ejemplo, si tomamos el algebra de Boole
$(B,\cup,\cap,^{c},\emptyset,\omega)$, donde%
\[
B=\{X\subseteq\omega:X\text{ es finito o }\omega-X\text{ es finito}\}
\]
podemos observar que%
\[
P=\{X\subseteq\omega:\omega-X\text{ es finito}\}
\]
es un filtro primo y que%
\[
S=\{\omega-\{n\}:n\in\omega\}
\]
esta contenido en $P$ pero $\inf S=\emptyset$ no pertenece a $P$.

\bigskip

El siguiente teorema sera clave en nuestra prueba del teorema de completitud
de la logica de primer orden.

\begin{theorem}
(Rasiova y Sikorski) Sea $(B,\mathsf{s},\mathsf{i},^{c},0,1)$ un algebra de
Boole. Sea $x\in B$, $x\neq0$. Supongamos que $(A_{1},A_{2},...)$ es una
infinitupla de subconjuntos de $B$ tal que existe $\inf(A_{j})$, para cada
$j=1,2....$ Entonces hay un filtro primo $P$ el cual cumple:

\begin{enumerate}
\item[(a)] $x\in P$

\item[(b)] $P\supseteq A_{j}\Rightarrow P\ni\inf(A_{j})$, para cada
$j=1,2,....$
\end{enumerate}
\end{theorem}

\begin{proof}
Sean $a_{j}=\inf(A_{j})$, $j=1,2,...$. Construiremos inductivamente una
infinitupla $(b_{0},b_{1},...)$ de elementos de $B$ tal que:

\begin{enumerate}
\item[(1)] $b_{0}=x$

\item[(2)] $b_{0}\;\mathsf{i\;}$...$\;\mathsf{i\;}b_{n}\neq0$, para cada
$n\geq0$

\item[(3)] $b_{j}=a_{j}$ o $b_{j}^{c}\in A_{j}$, para cada $j\geq1$.
\end{enumerate}

\noindent Definamos $b_{0}=x$. Supongamos ya definimos $b_{0},...,b_{n}$,
veamos como definir $b_{n+1}$. Si $(b_{0}\;\mathsf{i\;}...\;\mathsf{i\;}%
b_{n})\;\mathsf{i\;}a_{n+1}\neq0$, entonces definamos $b_{n+1}=a_{n+1}$. Si
$(b_{0}\;\mathsf{i\;}...\;\mathsf{i\;}b_{n})\;\mathsf{i\;}a_{n+1}=0$, entonces
por el lema anterior, tenemos que hay un $e\in A_{n+1}$ tal que $(b_{0}%
\;\mathsf{i\;}...\;\mathsf{i\;}b_{n})\;\mathsf{i\;}e^{c}\neq0$, lo cual nos
permite definir $b_{n+1}=e^{c}$.

Usando (2) se puede probar que el conjunto $S=\{b_{0},b_{1},...\}$ satisface
la hipotesis del primer corolario del Teorema del filtro primo, por lo cual
hay un filtro primo $P$ tal que $\{b_{0},b_{1},...\}\subseteq P$. Es claro que
$x\in P$ y es facil chequear usando (3) que $P$ satisface la propiedad (b).
\end{proof}

\bigskip

Cerramos la seccion con una convencion notacional que se usara mas que nada en
los ejercicios y la tombola.

\bigskip

\begin{enumerate}
\item[\textbf{Convencion notacional:}] Notese que hemos definido distintos
tipos de estructuras (i.e. posets, reticulados como ternas, etc) y en todas
ellas su primera coordenada es llamada el \textit{universo} de dicha
estructura. En general usaremos letras mayusculas en bold para denotar una
estructura dada y en tal caso usaremos la convension de que su correspondiente
mayuscula en italica denotara el universo de dicha estructura. Por ejemplo si
decimos "sea $\mathbf{L}$ un reticulado acotado", entonces ya queda implicita
la informacion de que denotaremos con $L$ al universo de $\mathbf{L}$. Ademas
deberia quedar claro que en tal caso $\mathbf{L}$ es una 5-upla.

Tambien si $\mathbf{L}^{\prime}$ denota una estructura, $L^{\prime}$ denotara
su universo. Similarmente si $\mathbf{\tilde{L}}$ denota una estructura,
$\tilde{L}$ denotara su universo, etc.

Notese que entonces, si escribimos "Sea $F:\mathbf{L}\rightarrow
\mathbf{L}^{\prime}$ es un homomorfismo de reticulados complementados",
estaremos suponiendo que $\mathbf{L}$ y $\mathbf{L}^{\prime}$ son reticulados
complementados (i.e. ciertas 6-uplas) y que $F$ es una funcion de $L$ en
$L^{\prime}$ la cual es un homomorfismo de $\mathbf{L}$ en $\mathbf{L}%
^{\prime}$. Aqui hay que tener cuidado ya que $D_{F}$ es $L$ y no $\mathbf{L}$
lo cual seria imposible ya que $\mathbf{L}$ no es un conjunto!

Tambien notese que si $\mathbf{L}$ denota un reticulado acotado y $\theta$ es
una congruencia de $\mathbf{L}$, entonces $\mathbf{L}/\theta$ denotara el
cociente de $\mathbf{L}$ sobre $\theta$, a saber cierto reticulado acotado
cuyo universo es $L/\theta$. Es decir que $Ti(\mathbf{L}/\theta
)=5\mathrm{-UPLA}$ y $Ti(L/\theta)=\mathrm{CONJUNTO}$
\end{enumerate}

\bigskip

\section{\label{Estructuras y su lenguaje elemental asociado}Estructuras y su
lenguaje elemental asociado}

Como ya vimos en la seccion anterior, hay distintos tipos de estructuras a las
cuales se las puede estudiar usando metodos similares. Con cada tipo de
estructura tenemos asociado cierto tipo de enunciados o sentencias que
llamaremos \textit{formulas elementales}. Tambien una ves que fijemos ciertas
sentencias elementales que axiomaticen tal tipo de estructura, tendremos
asociado un tipo de pruebas que llamaremos \textit{elementales} ya que solo
usaran dichos axiomas, deducciones muy simples y obvias de justificar con
peque\~{n}as fraces en castellano y ademas en su escritura lo concerniente a
la matematica misma se escribira usando solo sentencias elementales. No
daremos ahora una definicion matematica de estos conceptos pero los
describiremos en forma intuitiva mediante ejemplos. Cabe destacar que los
conceptos de formula elemental y prueba elemental son relativos al tipo de
estructura que se esta considerando.

\bigskip

\subsection{Formulas elementales para posets}

Se construyen en forma finitaria usando simbolos de la siguiente lista:

\begin{enumerate}
\item[ ] $\forall\ \exists\;\lnot\;\vee\;\wedge\;\rightarrow\;\leftrightarrow
\;(\;)\;,\;=\;\leq$

\item[ ] Variables: $x,y,z,w,...$

\item[ ] Nombres para elementos fijos: $a,b,c,d,...$
\end{enumerate}

Algunos ejemplos:

\begin{enumerate}
\item[ ] $(x\leq y)$

\item[ ] $(x=y)$

\item[ ] $(a\leq z)$

\item[ ] $(b=c)$

\item[ ] $\forall y(y\leq x)$

\item[ ] $\forall y(y\leq a)$

\item[ ] $\forall x(x\leq x)$

\item[ ] $\forall x\forall y\forall z\;((x\leq y\wedge y\leq z)\rightarrow
x\leq z)$

\item[ ] $\forall x\forall y((x\leq y\wedge y\leq x)\rightarrow x=y)$

\item[ ] $\exists x\forall y(y\leq x)$

\item[ ] $\lnot\exists y(x\leq y\wedge\lnot(y=x))$

\item[ ] $\forall y(y\leq x)\rightarrow\lnot\exists y(x\leq y\wedge
\lnot(y=x))$
\end{enumerate}

\bigskip

Notese que siempre "cuantificamos por delante", es decir que la palabra
$(x\leq a)\forall x$ NO es una formula elemental de posets (aunque es claro
que puede atribuirsele un sentido ya que podemos pensar que dice que $a$ es un
elemento maximo). Mas alla de que no hemos definido matematicamente el
concepto de formula elemental de posets, es importante entender que las
formulas elementales son palabras, es decir $Ti(\varphi)=\mathrm{PALABRA}$,
cada ves que $\varphi$ es una formula elemental de posets.

Para que una formula se vuelva verdadera o falsa tenemos que tener un poset
concreto $(P,\leq)$ y ademas asignarles valores concretos de $P$ a las
variables libres y a los nombres de elementos fijos que ocurren en dicha
formula. Cuando la formula no tiene variables libres diremos que es una
\textit{sentencia elemental de posets}. Notese que en tal caso sera verdadera
o falsa en un poset dado dependiendo solo de los valores que tomen los nombres
para elementos fijos que ocurren en ella. Tambien cabe destacar que los
cuantificadores siempre ranguean sobre $P$, es decir $\forall x$ se
interpretara como $\forall x\in P$ y $\exists x$ se interpretara como $\exists
x\in P$. La diferencia entre las variables y los nombres de elementos fijos es
que si bien ambos pueden variar su valor los nombres de elementos fijos suelen
denotar un valor fijo del poset durante todo un desarrollo o demostracion.
Tampoco se cuantificaran los nombres de elementos fijos, es decir solo
cuantificamos variables. O sea que $\forall a(a=x)$ no es una formula elemental.

Por ejemplo en el poset $(\mathbf{N},|)$

\begin{enumerate}
\item[-] La formula $(x\leq y)$ es verdadera cuando le asignamos a $x$ el
valor $6$ y a $y$ el valor $36$

\item[-] $\forall y(a\leq y)$ es verdadera cuando le asignamos a $a$ el valor
$1$ y falsa si le asignamos el valor $5$

\item[-] $\exists y(x\leq y\wedge\lnot(y=x))$ es verdadera cuando le asignamos
a $x$ cualquier valor de $\mathbf{N}$

\item[-] La sentencia $\forall x\forall y(x\leq y\vee y\leq x)$ es falsa en
$(\mathbf{N},|)$
\end{enumerate}

Tambien es bueno pensar que

\begin{enumerate}
\item[-] La formula $\forall y(a\leq y)$ dice que "$a$ es un elemento minimo
de $(P,\leq)$", la cual sera o no verdadera en $(P,\leq)$ dependiendo de que
valor tenga asignado $a$

\item[-] La formula $\lnot\exists y(x\leq y\wedge\lnot(y=x))$ dice que "$x $
es un elemento maximal de $(P,\leq)$", la cual sera o no verdadera en
$(P,\leq)$ dependiendo de que valor tenga asignado $x$
\end{enumerate}

\bigskip

\subsubsection{Variables libres}

Para tener una mejor intuicion del concepto de variable libre de una formula
elemental de posets, nos explayaremos un poco mas dando varios ejemplos.
Primero deberiamos notar que si una variable ocurre varias veces en una
formula, entonces algunas de aquellas ocurrencias seran libres y otras no. Por ejemplo

\begin{enumerate}
\item[-] En la formula $(x\leq a)\rightarrow\exists x\ ((a\leq x)\wedge(b\leq
x))$ la primer ocurrencia de $x$ es libre y las otras tres ocurrencias de $x$
no son libres
\end{enumerate}

Como es usual a las ocurrencias que no son libres las llamaremos
\textit{acotadas}. O sea que toda ocurrencia de una variable en una formula es
ya sea libre o acotada. Por ejemplo, en la formula%
\[
(z\leq y)\wedge\forall y(z\leq y)
\]
la variable $y$ ocurre tres veces, la primera ocurrencia es libre y la segunda
y tercera son acotadas.

Cuando digamos que $x$ \textit{es una variable libre de una formula} $\varphi$
nos estaremos refiriendo a que la variable $x$ ocurre al menos una ves
libremente en $\varphi$, aunque tambien puede ocurrir acotadamente en
$\varphi$. En el ejemplo anterior justamente se da ese caso y $x$ es una
variable libre de dicha formula. Por ejemplo las variables libres de la
formula%
\[
((x\leq z)\vee\exists x\forall y\ ((a\leq x)\wedge(y\leq x)))\rightarrow
\forall y(z\leq y)
\]
son $x$ y $z$. Las dos ocurrencias de $z$ son libres, todas las ocurrencias de
$y$ son acotadas, la primer ocurrencia de $x$ es libre y las otras tres
ocurrencias de $x$ son acotadas.

\paragraph{Alcance de la ocurrencia de un cuantificador}

Un \textit{cuantificador} sera una palabra formada por alguno de los simbolos%
\[
\forall\ \ \ \ \exists
\]
seguido de una variable. Es decir%
\[
\exists x\ \ \ \forall x\ \ \ \exists y\ \ \ \forall y\ \ \ \exists
z\ \ \ \forall z\ \ \ldots
\]
son los cuantificadores.

Una propiedad importante de las formulas elementales es que siempre que un
cuantificador ocurra en una formula, seguido a dicha ocurrencia ocurrira una
formula elemental (la cual ademas es unica). Ejemplos:

\begin{enumerate}
\item[-] En la formula%
\[
(((x\leq y)\wedge\forall y\lnot(y=a))\rightarrow\forall y(z\leq y))\vee(x\leq
z)
\]
seguido a la segunda ocurrencia del cuantificador $\forall y$ ocurre la
formula $(z\leq y)$ y seguido a la primer ocurrencia del cuantificador
$\forall y$ ocurre la formula $\lnot(y=a)$.

\item[-] En la formula%
\[
\forall x\forall y(x\leq y\vee y\leq x)
\]
seguido a la unica ocurrencia del cuantificador $\forall y$ ocurre la formula
$(x\leq y\vee y\leq x)$ y seguido a la unica ocurrencia del cuantificador
$\forall x$ ocurre la formula $\forall y(x\leq y\vee y\leq x)$
\end{enumerate}

\bigskip

Esto nos da lugar al concepto de \textit{alcance} de una ocurrencia de un
cuantificador en una formula elemental. Intuitivamente hablando, el alcance de
una ocurrencia de un cuantificador es el espacio ocupado por la unica formula
que ocurre inmediatamente despues de dicha ocurrencia del cuantificador.
Parece un trabalenguas pero con ejemplos se entendera la idea.

\begin{enumerate}
\item[-] En la formula%
\[
(((x\leq y)\wedge\forall y\underline{\lnot(y=a)})\rightarrow\forall y(z\leq
y))\vee(x\leq z)
\]
hemos subrayado el alcance de la primer ocurrencia del cuantificador $\forall
y$.

\item[-] En la formula%
\[
\forall x\underline{\forall y(x\leq y\vee y\leq x)}%
\]
hemos subrayado el alcance de la unica ocurrencia del cuantificador $\forall
x$.

\item[-] En la formula%
\[
((x\leq z)\vee\exists x\underline{\forall y((a\leq x)\wedge(y\leq
x))})\rightarrow\exists x(x\leq y)
\]
hemos subrayado el alcance de la primer ocurrencia del cuantificador $\exists
x$.
\end{enumerate}

\bigskip

Notese que una ocurrencia de una variable $v$ en una formula elemental
$\varphi$ sera acotada si y solo si ya sea es parte de la ocurrencia de un
cuantificador $Qv$ en $\varphi$, con $Q\in\{\forall,\exists\}$, o dicha
ocurrencia sucede dentro del alcance de la ocurrencia de un cuantificador
$Q^{\prime}v$ en $\varphi$, con $Q^{\prime}\in\{\forall,\exists\}$.

Deberia quedar claro que el roll jugado por una variable $v$ en sus
ocurrencias acotadas (dentro de la ocurrencia de un cuantificador $Qv$ y su
alcance) es en algun sentido "mudo" o "impersonal" en el sentido que podriamos
reemplazar dichas ocurrencias de $v$ por una variable $w$ que no ocurra en la
formula y el significado de la formula resultante seria el mismo que el de la
formula original. Por ejemplo la formula%
\[
\lnot(x=a)\wedge\forall y((x\leq y)\wedge(x\leq y))
\]
nos "dice" que $x$ es distinto a $a$ y que $x$ es comparable con todo otro
elemento del poset; y si reemplazamos cada ocurrencia de $y$ en el bloque
$\forall y((x\leq y)\wedge(x\leq y))$ por la variable $z$, obtenemos%
\[
\lnot(x=a)\wedge\forall z((x\leq z)\wedge(x\leq z))
\]
la cual claramente dice lo mismo acerca de $x$ y $a$

\bigskip

\subsection{Pruebas elementales de posets}

Notese que el concepto de poset se puede axiomatizar con sentencias
elementales de las recien descriptas, a saber, un \textit{poset} es un par
$(P,\leq)$ tal que $P$ es un conjunto no vacio, $\leq$ es una relacion binaria
sobre $P$ y se cumplen:

\begin{enumerate}
\item $\forall x(x\leq x)$

\item $\forall x\forall y\forall z\;((x\leq y\wedge y\leq z)\rightarrow x\leq
z)$

\item $\forall x\forall y((x\leq y\wedge y\leq x)\rightarrow x=y)$
\end{enumerate}

Muchas de las pruebas dadas sobre posets consisten en probar que cierta
sentencia elemental se cumple en todos los posets. Por ejemplo la sentencia
elemental%
\[
\mu=\forall x\forall y\left(  (\forall z\ z\leq x\wedge\forall z\ z\leq
y)\rightarrow x=y\right)
\]
claramente se cumple en todos los posets ya que ella expresa que si en un
poset $x$ e $y$ son elementos maximos, entonces $x=y$. Mas aun, la prueba de
este hecho puede escribirse usando solo formulas elementales de posets y
ciertas aclaraciones minimas de castellano que sirven para justificar los
distintos pasos en forma obvia. A este tipo de pruebas las llamaremos
\textit{pruebas elementales de posets}. A continuacion damos la prueba
elemental de $\mu$.

\bigskip

\begin{proof}
[Prueba elemental de $\mu$]Denotemos con $a$ y $b$ un par de elementos de $A$,
fijos. Supongamos%
\[
(\forall z\ z\leq a\wedge\forall z\ z\leq b)
\]
En particular $\forall z\ z\leq b$ nos dice que $a\leq b$ y $\forall z\ z\leq
a$ nos dice que $b\leq a$, por lo cual tenemos que%
\[
a\leq b\wedge b\leq a
\]
Pero el axioma%
\[
\forall x\forall y((x\leq y\wedge y\leq x)\rightarrow x=y)
\]
nos dice que%
\[
(a\leq b\wedge b\leq a)\rightarrow a=b
\]
obteniendo de esta forma que $a=b$. O sea que hemos probado que%
\[
(\forall z\ z\leq a\wedge\forall z\ z\leq b)\rightarrow a=b
\]
Como $a$ y $b$ eran elementos cualesquiera, obtenemos que vale $\mu$
\end{proof}

\bigskip

\subsection{Formulas elementales de reticulados terna}

Se construyen en forma finitaria usando simbolos de la siguiente lista:

\begin{enumerate}
\item[ ] $\forall\ \exists\;\lnot\;\vee\;\wedge\;\rightarrow\;\leftrightarrow
\;(\;)\;,\;=\;\mathsf{s\;\ i}$

\item[ ] Variables: $x,y,z,w,...$

\item[ ] Nombres para elementos fijos: $a,b,c,d,...$
\end{enumerate}

Algunos ejemplos:

\begin{enumerate}
\item[ ] $(x\;\mathsf{s\;}y=a)$

\item[ ] $(x=y)$

\item[ ] $((a\;\mathsf{s\;}z)\;\mathsf{i\;}x)=((x\;\mathsf{i\;}%
y)\;\mathsf{s\;}x))$

\item[ ] $(b=c)$

\item[ ] $\forall x\forall y\forall z((x\mathsf{\;s\;}y)\;\mathsf{s}%
\;z=x\;\mathsf{s}\;(y\;\mathsf{s}\;z))$

\item[ ] $\forall y(x\;\mathsf{s\;}y=a\rightarrow b=y)$

\item[ ] $\forall x\exists y(x\;\mathsf{s\;}y=a)$

\item[ ] $\forall x\forall y\forall z\;((x=y\wedge y=z)\rightarrow x=z)$

\item[ ] $\lnot\exists y(x\;\mathsf{s\;}y=y\wedge\lnot(x=y))$
\end{enumerate}

\bigskip

Notese que dado que estamos considerando ahora reticulados ternas, las
\textit{formulas elementales de reticulados ternas} no pueden usar el simbolo
$\leq$ ya que no esta explicito en la presentacion de este tipo de estructura.
Para que una formula se vuelva verdadera o falsa tenemos que tener un
reticulado concreto $(L,\mathsf{s},\mathsf{i})$ y ademas asignarles valores
concretos de $L$ a las variables libres y a los nombres de elementos fijos que
ocurran en dicha formula. Cuando la formula no tiene variables libres diremos
que es una \textit{sentencia elemental de reticulados ternas}. Notese que en
tal caso sera verdadera en un reticulado terna dado, dependiendo solo de los
valores que tomen los nombres de constantes fijas que ocurren en ella.

Por ejemplo en el reticulado $(\mathbf{N},mcm,mcd)$

\begin{enumerate}
\item[-] la formula $(x\;\mathsf{s\;}y=y)$ es verdadera cuando le asignamos a
$x$ el valor $6$ y a $y$ el valor $36$

\item[-] $\forall y(a=a\;\mathsf{i\;}y)$ es verdadera cuando le asignamos a
$a$ el valor $1$ y falsa si le asignamos el valor $5$

\item[-] La sentencia $\forall x\forall y(x\mathsf{\;i\;}y=y\mathsf{\;s\;}x)$
es falsa en $(\mathbf{N},mcm,mcd)$
\end{enumerate}

\bigskip

\subsection{Pruebas elementales de reticulados terna}

Igual que para el caso de los posets el concepto de reticulado terna fue
definido con sentencias elementales de las recien descriptas, es decir:

\begin{enumerate}
\item $\forall x\forall y(x\;\mathsf{s}\;x=x)$

\item $\forall x\forall y(x\mathsf{\;i\;}x=x)$

\item $\forall x\forall y(x\mathsf{\;s\;}y=y\;\mathsf{s}\;x)$

\item $\forall x\forall y(x\mathsf{\;i\;}y=y\mathsf{\;i\;}x)$

\item $\forall x\forall y\forall z((x\mathsf{\;s\;}y)\;\mathsf{s}%
\;z=x\;\mathsf{s}\;(y\;\mathsf{s}\;z))$

\item $\forall x\forall y\forall z((x\mathsf{\;i\;}y)\mathsf{\;i\;}%
z=x\mathsf{\;i\;}(y\mathsf{\;i\;}z))$

\item $\forall x\forall y(x\;\mathsf{s}\;(x\mathsf{\;i\;}y)=x)$

\item $\forall x\forall y(x\mathsf{\;i\;}(x\;\mathsf{s}\;y)=x)$
\end{enumerate}

Analogamente al caso de los posets, llamaremos \textit{pruebas elementales de
reticulados terna} a aquellas pruebas que partiendo de los axiomas anteriores
demuestran cierta sentencia elemental usando en el camino solo sentencias
elementales de reticulados ternas y ciertas aclaraciones minimas de castellano
que sirven para justificar los distintos pasos en forma obvia.

Como ejemplo daremos a continuacion una prueba elemental de reticulados terna
de la sentencia $\eta=\forall x\forall y(x\;\mathsf{s}\;y=y\rightarrow
x\;\mathsf{i}\;y=x)$.

\bigskip

\begin{proof}
[Prueba elemental de $\eta$]Denotemos con $a$ y $b$ un par de elementos de
$A$, fijos. Supongamos%
\[
(a\;\mathsf{s}\;b=b)
\]
El axioma%
\[
\forall x\forall y(x\mathsf{\;i\;}(x\;\mathsf{s}\;y)=x)
\]
nos dice que%
\[
a\mathsf{\;i\;}(a\;\mathsf{s}\;b)=a
\]
O sea que reemplazando en esta igualdad $a\;\mathsf{s}\;b$ por $b$ obtenemos:%
\[
a\mathsf{\;i\;}b=a
\]
Ya que habiamos supuesto que $a\;\mathsf{s}\;b=b$ hemos probado en realidad
que%
\[
a\;\mathsf{s}\;b=b\rightarrow a\mathsf{\;i\;}b=a
\]
Como $a$ y $b$ eran elementos cualesquiera, obtenemos que vale $\forall
x\forall y(x\;\mathsf{s}\;y=y\rightarrow x\;\mathsf{i}\;y=x)$
\end{proof}

\bigskip

Encontrar pruebas elementales de reticulados terna tiene cierta dificultad ya
que solo podemos usar los axiomas de reticulados terna y ademas no podemos
escribir el simbolo $\leq$. De todas maneras podemos escribir $t\;\mathsf{s}%
\;s=s$ en lugar de $t\leq s$ y de esta manera simular en nuestras formulas
elementales al simbolo $\leq$. Otro escollo para encontrar facilmente pruebas
elementales de reticulados ternas es que de los axiomas no es obvio que las
operaciones s e i sean supremo e infimo respecto del orden dado por la
ecuacion $x\;\mathsf{s}\;y=y$. Esto puede ser resuelto si nos inspiramos en la
prueba del Teorema de Dedeking pero de todas maneras las pruebas se complican
un poco en su escritura.

A continuacion introduciremos un tipo nuevo de estructura reticulada para las
cuales las pruebas elementales seran mas faciles de encontrar.

\bigskip

\subsection{Reticulados cuaterna}

Por un \textit{reticulado cuaterna} entenderemos una $4$-upla $(L,\mathsf{s}%
,\mathsf{i},\leq)$ tal que $L$ es un conjunto no vacio, $\mathsf{s}$ e
$\mathsf{i}$ son operaciones binarias sobre $L$, $\leq$ es una relacion
binaria sobre $L$ y se cumplen los siguientes axiomas:

\begin{enumerate}
\item[ ] $\mathrm{A}_{\leq R}=\forall x(x\leq x)$

\item[ ] $\mathrm{A}_{\leq T}=\forall x\forall y\forall z\;((x\leq y\wedge
y\leq z)\rightarrow x\leq z)$

\item[ ] $\mathrm{A}_{\leq A}=\forall x\forall y((x\leq y\wedge y\leq
x)\rightarrow x=y)$

\item[ ] $\mathrm{A}_{\mathsf{s}esC}=\forall x\forall y\;(x\leq x\;\mathsf{s}%
\;y\wedge y\leq x\;\mathsf{s}\;y)$

\item[ ] $\mathrm{A}_{\mathsf{s}\leq C}=\forall x\forall y\forall z\;\left(
(x\leq z\wedge y\leq z)\rightarrow x\;\text{$\mathsf{s\;}$}y\leq z\right)  $

\item[ ] $\mathrm{A}_{\mathsf{i}esC}=\forall x\forall y\;(x\;\mathsf{i}\;y\leq
x\wedge x\;\mathsf{i}\;y\leq y)$

\item[ ] $\mathrm{A}_{\mathsf{i}\geq C}=\forall x\forall y\forall z\;\left(
(z\leq x\wedge z\leq y)\rightarrow z\leq x\;\mathsf{i}\;y\right)  $
\end{enumerate}

Obviamente los tres primeros nos garantizan que $(L,\leq)$ es un poset. Ademas
notese que el axioma $\mathrm{A}_{\mathsf{s}esC}$ nos garantiza que
cualesquiera sean $x,y\in L$ se tiene que $x\;\mathsf{s}\;y$ es cota superior
del conjunto $\{x,y\}$. El axioma $\mathrm{A}_{\mathsf{s}\leq C}$ nos dice que
cualesquiera sean los elementos $x,y\in L$, se tiene que $x\;\mathsf{s}\;s$ es
menor o igual que que cualquier cota superior del conjunto $\{x,y\}$. Por
supuesto esto nos garaniza que $x\;\mathsf{s}\;y=\mathrm{sup}\{x,y\}$.
Similarmente los axiomas $\mathrm{A}_{\mathsf{i}esC}$ y $\mathrm{A}%
_{\mathsf{i}\geq C}$ garantizan que $x\;\mathsf{s}\;y=\inf\{x,y\}$.

O sea que, en virtud del teorema de Dedeking, tenemos que un reticulado
cuaterna no es ni mas ni menos que una $4$-upla $(L,\mathsf{s},\mathsf{i}%
,\leq)$ tal que $(L,\mathsf{s},\mathsf{i})$ es un reticulado terna y $\leq$ es
su orden parcial asociado.

\bigskip

\subsubsection{Formulas elementales de reticulados cuaterna}

Se construyen en forma finitaria usando simbolos de la siguiente lista:

\begin{enumerate}
\item[ ] $\forall\ \exists\;\lnot\;\vee\;\wedge\;\rightarrow\;\leftrightarrow
\;(\;)\;,\;=\;\mathsf{s\;}\;\mathsf{i\;}\leq$

\item[ ] Variables: $x,y,z,w,...$

\item[ ] Nombres para elementos fijos: $a,b,c,d,...$
\end{enumerate}

Algunos ejemplos:

\begin{enumerate}
\item[ ] $(x\leq y)$

\item[ ] $(x=y)$

\item[ ] $(a\leq z)$

\item[ ] $(x\;\mathsf{s\;}y=a)$

\item[ ] $((a\;\mathsf{s\;}z)\;\mathsf{i\;}x)=((x\;\mathsf{i\;}%
y)\;\mathsf{s\;}x))$

\item[ ] $\forall x\forall y\forall z((x\mathsf{\;s\;}y)\;\mathsf{s}\;z\leq
x\;\mathsf{s}\;(y\;\mathsf{s}\;z))$

\item[ ] $\lnot\exists y(x\;\mathsf{s\;}y=y\wedge\lnot(x=y))$

\item[ ] $\forall x\forall y\forall z\;((x\leq y\wedge y\leq z)\rightarrow
x\leq z)$

\item[ ] $(\forall x\exists y(x\;\mathsf{s\;}y=a))\rightarrow(a\leq a)$
\end{enumerate}

\bigskip

\subsubsection{Pruebas elementales de reticulados cuaterna}

Tal como para los otros tipos de estructuras, llamaremos \textit{pruebas
elementales de reticulados cuaterna} a aquellas pruebas que partiendo de los
axiomas $\mathrm{A}_{\leq R}$, $\mathrm{A}_{\leq A}$, $\mathrm{A}_{\leq T}$,
$\mathrm{A}_{\mathsf{s}esC}$, $\mathrm{A}_{\mathsf{s}\leq C}$, $\mathrm{A}%
_{\mathsf{i}esC}$ y $\mathrm{A}_{\mathsf{i}\geq C}$ demuestran cierta
sentencia elemental, usando en el camino solo sentencias elementales de
reticulados cuaternas y ciertas aclaraciones minimas de castellano que sirven
para justificar los distintos pasos en forma obvia.

Muchas de las pruebas dadas en la Guia 2 pueden adaptarse naturalmente para
ser pruebas elementales de reticulados cuaterna. Para hacer esta adaptacion
notese que el axioma $\mathrm{A}_{\leq A}$ puede ser usado en lugar de aplicar
la regla Igualdad en Posets y similarmente los axiomas $\mathrm{A}%
_{\mathsf{s}\leq C}$ y $\mathrm{A}_{\mathsf{i}\geq C}$ se pueden usar en lugar
de las reglas Superar un Supremo y Ser Menor o Igual que un Infimo. Por
ejemplo veamos una prueba elemental de la sentencia $\rho=\forall x\forall
y(x\;\mathsf{s}\;y=y\;\mathsf{s}\;x)$

\bigskip

\begin{proof}
[Prueba elemental de $\rho$:]Sean $a,b\in L$ elementos fijos. Por el axioma
$\mathrm{A}_{\mathsf{s}esC}$ (intanciado haciendo $x=b$ y $y=a$) tenemos que%
\[
b\leq b\;\mathsf{s}\;a\wedge a\leq b\;\mathsf{s}\;a
\]
De lo cual sacamos obviamente que%
\[
a\leq b\;\mathsf{s}\;a\wedge b\leq b\;\mathsf{s}\;a
\]
Ademas el axioma $\mathrm{A}_{\mathsf{s}\leq C}$ (instanciado haciendo $x=a$,
$y=b$ y $z=b\;\mathsf{s}\;a$) nos dice que%
\[
\left(  (a\leq b\;\mathsf{s}\;a\wedge b\leq b\;\mathsf{s}\;a)\rightarrow
a\;\text{$\mathsf{s\;}$}b\leq b\;\mathsf{s}\;a\right)
\]
O sea que de las ultimas dos sentencias obtenemos trivialmente que%
\[
a\;\text{$\mathsf{s\;}$}b\leq b\;\mathsf{s}\;a
\]
En forma analoga se puede probar que%
\[
b\;\mathsf{s}\;a\leq a\;\text{$\mathsf{s\;}$}b
\]
Lo cual nos dice trivialmente que%
\[
a\;\text{$\mathsf{s\;}$}b\leq b\;\mathsf{s}\;a\wedge b\;\mathsf{s}\;a\leq
a\;\text{$\mathsf{s\;}$}b
\]
Pero el axioma $\mathrm{A}_{\leq A}$ nos dice que%
\[
(a\;\text{$\mathsf{s\;}$}b\leq b\;\mathsf{s}\;a\wedge b\;\mathsf{s}\;a\leq
a\;\text{$\mathsf{s\;}$}b)\rightarrow a\;\text{$\mathsf{s\;}$}b=b\;\mathsf{s}%
\;a
\]
De lo cual obviamente obtenemos que%
\[
a\;\text{$\mathsf{s\;}$}b=b\;\mathsf{s}\;a
\]
Ya que $a,b$ eran elementos fijos pero arbitrarios, hemos probado que%
\[
\forall x\forall y(x\;\mathsf{s}\;y=y\;\mathsf{s}\;x)
\]

\end{proof}

\bigskip

Por supuesto, en la parte de la prueba en la que decimos "En forma analoga se
puede probar que $b\;\mathsf{s}\;a\leq a\;\mathsf{s\;}$$b$" deberiamos poner
las lineas que corresponden para obtener la prueba elemental completa.

Ahora daremos una prueba elemental de la sentencia $\mu=\forall x\forall
y(x\leq y\leftrightarrow x\;\mathsf{s}\;y=y)$.

\bigskip

\begin{proof}
[Prueba elemental de $\mu$:]Sean $a,b\in L$ elementos fijos. Supongamos que
$a\leq b$. Probaremos que $a\;\mathsf{s}\;b=b$. Notese que por el axioma
$\mathrm{A}_{\mathsf{s}esC}$ tenemos que%
\[
b\leq a\;\mathsf{s}\;b
\]
Por el axioma $\mathrm{A}_{\mathsf{s}\leq C}$ tenemos que%
\[
\left(  (a\leq b\wedge b\leq b)\rightarrow a\;\text{$\mathsf{s\;}$}b\leq
b\right)
\]
Pero por el axioma $\mathrm{A}_{\leq R}$ tenemos que $b\leq b$ y por hipotesis
tenemos que $a\leq b$ por lo cual%
\[
a\leq b\wedge b\leq b
\]
Obviamente esto nos dice que $a\;\mathsf{s\;}$$b\leq b$. O sea que hemos
probado%
\[
a\;\mathsf{s\;}b\leq b\wedge b\leq a\;\mathsf{s\;}b
\]
Lo cual por el axioma $\mathrm{A}_{\leq A}$ nos dice que $a\;\mathsf{s}\;b=b$.
Ya que habiamos asumido que $a\leq b$ en realidad hemos probado que%
\[
a\leq b\rightarrow a\;\mathsf{s}\;b=b
\]
Supongamos ahora que $a\;\mathsf{s}\;b=b$. Por el axioma $\mathrm{A}%
_{\mathsf{s}esC}$ tenemos que $a\leq a\;\mathsf{s}\;b$. Ya que $a\;\mathsf{s}%
\;b=b$ obtenemos que $a\leq b$. O sea que realmente hemos probado que%
\[
a\;\mathsf{s}\;b=b\rightarrow a\leq b
\]
Lo cual por la otra implicacion probada nos dice que%
\[
a\leq b\leftrightarrow a\;\mathsf{s}\;b=b
\]
Ya que $a,b$ eran elementos fijos pero arbitrarios, hemos probado que%
\[
\forall x\forall y(x\leq y\leftrightarrow x\;\mathsf{s}\;y=y)
\]

\end{proof}

\bigskip

\subsection{Formulas elementales de reticulados complementados}

Ya que los reticulados complementados son $6$-uplas $(L,\mathsf{s}%
,\mathsf{i},c,0,1)$ las formulas elementales se construyen en forma finitaria
usando simbolos de la siguiente lista:

\begin{enumerate}
\item[ ] $\forall\ \exists\;\lnot\;\vee\;\wedge\;\rightarrow\;\leftrightarrow
\;(\;)\;,\;=\;\mathsf{s\;\ i}$\ \ $c\ \ 0\ \ 1$

\item[ ] Variables: $x,y,z,w,...$

\item[ ] Nombres para elementos fijos: $a,b,c,d,...$
\end{enumerate}

Algunos ejemplos:

\begin{enumerate}
\item[ ] $(0\;\mathsf{s\;}c(y)=a)$

\item[ ] $(x\;\mathsf{s\;}1=a)$

\item[ ] $((c(a)\;\mathsf{s\;}z)\;\mathsf{i\;}x)=((x\;\mathsf{i\;}%
y)\;\mathsf{s\;}x))$

\item[ ] $(c(c(c(b)))=x)$

\item[ ] $\forall x\forall y\forall z((x\mathsf{\;s\;}y)\;\mathsf{s}%
\;z=x\;\mathsf{s}\;(y\;\mathsf{s}\;z))$

\item[ ] $(x\;\mathsf{s\;}y=1\wedge x\;\mathsf{i\;}y=0)\rightarrow x=c(y)$

\item[ ] $\forall x\forall y\forall z\;((x=y\wedge y=z)\rightarrow x=z)$

\item[ ] $\exists x\exists z\exists y\ c((a\;\mathsf{s\;}z)\;\mathsf{i\;}%
x))=((x\;\mathsf{i\;}c(y))\;\mathsf{s\;}x))$
\end{enumerate}

\bigskip

\subsection{Pruebas elementales de reticulados complementados}

Igual que para el caso de los reticulados terna el concepto de reticulado
complementado fue definido con sentencias elementales de las recien
descriptas, es decir:

\begin{enumerate}
\item $\forall x\forall y(x\;\mathsf{s}\;x=x)$

\item $\forall x\forall y(x\mathsf{\;i\;}x=x)$

\item $\forall x\forall y(x\mathsf{\;s\;}y=y\;\mathsf{s}\;x)$

\item $\forall x\forall y(x\mathsf{\;i\;}y=y\mathsf{\;i\;}x)$

\item $\forall x\forall y\forall z((x\mathsf{\;s\;}y)\;\mathsf{s}%
\;z=x\;\mathsf{s}\;(y\;\mathsf{s}\;z))$

\item $\forall x\forall y\forall z((x\mathsf{\;i\;}y)\mathsf{\;i\;}%
z=x\mathsf{\;i\;}(y\mathsf{\;i\;}z))$

\item $\forall x\forall y(x\;\mathsf{s}\;(x\mathsf{\;i\;}y)=x)$

\item $\forall x\forall y(x\mathsf{\;i\;}(x\;\mathsf{s}\;y)=x)$

\item $\forall x(0\mathsf{\;s\;}x=x)$

\item $\forall x(x\mathsf{\;s\;}1=1)$

\item $\forall x(x\mathsf{\;s\;}c(x)=1)$

\item $\forall x(x\mathsf{\;i\;}c(x)=0)$
\end{enumerate}

Llamaremos \textit{pruebas elementales de reticulados complementados }a
aquellas pruebas que partiendo de los axiomas anteriores demuestran cierta
sentencia elemental usando en el camino solo sentencias elementales de
reticulados complementados y ciertas aclaraciones minimas de castellano que
sirven para justificar los distintos pasos en forma obvia. Aqui tambien como
en el caso de reticulados terna tenemos el problema de no poder escribir el
simbolo $\leq$ en nuestras pruebas elementales de reticulados complementados y
tambien el escollo de que los axiomas no hacen referencia obvia a que las
operaciones s e i sean operciones supremo e infimo respecto del orden
asociado. Sin envargo muchas pruebas se pueden hacer en forma natural. Por
ejemplo, notar que toda prueba elemental de reticulados terna es tambien una
prueba elemental de reticulados complementados, por lo cual tenemos una prueba
elemental de reticulados complementados de la sentencia%
\[
\forall x\forall y(x\;\mathsf{s}\;y=y\rightarrow x\;\mathsf{i}\;y=x)
\]
Otro ejemplo:

\bigskip

\begin{proof}
[Prueba elemental de $\forall x(x\mathsf{\;i\;}1=x)$:]Sea $a\in L$ fijo. El
axioma%
\[
\forall x(x\mathsf{\;s\;}1=1)
\]
nos dice que%
\[
a\mathsf{\;s\;}1=1
\]
Ya que%
\[
\forall x\forall y(x\;\mathsf{s}\;y=y\rightarrow x\;\mathsf{i}\;y=x)
\]
(teorema ya probado) tenemos que%
\[
a\;\mathsf{s}\;1=1\rightarrow a\;\mathsf{i}\;1=a
\]
O sea que%
\[
a\;\mathsf{i}\;1=a
\]
Ya que $a$ era arbitrario, hemos probado que%
\[
\forall x(x\;\mathsf{i}\;1=x)
\]

\end{proof}

\bigskip

Notese que si realmente queremos tener la prueba elemental completa de
$\forall x(x\;\mathsf{i}\;1=x)$ deberiamos agregar en la prueba anterior las
lineas correspondientes a la prueba de $\forall x\forall y(x\;\mathsf{s}%
\;y=y\rightarrow x\;\mathsf{i}\;y=x)$.

\bigskip

Dejamos al lector decribir tal como se viene haciendo desde el comienso de
esta seccion los conceptos de formula elemental de reticulados acotados y
prueba elemental de reticulados acotados.

\bigskip

\subsection{Grafos}

Un \textit{grafo} es un par $(G,r)$ donde $G$ es un conjunto no vacio, $r$ es
una relacion binaria sobre $G$ y se cumple que:

\begin{enumerate}
\item[-] $\forall x\forall y(r(x,y)\rightarrow r(y,x))$
\end{enumerate}

(notese que escribimos $r(x,y)$ para expresar que $(x,y)\in r$). Hay varias
presentaciones del concepto de grafo no dirigido pero el lector no tardara en
darse cuenta que estas estructuras son equivalentes a las que el haya
estudiado bajo el nombre de grafos no dirigidos.

\bigskip

\subsubsection{Formulas elementales de grafos}

Las formulas elementales de grafos son aquellas que se pueden construir en
forma finitaria usando simbolos de la siguiente lista:

\begin{enumerate}
\item[ ] $\forall\ \exists\;\lnot\;\vee\;\wedge\;\rightarrow\;\leftrightarrow
\;(\;)\;,\;=r$

\item[ ] Variables: $x,y,z,w,...$

\item[ ] Nombres para elementos fijos: $a,b,c,d,...$
\end{enumerate}

Algunos ejemplos:

\begin{enumerate}
\item[ ] $r(x,y)$

\item[ ] $r(a,z)$

\item[ ] $\forall yr(a,y)$

\item[ ] $\forall x\forall y\forall z\;((r(x,y)\wedge r(y,z))\rightarrow
r(x,z))$

\item[ ] $\exists x\forall z\lnot r(x,z)$

\item[ ] $\forall x\exists y(r(x,y)\wedge\forall z\ r(y,z))$

\item[ ] $\forall x\forall y\forall z\;((x=y\wedge y=z)\rightarrow x=z)$
\end{enumerate}

\bigskip

\subsubsection{Pruebas elementales de grafos}

Llamaremos \textit{pruebas elementales de grafos }a aquellas pruebas que
partiendo del axioma $\forall x\forall y(r(x,y)\rightarrow r(y,x))$ demuestran
cierta sentencia elemental de grafos usando en el camino solo sentencias
elementales de grafos y ciertas aclaraciones minimas de castellano que sirven
para justificar los distintos pasos en forma obvia. Es dificil encontrar
pruebas elementales de grafos que no sean complicadas. Aceptando cierto grado
de complejidad hay muchas. Un dato interesante es que el conjunto de
sentencias elementales de grafos que tienen una prueba elemental es no
recursivo, es decir no hay un procedimiento efectivo que decida si una
sentencia elemental de grafos dada tiene una prueba elemental. Esto habla
acerca de cuan complicada puede ser la estructura o fisonomia de las
sentencias elementales de grafos que pueden ser probadas elementalmente.

\bigskip

\subsection{Median algebras}

Una \textit{median algebra} es un par $(A,M)$ donde $A$ es un conjunto no
vacio, $M$ es una operacion $3$-aria sobre $A$ (i.e. $M:A^{3}\rightarrow A$) y
se cumplen:

\begin{enumerate}
\item $\forall x\forall y\forall z(M(x,y,z)=M(x,z,y))$

\item $\forall x\forall y\forall z(M(x,y,z)=M(y,z,x))$

\item $\forall x\forall y(M(x,x,y)=x)$

\item $\forall x\forall y\forall z\forall u\forall
v(M(M(x,y,z),u,v))=M(x,M(y,u,v),M(z,u,v)))$
\end{enumerate}

Por ejemplo si tomamos un reticulado $(L,\mathsf{s},\mathsf{i})$ y definimos
$M(x,y,z)=(x\;\mathsf{i\;}y)\;\mathsf{s\;}(x\;\mathsf{i\;}z)\;\mathsf{s\;}%
(y\;\mathsf{i\;}z)$, para cada $x,y,z\in L$, entonces $(L,M)$ es una median algebra.

\bigskip

\subsubsection{Formulas elementales de median algebras}

Las formulas elementales de median algebras son aquellas que se pueden
construir en forma finitaria usando simbolos de la siguiente lista:

\begin{enumerate}
\item[ ] $\forall\ \exists\;\lnot\;\vee\;\wedge\;\rightarrow\;\leftrightarrow
\;(\;)\;,\;=M$

\item[ ] Variables: $x,y,z,w,...$

\item[ ] Nombres para elementos fijos: $a,b,c,d,...$
\end{enumerate}

Algunos ejemplos:

\begin{enumerate}
\item[ ] $M(x,y,z)=a$

\item[ ] $M(a,b,M(M(M(x,y,z),u,v),x,a)=M(y,u,a)$

\item[ ] $\forall y(M(x,y,z)=z)$

\item[ ] $\forall x\exists y(M(x,y,y)=y\rightarrow\forall
z\ M(a,y,z)=M(x,x,x))$

\item[ ] $\exists x\forall z\lnot(M(a,a,a)=a\vee\exists z(M(a,y,z)=b))$
\end{enumerate}

\bigskip

\subsubsection{Pruebas elementales de median algebras}

Llamaremos \textit{pruebas elementales de median algebras }a aquellas pruebas
que partiendo de los axiomas de median algebras demuestran cierta sentencia
elemental de median algebras usando en el camino solo sentencias elementales
de median algebras y ciertas aclaraciones minimas de castellano que sirven
para justificar los distintos pasos en forma obvia. Es dificil encontrar
pruebas elementales de median algebras que no sean complicadas. Un ejemplo:

\bigskip

\begin{proof}
[Prueba elemental de $\forall x\forall y(M(x,y,y)=y)$:]Sean $a,b\in A$ fijos.
Por el axioma $\forall x\forall y\forall z(M(x,y,z)=M(y,z,x))$ tenemos que%
\[
M(a,b,b)=M(b,b,a)
\]
Por el axioma $\forall x\forall y(M(x,x,y)=x)$ tenemos que%
\[
M(b,b,a)=b
\]
O sea que%
\[
M(a,b,b)=b
\]
Ya que $a,b$ eran cualesquiera, hemos probado que $\forall x\forall
y(M(x,y,y)=y)$
\end{proof}

\bigskip

\subsection{Grafos bicoloreados}

Recordemos que dado un grafo $(G,r)$, un \textit{coloreo de }$(G,r)$ es una
asignacion de colores a cada elemento de $G$ de manera que nunca dos elementos
de $G$ que esten reacionados tengan el mismo color. En el caso que solo usemos
dos colores, le llamaremos un \textit{bicoloreo de }$(G,r)$. Notese que un
bicoloreo puede ser representado con un subconjunto de $G$. Por ejemplo si el
bicoloreo coloreaba a los elementos de $G$ con dos colores, verde y rojo,
podemos tomar $R=\{g\in G:g$ es rojo$\}$ y esto determina nuestro bicoloreo ya
que $G-R$ sera justamente el conjunto de elemenos verdes. O sea que
matematicamente hablando podemos dar la siguiente definicion. Un
\textit{bicoloreo de }$(G,r)$ es un subconjunto $R$ de $G$ el cual cumple que
cualesquiera sean $x,y\in G$ se tiene que%
\[
\text{si }(x,y)\in r\text{, entonces }x\in R\text{ si y solo si }y\notin R
\]
Esto nos inspira para dar la definicion de un nuevo tipo de estructura.

Un \textit{grafo bicoloreado} es una terna $(G,r,R)$, donde $(G,r)$ es un
grafo y $R$ es un bicoloreo de $(G,r)$. Algunos ejemplos:

\begin{enumerate}
\item[-] $(\{1,2\},\{(1,2),(2,1)\},\{1\})$ es un grafo bicoloreado

\item[-] Tomemos%
\begin{align*}
G  & =\omega\\
r  & =\{(x,x+1):x\in\omega\}\cup\{(x+1,x):x\in\omega\}\\
R  & =\{x\in\omega:x\text{ es par}\}
\end{align*}
Entonces $(G,r,R)$ es un grafo bicoloreado

\item[-] $(\{1,2,3,4\},\emptyset,R)$ es un grafo bicoloreado, cualesquera sea
$R\subseteq\{1,2,3,4\}$
\end{enumerate}

\bigskip

\subsubsection{Formulas elementales de grafos bicoloreados}

Para escribir las formulas elementales de grafos bicoloreados, pensaremos a
$R$ como una "relacion unaria" y escribiremos $R(x)$ para expresar que $x\in R
$. Asi como cuando escribimos $r(x,y)$ pensamos "$x$ e $y$ estan conectados",
cuando escribamos $R(x)$ pensaremos "$x$ es rojo". Esto hace mas lejibles
nuestras expresiones.

Las formulas elementales de grafos bicoloreados son aquellas que se pueden
construir en forma finitaria usando simbolos de la siguiente lista:

\begin{enumerate}
\item[ ] $\forall\ \exists\;\lnot\;\vee\;\wedge\;\rightarrow\;\leftrightarrow
\;(\;)\;,\;=r\ \ R$

\item[ ] Variables: $x,y,z,w,...$

\item[ ] Nombres para elementos fijos: $a,b,c,d,...$
\end{enumerate}

Algunos ejemplos:

\begin{enumerate}
\item[ ] $R(a)\wedge r(x,y)$

\item[ ] $\exists z(r(a,z)\rightarrow R(z))$

\item[ ] $\forall yr(a,y)$

\item[ ] $\forall x\forall y((R(x)\wedge R(y))\rightarrow x=y)$

\item[ ] $\exists x\forall z(\lnot R(z)\rightarrow r(x,z))$

\item[ ] $\forall x\forall y(r(x,y)\rightarrow(R(x)\leftrightarrow\lnot
R(y)))$

\item[ ] $\forall x\forall y\forall z\;((x=y\wedge y=z)\rightarrow x=z)$
\end{enumerate}

\bigskip

\subsubsection{Pruebas elementales de grafos bicoloreados}

Notese que los grafos bicoloreados pueden axiomatizarse con las sentencias elementales:

\begin{enumerate}
\item $\forall x\forall y(r(x,y)\rightarrow r(y,x))$

\item $\forall x\forall y(r(x,y)\rightarrow(R(x)\leftrightarrow\lnot R(y))) $
\end{enumerate}

Llamaremos \textit{pruebas elementales de grafos bicoloreados }a aquellas
pruebas que partiendo de los dos axiomas anteriores demuestran cierta
sentencia elemental de grafos bicoloreados usando en el camino solo sentencias
elementales de grafos bicoloreados y ciertas aclaraciones minimas de
castellano que sirven para justificar los distintos pasos en forma obvia. Es
dificil encontrar pruebas elementales de grafos bicoloreados que no sean
complicadas. Un par de ejemplos simples:

\bigskip

\begin{proof}
[Prueba elemental de $\forall x\lnot r(x,x)$:]Sea $a\in G$ fijo. Supongamos
$r(a,a)$. Por el axioma $\forall x\forall y(r(x,y)\rightarrow
(R(x)\leftrightarrow\lnot R(y)))$ tenemos que $(r(a,a)\rightarrow
(R(a)\leftrightarrow\lnot R(a))$. O sea que tenemos que%
\[
R(a)\leftrightarrow\lnot R(a)
\]
lo cual claramente es un absurdo. El absurdo proviene de suponer $r(a,a)$ lo
cual nos dice que $\lnot r(a,a)$. Ya que $a$ era arbitrario, hemos probado que
$\forall x\lnot r(x,x)$.
\end{proof}

\bigskip

\begin{proof}
[Prueba elemental de $\forall x\forall y\forall z\lnot(r(x,y)\wedge
r(x,z)\wedge r(y,z))$:]Sean $a,b,c\in G$ fijos. Supongamos $r(a,b)\wedge
r(a,c)\wedge r(b,c)$. Supongamos que $R(a)$. Ya que $r(a,b)$ el axioma
$\forall x\forall y(r(x,y)\rightarrow(R(x)\leftrightarrow\lnot R(y)))$ nos
dice que se da $R(a)\leftrightarrow\lnot R(b)$, de lo cual sacamos que $\lnot
R(b)$. Ya que $r(a,c)$ el mismo axioma nos dice que se da $R(a)\leftrightarrow
\lnot R(c)$, de lo cual sacamos que $\lnot R(c)$. Ya que $r(b,c)$ el mismo
axioma nos dice que $R(b)\leftrightarrow\lnot R(c)$, de lo cual sacamos $R(b)$
(ya que se daba $\lnot R(c)$). O sea que probamos que se da $R(b)\wedge\lnot
R(b)$, lo cual es absurdo. El absurdo proviene de suponer que se daba $R(a)$,
por lo cual hemos probado que se da $\lnot R(a)$. Ya que $r(a,b)$ el axioma
$\forall x\forall y(r(x,y)\rightarrow(R(x)\leftrightarrow\lnot R(y)))$ nos
dice que se da $R(a)\leftrightarrow\lnot R(b)$, de lo cual sacamos que $R(b) $
(ya que se da $\lnot R(a)$). Ya que $r(a,c)$ el mismo axioma nos dice que se
da $R(a)\leftrightarrow\lnot R(c)$, de lo cual sacamos que $R(c)$. O sea que
se da $R(b)\wedge R(c)$. Ya que $r(b,c)$ el mismo axioma nos dice que
$R(b)\leftrightarrow\lnot R(c)$, lo cual contradice $R(b)\wedge R(c)$. El
absurdo proviene de suponer que se daba $r(a,b)\wedge r(a,c)\wedge r(b,c)$ por
lo cual hemos probado que $\lnot(r(a,b)\wedge r(a,c)\wedge r(b,c))$. Ya que
$a,b,c$ eran arbitrarios, hemos probado que $\forall x\forall y\forall
z\lnot(r(x,y)\wedge r(x,z)\wedge r(y,z))$.
\end{proof}

\bigskip

\section{Logica matematica}

En la Seccion \ref{Seccion estructuras algebraicas ordenadas} nos focalizamos
en aprender algebra con la intencion de volvernos lo mas "algebristas
profecionales" que podamos. Para esto fuimos exigentes a la hora de delimitar
y manejar nuestro lenguaje matematico y tambien a la hora de hacer pruebas
pusimos mucha atencion en hacerlas "perfectas" en el sentido de que sean
similares a las que haria un algebrista formado.

Pero para que hicimos esto? Muy simple: la logica matematica es
\textit{matematica aplicada} al estudio de los matematicos, su lenguaje y sus
metodos de demostracion, y que mas comodo para hacer logica matematica que
contar con un matematico dentro de uno mismo para estudiarlo! Tal como

\begin{enumerate}
\item[-] un biologo estudia la estructura y funcionamiento de los seres vivos

\item[-] un astronomo estudia los cuerpos celestes

\item[-] un fisico estudia la materia y su comportamiento
\end{enumerate}

\noindent un logico matematico estudia con herramientas matematicas a los
mismos matematicos en cuanto a sus caracteristicas en su roll haciendo
matematica. Es decir nos interesa dar un modelo matematico que describa en
forma matematica precisa el funcionamiento de un matematico en cuanto a su
lenguaje y sus metodos de demostracion. Pero algo debe quedar muy claro:
haremos matematica aplicada, es decir, no es nuestra intencion decirle a un
matematico como debe razonar! Todo lo contrario, sabemos que los matematicos
profecionales actuales razonan correctamente y que su estilo de prueba es
correcto, dado el avansado estado actual de la disciplina y de sus
profecionales. Simplemente los estudiaremos con herramientas matematicas para
tratar de dar una descripcion matematica de su lenguaje y de sus metodos de demostracion.

Por supuesto hacer logica matematica puede ser muy dificil o escurridizo ya
que como todos sabemos los matematicos tienen metodos dificiles de entender y
un lenguaje verdaderamente complicado.

La forma en la que encararemos el problema sera la siguiente. En lugar de
estudiar a un matematico en su actividad real crearemos un "contexto
matematico simplificado" en el cual tambien tenga sentido hacer matematica
profecional y luego estudiaremos a un matematico haciendo matematica en este
contexto. Por supueto esto baja mucho el nivel de nuestra ambicion cientifica
como logicos matematicos ya que en lugar de estudiar a los matematicos en su
vida real, los estudiaremos en un contexto simplificado. Sin envargo como
veremos mas adelante nuestra simplificacion no nos hara perder generalidad y
los resultados obtenidos daran un modelo matematico del quehacer matematico
real. Este hecho es uno de los logros mas importantes de la ciencia moderna.

Para crear este "contexto matematico simplificado" nos serviran los conceptos
de lenguaje elemental y prueba elemental. Mas concretamente fijaremos un tipo
de estructura, por ejemplo los reticulados cuaterna, y estudiaremos a un
matematico profecional haciendo matematica en este contexto elemental. Es
decir le pediremos que realice pruebas de propiedades que valgan en todos los
reticulados cuaterna pero lo restringirenos en su lenguaje, es decir le
pediremos que se restrinja a usar solo formulas elementales de reticulados
cuaterna y que las pruebas que realice sean tambien elementales de reticulados
cuaterna. El matematico rapidamente entendera la consigna y posiblemente
refunfu\~{n}e un poco porque claramente lo estamos restrinjiendo mucho en
relacion a su manera de hacer matematica (por ejemplo no podra hablar de
filtros primos, etc). De todas maneras las posibilidades de hacer matematica
profunda e interesante aun con esta restriccion son inmensas, es decir hay
verdades de reticulados cuaterna que son elementales en enunciado y prueba
pero son extremadamente dificiles, ingeniosas y profundas.

En este proyecto de hacer logica matematica estudiando a un matematico
haciendo matematica elemental de reticulados cuaterna hay varias cosas para
hacer y las establecemos a continuacion.

\bigskip

\textbf{Programa de logica sobre reticulados cuaterna}

\begin{enumerate}
\item[-] Dar una definicion matematica del concepto de formula elemental de
reticulados cuaterna

\item[-] Dar una definicion matematica de cuando una formula elemental es
verdadera en un reticulado cuaterna dado para una asignacion dada de valores
de dicho reticulado a las variables libres y a los nombres de constantes fijas
de la formula

\item[-] (Plato gordo) Dar un modelo matematico del concepto de prueba
elemental de reticulados cuaterna. A estos objetos matematicos que modelizaran
a las pruebas elementales de los matematicos los llamaremos pruebas formales
de reticulados cuaterna.

\item[-] (Sublime) Intentar probar matematicamente que nuestro concepto de
prueba es una correcta modelizacion matematica de la idea intuitiva de prueba
elemental que todo matematico profecional posee.
\end{enumerate}

Como veremos, los cuatro puntos anteriores pueden ser hechos
satisfactoriamente y constituyen el comienzo de la logica matematica con
cuantificadores. Cabe aclarar que la realizacion del cuarto punto es realmente
sorprendente ya que es un caso de una prueba matematica rigurosa de un hecho
que involucra un concepto intuitivo como lo es el de prueba elemental

$\bigskip$

Ya que la realizacion de los 4 puntos anteriores no depende en absoluto de que
hayamos elejido el tipo de estructura de los reticulados cuaterna (es decir,
el desarrollo que resuelve los 4 puntos anteriores para los reticulados
cuaterna puede adaptarse facilmente para cualquiera de los otros tipos de
estructuras descriptos en la Seccion
\ref{Estructuras y su lenguaje elemental asociado}), haremos las cosas con mas generalidad.

Primero, basados en la Seccion
\ref{Estructuras y su lenguaje elemental asociado}, generalizaremos el
concepto de estructura. La generalizacion que daremos del concepto de
estructura es realmente muy amplia y nos llevara mucho trabajo de
entrenamiento poder manejarla con madurez y naturalidad. Luego, estableceremos
para un tipo generico de estructura el programa de logica arriba escrito para
el caso particular de los reticulados cuaterna. En las subsiguientes secciones
nos dedicaremos a resolver dicho programa general.

\bigskip

\subsection{Tipos}

Para generalizar el concepto de estructura es conveniente primero dar
definiciones generales de los conceptos de operacion y de relacion sobre un conjunto.

Sea $A$ un conjunto y sea $n\in\mathbf{N}$. Por una \textit{operacion }%
$n$\textit{-aria sobre }$A$ entenderemos una funcion cuyo dominio es $A^{n}$ y
cuya imagen esta contenida en $A$. Por una \textit{relacion }$n$\textit{-aria
sobre }$A$ entenderemos un subconjunto de $A^{n}$. Notar que por la definicion
anterior una relacion unaria sobre $A$ no es ni mas ni menos que un
subconjunto de $A$.

Como venimos viendo hay una variedad de tipos de estructuras las cuales tienen
un sentido o interes matematico claro y todas son de un formato similar, a
saber uplas formadas por una primera coordenada que es un conjunto no vacio
(llamado el universo de la estructura) y luego ciertas relaciones, operaciones
y elementos distinguidos, dependiendo del caso. Otra cosa a notar es que para
cada tipo de estructura hay ciertos simbolos fijos que usamos en forma
generica para denotar sus relaciones, operaciones y elementos distinguidos.
Por ejemplo:

\begin{enumerate}
\item[-] Para los posets usamos el simbolo $\leq$ para denotar la relacion de
orden parcial en un sentido generico.

\item[-] Para el caso de los reticulados terna usamos en forma generica los
simbolos $\mathsf{s}$ e $\mathsf{i}$ para denotar sus operaciones supremo e infimo

\item[-] Para el caso de los reticulados acotados usamos en forma generica los
simbolos $\mathsf{s}$ e $\mathsf{i}$ para denotar sus operaciones supremo e
infimo y los numerales $0$ y $1$ para denotar sus elementos distinguidos, a
saber maximo y minimo respectivamente.

\item[-] Para el caso de los reticulados complementados usamos en forma
generica los simbolos $\mathsf{s}$ e $\mathsf{i}$ para denotar sus operaciones
supremo e infimo, el simbolo $c$ para denotar su operacion $1$-aria de
complementacion y los numerales $0$ y $1$ para denotar sus elementos
distinguidos, a saber maximo y minimo respectivamente.

\item[-] Para el caso de los reticulados cuaterna usamos en forma generica los
simbolos $\mathsf{s}$ e $\mathsf{i}$ para denotar sus operaciones supremo e
infimo y el simbolo $\leq$ para denotar su orden parcial

\item[-] Para las median algebras usamos genericamente el simbolo $M$ para
denotar su operacion $3$-aria.

\item[-] Para los grafos usamos el simbolo $r$ para denotar en forma generica
la relacion binaria del grafo en cuestion.

\item[-] Para los grafos bicoloreados usamos el simbolo $r$ para denotar en
forma generica la relacion binaria del grafo y el simbolo $R$ para denotar la
relacion unaria que determina el bicoloreo\bigskip
\end{enumerate}

O sea que para cada tipo de estructuras se distinguen tres conjuntos de simbolos:

\begin{enumerate}
\item[-] un conjunto $\mathcal{R}$ formado por los simbolos que denotaran
genericamente las relaciones de las estructuras

\item[-] un conjunto $\mathcal{F}$ formado por los simbolos que denotaran
genericamente las operaciones de las estructuras

\item[-] un conjunto $\mathcal{C}$ formado por los simbolos que denotaran
genericamente los elementos distinguidos de las estructuras
\end{enumerate}

Ademas otra informacion importante que se tiene para cada tipo de estructura
es la aridad de las operaciones que denotan los simbolos de $\mathcal{F}$ y la
aridad de las relaciones que denotan los simbolos de $\mathcal{R}$. A esto lo
representaremos con una funcion $a:\mathcal{F}\cup\mathcal{R}\rightarrow
\mathbf{N}$ la cual le asocia a cada simbolo de $\mathcal{F}\cup\mathcal{R}$
la aridad del objeto que denota.

Ejemplos:

\begin{enumerate}
\item[-] Posets: $\mathcal{C}=\emptyset\ \ \ \ \ \mathcal{F}=\emptyset
\ \ \ \ \ \mathcal{R}=\{\leq\}\ \ \ \ a=\{(\leq,2)\}$

\item[-] Reticulados terna: $\mathcal{C}=\emptyset\ \ \ \ \ \mathcal{F}%
=\{\mathsf{s},\mathsf{i}\}\ \ \ \ \ \mathcal{R}=\emptyset
\ \ \ \ \ a=\{(\mathsf{s},2),(\mathsf{i},2)\}$

\item[-] Reticulados acotados: $\mathcal{C}=\{0,1\}\ \ \ \ \ \mathcal{F}%
=\{\mathsf{s},\mathsf{i}\}\ \ \ \ \ \mathcal{R}=\emptyset
\ \ \ \ a=\{(\mathsf{s},2),(\mathsf{i},2)\}$

\item[-] Reticulados comp.: $\mathcal{C}=\{0,1\}\ \ \ \ \ \mathcal{F}%
=\{\mathsf{s},\mathsf{i},c\}\ \ \ \ \ \mathcal{R}=\emptyset
\ \ \ \ a=\{(\mathsf{s},2),(\mathsf{i},2),(c,1)\}$

\item[-] Reticulados cuaterna: $\mathcal{C}=\emptyset\ \ \ \ \ \mathcal{F}%
=\{\mathsf{s},\mathsf{i}\}\ \ \ \ \ \mathcal{R}=\{\leq
\}\ \ \ \ a=\{(\mathsf{s},2),(\mathsf{i},2),(\leq,2)\}$

\item[-] Median algebras: $\mathcal{C}=\emptyset\ \ \ \ \ \mathcal{F}%
=\{M\}\ \ \ \ \ \mathcal{R}=\emptyset\ \ \ \ a=\{(M,3)\}$

\item[-] Grafos: $\mathcal{C}=\emptyset\ \ \ \ \ \mathcal{F}=\emptyset
\ \ \ \ \ \mathcal{R}=\{r\}\ \ \ \ a=\{(r,2)\}$

\item[-] Grafos bicoloreados: $\mathcal{C}=\emptyset\ \ \ \ \ \mathcal{F}%
=\emptyset\ \ \ \ \ \mathcal{R}=\{r,R\}\ \ \ \ a=\{(r,2),(R,1)\}$
\end{enumerate}

Por supuesto aqui es muy importante no confundir los simbolos con las
operaciones que eventualmente ellos denotan. O sea en todos los ejemplos
anteriores los elementos de $\mathcal{C}$, $\mathcal{F}$ y $\mathcal{R}$ son
simbolos, es decir su $Ti$ es PALABRA. Es decir, segun el contexto si
escribimos $\leq$, puede ser que nos estemos refiriendo a un orden parcial o
simplemente al mismo simbolo $\leq$

Lo anterior motiva la siguiente definicion de tipo (de estructura). Antes de
darla recordemos que si $\alpha,\beta$ son palabras cualesquiera, decimos que
$\alpha$ \textit{es subpalabra (propia) de }$\beta$ cuando ($\alpha
\notin\{\varepsilon,\beta\}$ y) existen palabras $\delta,\gamma$ tales que
$\beta=\delta\alpha\gamma$.

Ahora si, nuestra definicion de tipo:

\bigskip

Por un \textit{tipo} (\textit{de primer orden}) entenderemos una 4-upla
$\tau=(\mathcal{C},\mathcal{F},\mathcal{R},a)$ tal que:

\begin{enumerate}
\item[(1)] Hay alfabetos finitos $\Sigma_{1}$, $\Sigma_{2}$ y $\Sigma_{3}$ tales:

\begin{enumerate}
\item $\mathcal{C}\subseteq\Sigma_{1}^{+}$, $\mathcal{F}\subseteq\Sigma
_{2}^{+}$ y $\mathcal{R}\subseteq\Sigma_{3}^{+}$

\item $\Sigma_{1}$, $\Sigma_{2}$ y $\Sigma_{3}$ son disjuntos de a pares.

\item $\Sigma_{1}\cup\Sigma_{2}\cup\Sigma_{3}$ no contiene ningun simbolo de
la lista

$\forall\ \exists\;\lnot\;\vee\;\wedge\;\rightarrow\;\leftrightarrow
\;(\;)\;,\;\equiv\mathsf{X\;}\mathit{0}\;\mathit{1\;}...\;\mathit{9}%
\;\mathbf{0}\;\mathbf{1}\ ...\;\mathbf{9}$
\end{enumerate}

\item[(2)] $a:\mathcal{F}\cup\mathcal{R}\rightarrow\mathbf{N}$ es una funcion
que a cada $p\in\mathcal{F}\cup\mathcal{R}$ le asocia un numero natural
$a(p)$, llamado la \textit{aridad} de $p$.

\item[(3)] Ninguna palabra de $\mathcal{C}$ (resp. $\mathcal{F}$,
$\mathcal{R}$) es subpalabra propia de otra palabra de $\mathcal{C}$ (resp.
$\mathcal{F} $, $\mathcal{R}$).
\end{enumerate}

\bigskip

Notese que los elementos de $\mathcal{C}$, $\mathcal{F}$ y $\mathcal{R}$
pueden ser palabras y no solo simbolos como en los casos de los tipos de
estructuras conocidas. Mas adelante cuando definamos las \textit{formulas de
tipo }$\tau$ se entenderan las restricciones puestas en c. de (1) y en (3).

A los elementos de $\mathcal{C}$ (resp. $\mathcal{F}$, $\mathcal{R}$) los
llamaremos \textit{nombres de constante }(resp. \textit{nombres de funcion,
nombres de relacion}) \textit{de tipo} $\tau$. Para cada $n\in\mathbf{N}$,
definamos%
\begin{align*}
\mathcal{F}_{n}  & =\{f\in\mathcal{F}:a(f)=n\}\\
\mathcal{R}_{n}  & =\{r\in\mathcal{R}:a(r)=n\}
\end{align*}
Al tipo $(\emptyset,\emptyset,\{\leq\},\{(\leq,2)\})$ lo llamaremos el
\textit{tipo de los posets}. Al tipo $(\emptyset,\{\mathsf{s},\mathsf{i}%
\},\emptyset,\{(\mathsf{s},2),(\mathsf{i},2)\})$ lo llamaremos el \textit{tipo
de los reticulados terna}. Al tipo%
\[
(\{0,1\},\{\mathsf{s},\mathsf{i}\},\emptyset,\{(\mathsf{s},2),(\mathsf{i}%
,2)\})
\]
lo llamaremos el \textit{tipo de los reticulados acotados}. Al tipo%
\[
(\{0,1\},\{\mathsf{s},\mathsf{i},c\},\emptyset,\{(\mathsf{s},2),(\mathsf{i}%
,2),(c,1)\})
\]
lo llamaremos el \textit{tipo de los reticulados complementados}. Al tipo%
\[
(\emptyset,\{\mathsf{s},\mathsf{i}\},\{\leq\},\{(\mathsf{s},2),(\mathsf{i}%
,2),(\leq,2)\})
\]
lo llamaremos el \textit{tipo de los reticulados cuaterna}. Al tipo
$(\emptyset,\{M\},\emptyset,\{(M,3)\})$ lo llamaremos el \textit{tipo de las
median algebras}. Al tipo $(\emptyset,\emptyset,\{r\},\{(r,2)\})$ lo
llamaremos el \textit{tipo de los grafos}. Al tipo%
\[
(\emptyset,\emptyset,\{r,R\},\{(r,2),(R,1)\})
\]
lo llamaremos el \textit{tipo de los grafos bicoloreados.}

Algunos ejemplos de tipos:

\begin{enumerate}
\item $(\{\mathrm{uno},\mathrm{doli}\},\{\mathrm{MAS},\mathrm{P}%
\},\{\mathrm{Her}\},a)$, con $a$ dado por $a(\mathrm{MAS})=4$, $a(\mathrm{P}%
)=1$ y $a(\mathrm{Her})=3$.

\item $(\{0,1\},\{+,\times\},\emptyset,a)$, con $a$ dado por $a(+)=2$ y
$a(\times)=2$.

\item $(\{\square\},\{\clubsuit\clubsuit,\mathrm{Pic}\},\{\vartriangleright
,\Vert\},a)$, con $a$ dado por $a(\clubsuit\clubsuit)=6$, $a(\mathrm{Pic})=1$,
$a(\vartriangleright)=4$ y $a(\Vert)=1$

\item $(\{\mathrm{dod},\mathrm{dood},\mathrm{doood},...\},\{\mathrm{Fu}%
\},\{\mathrm{He}\},a)$, con $a$ dado por $a(\mathrm{Fu})=1$ y $a(\mathrm{He}%
)=3$. Notese que este tipo tiene infinitos nombres de constante.
\end{enumerate}

\bigskip

\subsection{Estructuras de tipo $\tau$}

Ahora si estamos en condiciones de dar una definicion general de estructura.
Daremos una definicion matematica de "Estructura de tipo $\tau$". En virtud de
nuestras estructuras conocidas uno podria intentar definir estructura de tipo
$\tau$ como cierta $n$-upla pero esto trae problemas ya que en un tipo $\tau$
los nombres de $\mathcal{C}\cup\mathcal{F}\cup\mathcal{R}$ no tienen por que
estar ordenados. De todas maneras la idea es muy similar y nos aproximaremos
primero con ejemplos para entender mas facilmente el concepto.

Sea $\tau$ el tipo%
\[
(\{\mathrm{uno},\mathrm{doli}\},\{\mathrm{MAS},\mathrm{P}\},\{\mathrm{Her}%
\},\{(\mathrm{MAS},4),(\mathrm{P},1),(\mathrm{Her},3)\})
\]
Intuitivamente hablando, una estructura de tipo $\tau$ consiste en un conjunto
no vacio $A$ (que se llamara el universo de dicha estructura) junto con una
interpretacion de cada uno de los nombres del conjunto $\{\mathrm{uno}%
,\mathrm{doli},\mathrm{MAS},\mathrm{P},\mathrm{Her}\}$. Esta interpretacion
debe asignarle a

\begin{enumerate}
\item[-] $\mathrm{uno}$ un elemento de $A$

\item[-] $\mathrm{doli}$ un elemento de $A$

\item[-] $\mathrm{MAS}$ una operacion 4-aria sobre $A$

\item[-] $\mathrm{P}$ una operacion 1-aria sobre $A$

\item[-] $\mathrm{Her}$ una relacion 3-aria sobre $A$
\end{enumerate}

\bigskip

Lo que debe quedar claro es que estos elementos, operaciones y relaciones
pueden ser cualesquiera, es decir no deben cumplir nada en especial. Por
ejemplo si tomamos $\mathbf{R}$ como universo podemos interpretar

\begin{enumerate}
\item[-] $\mathrm{uno}$ como el numero $\pi$

\item[-] $\mathrm{doli}$ como el numero $0$

\item[-] $\mathrm{MAS}$ como la operacion%
\[%
\begin{array}
[c]{rcl}%
\mathbf{R}^{4} & \rightarrow & \mathbf{R}\\
(x,y,z,w) & \rightarrow & 2x+4y
\end{array}
\]


\item[-] $\mathrm{P}$ como la operacion%
\[%
\begin{array}
[c]{rcl}%
\mathbf{R} & \rightarrow & \mathbf{R}\\
x & \rightarrow & 5^{x}%
\end{array}
\]


\item[-] $\mathrm{Her}$ como la relacion%
\[
\{(x,y,z)\in\mathbf{R}^{3}:x.y.z=9\}
\]

\end{enumerate}

Analogamente, si $\tau$ es el tipo de los posets, es decir $\tau
=(\emptyset,\emptyset,\{\leq\},\{(\leq,2)\})$, una estructura de tipo $\tau$
consistira de un conjunto no vacio $A$ (que se llamara el universo de dicha
estructura) junto con una interpretacion del simbolo $\leq$, la cual nos dira
que relacion binaria sobre $A$ denotara $\leq$. Pero esta relacion binaria
puede ser cualquiera por lo cual habra muchas estructuras del tipo de los
posets que no se corresponderan con posets. Solo aquellas en las que $\leq$ se
interpreta como un orden parcial sobre su universo se corresponderan con los posets.

Ahora si daremos la definicion matematica de estructura de tipo $\tau$. Sea
$\tau$ un tipo. Una \textit{estructura o modelo de tipo }$\tau$ sera un par
$\mathbf{A}=(A,i)$ tal que:

\begin{enumerate}
\item[(1)] $A$ es un conjunto no vacio

\item[(2)] $i$ es una funcion con dominio $\mathcal{C}\cup\mathcal{F}%
\cup\mathcal{R},$ tal que:

\begin{enumerate}
\item[(a)] $i(c)$ es un elemento de $A$, para cada $c\in\mathcal{C}$

\item[(b)] $i(f)$ es una operacion $n$-aria sobre $A$, para cada
$f\in\mathcal{F}_{n}$, $n\geq1$

\item[(c)] $i(r)$ es una relacion $n$-aria sobre $A$, para cada $r\in
\mathcal{R}_{n}$, $n\geq1$
\end{enumerate}
\end{enumerate}

\bigskip

Si $\mathbf{A}=(A,i)$ es una estructura de tipo $\tau$, el conjunto $A$ es
llamado el \textit{universo }de $\mathbf{A}$ y la funcion $i$ es llamada la
\textit{funcion interpretacion} de $\mathbf{A}$. Si $s\in\mathcal{C}%
\cup\mathcal{F}\cup\mathcal{R}$, diremos que $i(s)$ es la interpretacion del
simbolo $s$ en $\mathbf{A}$. Algunos ejemplos:

\bigskip

\begin{enumerate}
\item[(E1)] Si $\tau$ es el tipo%
\[
(\{\mathrm{uno},\mathrm{doli}\},\{\mathrm{MAS},\mathrm{P}\},\{\mathrm{Her}%
\},\{(\mathrm{MAS},4),(\mathrm{P},1),(\mathrm{Her},3)\})
\]
entonces $(\mathbf{R},i)$ es una estructura de tipo $\tau$, si definimos $i$
igual a la funcion con dominio $\{\mathrm{uno},\mathrm{doli},\mathrm{MAS}%
,\mathrm{P},\mathrm{Her}\}$ dada por

\begin{enumerate}
\item $i(\mathrm{uno})=\pi$

\item $i(\mathrm{doli})=0$

\item $i(\mathrm{MAS})$ igual a la operacion%
\[%
\begin{array}
[c]{rcl}%
\mathbf{R}^{4} & \rightarrow & \mathbf{R}\\
(x,y,z,w) & \rightarrow & 2x+4y
\end{array}
\]


\item $i(\mathrm{P})$ igual a la operacion%
\[%
\begin{array}
[c]{rcl}%
\mathbf{R} & \rightarrow & \mathbf{R}\\
x & \rightarrow & 5^{x}%
\end{array}
\]


\item $i(\mathrm{Her})=\{(x,y,z)\in\mathbf{R}^{3}:x.y.z=9\}$
\end{enumerate}

\item[(E2)] Sea $\tau=(\emptyset,\emptyset,\{\leq\},\{(\leq,2)\})$. Notese que
por definicion una estructura de tipo $\tau$ es un par $(A,i)$ donde $A$ es un
conjunto no vacio y $i$ es una funcion con dominio $\{\leq\} $ tal que
$i(\leq)$ es una relacion binaria sobre $A$. Algunos ejemplos de estructuras
de tipo $\tau$:

\begin{enumerate}
\item $(\{1,2,3\},\{(\leq,\emptyset)\})$

\item $(\{1,2,3\},\{(\leq,\{2,3\}\times\{1\})\})$

\item $(\{1,\{2\},\emptyset\},\{(\leq,\{(1,\{2\})\})$

\item $(\mathbf{N},i)$, con $i$ dada por $i(\leq)=\{(1,2),(1000,1),(1,1)\}$
\end{enumerate}

Notese que aunque $\tau$ es llamado el tipo de los posets, ninguna de las
estructuras anteriores tiene mucho que ver con un poset. Consideremos ahora la
estructura $(\mathbf{N},i)$, donde $i$ es la funcion con dominio igual a
$\{\leq\}$ dada por%
\[
i(\leq)=\{(x,y)\in\mathbf{N}^{2}:x|y\}
\]
Notese que estrictamente hablando $(\mathbf{N},i)$ no es un poset ya que $i$
no es un orden parcial sobre $\mathbf{N}$ pero es claro que a nivel de
informacion $(\mathbf{N},i)$ y $(\mathbf{N},|)$ son la misma cosa. O sea que
aquellas estructuras de tipo $\tau$ en las cuales $\leq$ se interpreta como un
orden parcial sobre el universo de la estructura son "esencialmente posets".
Dejamos al lector dar una biyeccion entre el conjunto formado por todos los
posets y un subconjunto del conjunto de todas las estructuras de tipo $\tau$

\item[(E3)] Sea $\tau$ el tipo de los reticulados terna, es decir
$\tau=(\emptyset,\{\mathsf{s},\mathsf{i}\},\emptyset,\{(\mathsf{s}%
,2),(\mathsf{i},2)\})$. Entonces $(\mathbf{N},i)$, donde $i=\{(\mathsf{s}%
,\max),(\mathsf{i},\min)\}$, es una estructura de tipo $\tau$. Notese que
estrictamente hablando $(\mathbf{N},i)$ no es un reticulado terna ya que es
una $2$-upla y los reticulados ternas son $3$-uplas. Pero es claro que a nivel
de informacion $(\mathbf{N},i)$ y $(\mathbf{N},\max,\min)$ son la misma cosa.
Otras estructuras de tipo $\tau$ son por ejemplo:

\begin{enumerate}
\item $(\mathbf{R},\{(\mathsf{s},+),(\mathsf{i},\min)\})$

\item $(\{0,1,2\},\{(\mathsf{s},f),(\mathsf{i},g)\}$ donde $f:\{0,1,2\}^{2}%
\rightarrow\{0,1,2\}$ es la funcion constantemente 1 y $g:\{0,1,2\}^{2}%
\rightarrow\{0,1,2\}$ es la funcion constantemente 2
\end{enumerate}

Por supuesto, ninguna de las dos puede considerarse un reticulado terna ya que
en ambas los simbolos $\mathsf{s}$ y $\mathsf{i}$ no se interpretan como las
operaciones supremo e infimo provenientes de un orden parcial. Dejamos al
lector dar una biyeccion entre el conjunto formado por todos los reticulados
terna y un subconjunto del conjunto de todas las estructuras de tipo $\tau$

\item[(E4)] Sea $\tau$ el tipo de los grafos bicoloreados, es decir
$\tau=(\emptyset,\emptyset,\{r,R\},\{(r,2),(R,1)\})$. Entonces $(\{1,2\},i)$,
con $i=\{(r,\{(1,2),(2,1)\}),(R,\{1\})\}$, es una estructura de tipo $\tau$.
Notese que%
\[
(\{1,2\},i(r),i(R))=(\{1,2\},\{(1,2),(2,1)\},\{1\})
\]
es un grafo bicoloreado el cual esencialmente es lo mismo que la estructura
$(\{1,2\},i)$ (a nivel de informacion). De todas maneras estrictamente
hablando $(\{1,2\},i)$ no es un grafo bicoloreado. Otra estructura de tipo
$\tau$ la cual es "esencialmente" un grafo bicoloreado es el par $(\omega,i)
$, donde $i$ es la funcion con dominio $\{r,R\}$ dada por%
\begin{align*}
i(r)  & =\{(x,x+1):x\in\omega\}\cup\{(x+1,x):x\in\omega\}\\
i(R)  & =\{x\in\omega:x\text{ es par}\}
\end{align*}
Tal como en los otros ejemplos vistos, hay estructuras de tipo $\tau$ las
cuales no pueden considerarse grafos bicoloreados. Por ejemplo, la estructura
$(\mathbf{N},\{(r,\{(1,2)\}),(R,\{3\})\})$. Dejamos al lector dar una
biyeccion entre el conjunto formado por todos los grafos bicoloreados y un
subconjunto del conjunto de todas las estructuras de tipo $\tau$.
\end{enumerate}

\bigskip

\subsubsection{Independencia entre sintaxis y semantica}

Notese que la definicion de tipo es muy libre en lo que respecta a que
palabras componen los conjuntos $\mathcal{C}$, $\mathcal{F}$ y $\mathcal{R}$,
es decir salvo por ciertas restricciones leves, ellas pueden ser cualquier
palabra. Ademas no es necesario que las palabras de $\mathcal{C}%
\cup\mathcal{F}\cup\mathcal{R}$ se interpreten en la estructura de tipo $\tau$
(via la funcion $i$) como usualmente se interpretan en matematica. Algunos ejemplos:

\bigskip

\begin{enumerate}
\item $\tau=(\{\leq\},\emptyset,\emptyset,\emptyset)$ es un tipo y en las
estructuras de tipo $\tau$ el simbolo $\leq$ se interpretara como un elemento
del universo y no un orden parcial. Por ejemplo $(\{1,2,3\},\{(\leq,2)\})$ es
una estructura de tipo $\tau$.

\item $\tau^{\prime}=(\emptyset,\emptyset,\{\leq\},\{(\leq,3)\})$ es un tipo
pero en las estructuras de tipo $\tau^{\prime}$ el simbolo $\leq$ se
interpreta como una relacion 3-aria sobre el universo. Por ejemplo
$(\mathbf{N},i)$, con $i$ dada por $i(\leq)=\{(x,y,z)\in\mathbf{N}%
^{3}:x=y=z\}$, es una estructura de tipo $\tau^{\prime}$. En esta estructura
el simbolo $\leq$ no se interpreta como un orden parcial sino como una
relacion ternaria ya que en $\tau^{\prime}$ el simbolo $\leq$ es un simbolo de
relacion de aridad $3$

\item $\tau^{\prime\prime}=(\emptyset,\{1\},\emptyset,\{(1,3)\})$ es un tipo y
en las estructuras de tipo $\tau^{\prime\prime}$ el simbolo $1$ se
interpretara como una funcion 3-aria sobre el universo (tener cuidado al leer
$(\emptyset,\{1\},\emptyset,\{(1,3)\})$ ya que en esta expresion $1$ es el
"numeral uno" y $3$ es el numero tres). Por ejemplo si denotamos con $f $ a la
operacion%
\[%
\begin{array}
[c]{rcl}%
\mathbf{Z}^{3} & \rightarrow & \mathbf{Z}\\
(x,y,z) & \rightarrow & x+y+z
\end{array}
\]
entonces $(\mathbf{Z},i)$, con $i$ dada por $i(1)=f$, es una estructura de
tipo $\tau^{\prime\prime}$
\end{enumerate}

\bigskip

Esta libertad en la definicion de tipo y tambien en la definicion de
estructura de tipo $\tau$ (i.e. las estructuras interpretan a los nombres de
$\mathcal{C}\cup\mathcal{F}\cup\mathcal{R}$ con total independencia de la
fisonomia de las palabras de $\mathcal{C}\cup\mathcal{F}\cup\mathcal{R}$) es
clave a la hora de fortalecer la separacion entre sintaxis y semantica, idea
fundamental en el desarrollo de la logica. Para reforzar aun mas esta idea de
independencia entre semantica y sintaxis veremos algunos ejemplos de conteo de
estructuras. Antes un lema de conteo que nos sera de suma utilidad.

\bigskip

\begin{lemma}
Se tiene que:

\begin{enumerate}
\item[(1)] Dados $A,B$ conjuntos finitos no vacios, hay $\left\vert
B\right\vert ^{\left\vert A\right\vert }$ funciones tales que su dominio es
$A$ y su imagen esta contenida en $B$

\item[(2)] si $A$ es un conjunto cualquiera, entonces hay $2^{\left\vert
A\right\vert }$ subconjuntos de $A$
\end{enumerate}
\end{lemma}

\begin{proof}
(1) Supongamos $A=\{a_{1},...,a_{n}\}$, con $n=\left\vert A\right\vert $. Sea
$Fu=\{f:D_{f}=A$ y $I_{f}\subseteq B\}$. Es facil ver que la siguiente funcion
es biyectiva%
\[%
\begin{array}
[c]{rcl}%
Fu & \rightarrow & B^{n}\\
f & \rightarrow & (f(a_{1}),...,f(a_{n}))
\end{array}
\]


(2) Ya que los subconjuntos de $A$ estan en correspondencia biunivoca con las
funciones de $A$ en $\{0,1\}$ (por que?) podemos aplicar (1)
\end{proof}

\bigskip

Daremos a continuacion algunos ejemplos de conteo de estructuras. Sea%
\[
\tau=(\emptyset,\emptyset,\{\leq\},\{(\leq,2)\})
\]
Nos interesa saber cuantas estructuras de tipo $\tau$ hay que tengan al
conjunto $\{1,2,3\}$ como universo. Una estructura de tipo $\tau$ con universo
$\{1,2,3\}$ es un par $(\{1,2,3\},i)$ donde $i$ es una funcion tal que su
dominio es $\{\leq\}$ y tal que

\begin{enumerate}
\item[-] $i(\leq)$ es una relacion 2-aria sobre $\{1,2,3\}$, es decir es un
subconjunto de $\{1,2,3\}^{2}$
\end{enumerate}

O sea que una estructura de tipo $\tau$ con universo $\{1,2,3\}$ es un par de
la forma%
\[
(\{1,2,3\},\{(\leq,S)\})
\]
donde $S$ es cualquier subconjunto de $\{1,2,3\}^{2}$. Ya que, por el lema
anterior, hay $2^{9}$ subconjuntos del conjunto $\{1,2,3\}^{2}$, tenemos que
hay exactamente $2^{9}$ estructuras de tipo $\tau$ cuyo universo es
$\{1,2,3\}$. Notese que, estrictamente hablando, ninguna de estas estructuras
es un poset. Sin embargo aquellas en las cuales $S$ es un orden parcial sobre
$\{1,2,3\}$ pueden considerarse como posets ya que esencialmente estan
determinadas por un orden parcial.

Otro ejemplo, tomemos%
\[
\tau=(\{\mathrm{un},\mathrm{do}\},\{\mathrm{MAS},\mathrm{P}\},\{\mathrm{Her}%
\},\{(\mathrm{MAS},4),(\mathrm{P},1),(\mathrm{Her},3)\}
\]
Nos interesa saber cuantas estructuras de tipo $\tau$ hay que tengan al
conjunto $\{1,2,3\}$ como universo. Una estructura de tipo $\tau$ con universo
$\{1,2,3\}$ es un par $(\{1,2,3\},i)$ donde $i$ es una funcion tal que su
dominio es $\{\mathrm{un},\mathrm{do},\mathrm{MAS},\mathrm{P},\mathrm{Her}\}$
y tal que

\begin{enumerate}
\item $i(\mathrm{un})$ y $i(\mathrm{do})$ pertenecen a $\{1,2,3\}$

\item $i(\mathrm{MAS})$ es una operacion 4-aria sobre $\{1,2,3\}$

\item $i(\mathrm{P})$ es una operacion 1-aria sobre $\{1,2,3\}$

\item $i(\mathrm{Her})$ es una relacion 3-aria sobre $\{1,2,3\}$, es decir es
un subconjunto de $\{1,2,3\}^{3}$
\end{enumerate}

Notese que hay

\begin{enumerate}
\item 3 posibilidades para $i(\mathrm{un})$

\item 3 posibilidades para $i(\mathrm{do})$

\item 3$^{(3^{4})}$ posibilidades para $i(\mathrm{MAS})$ (por (1) del lema
anterior con $A=\{1,2,3\}^{4}$ y $B=\{1,2,3\}$)

\item 3$^{3}$ posibilidades para $i(\mathrm{P})$ (por (1) del lema anterior
con $A=\{1,2,3\}$ y $B=\{1,2,3\}$)

\item 2$^{(3^{3})}$ posibilidades para $i(\mathrm{Her})$ (por (2) del lema
anterior con $A=\{1,2,3\}^{3}$)
\end{enumerate}

O sea que hay exactamente $3.3.3^{(3^{4})}.3^{3}.2^{(3^{3})}$ estructuras de
tipo $\tau$ que tienen al conjunto $\{1,2,3\}$ como universo.

\bigskip

\subsection{Un poco de arrogancia}

Hemos dado, via las definiciones de \textit{tipo} y de \textit{estructura de
tipo }$\tau$, un modelo matematico preciso del concepto intuitivo de
estructura que veniamos acu\~{n}ando en las guias anteriores. Esto es un salto
importante ya que ahora tenemos una definicion matematica de lo que es una
estructura en general y no solo un pu\~{n}ado de definiciones matematicas de
ciertas estructuras particulares. Hemos encontrado la esencia del concepto
intuitivo de estructura que veniamos acu\~{n}ando con casos particulares en
las primeras guias. La modelizacion es bastante sofisticada al punto que
ninguna de las estructuras concretas antes estudiadas es estrictamente
hablando una estructura de tipo $\tau$, aunque cada tipo de estructura
concreta estudiada tiene su "version" dentro de esta definicion general de
estructura de tipo $\tau$, la cual es esencialmente el mismo objeto. Por
ejemplo, para el tipo de los reticulados complementados%
\[
\tau=(\{0,1\},\{\mathsf{s},\mathsf{i},c\},\emptyset,\{(\mathsf{s}%
,2),(\mathsf{i},2),(c,1)\})
\]
las estructuras de tipo $\tau$ que modelizan a los reticulados complementados
son precisamente aquellas estructuras $(A,i)$ tales que%
\[
(A,i(\mathsf{s}),i(\mathsf{i}),i(c),i(0),i(1))
\]
es un reticulado complementado. Obviamente estas estructuras no son
estrictamente hablando reticulados complementados, pero esencialmente son la
misma cosa.

La utilidad de este nuevo concepto general de estructura ira quedando clara a
medida que avancemos. Cabe destacar que este concepto general de estructura no
solo ha sido clave en el desarrollo de la logica matematica sino que tambien
ha sido crusial en el desarrollo de la informatica teorica, mas precisamente
en el area de especificaciones algebraicas, ya que la versatilidad del
concepto de estructura eterogenea ha permitido crear una teoria de amplio
alcance y modelizacion de la idea de la especificacion de tipos abstractos de datos.

\bigskip

\subsection{Formulas elementales de tipo $\tau$}

Recordemos que cada una de las estructuras consideradas en la Guia 6 tiene su
tipo asociado. Es decir:%
\begin{align*}
\text{Tipo de los posets}  & =(\emptyset,\emptyset,\{\leq\},\{(\leq,2)\})\\
\text{Tipo de los ret. ternas}  & =(\emptyset,\{\mathsf{s},\mathsf{i}%
\},\emptyset,\{(\mathsf{s},2),(\mathsf{i},2)\})\\
\text{Tipo de los ret. acotados}  & =(\{0,1\},\{\mathsf{s},\mathsf{i}%
\},\emptyset,\{(\mathsf{s},2),(\mathsf{i},2)\})\\
\text{Tipo de los ret. comp.}  & =(\{0,1\},\{\mathsf{s},\mathsf{i}%
,c\},\emptyset,\{(\mathsf{s},2),(\mathsf{i},2),(c,1)\})\\
\text{Tipo de los ret. cuaternas}  & =(\emptyset,\{\mathsf{s},\mathsf{i}%
\},\{\leq\},\{(\mathsf{s},2),(\mathsf{i},2),(\leq,2)\})\\
\text{Tipo de las median algebras}  & =(\emptyset,\{M\},\emptyset,\{(M,3)\})\\
\text{Tipo de los grafos}  & =(\emptyset,\emptyset,\{r\},\{(r,2)\})\\
\text{Tipo de los grafos bicoloreados}  & =(\emptyset,\emptyset
,\{r,R\},\{(r,2),(R,1)\})
\end{align*}
Notese que en cada uno de los casos anteriores los simbolos de $\mathcal{C}%
\cup\mathcal{F}\cup\mathcal{R}$ son los que se usan (junto con los simbolos
logicos, las variables y los simbolos de elementos fijos) para formar sus
correspondientes formulas elementales. Es decir, lo particular de las formulas
elementales de cada tipo de estructura estaba dado por los correspondientes
simbolos de $\mathcal{C}\cup\mathcal{F}\cup\mathcal{R}$. Esto nos permite
generalizar nuestro concepto intuitivo de formula elemental para el caso de
cualquier tipo $\tau$ de estructuras.

Si $\tau=(\mathcal{C},\mathcal{F},\mathcal{R},a)$ es un tipo, las
\textit{formulas elementales de tipo }$\tau$ se construyen en forma finitaria
usando los nombres de $\mathcal{C}\cup\mathcal{F}\cup\mathcal{R}$ junto con
los simbolos de la siguiente lista:

\begin{enumerate}
\item[ ] $\forall\ \exists\;\lnot\;\vee\;\wedge\;\rightarrow\;\leftrightarrow
\;(\;)\;,\;=$

\item[ ] Variables: $x,y,z,w,...$

\item[ ] Nombres para elementos fijos: $a,b,c,d,...$
\end{enumerate}

\bigskip

La manera en la que se construyen las formulas elementales de tipo $\tau$ a
partir de los simbolos anteriores y de las palabras de $\mathcal{C}%
\cup\mathcal{F}\cup\mathcal{R}$ es esencialmente la misma que usamos en la
Seccion \ref{Estructuras y su lenguaje elemental asociado} para construir las
formulas elementales de los distintos tipos de estructuras clasicas.
Mostraremos esto con varios ejemplos asi el lector queda con una idea clara
del concepto.

Por supuesto el concepto de formula elemental de tipo $\tau$ no es un concepto
definido en forma precisa sino mas bien una idea basada en ciertos ejemplos de
la vida real de los matematicos.

\bigskip

\begin{enumerate}
\item[(E1)] Si $\tau$ es el tipo%
\[
(\{\mathrm{un},0\},\{\mathrm{MAS},\mathrm{P}\},\{\mathrm{Her},\mathrm{Verde}%
\},\{(\mathrm{MAS},4),(\mathrm{P},1),(\mathrm{Her},3),(\mathrm{Verde},1)\})
\]
entonces las siguientes son formulas elementales de tipo $\tau$:

\begin{enumerate}
\item $\mathrm{Her}(x,y,z)$

\item $\mathrm{Verde}(x)$

\item $\mathrm{Verde}(\mathrm{MAS}(a,b,\mathrm{un},z))$

\item $\mathrm{Her}(0,\mathrm{MAS}(a,b,\mathrm{un},z),\mathrm{P}%
(\mathrm{P}(z))))$

\item $(\mathrm{un}=\mathrm{do})$

\item $(\mathrm{Verde}(\mathrm{MAS}(a,b,\mathrm{un},z))\wedge(\mathrm{un}%
=\mathrm{do}))$

\item $(\mathrm{MAS}(a,b,\mathrm{un},z)=b)$

\item $(\mathrm{MAS}(a,b,\mathrm{un},\mathrm{P}(z))=\mathrm{P}(\mathrm{P}%
(\mathrm{P}(z))))$

\item $\exists z(\mathrm{MAS}(a,b,\mathrm{un},z)=b)$

\item $\forall x\forall y\mathrm{Her}(0,y,\mathrm{P}(\mathrm{P}(x)))$

\item $\forall y\ ((\mathrm{P}(\mathrm{P}(z))=x)\rightarrow\exists
z\ (\mathrm{Verde}(z)\wedge\mathrm{Her}(x,y,z)))$
\end{enumerate}

Por supuesto las aridades de los nombres de $\mathcal{F}\cup\mathcal{R}$ son
importantes y deben ser respetadas. Por ejemplo%
\[
(\mathrm{P}(x,y)=x)\ \ \ \ \ \ \mathrm{Her}(x,y)\ \ \ \ \ \ \ \ \mathrm{Verde}%
(x,y)
\]
no son formulas elementales de tipo $\tau$.

\item[(E2)] Si $\tau$ es el tipo%
\[
(\{0,1\},\{+,\bigtriangleup\},\{\leq,r\},\{(+,2),(\bigtriangleup
,5),(\leq,2),(r,2)\})
\]
entonces las siguientes son formulas elementales de tipo $\tau$:

\begin{enumerate}
\item $r(x,z)$

\item $\mathrm{\leq}(x,y)$

\item $\mathrm{\leq}(\bigtriangleup(x,y,z,0,0),+(x,x))$

\item $(+(a,b)=\bigtriangleup(x,y,z,0,0))$

\item $(\bigtriangleup(x,y,z,0,0)=\bigtriangleup(1,1,0,x,z))$

\item $(+(\bigtriangleup(x,y,z,0,0),z)=1)$

\item $\lnot r(x,+(a,+(a,b)))$

\item $\lnot\forall y(+(x,y)=x)$

\item $\exists z\forall x\ (r(x,+(z,z)\wedge\lnot\mathrm{\leq}(x,z))$

\item $\forall x\forall y\forall z\;((r(x,y)\wedge r(y,z))\rightarrow r(x,z))
$
\end{enumerate}

Notese que hay algunas peque\~{n}as diferencias con las formulas elementales
de las estructuras clasicas ya que aqui respondemos a un formato mas general.
Por ejemplo hemos escrito $\mathrm{\leq}(x,y)$ en lugar de $x\leq y $ y
$+(x,y)$ en lugar de $x+y$. Esto es a los fines de homogeneisar la escritura y
no hacer un uso distinto para las operaciones binarias y las relaciones binarias.

Por supuesto las aridades de los nombres de $\mathcal{F}\cup\mathcal{R}$ son
importantes y deben ser respetadas. Por ejemplo%
\[
(+(x,y,z)=x)\ \ \ \ \ \ r(x,y,z)\ \ \ \ \ \ \ \ \mathrm{\leq}(x,y,z)
\]
no son formulas elementales de tipo $\tau$.

\item[(E3)] Si $\tau$ es el tipo%
\[
(\{\mathrm{er}\},\{+\},\{\leq\},\{(+,4),(\leq,5)\})
\]
entonces las siguientes son formulas elementales de tipo $\tau$:

\begin{enumerate}
\item $\mathrm{\leq}(x,y,\mathrm{er},\mathrm{er},\mathrm{er})$

\item $\mathrm{\leq}(+(x,y,z,\mathrm{er}),+(x,x,\mathrm{er},x),a,b,z)$

\item $\exists z(+(x,z,x,+(x,x,x,x))=z)$
\end{enumerate}

\item[(E4)] Si $\tau$ es el tipo%
\[
(\{\mathrm{er}\},\{\leq\},\{+\},\{(\leq,3),(+,2)\})
\]
entonces las siguientes son formulas elementales de tipo $\tau$:

\begin{enumerate}
\item $(\mathrm{\leq}(x,y,\mathrm{er})=x)$

\item $+(z,\mathrm{er})$

\item $\exists z\lnot\mathrm{+}(z,\mathrm{er})$
\end{enumerate}

(aqui hay que tener en cuenta que $\leq$ es un nombre de funcion de aridad 3 y
que $+$ es un nombre de relacion de aridad 2, lo cual es inusual pero
perfectamente posible en nuestra muy general definicion de tipo)

\item[(E5)] Si $\tau$ es el tipo%
\[
(\{\leq\},\{+\},\emptyset,\{(+,3)\})
\]
entonces las siguientes son formulas elementales de tipo $\tau$:

\begin{enumerate}
\item $(\mathrm{\leq}=x)$

\item $(+(z,\leq,a)=\mathrm{\leq})$

\item $(+(+(z,\leq,\leq),x,a)=b)$
\end{enumerate}
\end{enumerate}

(aqui hay que tener en cuenta que $\leq$ es un nombre de constante, lo cual es
inusual pero perfectamente posible en nuestra muy general definicion de tipo)

\bigskip

Para que una formula elemental de tipo $\tau$ se vuelva verdadera o falsa
tenemos que tener una estructura $(A,i)$ de tipo $\tau$ y ademas asignarles
valores concretos de $A$ a las variables libres y a los nombres de elementos
fijos que figuran en dicha formula. Cuando la formula no tiene variables
libres diremos que es una \textit{sentencia elemental de tipo }$\tau$. Notese
que en tal caso sera verdadera o falsa en una estructura dada dependiendo solo
de los valores que tomen los nombres para elementos fijos que ocurren en ella.
Tambien cabe destacar que los cuantificadores siempre ranguean sobre $A$, es
decir $\forall x$ se interpretara como $\forall x\in A $ y $\exists x$ se
interpretara como $\exists x\in A$. La diferencia entre las variables y los
nombres de elementos fijos es que si bien ambos pueden variar su valor los
nombres de elementos fijos suelen denotar un valor fijo de la estructura
durante todo un desarrollo o demostracion. Tampoco se cuantificaran los
nombres de elementos fijos, es decir solo cuantificamos variables (por
ejemplo, $\forall a(a=x)$ no es una formula elemental de tipo $\tau$,
cualquiera sea el tipo $\tau$).

Daremos algunos ejemplos para reafirmar la idea intuitiva de cuando una
formula elemental es verdadera en una estructura dada para una asignacion de
valores de sus variables libres y de sus nombres de elementos fijos:

\begin{enumerate}
\item[(E1)] Sea $\tau$ el tipo%
\[
(\{\mathrm{un},0\},\{\mathrm{MAS},\mathrm{P}\},\{\mathrm{Her},\mathrm{Verde}%
\},\{(\mathrm{MAS},4),(\mathrm{P},1),(\mathrm{Her},3),(\mathrm{Verde},1)\})
\]
y sea $(A,i)$ la estructura de tipo $\tau$ dada por:

\begin{enumerate}
\item[-] $A=\mathbf{R}$, $i(\mathrm{un})=\pi$, $i(0)=0$ (ojo que aqui el
primer cero es un simbolo y el segundo un numero real!)

\item[-]
\begin{align*}
&
\begin{array}
[t]{rcl}%
i(\mathrm{MAS}):\mathbf{R}^{4} & \rightarrow & \mathbf{R}\\
(x,y,z,w) & \rightarrow & x.y
\end{array}
\\
&
\begin{array}
[t]{rcl}%
i(\mathrm{P}):\mathbf{R} & \rightarrow & \mathbf{R}\\
x & \rightarrow & x^{2}%
\end{array}
\end{align*}%
\begin{align*}
i(\mathrm{Her})  & =\{(x,y,z)\in\mathbf{R}^{3}:x.y.z=9\}\\
i(\mathrm{Verde})  & =\mathbf{Q}%
\end{align*}

\end{enumerate}

Entonces:

\begin{enumerate}
\item la formula $\mathrm{Her}(x,y,z)$ es verdadera en $(A,i)$ cuando le
asignamos a $x$ el valor $9$, a $y$ el valor $1$ y a $z$ el valor $1$

\item $\mathrm{Verde}(x)$ es falsa en $(A,i)$ cuando le asignamos a $x$ el
valor $\sqrt{2}$

\item $\mathrm{Verde}(\mathrm{MAS}(a,b,\mathrm{un},z))$ es verdadera en
$(A,i)$ cuando le asignamos a $a$ el valor $\sqrt{2}$, a $b$ el valor
$\sqrt{2}$ y a $z$ el valor $16$ (o cualquier otro valor)

\item la formula $\exists y\exists z\ \mathrm{Her}(a,y,z))$ es una sentencia
ya que no tiene variables libres y es veradera en $(A,i)$ cuando a $a$ le
asignamos un valor no nulo

\item la formula $\exists y\exists z\ \mathrm{Her}(x,y,z))$ es una formula y
es veradera en $(A,i)$ cuando a $x$ le asignamos un valor no nulo

\item la formula $\forall x\ (\lnot(x=0)\rightarrow\exists y\exists
z\ \mathrm{Her}(x,y,z))$ es una sentencia ya que no tiene variables libres y
es veradera en $(A,i)$

\item la formula $\forall x\forall y\ ((\mathrm{Verde}(x)\wedge\mathrm{Verde}%
(y))\rightarrow\mathrm{Verde}(\mathrm{MAS}(x,y,\mathrm{un},z)))$ es verdadera
en $(A,i)$ independientemente de que valor le asignemos a $z$, ya que el
producto de racionales es racional

\item la formula $\exists y(\mathrm{MAS}(z,z,y,\mathrm{un})=\mathrm{P}(z))$ es
veradera en $(A,i)$ cualquiera sea el valor que le asignemos a $z$

\item \textbf{Error frecuente:} En la estructura anterior hay varios elementos
que tienen su notacion clasica en la matematica, por ejemplo, con la letra
griega $\pi$ denotamos la cantidad de veces que entra el diametro en la
circunferencia o con el numeral $3$ denotamos al numero entero tres. Esto no
debe confundirnos y pensar que por ejemplo las palabras%
\[
\lnot\mathrm{Verde}(\pi)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \exists
y\mathrm{Her}(3,3,y)
\]
son formulas elementales de tipo $\tau$ (aunque es claro que son verdaderas en
la estructura $(A,i)$)
\end{enumerate}

\item[(E2)] Sea $\tau$ el tipo%
\[
(\{\mathrm{er}\},\{+\},\{\leq\},\{(+,4),(\leq,5)\})
\]
y sea $(A,i)$ la estructura de tipo $\tau$ dada por:

\begin{enumerate}
\item[-] $A=\{1,2,3,4,5\}$, $i(\mathrm{er})=4$

\item[-]
\[%
\begin{array}
[t]{rcl}%
i(+):A^{4} & \rightarrow & A\\
(x,y,z,w) & \rightarrow & \max\{x,y,z,w\}
\end{array}
\]%
\[
i(\leq)=\{(x,y,z,u,v)\in A^{5}:x+y+z+u+v\geq17\}
\]

\end{enumerate}

Entonces:

\begin{enumerate}
\item $\mathrm{\leq}(\mathrm{er},\mathrm{er},\mathrm{er},\mathrm{er}%
,\mathrm{er})$ es una sentencia verdadera en $(A,i)$

\item $\mathrm{\leq}(x,y,\mathrm{er},\mathrm{er},\mathrm{er})$ es verdadera en
$(A,i)$ cuando le asignamos a las variables $x$ e $y$ valores que sumados den
al menos $5$

\item $\forall x\exists y\ \mathrm{\leq}(x,x,x,x,y)$ es una sentencia la cual
es falsa en $(A,i)$, ya que la formula $\exists y\ \mathrm{\leq}(x,x,x,x,y)$
es falsa en $(A,i)$ cuando le asignamos a $x$ el valor $1$

\item la sentencia $\forall x\exists z\ \mathrm{\leq}(x,x,x,x,+(x,x,x,z))$ es
falsa en $(A,i)$
\end{enumerate}

\item[(E3)] Sea $\tau$ el tipo%
\[
(\{\mathrm{epa}\},\{\leq,r\},\emptyset,\{(\leq,1),(r,1)\})
\]
y sea $(A,i)$ la estructura de tipo $\tau$ dada por:

\begin{enumerate}
\item[-] $A=\omega$, $i(\mathrm{epa})=71$

\item[-]
\begin{align*}
&
\begin{array}
[t]{rcl}%
i(\mathrm{\leq}):\omega & \rightarrow & \omega\\
x & \rightarrow & x^{2}%
\end{array}
\\
&
\begin{array}
[t]{rcl}%
i(r):\omega & \rightarrow & \omega\\
x & \rightarrow & \left\lfloor \sqrt{x}\right\rfloor
\end{array}
\end{align*}
(Notese que aqui contrario al uso estandard en la matematica, el simbolo
$\leq$ se interpreta como una funcion.) Entonces:

\item $(\mathrm{\leq}(\mathrm{epa})=x)$ es veradera en $(A,i)$ cuando le
asignamos a la variable $x$ el valor $71^{2}$ y falsa en caso contrario

\item la sentencia $\exists z(\mathrm{\leq}(z)=x)$ es verdadera en $(A,i)$
cuando le asignamos a $x$ el valor $16$

\item la sentencia $\forall x\ (r(\mathrm{\leq}(x))=x)$ es verdadera en
$(A,i)$

\item la sentencia $\exists x\ \lnot(\mathrm{\leq}(r(x))=x)$ es verdadera en
$(A,i)$
\end{enumerate}
\end{enumerate}

\bigskip

\subsection{\label{Teorias elementales y pruebas elementales}Teorias
elementales y pruebas elementales}

Tal como vimos en la Seccion
\ref{Estructuras y su lenguaje elemental asociado}, el concepto de prueba
elemental dependia del tipo de estructura en cuestion y ademas de tener fijado
un conjunto de sentencas elementales que llamabamos axiomas y eran el punto de
partida de dichas pruebas. Cabe destacar que dichos axiomas eran sentencias
elementales sin nombres de elementos fijos ya que estos se usaban solo en las
pruebas elementales para denotar hipoteticos elementos dentro del argumento de
la prueba misma. Ademas cuando haciamos una prueba elemental teniamos en mente
una estructura generica de la cual solo sabiamos que satisfacia los axiomas,
es decir solo podiamos usar la informacion particular que dichos axiomas nos
proveian y pasos elementales obvios de los cuales nadie dudaria. Esto nos
inspira a hacer las siguientes dos definiciones.

Una \textit{teoria elemental} sera un par $(\Sigma,\tau)$ tal que $\tau$ es un
tipo cualquiera y $\Sigma$ es un conjunto de sentencias elementales de tipo
$\tau$, las cuales no tienen nombres de elementos fijos. Un \textit{modelo de}
$(\Sigma,\tau)$ sera una estructura de tipo $\tau$ la cual haga verdaderos a
todos los elementos de $\Sigma$. Veamos algunos ejemplos:

\begin{enumerate}
\item[(E1)] La \textit{teoria elemental de los posets} es el par $(\Sigma
,\tau)$, donde $\tau=(\emptyset,\emptyset,\{\leq\},\{(\leq,2)\})$ y $\Sigma$
es el conjunto formado por las siguientes tres sentencias elementales de tipo
$\tau$:

\begin{enumerate}
\item $\forall x\ \mathrm{\leq}(x,x)$

\item $\forall x\forall y\forall z\;((\mathrm{\leq}(x,y)\wedge\mathrm{\leq
}(y,z))\rightarrow\mathrm{\leq}(x,z))$

\item $\forall x\forall y((\mathrm{\leq}(x,y)\wedge\mathrm{\leq}%
(y,x))\rightarrow x=y)$
\end{enumerate}

Notese que los modelos de esta teoria elemental son exactamente aquellas
estructuras de tipo $\tau$ las cuales son "esencialmente" posets.

\item[(E2)] La \textit{teoria elemental de los reticulados terna} es el par
$(\Sigma,\tau)$, donde $\tau=(\emptyset,\{\mathsf{s},\mathsf{i}\},\emptyset
,\{(\mathsf{s},2),(\mathsf{i},2)\})$ y $\Sigma$ es el conjunto formado por las
siguientes sentencias elementales de tipo $\tau$:

\begin{enumerate}
\item $\forall x\forall y\ (\mathsf{s}(x,x)=x)$

\item $\forall x\forall y\ (\mathsf{i}(x,x)=x)$

\item $\forall x\forall y\ (\mathsf{s}(x,y)=\mathsf{s}(y,x))$

\item $\forall x\forall y\ (\mathsf{i}(x,y)=\mathsf{i}(y,x))$

\item $\forall x\forall y\forall z\ (\mathsf{s}(\mathsf{s}(x,y),z)=\mathsf{s}%
(x,\mathsf{s}(y,z)))$

\item $\forall x\forall y\forall z\ (\mathsf{i}(\mathsf{i}(x,y),z)=\mathsf{i}%
(x,\mathsf{i}(y,z)))$

\item $\forall x\forall y\ \mathsf{s}(x,\mathsf{i}(x,y))=x)$

\item $\forall x\forall y\ \mathsf{i}(x,\mathsf{s}(x,y))=x)$
\end{enumerate}

Notese que los modelos de esta teoria elemental son exactamente aquellas
estructuras de tipo $\tau$ las cuales son "esencialmente" reticulados terna.

\item[(E3)] La \textit{teoria elemental de los reticulados cuaterna} es el par
$(\Sigma,\tau)$, donde $\tau=(\emptyset,\{\mathsf{s},\mathsf{i}\},\{\leq
\},\{(\mathsf{s},2),(\mathsf{i},2),(\leq,2)\})$ y $\Sigma$ es el conjunto
formado por las siguientes sentencias elementales de tipo $\tau$:

\begin{enumerate}
\item $\mathrm{A}_{\leq R}=\forall x\ \mathrm{\leq}(x,x)$

\item $\mathrm{A}_{\leq T}=\forall x\forall y\forall z\;((\mathrm{\leq
}(x,y)\wedge\mathrm{\leq}(y,z))\rightarrow\mathrm{\leq}(x,z))$

\item $\mathrm{A}_{\leq A}=\forall x\forall y\ ((\mathrm{\leq}(x,y)\wedge
\mathrm{\leq}(y,x))\rightarrow x=y)$

\item $\mathrm{A}_{\mathsf{s}esC}=\forall x\forall y\;(\mathrm{\leq
}(x,\mathsf{s}(x,y))\wedge\mathrm{\leq}(y,\mathsf{s}(x,y)))$

\item $\mathrm{A}_{\mathsf{s}\leq C}=\forall x\forall y\forall z\;\left(
(\mathrm{\leq}(x,z)\wedge\mathrm{\leq}(y,z))\rightarrow\mathrm{\leq
}(\mathsf{s}(x,y),z\right)  )$

\item $\mathrm{A}_{\mathsf{i}esC}=\forall x\forall y\;(\mathrm{\leq
}(\mathsf{i}(x,y),x)\wedge\mathrm{\leq}(\mathsf{i}(x,y),y))$

\item $\mathrm{A}_{\mathsf{i}\geq C}=\forall x\forall y\forall z\;\left(
(\mathrm{\leq}(z,x)\wedge\mathrm{\leq}(z,y))\rightarrow\mathrm{\leq
}(z,\mathsf{i}(x,y))\right)  $
\end{enumerate}

Notese que los modelos de esta teoria elemental son exactamente aquellas
estructuras de tipo $\tau$ las cuales son "esencialmente" reticulados cuaterna.

\item[(E4)] La \textit{teoria elemental de los grafos} es el par $(\Sigma
,\tau)$, donde $\tau=(\emptyset,\emptyset,\{r\},\{(r,2)\})$ y $\Sigma$ es el
conjunto formado por la siguiente sentencia elemental de tipo $\tau$:

\begin{enumerate}
\item $\forall x\forall y(r(x,y)\rightarrow r(y,x))$
\end{enumerate}

Notese que los modelos de esta teoria elemental son exactamente aquellas
estructuras de tipo $\tau$ las cuales son "esencialmente" grafos.

\item[(E5)] La \textit{teoria elemental de los grafos bicoloreados} es el par
$(\Sigma,\tau)$, donde $\tau=(\emptyset,\emptyset,\{r,R\},\{(r,2),(R,1)\})$ y
$\Sigma$ es el conjunto formado por las siguientes sentencias elementales de
tipo $\tau$:

\begin{enumerate}
\item $\forall x\forall y(r(x,y)\rightarrow r(y,x))$

\item $\forall x\forall y(r(x,y)\rightarrow(R(x)\leftrightarrow\lnot R(y))) $
\end{enumerate}

Notese que los modelos de esta teoria elemental son exactamente aquellas
estructuras de tipo $\tau$ las cuales son "esencialmente" grafos bicoloreados.
\end{enumerate}

\bigskip

Es muy importante notar que una teoria elemental $(\Sigma,\tau)$ es en algun
sentido un objeto esencialmente sintactico ya que $\Sigma$, $\mathcal{C}$,
$\mathcal{F}$ y $\mathcal{R}$ son conjuntos de palabras. Los modelos de
$(\Sigma,\tau)$ constituyen la semantica de la teoria.

Las anteriores son las teorias elementales que se corresponden con los tipos
de estructuras consideradas en la Seccion
\ref{Estructuras y su lenguaje elemental asociado} pero nuestra definicion de
teoria elemental es muy general y nos permite considerar una gran diversidad
de teorias. Veamos algunos ejemplos de teorias elementales interesantes y no
consideradas en la Seccion \ref{Estructuras y su lenguaje elemental asociado}:

\bigskip

\begin{enumerate}
\item[(E6)] Consideremos la teoria elemental $(\Sigma,\tau)$, donde
$\tau=(\{\mathrm{ex}\},\{\mathrm{F}\},\emptyset,\{(\mathrm{F},1)\})$ y
$\Sigma$ es el conjunto formado por las siguientes dos sentencias elementales:

\begin{enumerate}
\item $\forall x\forall y\ (\lnot(x=y)\rightarrow\lnot(\mathrm{F}%
(x)=\mathrm{F}(y)))$

\item $\forall x\ \lnot(\mathrm{F}(x)=\mathrm{ex})$
\end{enumerate}

Notese que una estructura $\mathbf{A}=(A,i)$ de tipo $\tau$ es un modelo de
$(\Sigma,\tau)$ si y solo si $i(\mathrm{F})$ es inyectiva y $i(\mathrm{ex}%
)\notin\operatorname{Im}(i(\mathrm{F}))$. Esto obviamente nos dice que el
universo de cada modelo de esta teoria es infinito. Un modelo de la teoria es
por ejemplo $(\omega,\{(\mathrm{ex},0),(\mathrm{F},Suc)\})$

\item[(E7)] Sea $\tau=(\emptyset,\{\times\},\{\mathrm{Com}\},\{(\times
,2),(\mathrm{Com},1)\})$ y sea $\Sigma$ el conjunto formado por las siguientes
sentencias elementales de tipo $\tau$:

\begin{enumerate}
\item $\forall x\forall y\forall z\ (\times(\times(x,y),z)=\times
(x,\times(y,z)))$

\item $\forall z\ (\mathrm{Com}(z)\rightarrow\forall x\ (\times(x,z)=\times
(z,x)))$

\item $\forall x\exists z\ (x=\times(z,z)\wedge\mathrm{Com}(z))$
\end{enumerate}

Supongamos $\mathbf{A}=(A,i)$ es un modelo de la teoria $(\Sigma,\tau)$.
Notese que el primer axioma nos dice que $i(\times)$ es una operacion binaria
asociativa, esto se ve mas facilmente si escribimos dicho axioma con la
notacion mas usual para operaciones:%
\[
\forall x\forall y\forall z\ (x\times y)\times z=x\times(y\times z)
\]


El segundo axioma nos dice que si $a\in i(\mathrm{Com})$, entonces
$a\ i(\times)\ b=b\ i(\times)\ a$, cualesquiera sea $b\in A$. O sea nos dice
que los elementos de $i(\mathrm{Com})$ conmutan con todos los otros elementos
relativo a la operacion $i(\times)$. El tercer axioma nos dice que cualquiera
sea $a\in A$, debe haber un $b\in i(\mathrm{Com})$ tal que $b\ i(\times
)\ b=a$. En algun sentido nos dice que todo elemento de $A$ tiene en el
conjunto $i(\mathrm{Com})$ una "raiz cuadrada" relativo a la operacion
$i(\times)$. Ejemplos de modelos de esta teoria son:

\begin{enumerate}
\item $(\{r\in\mathbf{R}:r\geq0\},i)$, con $i(\times)=$ operacion producto
usual de $\mathbf{R}$ restringida a $\{r\in\mathbf{R}:r\geq0\}^{2} $ y
$i(\mathrm{Com})=\{r\in\mathbf{R}:r\geq0\}$

\item $(\mathbf{R},i)$, con $i(\times)=\max$ y $i(\mathrm{Com})=\mathbf{R}$

\item $(\mathbf{R},i)$, con $i(\times)=\min$ y $i(\mathrm{Com})=\mathbf{R}$

\item $(\mathcal{P}(\{1,2,3\}),i)$, con $i(\times)=\cup$ y $i(\mathrm{Com}%
)=\mathcal{P}(\{1,2,3\})$
\end{enumerate}

\item[(E8)] La \textit{teoria elemental de los reticulados cuaterna
distributivos} es el par $(\Sigma,\tau)$, donde $\tau=(\emptyset
,\{\mathsf{s},\mathsf{i}\},\{\leq\},\{(\mathsf{s},2),(\mathsf{i}%
,2),(\leq,2)\})$ y $\Sigma$ es el conjunto formado por los axiomas de la
teoria elemental de los reticulados cuaterna junto con el axioma

\begin{enumerate}
\item $\forall x\forall y\forall z\ (\mathsf{i}(x,\mathsf{s}(y,z))=\mathsf{s}%
(\mathsf{i}(x,y),\mathsf{i}(x,z)))$
\end{enumerate}

Notese que los modelos de esta teoria elemental son exactamente aquellas
estructuras de tipo $\tau$ las cuales son "esencialmente" reticulados cuaterna distributivos

\item[(E9)] La \textit{teoria elemental de los reticulados terna
distributivos} es el par $(\Sigma,\tau)$, donde $\tau=(\emptyset
,\{\mathsf{s},\mathsf{i}\},\emptyset,\{(\mathsf{s},2),(\mathsf{i},2)\})$ y
$\Sigma$ es el conjunto formado por los axiomas de la teoria elemental de los
reticulados terna junto con el axioma

\begin{enumerate}
\item $\forall x\forall y\forall z\ (\mathsf{i}(x,\mathsf{s}(y,z))=\mathsf{s}%
(\mathsf{i}(x,y),\mathsf{i}(x,z)))$
\end{enumerate}

Notese que los modelos de esta teoria elemental son exactamente aquellas
estructuras de tipo $\tau$ las cuales son "esencialmente" reticulados terna distributivos
\end{enumerate}

\bigskip

\subsubsection{Pruebas elementales}

Podemos generalizar el concepto de prueba elemental, introducido en la Seccion
\ref{Estructuras y su lenguaje elemental asociado}, a cualquier teoria
elemental. Dada una teoria elemental $(\Sigma,\tau)$ y una sentencia elemental
$\varphi$ la cual no posea nombres de constantes auxiliares, una
\textit{prueba elemental de }$\varphi$ \textit{en }$(\Sigma,\tau)$ sera una
prueba de $\varphi$ que posea las siguientes caracteristicas:

\begin{enumerate}
\item[-] salvo por ciertas aclaraciones simples y concretas en castellano la
prueba se escribe usando solo sentencias elementales de tipo $\tau$

\item[-] cada paso de la demostracion debe ser obvio y solido

\item[-] cuando el matematico comiensa la prueba tiene en mente una estructura
generica de tipo $\tau$ que satisface los axiomas de la teoria (i.e. es un
modelo de la teoria) y esa es la unica particularidad que supone de dicho
modelo generico al comensar la prueba. Es decir las pruebas elementales
siempre son formas solidas de justificar que \textit{cualquier} estructura que
satisfaga los axiomas tambien satisfacera la sentencia probada
\end{enumerate}

\bigskip

Por supuesto el concepto de prueba elemental en una teoria $(\Sigma,\tau)$ no
es un concepto definido en forma precisa sino mas bien una idea basada en
ciertos ejemplos de la vida real de los matematicos.

Veamos algunos ejemplos:

\begin{enumerate}
\item[(E10)] Consideremos la teoria elemental del ejemplo (E6). Sea%
\[
\varphi=\exists x\exists y\exists z\ (\lnot(x=y)\wedge\lnot(x=z)\wedge
\lnot(y=z))
\]


($\varphi$ dice que el universo tiene al menos tres elementos.) Tenemos la siguiente:

\begin{enumerate}
\item[Prueba elemental de $\varphi$ en $(\Sigma,\tau)$:] Por el segundo axioma
tenemos que $\lnot(\mathrm{F}(\mathrm{ex})=\mathrm{ex})$. Obviamente entonces
tenemos que

(1) $\lnot(\mathrm{ex}=\mathrm{F}(\mathrm{ex}))$

Por el segundo axioma tambien tenemos que $\lnot(\mathrm{F}(\mathrm{F}%
(\mathrm{ex}))=\mathrm{ex})$ por lo que

(2) $\lnot(\mathrm{ex}=\mathrm{F}(\mathrm{F}(\mathrm{ex})))$

Ya que se da (2), el primer axioma nos dice que

(3) $\lnot(\mathrm{F}(\mathrm{ex})=\mathrm{F}(\mathrm{F}(\mathrm{ex})))$

Poniendo (1), (2) y (3) juntos tenemos que%
\[
\lnot(\mathrm{ex}=\mathrm{F}(\mathrm{ex}))\wedge\lnot(\mathrm{ex}%
=\mathrm{F}(\mathrm{F}(\mathrm{ex})))\wedge\lnot(\mathrm{F}(\mathrm{ex}%
)=\mathrm{F}(\mathrm{F}(\mathrm{ex})))
\]
de lo cual es obvio que vale $\varphi$.
\end{enumerate}

\item[(E11)] Consideremos la teoria elemental del ejemplo (E7). A continuacion
daremos una prueba elemental de $\varphi=\forall x\forall y\ (\times
(x,y)=\times(y,x))$ en la teoria $(\Sigma,\tau)$. Para facilitar la lectura
usaremos la notacion clasica para operaciones binarias, es decir escribiremos
$x\times y$ en lugar de $\times(y,x)$, etc.

\begin{enumerate}
\item[Prueba elemental de $\varphi$ en $(\Sigma,\tau)$:] Sean $a,b\in A$,
fijos pero arbitrarios. Por el tercer axioma tenemos que

1. $\exists z\ (a=z\times z\wedge\mathrm{Com}(z))$

Sea $c$ tal que

2. $a=c\times c\wedge\mathrm{Com}(c)$

Nuevamente, por el tercer axioma tenemos que

3. $\exists z\ (b=z\times z\wedge\mathrm{Com}(z))$

Sea $d$ tal que

4. $b=d\times d\wedge\mathrm{Com}(d)$

Ya que vale $\mathrm{Com}(c)$, el segundo axioma nos dice que

5. $\forall x\ (x\times c=c\times x)$

Ya que $a=c\times c$ y $b=d\times d$, tenemos que

6. $a\times b=(c\times c)\times(d\times d)$

Pero por el primer axioma (asociatividad) tenemos que

7. $(c\times c)\times(d\times d)=c\times(c\times(d\times d))$

Pero por 5. tenemos que

8. $c\times(c\times(d\times d))=c\times((d\times d)\times c)$

Por asociatividad

9. $c\times((d\times d)\times c)=(c\times(d\times d))\times c$

Por 5. tenemos que

10. $(c\times(d\times d))\times c=((d\times d)\times c)\times c$

Por asociatividad tenemos que

11. $((d\times d)\times c)\times c=(d\times d)\times(c\times c)$

Ya que $a=c\times c$ y $b=d\times d$, tenemos que

12. $(d\times d)\times(c\times c)=b\times a$.

Siguiendo la cadena de igualdades desde 6. hasta 12. tenemos que

13. $a\times b=b\times a$.

Ya que $a$ y $b$ eran elementos arbitrarios, hemos probado que $\forall
x\forall y\ x\times y=y\times x$
\end{enumerate}
\end{enumerate}

\bigskip

\subsection{\label{programa}Programa}

Ahora que hemos generalizado los conceptos de estructura, formula elemental y
prueba elemental via el concepto de tipo, podemos enunciar en forma mucho mas
general el programa de logica para reticulados cuaterna dado al principio de
la seccion.

\bigskip

\textbf{Programa de logica matematica}

\begin{enumerate}
\item[(1)] Dar un modelo matematico del concepto de formula elemental de tipo
$\tau$

\item[(2)] Dar una definicion matematica de cuando una formula elemental de
tipo $\tau$ es verdadera en una estructura de tipo $\tau$ para una asignacion
dada de valores a las variables libres y a los nombres de constantes fijas de
la formula

\item[(3)] (Plato gordo) Dar un modelo matematico del concepto de prueba
elemental en una teoria elemental de tipo $\tau$. A estos objetos matematicos
los llamaremos pruebas formales de tipo $\tau$

\item[(4)] (Sublime) Intentar probar matematicamente que nuestro concepto de
prueba formal de tipo $\tau$ es una correcta modelizacion matematica de la
idea intuitiva de prueba elemental en una teoria elemental de tipo $\tau$
\end{enumerate}

\bigskip

Como veremos, los cuatro puntos anteriores pueden ser hechos
satisfactoriamente y constituyen el comienzo de la logica matematica con
cuantificadores. Cabe aclarar que la realizacion del cuarto punto es realmente
sorprendente ya que es un caso de una prueba matematica rigurosa de un hecho
que involucra un concepto intuitivo como lo es el de prueba elemental.

El punto (1) se resuelve en la seccion siguiente y si bien produce
interesantes conceptos y resultados matematicos su resolucion es rutinaria. El
punto (2) es resuelto por Tarski. El punto (3) por Fregue. El (4) es una
concecuencia de dos importantes teoremas, el Teorema de Correccion y el
Teorema de Completitud de Godel.

\bigskip

\subsection{Modelo matematico de la sintaxis elemental}

En esta seccion daremos un modelo matematico del concepto de formula elemental
de tipo $\tau$. Esto coresponde al punto (1) del programa de logica enunciado
anteriormente.\bigskip

\subsubsection{Variables}

Las variables usadas en las formulas elementales no estaban del todo
especificadas. Para hacer bien preciso este concepto definiremos un conjunto
concreto de variables. Sea $Var$ el siguiente conjunto de palabras del
alfabeto $\{\mathsf{X},\mathit{0},\mathit{1},...,\mathit{9},\mathbf{0}%
,\mathbf{1},...,\mathbf{9}\}$:%
\[
Var=\{\mathsf{X}\mathbf{1},\mathsf{X}\mathbf{2},...,\mathsf{X}\mathbf{9}%
,\mathsf{X}\mathit{1}\mathbf{0},\mathsf{X}\mathit{1}\mathbf{1},...,\mathsf{X}%
\mathit{1}\mathbf{9},\mathsf{X}\mathit{2}\mathbf{0},\mathsf{X}\mathit{2}%
\mathbf{1},...\}
\]
Es decir el elemento $n$-esimo de $Var$ es la palabra de la forma
$\mathsf{X}\alpha$ donde $\alpha$ es el resultado de reemplazar en la palabra
que denota $n$ en notacion decimal, el ultimo numeral por su correspondiente
numeral bold y los otros por sus correspondientes italicos. A los elementos de
$Var$ los llamaremos \textit{variables.} La razon por la cual usamos numerales
italicos y bold es que a los numerales normales los usamos habitualmente en
los tipos y sera conveniente que entonces no ocurran en las variables. Ademas
tomamos el ultimo simbolo de cada variable en bold para que de esta manera
nunca una variable sea una subpalabra de otra variable distinta a ella, lo
cual contribuye a simplificar los resultados.

Denotaremos con $x_{i}$ al $i$-esimo elemento de $Var$, para cada
$i\in\mathbf{N}$.

\bigskip

\subsubsection{Terminos}

Dado un tipo $\tau$, definamos recursivamente los conjuntos de palabras
$T_{k}^{\tau}$, con $k\geq0$, de la siguiente manera:%
\begin{align*}
T_{0}^{\tau}  & =Var\cup\mathcal{C}\\
T_{k+1}^{\tau}  & =T_{k}^{\tau}\cup\{f(t_{1},...,t_{n}):f\in\mathcal{F}%
_{n}\text{, }n\geq1\text{ y }t_{1},...,t_{n}\in T_{k}^{\tau}\}.
\end{align*}
Sea%
\[
T^{\tau}=\bigcup_{k\geq0}T_{k}^{\tau}%
\]
Los elementos de $T^{\tau}$ seran llamados \textit{terminos de tipo }$\tau$.
Un termino $t$ es llamado \textit{cerrado} si $x_{i}$ no es subpalabra de $t$,
para cada $i\in\mathbf{N}$. Definamos%
\[
T_{c}^{\tau}=\{t\in T^{\tau}:t\text{ es cerrado}\}
\]


Algunos ejemplos:

\begin{enumerate}
\item[(E1)] Sea $\tau=(\{\mathrm{uno},\mathrm{doli}\},\{\mathrm{MAS}%
,\mathrm{P}\},\{\mathrm{Her}\},a)$, con $a$ dado por $a(\mathrm{MAS})=4$,
$a(\mathrm{P})=1$ y $a(\mathrm{Her})=3$. Entonces

\begin{enumerate}
\item Las palabras $\mathrm{uno}$, $\mathrm{doli}$ y $\mathsf{X}%
\mathit{15666}\mathbf{9}$ son terminos de tipo $\tau$ ya que pertenecen a
$T_{0}^{\tau}$

\item $\mathrm{MAS}(\mathrm{uno},\mathrm{doli},\mathsf{X}\mathit{1}%
\mathbf{9},\mathsf{X}\mathbf{5})$ y $\mathrm{P}(\mathrm{uno})\ $son terminos
de tipo $\tau$ ya que pertenecen a $T_{1}^{\tau}$ (por que?)

\item Las palabras%
\[
\mathrm{P}(\mathrm{P}(\mathrm{uno}))\ \ \ \ \ \mathrm{MAS}(\mathrm{P}%
(\mathsf{X}\mathbf{4}),\mathrm{doli},\mathsf{X}\mathit{1}\mathbf{9}%
,\mathsf{X}\mathbf{5})
\]
son terminos de tipo $\tau$ ya que pertenecen a $T_{2}^{\tau}$

\item $\mathrm{P}(\mathrm{MAS}(\mathrm{P}(\mathsf{X}\mathbf{4}),\mathrm{MAS}%
(\mathsf{X}\mathbf{1},\mathsf{X}\mathbf{2},\mathsf{X}\mathbf{3},\mathsf{X}%
\mathbf{4}),\mathsf{X}\mathit{1}\mathbf{9},\mathsf{X}\mathbf{5}))$ es un
termino ya que pertenece a $T_{3}^{\tau}$

\item $\mathrm{uno}$, $\mathrm{doli}$, $\mathrm{P}(\mathrm{uno})$ y
$\mathrm{MAS}(\mathrm{uno},\mathrm{doli},\mathrm{doli},\mathrm{doli})$ son
terminos cerrados de tipo $\tau$
\end{enumerate}

Lo que debe quedar claro es que como objetos matematicos los terminos son
meras palabras, por ejemplo $\mathrm{MAS}(\mathrm{uno},\mathrm{doli}%
,\mathsf{X}\mathit{1}\mathbf{9},\mathsf{X}\mathbf{5})$ es una palabra (de
longitud 20)

\item[(E2)] Sea $\tau=(\{0,1\},\{+,\times,\uparrow\},\emptyset,a)$, con $a$
dado por $a(+)=2$, $a(\times)=3$ y $a(\uparrow)=1$. Entonces%
\[
\mathsf{X}\mathit{111}\mathbf{9}%
\ \ \ \ \ \ \ \ 0\ \ \ \ \ \ \ \ 1\ \ \ \ \ \ \ \ +(+(\mathrm{\uparrow
}(\mathsf{X}\mathbf{4}),\times(\mathsf{X}\mathbf{2},1,0)),\times
(1,\mathsf{X}\mathbf{2},\mathsf{X}\mathbf{3}))
\]
son terminos de tipo $\tau$. Tambien $\mathrm{\uparrow}(+(\mathrm{\uparrow
}(0),\times(0,1,0)))$ es un termino cerrado de tipo $\tau$

\item[(E3)] Sea $\tau=(\emptyset,\{\mathsf{s},\mathsf{i}\},\emptyset
,\{(\mathsf{s},2),(\mathsf{i},2)\})$ el tipo de los reticulados terna.
Entonces%
\[
\mathsf{s}(\mathsf{X}\mathbf{2},\mathsf{X}\mathbf{3}%
)\ \ \ \ \ \ \ \ \ \mathsf{s}(\mathsf{s}(\mathsf{X}\mathbf{4},\mathsf{X}%
\mathit{1}\mathbf{4}),\mathsf{i}(\mathsf{X}\mathbf{2},\mathsf{X}%
\mathit{111}\mathbf{9}))
\]
son terminos de tipo $\tau$. No hay terminos cerrados de tipo $\tau$. Cabe
destacar que $\mathsf{X}\mathbf{2\ }\mathsf{s\ X}\mathbf{3}$ no es un termino
de tipo $\tau$ aunque, como veremos en los ejercicios esto no es trivial de la
definicion de termino y requiere de una demostracion.
\end{enumerate}

\bigskip

El siguiente lema es la herramienta basica para probar propiedades de los terminos.

\begin{lemma}
[Menu para terminos]\label{basic0}Supongamos $t\in T_{k}^{\tau}$, con $k\geq
1$. Entonces se da alguna de las siguientes:

\begin{enumerate}
\item[(a)] $t\in Var\cup\mathcal{C}$

\item[(b)] $t=f(t_{1},...,t_{n})$, con $f\in\mathcal{F}_{n}$, $n\geq1$ y
$t_{1},...,t_{n}\in T_{k-1}^{\tau}$.
\end{enumerate}
\end{lemma}

\begin{proof}
Por induccion en $k$.

CASO $k=1$: Es directo ya que por definicion%
\[
T_{1}^{\tau}=Var\cup\mathcal{C}\cup\{f(t_{1},...,t_{n}):f\in\mathcal{F}%
_{n}\text{, }n\geq1\text{ y }t_{1},...,t_{n}\in T_{0}^{\tau}\}.
\]


CASO $k\Rightarrow k+1$: Sea $t\in T_{k+1}^{\tau}$. Por definicion de
$T_{k+1}^{\tau}$ tenemos que $t\in T_{k}^{\tau}$ o $t=f(t_{1},...,t_{n})$ con
$f\in\mathcal{F}_{n}$, $n\geq1$ y $t_{1},...,t_{n}\in T_{k}^{\tau}$. Si se da
que $t\in T_{k}^{\tau}$, entonces podemos aplicar hipotesis inductiva y usar
que $T_{k-1}^{\tau}\subseteq T_{k}^{\tau}$. Esto completa el caso.
\end{proof}

\bigskip

Algunos ejemplos de propiedades de los terminos las cuales se pueden probar
facilmente usando el lema anterior son

\begin{enumerate}
\item[-] Si $t\in T^{\tau}$ es tal que en $t$ ocurre el simbolo $)$, entonces
$t=f(t_{1},...,t_{n})$ con $f\in\mathcal{F}_{n}$, $n\geq1$ y $t_{1}%
,...,t_{n}\in T^{\tau}$.

\item[-] Ningun termino comienza con un simbolo del alfabeto $\{\mathit{0}%
,\mathit{1},...,\mathit{9}\}$

\item[-] Si $t\in T^{\tau}$ comienza con $\mathsf{X}$ entonces $t\in Var$

\item[-] Si $t\in T^{\tau}$ y $\left[  t\right]  _{i}=)$, con $i<\left\vert
t\right\vert $, entonces $\left[  t\right]  _{i+1}=$ $,$ o $\left[  t\right]
_{i+1}=$ $)$

\item[-] Si $t\in T^{\tau}$, entonces $\left\vert t\right\vert _{(}=\left\vert
t\right\vert _{)}$.
\end{enumerate}

\bigskip

Una posible forma de probar que una palabra dada no es un termino es encontrar
una propiedad que posean todos los terminos la cual no cumpla dicha palabra.
Por ejemplo si $\tau=(\emptyset,\{glp\},\emptyset,a)$, con $a(glp)=1$, la
palabra $\alpha=glp((\mathsf{X}\mathit{13}\mathbf{3})$ no es un termino ya que
$\left\vert \alpha\right\vert _{(}\neq\left\vert \alpha\right\vert _{)}$.

\bigskip

\paragraph{Unicidad de la lectura de terminos}

Definamos conjuntos $Bal_{k}$, con $k\geq1$ de la siguiente manera:%
\begin{align*}
Bal_{1}  & =\{()\}\\
Bal_{k+1}  & =Bal_{k}\cup\{(b_{1}...b_{n}):b_{1},...,b_{n}\in Bal_{k}%
,n\geq1\}.
\end{align*}
Sea%
\[
Bal=\bigcup_{k\geq1}Bal_{k}%
\]
Recordemos que $\beta$ es un \textit{tramo inicial (propio) }de $\alpha$ si
hay una palabra $\gamma$ tal que $\alpha=\beta\gamma$ (y $\beta\notin
\{\varepsilon,\alpha\}$). En forma similar se define \textit{tramo final
(propio).}

\begin{lemma}
\label{basicas de balanceadas}Sea $b\in Bal$. Se tiene:

\begin{enumerate}
\item[(1)] $\left\vert b\right\vert _{(}-\left\vert b\right\vert _{)}=0$

\item[(2)] Si $x$ es tramo inicial propio de $b$, entonces $\left\vert
x\right\vert _{(}-\left\vert x\right\vert _{)}>0$

\item[(3)] Si $x$ es tramo final propio de $b$, entonces $\left\vert
x\right\vert _{(}-\left\vert x\right\vert _{)}<0$
\end{enumerate}
\end{lemma}

\begin{proof}
Probaremos por induccion en $k$, que valen (1), (2) y (3) para cada $b\in
Bal_{k}$. El caso $k=1$ es trivial. Supongamos $b\in Bal_{k+1}$. Si $b\in
Bal_{k}$, se aplica directamente HI. Supongamos entonces que $b=(b_{1}%
...b_{n})$, con $b_{1},...,b_{n}\in Bal_{k}$, $n\geq1$. Por HI, $b_{1}%
,...,b_{n}$ cumplen (1) por lo cual $b$ cumple (1). Veamos que $b$ cumple (2).
Sea $x$ un tramo inicial propio de $b$. Notese que $x$ es de la forma
$x=(b_{1}...b_{i}x_{1}$ con $0\leq i\leq n-1$ y $x_{1}$ un tramo inicial de
$b_{i+1}$ (en el caso $i=0$ interpretamos $b_{1}...b_{i}=\varepsilon)$. Pero
entonces ya que%
\[
\left\vert x\right\vert _{(}-\left\vert x\right\vert _{)}=1+\left(  \sum
_{j=1}^{i}\left\vert b_{j}\right\vert _{(}-\left\vert b_{j}\right\vert
_{)}\right)  +\left\vert x_{1}\right\vert _{(}-\left\vert x_{1}\right\vert
_{)}%
\]
tenemos que por HI, se da que $\left\vert x\right\vert _{(}-\left\vert
x\right\vert _{)}>0$. En forma analoga se puede ver que $b$ cumple (3).
\end{proof}

\bigskip

Dado un alfabeto $\Sigma$ tal que $($ y $)$ pertenecen a $\Sigma$, definamos
$del:\Sigma^{\ast}\rightarrow\Sigma^{\ast}$, de la siguiente manera%
\begin{align*}
del(\varepsilon)  & =\varepsilon\\
del(\alpha a)  & =del(\alpha)a\text{, si }a\in\{(,)\}\\
del(\alpha a)  & =del(\alpha)\text{, si }a\in\Sigma-\{(,)\}
\end{align*}


\begin{lemma}
$del(xy)=del(x)del(y)$, para todo $x,y\in\Sigma^{\ast}$
\end{lemma}

\begin{lemma}
Supongamos que $\Sigma$ es tal que $T^{\tau}\subseteq\Sigma^{\ast}$. Entonces
$del(t)\in Bal$, para cada $t\in T^{\tau}-(Var\cup\mathcal{C})$
\end{lemma}

\bigskip

Notese que en la definicion de tipo se exige que nunca un nombre de cte sea
subpalabra de otro nombre de cte, lo cual garantiza que nunca puede ser un
nombre de cte un tramo inicial o final propio de otro nombre de cte. Lo que si
puede suceder es que un tramo final propio de un nombre de cte $c$ sea un
tramo inicial propio de otro nombre de cte $d$. Mas formalmente puede suceder
que haya palabras $x,y,z$, las tres distintas de $\varepsilon$ tales que
$c=xy$ y $d=yz$. En tal caso solemos decir que las palabras $c$ y $d$ se
\textit{mordizquean}. Por ejemplo si $\tau=(\{\mathrm{uno}$,$\mathrm{noli}%
\},\emptyset,\emptyset,\emptyset)$, es facil ver que $\tau$ es un tipo y que
$\mathrm{uno}$ y $\mathrm{noli}$ se mordizquean. El lema siguiente nos dice
que este es el unico caso de mordizqueo de terminos.

\begin{lemma}
[Mordizqueo de Terminos]\label{superposicion}Sean $s,t\in T^{\tau}$ y
supongamos que hay palabras $x,y,z$, con $y\neq\varepsilon$ tales que $s=xy$ y
$t=yz$ . Entonces $x=z=\varepsilon$ o $s,t\in\mathcal{C}$. En particular si un
termino es tramo inicial o final de otro termino, entonces dichos terminos son iguales.
\end{lemma}

\begin{proof}
Supongamos $s\in\mathcal{C}$. Ya que $y\neq\varepsilon$ tenemos que $t$ debe
comenzar con un simbolo que ocurre en un nombre de cte, lo cual dice que $t$
no puede ser ni una variable ni de la forma $g(t_{1},...,t_{m})$, es decir
$t\in\mathcal{C}$. Supongamos $s\in Var$. Si $x\neq\varepsilon$ tenemos que
$t$ debe comenzar con alguno de los siguientes simbolos%
\[
\mathit{0}\;\mathit{1\;}...\;\mathit{9}\;\mathbf{0}\;\mathbf{1}%
\ ...\;\mathbf{9}%
\]
lo cual es absurdo. O sea que $x=\varepsilon$ y por lo tanto $t$ debe comenzar
con $\mathsf{X}$. Pero esto dice que $t\in Var$ de lo que sigue facilmente que
$z=\varepsilon$. Supongamos entonces que $s$ es de la forma $f(s_{1}%
,...,s_{n})$. Ya que $)$ debe ocurrir en $t$, tenemos que $t$ es de la forma
$g(t_{1},...,t_{m})$. O sea que $del(s),del(t)\in Bal$. Ya que $)$ ocurre en
$y$, $del(y)\neq\varepsilon$. Tenemos tambien que%
\begin{align*}
del(s)  & =del(x)del(y)\\
del(t)  & =del(y)del(z)
\end{align*}
La primera igualdad, por (1) y (3) del Lema \ref{basicas de balanceadas}, nos
dice que%
\[
\left\vert del(y)\right\vert _{(}-\left\vert del(y)\right\vert _{)}\leq0,
\]
y la segunda que%
\[
\left\vert del(y)\right\vert _{(}-\left\vert del(y)\right\vert _{)}\geq0,
\]
por lo cual%
\[
\left\vert del(y)\right\vert _{(}-\left\vert del(y)\right\vert _{)}=0
\]
Pero entonces (3) del Lema \ref{basicas de balanceadas} nos dice que $del(y)$
no puede ser tramo final propio de $del(s)$, por lo cual debe suceder que
$del(y)=del(s)$, ya que $del(y)\neq\varepsilon$. Claramente entonces obtenemos
que $del(x)=\varepsilon$. Similarmente se puede ver que $del(z)=\varepsilon$.
Ya que que $t$ termina con $)$ tenemos que $z=\varepsilon$. O sea que
$f(s_{1},...,s_{n})=xg(t_{1},...,t_{m})$ con $del(x)=\varepsilon$, de lo que
se saca que $f=xg$ ya que $($ no ocurre en $x $. De la definicion de tipo se
desprende que $x=\varepsilon$.
\end{proof}

\bigskip

\begin{theorem}
[Lectura unica de terminos]\textit{\label{le-un-de-te}Dado }$t\in T^{\tau}%
$\textit{\ se da una de las siguientes:}

\begin{enumerate}
\item[(1)] $t\in Var\cup\mathcal{C}$

\item[(2)] \textit{Hay unicos }$n\geq1$,$\;f\in\mathcal{F}_{n}$,$\;t_{1}%
,...,t_{n}\in T^{\tau}$\textit{\ tales que }$t=f(t_{1},...,t_{n})$.
\end{enumerate}
\end{theorem}

\begin{proof}
En virtud del Lema \ref{basic0} solo nos falta probar la unicidad en el punto
(2). Supongamos que%
\[
t=f(t_{1},...,t_{n})=g(s_{1},...,s_{m})
\]
con $n,m\geq1,\;f\in\mathcal{F}_{n}$, $g\in\mathcal{F}_{m}$, $t_{1}%
,...,t_{n},s_{1},...,s_{m}\in T^{\tau}$. Notese que $f=g$. O sea que
$n=m=a(f)$. Notese que $t_{1}$ es tramo inicial de $s_{1}$ o $s_{1}$ es tramo
inicial de $t_{1}$, lo cual por el lema anterior nos dice que $t_{1}=s_{1}$.
Con el mismo razonamiento podemos probar que debera suceder $t_{2}%
=s_{2},...,t_{n}=s_{n}$.
\end{proof}

\bigskip\ 

El teorema anterior es importante ya que nos permite definir recursivamente
funciones con dominio contenido en $T^{\tau}$. Por ejemplo podemos definir una
funcion $F:T^{\tau}\rightarrow T^{\tau}$, de la siguiente manera:

\begin{enumerate}
\item[-] $F(c)=c$, para cada $c\in\mathcal{C}$

\item[-] $F(v)=v$, para cada $v\in Var$

\item[-] $F(f(t_{1},...,t_{n}))=f(F(t_{1}),...,F(t_{n}))$, si $f\in
\mathcal{F}_{n}$, con $n\neq2$

\item[-] $F(f(t_{1},t_{2}))=f(t_{2},t_{1})$, si $f\in\mathcal{F}_{2}.$
\end{enumerate}

\noindent Notese que si la unicidad de la lectura no fuera cierta, entonces
las ecuaciones anteriores no estarian definiendo en forma correcta una funcion
ya que el valor de la imagen de un termino $t$ estaria dependiendo de cual
descomposicion tomemos para $t$.

\bigskip

\paragraph{Subterminos}

Sean $s,t\in T^{\tau}$. Diremos que $s$ es \textit{subtermino }%
(\textit{propio})\textit{\ }de $t$ si (no es igual a $t$ y) $s$ es subpalabra
de $t$. A continuacion veremos de que manera ocurren los subterminos de un
termino. Para esto recordemos un poco el concepto de ocurrencia.

Dadas palabras $\alpha,\beta\in\Sigma^{\ast}$, con $\left\vert \alpha
\right\vert ,\left\vert \beta\right\vert \geq1$, y un natural $i\in
\{1,...,\left\vert \beta\right\vert \}$, se dice que $\alpha$ \textit{ocurre a
partir de }$i$ \textit{en }$\beta$ cuando se de que existan palabras
$\delta,\gamma$ tales que $\beta=\delta\alpha\gamma$ y $\left\vert
\delta\right\vert =i-1$. Intuitivamente hablando $\alpha$ ocurre a partir de
$i$ en $\beta$ cuando se de que si comensamos a leer desde el lugar $i$-esimo
de $\beta$ en adelante, leeremos la palabra $\alpha$ completa y luego
posiblemente seguiran otros simbolos.

Notese que una palabra $\alpha$ puede ocurrir en $\beta$, a partir de $i$, y
tambien a partir de $j$, con $i\neq j$. En virtud de esto, hablaremos de las
distintas ocurrencias de $\alpha$ en $\beta$. Por ejemplo hay dos ocurrencias
de la palabra $aba$\ en la palabra%
\[
cccccccabaccccabaccccc
\]
y tambien hay dos ocurrencias de la palabra $aba$\ en la palabra%
\[
cccccccababacccccccccc
\]
En el primer caso diremos que dichas ocurrencias de $aba$ son
\textit{disjuntas}, en cambio en el segundo caso puede apreciarse que las dos
ocurrencias se superponen en una posicion. No definiremos en forma matematica
precisa el concepto de ocurrencia pero lo describiremos con ejemplos de manera
que el lector no tendra problemas en comprenderlo y manejarlo en forma correcta.

A veces diremos que una ocurrencia esta \textit{contenida} o \textit{sucede}
dentro de otra. Por ejemplo la segunda ocurrencia de $ab$ en
$babbbfabcccfabccc$ esta contenida en la primer ocurrencia de $fabc$ en
$babbbfabcccfabccc$. Tambien haremos \textit{reemplazos} de ocurrencias por
palabras. Por ejemplo el resultado de reemplazar la primer ocurrencia de $abb
$ en $ccabbgfgabbgg$ por $oolala$ es la palabra $ccoolalagfgabbgg$. El
resultado de reemplazar todas las ocurrencias de $aba$ en $ccabagfgabaggaba$
por $\$\$$ es la palabra $cc\$\$gfg\$\$gg\$\$$. En algunos casos deberemos
especificar que los reemplazos se hagan \textit{simultaneamente}. Por ejemplo
hablaremos del resultado de reemplazar en $\gamma$, simultaneamente, todas las
ocurrencias de $\alpha_{1}$ por $\beta_{1}$ y todas las de $\alpha_{2}$ por
$\beta_{2}$. Aqui la aclaracion de simultaneidad es importante ya que si
primero reemplazaramos las ocurrencias de $\alpha_{1}$ por $\beta_{1}$ y
despues las de $\alpha_{2}$ por $\beta_{2}$, el resultado puede cambiar porque
en $\beta_{1}$ puede haber ocurrencias de $\alpha_{2}$. Dejamos al lector dar
un ejemplo en el cual el reemplazo secuencial y el simultaneo dan distintos resultados.

\bigskip

\begin{lemma}
[Ocurrencias de terminos en terminos]\label{reemp-ter-en-ter}Sean $r,s,t\in
T^{\tau}$.

\begin{enumerate}
\item[(a)] Si $s\neq t=f(t_{1},...,t_{n})$ y $s$ ocurre en $t$, entonces dicha
ocurrencia sucede dentro de algun $t_{j}$, $j=1,...,n$.

\item[(b)] Si $r,s$\ ocurren en $t$, entonces dichas ocurrencias son disjuntas
o una ocurre dentro de otra. En particular, las distintas ocurrencias de $r$
en $t$ son disjuntas.

\item[(c)] Si $t^{\prime}$\ es el resultado de reemplazar una ocurrencia de
$s$\ en $t$\ por $r$, entonces $t^{\prime}\in T^{\tau}$.
\end{enumerate}
\end{lemma}

\begin{proof}
(a) Supongamos la ocurrencia de $s$ comienza en algun $t_{j}$. Entonces el
Lema \ref{superposicion} nos conduce a que dicha ocurrencia debera estar
contenida en $t_{j}$. Veamos que la ocurrencia de $s$ no puede ser a partir de
un $i\in\{1,...,\left\vert f\right\vert \}$. Supongamos lo contrario. Tenemos
entonces que $s$ debe ser de la forma $g(s_{1},...,s_{m})$ ya que no puede
estar en $Var\cup\mathcal{C}$. Notese que $i\neq1$ ya que en caso contrario
$s$ seria un tramo inicial propio de $t$. Pero entonces $g$ debe ser un tramo
final propio de $f$, lo cual es absurdo. Ya que $s$ no puede comenzar con
parentesis o coma, hemos contemplado todos los posibles casos de comienzo de
la ocurrencia de $s$ en $t$.

(b) y (c) pueden probarse por induccion, usando (a).
\end{proof}

\smallskip\smallskip

\textbf{Nota:} Es importante notar que si bien no hemos definido en forma
presisa el concepto de ocurrencia o de reemplazo de ocurrencias, la prueba del
lema anterior es rigurosa en el sentido de que solo usa propiedades del
concepto de ocurrencia y reemplazo de ocurrencias las cuales deberan ser
comunes a cualquier definicion o formulacion matematica que se hiciera de
aquellos conceptos. En este caso, es posible dar una defincion presisa y
satisfactoria de dichos conceptos aunque para otros conceptos tales como el de
prueba absoluta de consistencia, aun no se ha encontrado una formulacion
matematica adecuada.

\bigskip

\subsubsection{Formulas}

Sea $\tau$ un tipo. Las palabras de alguna de las siguientes dos formas%
\[%
\begin{array}
[c]{l}%
(t\equiv s),\;\text{con }t,s\in T^{\tau}\\
r(t_{1},...,t_{n})\text{, con }r\in\mathcal{R}_{n}\text{,}\ n\geq1\text{ y
}t_{1},...,t_{n}\in T^{\tau}%
\end{array}
\]
seran llamadas \textit{formulas atomicas de tipo }$\tau$. Por ejemplo si
$\tau=(\{\mathrm{uno},\mathrm{doli}\},\{\mathrm{MAS},\mathrm{P}%
\},\{\mathrm{Her}\},a)$, con $a$ dado por $a(\mathrm{MAS})=4$, $a(\mathrm{P}%
)=1$ y $a(\mathrm{Her})=3$, entonces

\begin{enumerate}
\item[-] $(\mathrm{uno}\equiv\mathrm{doli})$

\item[-] $(\mathsf{X}\mathit{15666}\mathbf{9}\equiv\mathrm{doli})$

\item[-] $\mathrm{Her}(\mathrm{uno},\mathsf{X}\mathbf{4},\mathrm{doli})$

\item[-] $(\mathrm{MAS}(\mathrm{uno},\mathrm{doli},\mathsf{X}\mathit{1}%
\mathbf{9},\mathsf{X}\mathbf{5})\equiv\mathrm{uno})$

\item[-] $\mathrm{Her}(\mathrm{P}(\mathrm{P}(\mathrm{uno})),\mathrm{MAS}%
(\mathrm{P}(\mathsf{X}\mathbf{4}),\mathrm{doli},\mathsf{X}\mathit{1}%
\mathbf{9},\mathsf{X}\mathbf{5}),\mathsf{X}\mathit{1}\mathbf{9})$
\end{enumerate}

son formulas atomicas de tipo $\tau$.

\bigskip

Dado un tipo $\tau$, definamos recursivamente los conjuntos de palabras
$F_{k}^{\tau}$, con $k\geq0$, de la siguiente manera:%
\[%
\begin{array}
[c]{ccl}%
F_{0}^{\tau} & = & \{\text{formulas atomicas de tipo }\tau\}\\
F_{k+1}^{\tau} & = & F_{k}^{\tau}\cup\{\lnot\varphi:\varphi\in F_{k}^{\tau
}\}\cup\{(\varphi\vee\psi):\varphi,\psi\in F_{k}^{\tau}\}\cup\\
&  & \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \{(\varphi\wedge\psi):\varphi,\psi\in
F_{k}^{\tau}\}\cup\{(\varphi\rightarrow\psi):\varphi,\psi\in F_{k}^{\tau
}\}\cup\\
&  & \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \{(\varphi\leftrightarrow
\psi):\varphi,\psi\in F_{k}^{\tau}\}\cup\{\forall v\varphi:\varphi\in
F_{k}^{\tau}\text{ y }v\in Var\}\cup\\
&  &
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \{\exists
v\varphi:\varphi\in F_{k}^{\tau}\text{ y }v\in Var\}
\end{array}
\]
Sea%
\[
F^{\tau}=\bigcup_{k\geq0}F_{k}^{\tau}%
\]
Los elementos de $F^{\tau}$ seran llamados \textit{formulas de tipo }$\tau$.

Algunos ejemplos:

\begin{enumerate}
\item[(E1)] Sea $\tau=(\{\mathrm{uno},\mathrm{doli}\},\{\mathrm{MAS}%
,\mathrm{P}\},\{\mathrm{Her}\},a)$, con $a$ dado por $a(\mathrm{MAS})=4$,
$a(\mathrm{P})=1$ y $a(\mathrm{Her})=3$. Entonces

\begin{enumerate}
\item $\lnot((\mathsf{X}\mathbf{1}\equiv\mathsf{X}\mathbf{2})\wedge
\mathrm{Her}(\mathrm{P}(\mathrm{doli}),\mathrm{doli},\mathsf{X}\mathit{1}%
\mathbf{9}))$

\item $\exists\mathsf{X}\mathbf{9}\mathrm{Her}(\mathrm{doli},\mathrm{doli}%
,\mathsf{X}\mathbf{9})$

\item $\exists\mathsf{X}\mathbf{9}\lnot(\mathrm{uno}\equiv\mathrm{doli})$

\item $\lnot\exists\mathsf{X}\mathbf{9}\forall\mathsf{X}\mathbf{7(}%
\mathrm{Her}(\mathsf{X}\mathbf{9},\mathrm{doli},\mathsf{X}\mathbf{7}%
)\rightarrow(\mathrm{P}(\mathrm{doli})\equiv\mathsf{X}\mathbf{7}))$

\item $\forall\mathsf{X}\mathit{555}\mathbf{9}\forall\mathsf{X}\mathbf{7}%
\exists\mathsf{X}\mathit{5}\mathbf{1}(\mathrm{MAS}(\mathrm{uno},\mathrm{doli}%
,\mathsf{X}\mathit{1}\mathbf{9},\mathsf{X}\mathbf{5})\equiv\mathrm{uno}%
)\rightarrow\mathrm{Her}(\mathrm{doli},\mathrm{doli},\mathrm{doli}))$
\end{enumerate}

son formulas de tipo $\tau$

\item[(E2)] Sea $\tau=(\{0,1\},\{\mathsf{s},\mathsf{i}\},\{\leq
\},\{(\mathsf{s},2),(\mathsf{i},2),(\leq,2)\})$ el tipo de los reticulados
cuaterna. Entonces

\begin{enumerate}
\item $\mathrm{\leq}(1,0)$

\item $\mathrm{\leq}(\mathsf{X}\mathbf{1},\mathsf{X}\mathbf{2})$

\item $\lnot(\mathsf{s}(\mathsf{X}\mathbf{2},\mathsf{X}\mathbf{1}%
)\equiv\mathsf{X}\mathbf{2}))$

\item $\forall\mathsf{X}\mathbf{2}\forall\mathsf{X}\mathbf{1}\mathrm{\leq
}(\mathsf{X}\mathbf{2},\mathsf{s}(\mathsf{X}\mathbf{2},\mathsf{X}\mathbf{1}))$

\item $((\mathsf{i}(\mathsf{X}\mathbf{1},\mathsf{X}\mathbf{2})\equiv
0)\wedge(\mathsf{s}(\mathsf{X}\mathbf{1},\mathsf{X}\mathbf{2})\equiv1))$

\item $\forall\mathsf{X}\mathbf{9}\exists\mathsf{X}\mathbf{1}((0\equiv
\mathsf{X}\mathbf{1})\rightarrow\exists\mathsf{X}\mathbf{1}\lnot\mathrm{\leq
}(\mathsf{X}\mathbf{2},\mathsf{s}(\mathsf{X}\mathbf{2},\mathsf{X}%
\mathbf{1})))$
\end{enumerate}

son formulas de tipo $\tau$. Cabe destacar que $(\mathsf{X}\mathbf{1}%
\leq\mathsf{X}\mathbf{2})$ no es una formula de tipo $\tau$ aunque, como
veremos en los ejercicios esto no es trivial de la definicion de formula y
requiere de una demostracion.
\end{enumerate}

\bigskip

El siguiente lema es la herramienta basica que usaremos para probar
propiedades acerca de los elementos de $F^{\tau}$.

\begin{lemma}
[Menu para formulas]\label{basic2}Supongamos $\varphi\in F_{k}^{\tau}$, con
$k\geq1$. Entonces $\varphi$ es de alguna de las siguientes formas

$\varphi=(t\equiv s),$ con $t,s\in T^{\tau}$\textit{.}

$\varphi=r(t_{1},...,t_{n}),$ con $r\in\mathcal{R}_{n}$\textit{, }%
$t_{1},...,t_{n}\in T^{\tau}$\textit{\ }

$\varphi=(\varphi_{1}\eta\varphi_{2}),$ con $\eta\in\{\wedge,\vee
,\rightarrow,\leftrightarrow\},\;\varphi_{1},\varphi_{2}\in F_{k-1}^{\tau}$

$\varphi=\lnot\varphi_{1},$ con $\varphi_{1}\in F_{k-1}^{\tau}$

$\varphi=Qv\varphi_{1},$ con $Q\in\{\forall,\exists\},\;v\in Var$ y
$\varphi_{1}\in F_{k-1}^{\tau}.$
\end{lemma}

\begin{proof}
Induccion en $k$.
\end{proof}

\bigskip

\paragraph{Unicidad de la lectura de formulas}

Tal como para el caso de terminos veremos que las formulas tambien tienen su
unicidad de lectura.

\begin{lemma}
\label{basic3}Sea $\tau$ un tipo.

\begin{enumerate}
\item[(a)] Supongamos que $\Sigma$ es tal que $F^{\tau}\subseteq\Sigma^{\ast}%
$. Entonces $del(\varphi)\in Bal$, para cada $\varphi\in F^{\tau}$.

\item[(b)] Sea $\varphi\in F_{k}^{\tau}$, con $k\geq0$. Existen $x\in
(\{\lnot\}\cup\{Qv:Q\in\{\forall,\exists\}$ y $v\in Var\})^{\ast}$ y
$\varphi_{1}\in F^{\tau}$ tales que $\varphi=x\varphi_{1}$ y $\varphi_{1} $ es
de la forma $(\psi_{1}\eta\psi_{2})$ o atomica. En particular toda formula
termina con el simbolo $)$.
\end{enumerate}
\end{lemma}

\begin{proof}
(b) Induccion en $k$. El caso $k=0$ es trivial. Supongamos (b) vale para cada
$\varphi\in F_{k}^{\tau}$ y sea $\varphi\in F_{k+1}^{\tau}$. Hay varios casos
de los cuales haremos solo dos

CASO $\varphi=(\psi_{1}\eta\psi_{2})$, con $\psi_{1},\psi_{2}\in F_{k}^{\tau}$
y $\eta\in\{\vee,\wedge,\rightarrow,\leftrightarrow\}$.

\noindent Podemos tomar $x=\varepsilon$ y $\varphi_{1}=\varphi$.

CASO $\varphi=Qx_{i}\psi$, con $\psi\in F_{k}^{\tau}$, $i\geq1$ y
$Q\in\{\forall,\exists\}$.

\noindent Por HI hay $\bar{x}\in(\{\lnot\}\cup\{Qv:Q\in\{\forall,\exists\}$ y
$v\in Var\})^{\ast}$ y $\psi_{1}\in F^{\tau}$ tales que $\psi=\bar{x}\psi_{1}$
y $\psi_{1}$ es de la forma $(\gamma_{1}\eta\gamma_{2})$ o atomica. Entonces
es claro que $x=Qx_{i}\bar{x}$ y $\varphi_{1}=\psi_{1}$ cumplen (b).
\end{proof}

\bigskip

\begin{lemma}
Ninguna formula es tramo final propio de una formula atomica, es decir, si
$\varphi=x\psi$, con $\varphi\in F_{0}^{\tau}$ y $\psi\in F^{\tau}$, entonces
$x=\varepsilon$.
\end{lemma}

\begin{proof}
Si $\varphi$ es de la forma $(t\equiv s)$, entonces $\left\vert
del(y)\right\vert _{(}-\left\vert del(y)\right\vert _{)}<0$ para cada tramo
final propio $y$ de $\varphi$, lo cual termina el caso ya que $del(\psi)$ es
balanceada. Supongamos entonces $\varphi=r(t_{1},...,t_{n})$. Notese que
$\psi$ no puede ser tramo final de $t_{1},...,t_{n})$ ya que $del(\psi)$ es
balanceada y $\left\vert del(y)\right\vert _{(}-\left\vert del(y)\right\vert
_{)}<0$ para cada tramo final $y$ de $t_{1},...,t_{n})$. Es decir que
$\psi=y(t_{1},...,t_{n})$, para algun tramo final $y$ de $r$. Ya que en $\psi$
no ocurren cuantificadores ni nexos ni el simbolo $\equiv$ el Lema
\ref{basic2} nos dice $\psi=\tilde{r}(s_{1},...,s_{m})$, con $\tilde{r}%
\in\mathcal{R}_{m}$\textit{, }$m\geq1$ y $s_{1},...,s_{m}\in T^{\tau}$. Ahora
es facil usando un argumento paresido al usado en la prueba del Teorema
\ref{le-un-de-te} concluir que $m=n$, $s_{i}=t_{i}$, $i=1,...,n$ y $\tilde{r}$
es tramo final de $r$. Por (3) de la definicion de tipo tenemos que $\tilde
{r}=r$ lo cual nos dice que $\varphi=\psi$ y $x=\varepsilon$
\end{proof}

\bigskip

\begin{lemma}
\textit{Si }$\varphi=x\psi$, con $\varphi,\psi\in F^{\tau}$ y $x$ sin
parentesis, entonces $x\in(\{\lnot\}\cup\{Qv:Q\in\{\forall,\exists\}$ y $v\in
Var\})^{\ast}$
\end{lemma}

\begin{proof}
Por induccion en el $k$ tal que $\varphi\in F_{k}^{\tau}$. El caso $k=0$ es
probado en el lema anterior. Asumamos que el resultado vale cuando $\varphi\in
F_{k}^{\tau}$ y veamos que vale cuando $\varphi\in F_{k+1}^{\tau}$. Mas aun
supongamos $\varphi\in F_{k+1}^{\tau}-F_{k}^{\tau}$. Primero haremos el caso
en que $\varphi=Qv\varphi_{1},$ con $Q\in\{\forall,\exists\},\;v\in Var$ y
$\varphi_{1}\in F_{k}^{\tau}$. Supongamos $x\neq\varepsilon$. Ya que $\psi$ no
comienza con simbolos de $v$, tenemos que $\psi$ debe ser tramo final de
$\varphi_{1}$ lo cual nos dice que hay una palabra $x_{1}$ tal que $x=Qvx_{1}$
y $\varphi_{1}=x_{1}\psi$. Por HI tenemos que $x_{1}\in(\{\lnot\}\cup
\{Qv:Q\in\{\forall,\exists\}$ y $v\in Var\})^{\ast}$ con lo cual $x\in
(\{\lnot\}\cup\{Qv:Q\in\{\forall,\exists\}$ y $v\in Var\})^{\ast}$. El caso en
el que $\varphi=\lnot\varphi_{1}$ con $\varphi_{1}\in F_{k}^{\tau}$, es
similar. Note que no hay mas casos posibles ya que $\varphi$ no puede comenzar
con $($ porque en $x$ no ocurren parentesis por hipotesis
\end{proof}

\bigskip

\begin{proposition}
[Mordisqueo de formulas]\label{mordisqueo}\textit{Si }$\varphi,\psi\in
F^{\tau}$\textit{\ y }$x,y,z$\textit{\ son tales que }$\varphi=xy,$%
\textit{\ }$\psi=yz$\textit{\ y }$y\neq\varepsilon,$\textit{\ entonces
}$z=\varepsilon$\textit{\ y }$x\in(\{\lnot\}\cup\{Qv:Q\in\{\forall,\exists\}$
y $v\in Var\})^{\ast}$.\textit{\ En particular ningun tramo inicial propio de
una formula es una formula.}
\end{proposition}

\begin{proof}
Ya que $\varphi$ termina con $)$ tenemos que $del(y)\neq\varepsilon.$ Por un
lema anterior tenemos que $del(\varphi),del(\psi)\in Bal$. Ademas%
\begin{align*}
del(\varphi)  & =del(x)del(y)\\
del(\psi)  & =del(y)del(z)
\end{align*}
La primera igualdad, por (1) y (3) del Lema \ref{basicas de balanceadas}, nos
dice que%
\[
\left\vert del(y)\right\vert _{(}-\left\vert del(y)\right\vert _{)}\leq0,
\]
y la segunda que%
\[
\left\vert del(y)\right\vert _{(}-\left\vert del(y)\right\vert _{)}\geq0,
\]
por lo cual%
\[
\left\vert del(y)\right\vert _{(}-\left\vert del(y)\right\vert _{)}=0
\]
Pero entonces (3) del Lema \ref{basicas de balanceadas} nos dice que $del(y)$
no puede ser tramo final propio de $del(\varphi)$, por lo cual debe suceder
que $del(y)=del(\varphi)$, ya que $del(y)\neq\varepsilon$. Claramente entonces
obtenemos que $del(x)=\varepsilon$. Similarmente se puede ver que
$del(z)=\varepsilon$. Pero $\psi$ termina con $)$ lo cual nos dice que
$z=\varepsilon$. Es decir que $\varphi=x\psi$. Por el lema anterior tenemos
que $x\in(\{\lnot\}\cup\{Qv:Q\in\{\forall,\exists\}$ y $v\in Var\})^{\ast}$
\end{proof}

\bigskip

\begin{theorem}
[Lectura unica de formulas]Dada $\varphi\in F^{\tau}$\ se da una y solo una de
las siguientes:

\begin{enumerate}
\item[(1)] $\varphi=(t\equiv s),$ con $t,s\in T^{\tau}$

\item[(2)] $\varphi=r(t_{1},...,t_{n}),$ con $r\in\mathcal{R}_{n}$\textit{,
}$t_{1},...,t_{n}\in T^{\tau}$

\item[(3)] $\varphi=(\varphi_{1}\eta\varphi_{2}),$ con $\eta\in\{\wedge
,\vee,\rightarrow,\leftrightarrow\},\;\varphi_{1},\varphi_{2}\in F^{\tau}$

\item[(4)] $\varphi=\lnot\varphi_{1},$ con $\varphi_{1}\in F^{\tau}$

\item[(5)] $\varphi=Qv\varphi_{1},$ con $Q\in\{\forall,\exists\},\;\varphi
_{1}\in F^{\tau}$ y $v\in Var.$
\end{enumerate}

\noindent Mas aun, en los puntos (1), (2), (3), (4) y (5) tales
descomposiciones son unicas.
\end{theorem}

\begin{proof}
Si una formula $\varphi$ satisface (1), entonces $\varphi$ no puede contener
simbolos del alfabeto $\{\wedge,\vee,\rightarrow,\leftrightarrow\}$ lo cual
garantiza que $\varphi$ no puede satisfacer (3). Ademas $\varphi$ no puede
satisfacer (2) o (4) o (5) ya que $\varphi$ comienza con $($. En forma analoga
se puede terminar de ver que las propiedades (1),...,(5) son excluyentes.

La unicidad en las descomposiciones de (4) y (5) es obvia. La de (3) se
desprende facilmente del lema anterior y la de los puntos (1) y (2) del lema
analogo para terminos.
\end{proof}

\bigskip

\paragraph{Subformulas}

Una formula $\varphi$ sera llamada una \textit{subformula }(\textit{propia}%
)\textit{\ }de una formula $\psi$, cuando $\varphi$ (sea no igual a $\psi$ y)
sea subpalabra de $\psi$.

\begin{lemma}
[Ocurrencias de formulas en formulas]\label{ocurrencias}Sea $\tau$ un tipo.

\begin{enumerate}
\item[(a)] Las formulas atomicas no tienen subformulas propias.

\item[(b)] Si $\varphi$\ ocurre propiamente en $(\psi\eta\gamma),$\ entonces
tal ocurrencia es en $\psi$\ o en $\gamma.$

\item[(c)] Si $\varphi$\ ocurre propiamente en $\lnot\psi$,\ entonces tal
ocurrencia es en $\psi.$

\item[(d)] Si $\varphi$\ ocurre propiamente en $Qx_{k}\psi,$\ entonces tal
ocurrencia es en $\psi.$

\item[(e)] Si $\varphi_{1},\varphi_{2}$\ ocurren en $\varphi,$\ entonces
dichas ocurrencias son disjuntas o una contiene a la otra.

\item[(f)] Si $\lambda^{\prime}$\ es el resultado de reemplazar alguna
ocurrencia de $\varphi$\ en $\lambda$\ por $\psi$, entonces $\lambda^{\prime
}\in F^{\tau}$.
\end{enumerate}
\end{lemma}

\begin{proof}
Ejercicio.
\end{proof}

\bigskip

\paragraph{\label{VariablesLibres}Variables libres}

Recordemos que dadas palabras $\alpha,\beta\in\Sigma^{\ast}$, con $\left\vert
\alpha\right\vert ,\left\vert \beta\right\vert \geq1$, y un natural
$i\in\{1,...,\left\vert \beta\right\vert \}$, se dice que $\alpha$
\textit{ocurre a partir de }$i$ \textit{en }$\beta$ cuando se de que existan
palabras $\delta,\gamma$ tales que $\beta=\delta\alpha\gamma$ y $\left\vert
\delta\right\vert =i-1$. Intuitivamente hablando $\alpha$ ocurre a partir de
$i$ en $\beta$ cuando se de que si comensamos a leer desde el lugar $i$-esimo
de $\beta$, en adelante, entonces leeremos la palabra $\alpha$ completa y
luego posiblemente seguiran otros simbolos.

Definamos recursivamente la relacion $"v\mathit{\ ocurre\ libremente\ en\ }%
\varphi\mathit{\ a\ partir\ de\ }i"$, donde $v\in Var$, $\varphi\in F^{\tau}$
y $i\in\{1,...,\left\vert \varphi\right\vert \}$, de la siguiente manera:

\begin{enumerate}
\item[(1)] Si $\varphi$ es atomica, entonces $v$ ocurre libremente en
$\varphi$ a partir de $i$ sii $v$ ocurre en $\varphi$ a partir de $i$

\item[(2)] Si $\varphi=(\varphi_{1}\eta\varphi_{2})$, entonces $v$ ocurre
libremente en $\varphi$ a partir de $i$ sii se da alguna de las siguientes

\begin{enumerate}
\item[(a)] $v$ ocurre libremente en $\varphi_{1}$ a partir de $i-1$

\item[(b)] $v$ ocurre libremente en $\varphi_{2}$ a partir de $i-\left\vert
(\varphi_{1}\eta\right\vert $
\end{enumerate}

\item[(3)] Si $\varphi=\lnot\varphi_{1}$, entonces $v$ ocurre libremente en
$\varphi$ a partir de $i$ sii $v$ ocurre libremente en $\varphi_{1}$ a partir
de $i-1$

\item[(4)] Si $\varphi=Qw\varphi_{1}$, entonces $v$ ocurre libremente en
$\varphi$ a partir de $i$ sii $v\neq w$ y $v$ ocurre libremente en
$\varphi_{1}$ a partir de $i-\left\vert Qw\right\vert $
\end{enumerate}

Dados $v\in Var$, $\varphi\in F^{\tau}$ y $i\in\{1,...,\left\vert
\varphi\right\vert \}$, diremos que $"v$ \textit{ocurre acotadamente en}
$\varphi$ \textit{a partir de} $i"$ cuando $v$ ocurre en $\varphi$ a partir de
$i$ y $v$ no ocurre libremente en $\varphi$ a partir de $i$.

\bigskip

Algunos ejemplos:

\begin{enumerate}
\item Sea $\tau=(\{\mathrm{uno},\mathrm{doli}\},\{\mathrm{MAS},\mathrm{P}%
\},\{\mathrm{Her}\},a)$, con $a$ dado por $a(\mathrm{MAS})=4$, $a(\mathrm{P}%
)=1$ y $a(\mathrm{Her})=3$.

\begin{enumerate}
\item $\mathsf{X}\mathbf{9}$ ocurre libremente en $\mathrm{Her}(\mathrm{doli}%
,\mathrm{doli},\mathsf{X}\mathbf{9})$ a partir de $15$

\item $\mathsf{X}\mathbf{9}$ ocurre acotadamente en $\exists\mathsf{X}%
\mathbf{9}\mathrm{Her}(\mathrm{doli},\mathrm{doli},\mathsf{X}\mathbf{9})$ a
partir de $2$ y de $18$

\item $\mathsf{X}\mathbf{2}$ ocurre libremente en $(\exists\mathsf{X}%
\mathbf{2}\mathrm{Her}(\mathsf{X}\mathbf{2},\mathsf{X}\mathbf{7}%
,\mathrm{uno})\rightarrow\mathrm{Her}(\mathsf{X}\mathbf{2},\mathsf{X}%
\mathbf{7},\mathrm{uno}))$ a partir de $16$ y acotadamente a partir de $3$ y
$7$.

\item Sea $\varphi=((\mathsf{X}\mathbf{1}\equiv\mathsf{X}\mathbf{2}%
)\wedge\exists\mathsf{X}\mathbf{2}\mathrm{Her}(\mathrm{P}(\mathrm{doli}%
),\mathrm{doli},\mathsf{X}\mathbf{2}))$. La variable $\mathsf{X}\mathbf{2}$
ocurre libremente en $\varphi$ a partir de $6$ y ocurre acotadamente en
$\varphi$ a partir de $11$ y de $30$.
\end{enumerate}
\end{enumerate}

\bigskip

Dada una formula $\varphi$, sea%

\[
Li(\varphi)=\{v\in Var:\text{hay un }i\text{ tal que }v\text{ ocurre
libremente en }\varphi\text{ a partir de }i\}\text{.}%
\]
Los elementos de $Li(\varphi)$ seran llamados \textit{variables libres de
}$\varphi$. Por ejemplo, si $\varphi$ es la formula%
\[
(\exists\mathsf{X}\mathbf{7}(\mathsf{X}\mathbf{7}\equiv\mathsf{X}%
\mathbf{6})\rightarrow((\mathsf{X}\mathbf{1}\equiv\mathsf{X}\mathbf{2}%
)\wedge\exists\mathsf{X}\mathbf{2}\mathrm{Her}(\mathrm{doli},\mathrm{doli}%
,\mathsf{X}\mathbf{2})))
\]
tenemos que $Li(\varphi)=\{\mathsf{X}\mathbf{1},\mathsf{X}\mathbf{2}%
,\mathsf{X}\mathbf{6}\}$ (justifique). Tambien si%
\[
\varphi=(\exists\mathsf{X}\mathbf{2}\mathrm{Her}(\mathsf{X}\mathbf{2}%
,\mathsf{X}\mathbf{7},\mathrm{uno})\rightarrow\mathrm{Her}(\mathsf{X}%
\mathbf{2},\mathsf{X}\mathbf{7},\mathrm{uno}))
\]
entonces $Li(\varphi)=\{\mathsf{X}\mathbf{2},\mathsf{X}\mathbf{7}\}$.

Una \textit{sentencia }sera una formula $\varphi$ tal que $Li(\varphi
)=\emptyset$. Usaremos $S^{\tau}$ para denotar el conjunto de las sentencias
de tipo $\tau$.

\begin{lemma}
\begin{enumerate}
\item[(a)] $Li((t\equiv s))=\{v\in Var:v$ ocurre en $t$ o $v$ ocurre en $s\}.
$

\item[(b)] $Li(r(t_{1},...,t_{n}))=\{v\in Var:v$ ocurre en algun $t_{i}\}.$

\item[(c)] $Li(\lnot\varphi)=Li(\varphi)$

\item[(d)] $Li((\varphi\eta\psi))=Li(\varphi)\cup Li(\psi).$

\item[(e)] $Li(Qx_{j}\varphi)=Li(\varphi)-\{x_{j}\}.$
\end{enumerate}
\end{lemma}

\begin{proof}
(a) y (b) son triviales de las definiciones y dejadas al lector

(d) Supongamos $v\in Li((\varphi\eta\psi))$, entonces hay un $i$ tal que $v$
ocurre libremente en $(\varphi\eta\psi)$ a partir de $i$. Por definicion
tenemos que ya sea $v$ ocurre libremente en $\varphi$ a partir de $i-1$ o $v$
ocurre libremente en $\psi$ a partir de $i-\left\vert (\varphi\eta\right\vert
$, con lo cual $v\in Li(\varphi)\cup Li(\psi)$

Supongamos ahora que $v\in Li(\varphi)\cup Li(\psi)$. S.p.d.g. supongamos
$v\in Li(\psi)$. Por definicion tenemos que hay un $i$ tal que $v$ ocurre
libremente en $\psi$ a partir de $i$. Pero notese que esto nos dice por
definicion que $v$ ocurre libremente en $(\varphi\eta\psi)$ a partir de
$i+\left\vert (\varphi\eta\right\vert $ con lo cual $v\in Li((\varphi\eta
\psi))$.

(c) es similar a (d)

(e) Supongamos $v\in Li(Qx_{j}\varphi)$, entonces hay un $i$ tal que $v$
ocurre libremente en $Qx_{j}\varphi$ a partir de $i$. Por definicion tenemos
que $v\neq x_{j}$ y $v$ ocurre libremente en $\varphi$ a partir de
$i-\left\vert Qx_{j}\right\vert $, con lo cual $v\in Li(\varphi)-\{x_{j}\}$

Supongamos ahora que $v\in Li(\varphi)-\{x_{j}\}$. Por definicion tenemos que
hay un $i$ tal que $v$ ocurre libremente en $\varphi$ a partir de $i$. Ya que
$v\neq x_{j}$ esto nos dice por definicion que $v$ ocurre libremente en
$Qx_{j}\varphi$ a partir de $i+\left\vert Qx_{j}\right\vert $, con lo cual
$v\in Li(Qx_{j}\varphi)$.
\end{proof}

\bigskip

\subsubsection{Modelo matematico de las formulas elementales de tipo $\tau$}

Si $\tau=(\mathcal{C},\mathcal{F},\mathcal{R},a)$ es un tipo, diremos que
$\tau^{\prime}$ es una \textit{extension de }$\tau$\textit{\ por nombres de
constante} si $\tau^{\prime}$ es de la forma $(\mathcal{C}^{\prime
},\mathcal{F},\mathcal{R},a)$ con $\mathcal{C}^{\prime}$ tal que
$\mathcal{C}\subseteq\mathcal{C}^{\prime}$.

Hemos definido las formulas de tipo $\tau$ con la intencion de dar un modelo
matematico del concepto de formula elemental de tipo $\tau$ pero deberiamos
notar que en las formulas de tipo $\tau$ no hay nombres de elementos fijos por
lo cual dichas formulas son un modelo matematico solo de ciertas formulas
elementales de tipo $\tau$, a saber aquellas en las cuales no hay nombres de
elementos fijos. Recordemos que estos nombres se usaban en las pruebas
elementales para denotar elementos fijos (a veces arbitrarios y otras veces
que cumplian alguna propiedad).

Cuando un matematico realiza una prueba elemental en una teoria elemental
$(\Sigma,\tau)$ comienza la misma imaginando una estructura de tipo $\tau$ de
la cual lo unico que sabe es que cumple las sentencias de $\Sigma$. Luego
cuando fija un elemento y le pone nombre, digamos $b$, podemos pensar que
expandio su estructura imaginaria a una de tipo $(\mathcal{C}\cup
\{b\},\mathcal{F},\mathcal{R},a)$ y continua su razonamiento. Esto lo puede
hacer muchas veces a lo largo de una prueba por lo cual su estructura
imaginaria va cambiando de tipo. Esta mecanica de prueba del matematico nos
deja ver que es natural modelizar las formulas elementales de tipo $\tau$ con
formulas de tipo $\tau_{1}$, donde $\tau_{1}$ es alguna extension de $\tau$
por constantes.

\bigskip

\subsection{Modelo matematico del valor de verdad de una formula}

En esta seccion daremos una definicion matematica que modeliza la idea
intuitiva de cuando una formula de tipo $\tau$ es verdadera en una estructura
dada para una asignacion de elementos a las variables libres de dicha formula.
Esto corresponde al punto (2) del programa de logica enunciado en la Seccion
\ref{programa}.

\subsubsection{El valor de un termino en una estructura}

Sea $\mathbf{A}=(A,i)$ una estructura de tipo $\tau$. Una \textit{asignacion
de }$\mathbf{A}$\textit{\ }sera un elemento de $A^{\mathbf{N}}=\{$infinituplas
de elementos de $A\}$. Si $\vec{a}=(a_{1},a_{2},...)$ es una asignacion,
entonces diremos que $a_{j}$ \textit{es el valor que }$\vec{a}$\textit{\ le
asigna a la variable }$x_{j}$.

Dada una estructura $\mathbf{A}$ de tipo $\tau$, un termino $t\in T^{\tau}$ y
una asignacion $\vec{a}=(a_{1},a_{2},...)\in A^{\mathbf{N}}$ definamos
recursivamente $t^{\mathbf{A}}[\vec{a}]$ de la siguiente manera

\begin{enumerate}
\item[(1)] Si $t=x_{i}\in Var$, entonces $t^{\mathbf{A}}[\vec{a}]=a_{i}$

\item[(2)] Si $t=c\in\mathcal{C}$, entonces $t^{\mathbf{A}}[\vec{a}]=i(c)$

\item[(3)] Si $t=f(t_{1},...,t_{n})$, con $f\in\mathcal{F}_{n},\;n\geq1$ y
$t_{1},...,t_{n}\in T^{\tau}$, entonces $t^{\mathbf{A}}[\vec{a}]=i(f)(t_{1}%
^{\mathbf{A}}[\vec{a}],...,t_{n}^{\mathbf{A}}[\vec{a}])$
\end{enumerate}

\noindent El elemento $t^{\mathbf{A}}[\vec{a}]$ sera llamado el \textit{valor
de }$t$\textit{\ en la estructura }$\mathbf{A}$\textit{\ para la asignacion
}$\vec{a}$.

Veamos un ejemplo. Sea $\tau$ el tipo%
\[
(\{\mathrm{uno},\mathrm{doli}\},\{\mathrm{MAS},\mathrm{P}\},\{\mathrm{Her}%
\},\{(\mathrm{MAS},4),(\mathrm{P},1),(\mathrm{Her},3)\})
\]
y sea $\mathbf{A}=(A,i)$ la estructura de tipo $\tau$ con universo
$A=\mathbf{R}$ y

\begin{enumerate}
\item[-] $i(\mathrm{uno})=9$

\item[-] $i(\mathrm{doli})=0$

\item[-] $i(\mathrm{MAS})$ igual a la operacion%
\[%
\begin{array}
[c]{rcl}%
\mathbf{R}^{4} & \rightarrow & \mathbf{R}\\
(x,y,z,w) & \rightarrow & 2x+4y
\end{array}
\]


\item[-] $i(\mathrm{P})$ igual a la operacion%
\[%
\begin{array}
[c]{rcl}%
\mathbf{R} & \rightarrow & \mathbf{R}\\
x & \rightarrow & 5^{x}%
\end{array}
\]


\item[-] $i(\mathrm{Her})=\{(x,y,z)\in\mathbf{R}^{3}:x.y.z=9\}$
\end{enumerate}

Sea $\vec{a}=(1,2,3,4,5,...)$. Claramente $\vec{a}$ es una asignacion de
$\mathbf{A}$. Se tiene que:

\begin{enumerate}
\item Si $t=\mathsf{X}\mathit{55}\mathbf{4}$, entonces $t^{\mathbf{A}}[\vec
{a}]=\mathsf{X}\mathit{55}\mathbf{4}^{\mathbf{A}}[\vec{a}]=554$ (por (1) de la
definicion recursiva de $t^{\mathbf{A}}[\vec{a}]$)

\item Si $t=\mathrm{uno}$, entonces $t^{\mathbf{A}}[\vec{a}]=\mathrm{uno}%
^{\mathbf{A}}[\vec{a}]=9$ (por (2) de la definicion recursiva de
$t^{\mathbf{A}}[\vec{a}]$)

\item Si $t=\mathrm{P}(\mathsf{X}\mathbf{3})$, entonces%
\begin{align*}
t^{\mathbf{A}}[\vec{a}]  & =\mathrm{P}(\mathsf{X}\mathbf{3})^{\mathbf{A}}%
[\vec{a}]\\
& =i(\mathrm{P})(\mathsf{X}\mathbf{3}^{\mathbf{A}}[\vec{a}])\text{ (por (3) de
la definicion de }t^{\mathbf{A}}[\vec{a}]\text{)}\\
& =i(\mathrm{P})(3)\\
& =5^{3}=125
\end{align*}


\item Si $t=\mathrm{MAS}(\mathsf{X}\mathbf{1},\mathrm{uno},\mathsf{X}%
\mathbf{3},\mathsf{X}\mathit{55}\mathbf{4})$, entonces%
\begin{align*}
t^{\mathbf{A}}[\vec{a}]  & =\mathrm{MAS}(\mathsf{X}\mathbf{1},\mathrm{uno}%
,\mathsf{X}\mathbf{3},\mathsf{X}\mathit{55}\mathbf{4})^{\mathbf{A}}[\vec{a}]\\
& =i(\mathrm{MAS})(\mathsf{X}\mathbf{1}^{\mathbf{A}}[\vec{a}],\mathrm{uno}%
^{\mathbf{A}}[\vec{a}],\mathsf{X}\mathbf{3}^{\mathbf{A}}[\vec{a}%
],\mathsf{X}\mathit{55}\mathbf{4}^{\mathbf{A}}[\vec{a}])\\
& =i(\mathrm{MAS})(1,9,3,554)\\
& =2.1+4.9=38
\end{align*}
\bigskip
\end{enumerate}

\begin{lemma}
\label{independencia del valor}Sea $\mathbf{A}$ una estructura de tipo $\tau$
y sea $t\in T^{\tau}$. Supongamos que $\vec{a},\vec{b}$ son asignaciones tales
que $a_{i}=b_{i},$ cada vez que $x_{i}$ ocurra en $t$. Entonces $t^{\mathbf{A}%
}[\vec{a}]=t^{\mathbf{A}}[\vec{b}]$.
\end{lemma}

\begin{proof}
Sea

\begin{enumerate}
\item[-] Teo$_{k}$: El lema vale para $t\in T_{k}^{\tau}$.
\end{enumerate}

Teo$_{0}$ es facil de probar. Veamos Teo$_{k}\Rightarrow$Teo$_{k+1}$.
Supongamos $t\in T_{k+1}^{\tau}-T_{k}^{\tau}$ y sean $\vec{a},\vec{b}$
asignaciones tales que $a_{i}=b_{i}$, cada vez que $x_{i}$ ocurra en $t$.
Notese que $t=f(t_{1},...,t_{n})$, con $f\in\mathcal{F}_{n}$,$\;n\geq1$ y
$t_{1},...,t_{n}\in T_{k}^{\tau}$. Notese que para cada $j=1,...,n$, tenemos
que $a_{i}=b_{i},$ cada vez que $x_{i}$ ocurra en $t_{j}$, lo cual por
Teo$_{k}$ nos dice que%
\[
t_{j}^{\mathbf{A}}[\vec{a}]=t_{j}^{\mathbf{A}}[\vec{b}]\text{, }j=1,...,n
\]
Se tiene entonces que%
\[%
\begin{array}
[c]{ccl}%
t^{\mathbf{A}}[\vec{a}] & = & i(f)(t_{1}^{\mathbf{A}}[\vec{a}],...,t_{n}%
^{\mathbf{A}}[\vec{a}])\text{ (por def de }t^{\mathbf{A}}[\vec{a}]\text{)}\\
& = & i(f)(t_{1}^{\mathbf{A}}[\vec{b}],...,t_{n}^{\mathbf{A}}[\vec{b}])\\
& = & t^{\mathbf{A}}[\vec{b}]\text{ (por def de }t^{\mathbf{A}}[\vec
{b}]\text{)}%
\end{array}
\]
\ 
\end{proof}

\bigskip

\subsubsection{La relacion $\models$}

Dada una asignacion $\vec{a}\in A^{\mathbf{N}}$ y $a\in A$, con $\downarrow
_{i}^{a}(\vec{a})$ denotaremos la asignacion que resulta de reemplazar en
$\vec{a}$ el $i$-esimo elemento por $a$. A continuacion definiremos
recursivamente la relacion $\mathbf{A}\models\varphi\lbrack\vec{a}]$, donde
$\mathbf{A}$ es una estructura de tipo $\tau$, $\vec{a}$ es una asignacion y
$\varphi\in F^{\tau}$. Escribiremos $\mathbf{A}\not \models \varphi\lbrack
\vec{a}]$ para expresar que no se da $\mathbf{A}\models\varphi\lbrack\vec{a}]$.

\begin{enumerate}
\item[(1)] Si $\varphi=(t\equiv s),$ entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ si y solo si $t^{\mathbf{A}%
}[\vec{a}]=s^{\mathbf{A}}[\vec{a}]$
\end{enumerate}

\item[(2)] Si $\varphi=r(t_{1},...,t_{m})$, entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ si y solo si $(t_{1}%
^{\mathbf{A}}[\vec{a}],...,t_{m}^{\mathbf{A}}[\vec{a}])\in i(r)$
\end{enumerate}

\item[(3)] Si $\varphi=(\varphi_{1}\wedge\varphi_{2})$, entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ si y solo si $\mathbf{A}%
\models\varphi_{1}[\vec{a}]$ y $\mathbf{A}\models\varphi_{2}[\vec{a}]$
\end{enumerate}

\item[(4)] Si $\varphi=(\varphi_{1}\vee\varphi_{2})$, entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ si y solo si $\mathbf{A}%
\models\varphi_{1}[\vec{a}]$ o $\mathbf{A}\models\varphi_{2}[\vec{a}]$
\end{enumerate}

\item[(5)] Si $\varphi=(\varphi_{1}\rightarrow\varphi_{2})$, entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ si y solo si $\mathbf{A}%
\not \models \varphi_{1}[\vec{a}]$ o $\mathbf{A}\models\varphi_{2}[\vec{a}]$
\end{enumerate}

\item[(6)] Si $\varphi=(\varphi_{1}\leftrightarrow\varphi_{2})$, entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ si y solo si ya sea se dan
$\mathbf{A}\models\varphi_{1}[\vec{a}]$ y $\mathbf{A}\models\varphi_{2}%
[\vec{a}]$ o se dan $\mathbf{A}\not \models \varphi_{1}[\vec{a}]$ y
$\mathbf{A}\not \models \varphi_{2}[\vec{a}]$
\end{enumerate}

\item[(7)] Si $\varphi=\lnot\varphi_{1},$ entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ si y solo si $\mathbf{A}%
\not \models \varphi_{1}[\vec{a}]$
\end{enumerate}

\item[(8)] Si $\varphi=\forall x_{i}\varphi_{1}$, entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ si y solo si para cada
$a\in A$, se da que $\mathbf{A}\models\varphi_{1}[\downarrow_{i}^{a}(\vec
{a})]$
\end{enumerate}

\item[(9)] Si $\varphi=\exists x_{i}\varphi_{1}$, entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ si y solo si hay un $a\in
A$ tal que $\mathbf{A}\models\varphi_{1}[\downarrow_{i}^{a}(\vec{a})] $
\end{enumerate}
\end{enumerate}

\noindent Cuando se de $\mathbf{A}\models\varphi\lbrack\vec{a}]$ diremos que
\textit{la estructura }$\mathbf{A}$\textit{\ satisface }$\varphi$ \textit{en
la asignacion }$\vec{a}$ y en tal caso diremos que $\varphi$ \textit{es
verdadera en }$\mathbf{A}$\textit{\ para la asignacion }$\vec{a}$. Cuando no
se de $\mathbf{A}\models\varphi\lbrack\vec{a}]$ diremos que \textit{la
estructura }$\mathbf{A}$\textit{\ no satisface }$\varphi$ \textit{en la
asignacion }$\vec{a}$ y en tal caso diremos que $\varphi$ \textit{es falsa en
}$\mathbf{A}$\textit{\ para la asignacion }$\vec{a}$. Tambien hablaremos del
\textit{valor de verdad de }$\varphi$\textit{\ en }$\mathbf{A}$\textit{\ para
la asignacion }$\vec{a}$ el cual sera igual a $1$ si se da $\mathbf{A}%
\models\varphi\lbrack\vec{a}]$ y $0$ en caso contrario.

Veamos algunos ejemplos. Sea $\tau$ el tipo%
\[
(\{\mathrm{uno},\mathrm{doli}\},\{\mathrm{MAS},\mathrm{P}\},\{\mathrm{Her}%
\},\{(\mathrm{MAS},4),(\mathrm{P},1),(\mathrm{Her},3)\})
\]
y sea $\mathbf{A}=(A,i)$ la estructura de tipo $\tau$ con universo
$A=\mathbf{R}$ y

\begin{enumerate}
\item[-] $i(\mathrm{uno})=9$

\item[-] $i(\mathrm{doli})=0$

\item[-] $i(\mathrm{MAS})$ igual a la operacion%
\[%
\begin{array}
[c]{rcl}%
\mathbf{R}^{4} & \rightarrow & \mathbf{R}\\
(x,y,z,w) & \rightarrow & 2x+4y
\end{array}
\]


\item[-] $i(\mathrm{P})$ igual a la operacion%
\[%
\begin{array}
[c]{rcl}%
\mathbf{R} & \rightarrow & \mathbf{R}\\
x & \rightarrow & 5^{x}%
\end{array}
\]


\item[-] $i(\mathrm{Her})=\{(x,y,z)\in\mathbf{R}^{3}:x.y.z=9\}$
\end{enumerate}

Sea $\vec{a}=(1,2,3,4,5,...)$. Claramente $\vec{a}$ es una asignacion de
$\mathbf{A}$. Consideremos los siguientes ejemplos:

\begin{enumerate}
\item[(E1)] Si $\varphi=(\mathrm{MAS}(\mathsf{X}\mathbf{1},\mathrm{uno}%
,\mathsf{X}\mathbf{3},\mathsf{X}\mathit{55}\mathbf{4})\equiv\mathrm{P}%
(\mathsf{X}\mathbf{3}))$, entonces ya que

\begin{enumerate}
\item $\mathrm{MAS}(\mathsf{X}\mathbf{1},\mathrm{uno},\mathsf{X}%
\mathbf{3},\mathsf{X}\mathit{55}\mathbf{4})^{\mathbf{A}}[\vec{a}]=38$

\item $\mathrm{P}(\mathsf{X}\mathbf{3})^{\mathbf{A}}[\vec{a}]=125$
\end{enumerate}

tenemos que (1) de la definicion nos dice que $\mathbf{A}\models\varphi
\lbrack\vec{a}]$ si y solo si $38=125$ por lo cual se saca que $\mathbf{A}%
\not \models \varphi\lbrack\vec{a}]$.

\item[(E2)] Si $\varphi=\lnot\mathrm{Her}(\mathrm{P}(\mathrm{P}(\mathsf{X}%
\mathbf{6})),\mathsf{X}\mathbf{3},\mathrm{doli})$, entonces ya que

\begin{enumerate}
\item[-] $\mathrm{P}(\mathrm{P}(\mathsf{X}\mathbf{6}))^{\mathbf{A}}[\vec
{a}]=5^{(5^{6})}$

\item[-] $\mathsf{X}\mathbf{3}^{\mathbf{A}}[\vec{a}]=3$

\item[-] $\mathrm{doli}^{\mathbf{A}}[\vec{a}]=0$
\end{enumerate}

tenemos que (7) de la definicion nos dice que $\mathbf{A}\models\varphi
\lbrack\vec{a}]$ si y solo si $\mathbf{A}\not \models \mathrm{Her}%
(\mathrm{P}(\mathrm{P}(\mathsf{X}\mathbf{6})),\mathsf{X}\mathbf{3}%
,\mathrm{doli})[\vec{a}]$. Pero (2) de la definicion nos dice que
$\mathbf{A}\models\mathrm{Her}(\mathrm{P}(\mathrm{P}(\mathsf{X}\mathbf{6}%
)),\mathsf{X}\mathbf{3},\mathrm{doli})[\vec{a}]$ si y solo si $(5^{(5^{6}%
)},3,0)\in i(\mathrm{Her})$ ya que no se da que $(5^{(5^{6})},3,0)\in
i(\mathrm{Her})$, tenemos que $\mathbf{A}\not \models \mathrm{Her}%
(\mathrm{P}(\mathrm{P}(\mathsf{X}\mathbf{6})),\mathsf{X}\mathbf{3}%
,\mathrm{doli})[\vec{a}]$ lo cual nos dice que $\mathbf{A}\models
\varphi\lbrack\vec{a}]$.

\item[(E3)] Si $\varphi=\exists\mathsf{X}\mathbf{3}\mathrm{Her}(\mathsf{X}%
\mathbf{6},\mathsf{X}\mathbf{3},\mathrm{uno})$, entonces por (9) de la
definicion tenemos que

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ sii hay un $r\in\mathbf{R}$
tal que $\mathbf{A}\models\mathrm{Her}(\mathsf{X}\mathbf{6},\mathsf{X}%
\mathbf{3},\mathrm{uno})[\downarrow_{3}^{r}(\vec{a})]$
\end{enumerate}

es decir que

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ sii hay un $r\in\mathbf{R}$
tal que $\mathbf{A}\models\mathrm{Her}(\mathsf{X}\mathbf{6},\mathsf{X}%
\mathbf{3},\mathrm{uno})[(1,2,r,4,5,6,...)]$
\end{enumerate}

Pero (2) de la definicion nos dice que cualquiera sea $r\in\mathbf{R}$ se
tiene que

\begin{enumerate}
\item[-] $\mathbf{A}\models\mathrm{Her}(\mathsf{X}\mathbf{6},\mathsf{X}%
\mathbf{3},\mathrm{uno})[(1,2,r,4,5,6,...)]$ sii $(6,r,9)\in i(\mathrm{Her})$
\end{enumerate}

O sea que obtenemos finalmente que

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ sii hay un $r\in\mathbf{R}$
tal que $6.r.9=9$
\end{enumerate}

Lo cual claramente implica que $\mathbf{A}\models\varphi\lbrack\vec{a}]$ ya
que podemos tomar $r=1/6$.

\item[(E4)] Si $\varphi=\forall\mathsf{X}\mathbf{3(}(\mathsf{X}\mathbf{4}%
\equiv\mathsf{X}\mathbf{3})\rightarrow\exists\mathsf{X}\mathbf{6}%
\mathrm{Her}(\mathsf{X}\mathbf{6},\mathsf{X}\mathbf{3},\mathrm{uno}))$,
entonces por (8) de la definicion tenemos que

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ sii para cada
$r\in\mathbf{R}$ se da que%
\[
\mathbf{A}\models\mathbf{(}(\mathsf{X}\mathbf{4}\equiv\mathsf{X}%
\mathbf{3})\rightarrow\exists\mathsf{X}\mathbf{6}\mathrm{Her}(\mathsf{X}%
\mathbf{6},\mathsf{X}\mathbf{3},\mathrm{uno}))[\downarrow_{3}^{r}(\vec{a})]
\]

\end{enumerate}

es decir que

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ sii para cada
$r\in\mathbf{R}$ se da que%
\[
\mathbf{A}\models\mathbf{(}(\mathsf{X}\mathbf{4}\equiv\mathsf{X}%
\mathbf{3})\rightarrow\exists\mathsf{X}\mathbf{6}\mathrm{Her}(\mathsf{X}%
\mathbf{6},\mathsf{X}\mathbf{3},\mathrm{uno}))[(1,2,r,4,5,6,...)]
\]

\end{enumerate}

Pero entonces (5) de la definicion nos dice que

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ sii para cada
$r\in\mathbf{R}$ se da que%
\[
\mathbf{A}\not \models (\mathsf{X}\mathbf{4}\equiv\mathsf{X}\mathbf{3}%
)[(1,2,r,4,5,6,...)]\text{ o }\mathbf{A}\models\exists\mathsf{X}%
\mathbf{6}\mathrm{Her}(\mathsf{X}\mathbf{6},\mathsf{X}\mathbf{3}%
,\mathrm{uno}))[(1,2,r,4,5,6,...)]
\]
O sea que

\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ sii para cada
$r\in\mathbf{R}$ se da que%
\[
r\neq4\text{ o }\mathbf{A}\models\exists\mathsf{X}\mathbf{6}\mathrm{Her}%
(\mathsf{X}\mathbf{6},\mathsf{X}\mathbf{3},\mathrm{uno}))[(1,2,r,4,5,6,...)]
\]
Es decir que debemos ver cuando se da que $\mathbf{A}\models\exists
\mathsf{X}\mathbf{6}\mathrm{Her}(\mathsf{X}\mathbf{6},\mathsf{X}%
\mathbf{3},\mathrm{uno}))[(1,2,r,4,5,6,...)]$. Por (9) y (2) de la definicion
tenemos que cualquiera sea el $r\in\mathbf{R}$ se da que

\item[-] $\mathbf{A}\models\exists\mathsf{X}\mathbf{6}\mathrm{Her}%
(\mathsf{X}\mathbf{6},\mathsf{X}\mathbf{3},\mathrm{uno}))[(1,2,r,4,5,6,...)]$
sii hay un $s\in\mathbf{R}$ tal que $s.r.9=9$.
\end{enumerate}

Esto nos dice finalmente que

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ sii para cada
$r\in\mathbf{R}$ se da que%
\[
r\neq4\text{ o hay un }s\in\mathbf{R}\text{ tal que }s.r.9=9
\]

\end{enumerate}

Pensando un poco esto nos dice que $\mathbf{A}\models\varphi\lbrack\vec{a}]$
(separar los casos $r=4$ y $r\neq4$)
\end{enumerate}

\bigskip

\begin{lemma}
\label{independencia1}Supongamos que $\vec{a},\vec{b}$ son asignaciones tales
que si $x_{i}\in Li(\varphi),$ entonces $a_{i}=b_{i}.$ Entonces $\mathbf{A}%
\models\varphi\lbrack\vec{a}]$ sii $\mathbf{A}\models\varphi\lbrack\vec{b}]$
\end{lemma}

\begin{proof}
Probaremos por induccion en $k$ que el lema vale para cada $\varphi\in
F_{k}^{\tau}$. El caso $k=0$ se desprende del Lema
\ref{independencia del valor}. Veamos que Teo$_{k}$ implica Teo$_{k+1}.$ Sea
$\varphi\in F_{k+1}^{\tau}-F_{k}^{\tau}.$ Hay varios casos:

CASO $\varphi=(\varphi_{1}\wedge\varphi_{2})$.

\noindent Ya que $Li(\varphi_{i})\subseteq Li(\varphi)$, $i=1,2$, Teo$_{k}$
nos dice que $\mathbf{A}\models\varphi_{i}[\vec{a}]$ sii $\mathbf{A}%
\models\varphi_{i}[\vec{b}]$, para $i=1,2$. Se tiene entonces que%
\[%
\begin{array}
[c]{l}%
\mathbf{A}\models\varphi\lbrack\vec{a}]\\
\ \ \Updownarrow\text{ (por (3) en la def de }\mathbf{A}\models\varphi
\lbrack\vec{a}]\text{)}\\
\mathbf{A}\models\varphi_{1}[\vec{a}]\text{ y }\mathbf{A}\models\varphi
_{2}[\vec{a}]\\
\ \ \Updownarrow\text{ (por Teo}_{k}\text{)}\\
\mathbf{A}\models\varphi_{1}[\vec{b}]\text{ y }\mathbf{A}\models\varphi
_{2}[\vec{b}]\\
\ \ \Updownarrow\text{(por (3) en la def de }\mathbf{A}\models\varphi
\lbrack\vec{a}]\text{)}\\
\mathbf{A}\models\varphi\lbrack\vec{b}]
\end{array}
\]


CASO $\varphi=(\varphi_{1}\vee\varphi_{2})$.

\noindent Es completamente similar al anterior.

CASO $\varphi=(\varphi_{1}\rightarrow\varphi_{2})$.

\noindent Es completamente similar al anterior.

CASO $\varphi=(\varphi_{1}\leftrightarrow\varphi_{2})$.

\noindent Es completamente similar al anterior.

CASO $\varphi=\lnot\varphi_{1}.$

\noindent Es completamente similar al anterior.

CASO $\varphi=\forall x_{j}\varphi_{1}.$

\noindent Supongamos $\mathbf{A}\models\varphi\lbrack\vec{a}]$. Entonces por
(8) en la def de $\mathbf{A}\models\varphi\lbrack\vec{a}]$ se tiene que
$\mathbf{A}\models\varphi_{1}[\downarrow_{j}^{a}(\vec{a})]$, para todo $a\in
A$. Notese que $\downarrow_{j}^{a}(\vec{a})$ y $\downarrow_{j}^{a}(\vec{b})$
coinciden en toda $x_{i}\in Li(\varphi_{1})$ ya que $Li(\varphi_{1})\subseteq
Li(\varphi)\cup\{x_{j}\}$. O sea que por Teo$_{k} $ se tiene que
$\mathbf{A}\models\varphi_{1}[\downarrow_{j}^{a}(\vec{b})]$, para todo $a\in
A$, lo cual por (8) en la def de $\mathbf{A}\models\varphi\lbrack\vec{a}]$ nos
dice que $\mathbf{A}\models\varphi\lbrack\vec{b}]$. La prueba de que
$\mathbf{A}\models\varphi\lbrack\vec{b}]$ implica que $\mathbf{A}%
\models\varphi\lbrack\vec{a}]$ es similar.

CASO $\varphi=\exists x_{j}\varphi_{1}$.

\noindent Es similar al anterior.
\end{proof}

\begin{corollary}
Si $\varphi$ es una sentencia, entonces $\mathbf{A}\models\varphi\lbrack
\vec{a}]$ sii $\mathbf{A}\models\varphi\lbrack\vec{b}]$, cualesquiera sean las
asignaciones $\vec{a},\vec{b}$.
\end{corollary}

\bigskip

En virtud del corolario anterior tenemos que el valor de verdad de una
sentencia $\varphi$ en una estructura dada $\mathbf{A}$ para una asignacion
$\vec{a}$ no depende de $\vec{a}$, es decir este valor es ya sea $1$ para
todas las asignaciones o $0$ para todas las asignaciones. En el primer caso
diremos que $\varphi$ \textit{es verdadera en }$\mathbf{A}$ (y escribiremos
$\mathbf{A}\models\varphi$) y en el segundo caso diremos que $\varphi$
\textit{es falsa en }$\mathbf{A}$ (y escribiremos $\mathbf{A}\not \models
\varphi$)

Una sentencia de tipo $\tau$ sera llamada \textit{universalmente valida} si es
verdadera en cada modelo de tipo $\tau$.

\subsubsection{Equivalencia de formulas}

Dadas $\varphi,\psi\in F^{\tau}$ diremos que $\varphi$ y $\psi$ son
\textit{equivalentes} cuando se de la siguiente condicion

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack\vec{a}]$ si y solo si $\mathbf{A}%
\models\psi\lbrack\vec{a}]$, para cada modelo de tipo $\tau$, $\mathbf{A}$ y
cada $\vec{a}\in A^{\mathbf{N}}$
\end{enumerate}

\noindent Escribiremos $\varphi\thicksim\psi$ cuando $\varphi$ y $\psi$ sean
equivalentes. Notese que%
\[
\{(\varphi,\psi)\in F^{\tau}:\varphi\thicksim\psi\}
\]
es una relacion de equivalencia sobre $F^{\tau}$.

\begin{lemma}


\begin{enumerate}
\item[(a)] Si $Li(\phi)\cup Li(\psi)\subseteq\{x_{i_{1}},...,x_{i_{n}}\},$
entonces $\phi\thicksim\psi$ si y solo si la sentencia $\forall x_{i_{1}%
}...\forall x_{i_{n}}(\phi\leftrightarrow\psi)$ es universalmente valida.

\item[(b)] Si $\phi_{i}\thicksim\psi_{i},$ $i=1,2,$ entonces $\lnot\phi
_{1}\thicksim\lnot\psi_{1},$ $(\phi_{1}\eta\phi_{2})\thicksim(\psi_{1}\eta
\psi_{2})$ y $Qv\phi_{1}\thicksim Qv\psi_{1}.$

\item[(c)] Si $\phi\thicksim\psi$ y $\alpha^{\prime}$ es el resultado de
reemplazar en una formula $\alpha$ algunas (posiblemente $0$) ocurrencias de
$\phi$ por $\psi$, entonces $\alpha\thicksim\alpha^{\prime}.$
\end{enumerate}
\end{lemma}

\begin{proof}
(a) Tenemos que%
\[%
\begin{array}
[c]{l}%
\gamma\thicksim\psi\\
\ \ \Updownarrow\text{ (por (6) de la def de}\models\text{)}\\
\mathbf{A}\models(\gamma\leftrightarrow\psi)[\vec{a}]\text{, para todo
}\mathbf{A}\text{ y toda }\vec{a}\in A^{\mathbf{N}}\\
\ \ \Updownarrow\\
\mathbf{A}\models(\gamma\leftrightarrow\psi)[\downarrow_{i_{n}}^{a}(\vec
{a})]\text{, para todo }\mathbf{A}\text{, }a\in A\text{ y toda }\vec{a}\in
A^{\mathbf{N}}\\
\ \ \Updownarrow(\text{por (8) de la def de}\models)\\
\mathbf{A}\models\forall x_{i_{n}}(\gamma\leftrightarrow\psi)[\vec{a}]\text{,
para todo }\mathbf{A}\text{ y toda }\vec{a}\in A^{\mathbf{N}}\\
\ \ \Updownarrow\\
\mathbf{A}\models\forall x_{i_{n}}(\gamma\leftrightarrow\psi)[\downarrow
_{i_{n-1}}^{a}(\vec{a})]\text{, para todo }\mathbf{A}\text{, }a\in A\text{ y
toda }\vec{a}\in A^{\mathbf{N}}\\
\ \ \Updownarrow\text{ (por (8) de la def de}\models\text{)}\\
\mathbf{A}\models\forall x_{i_{n-1}}\forall x_{i_{n}}(\gamma\leftrightarrow
\psi)[\vec{a}]\text{, para todo }\mathbf{A}\text{ y toda }\vec{a}\in
A^{\mathbf{N}}\\
\ \ \Updownarrow\\
\ \ \ \ \vdots\\
\ \ \Updownarrow\\
\mathbf{A}\models\forall x_{i_{1}}...\forall x_{i_{n}}(\gamma\leftrightarrow
\psi)[\vec{a}]\text{, para todo }\mathbf{A}\text{ y toda }\vec{a}\in
A^{\mathbf{N}}\\
\ \ \Updownarrow\\
\forall x_{i_{1}}...\forall x_{i_{n}}(\gamma\leftrightarrow\psi)\text{ es
universalmente valida}%
\end{array}
\]


(b) Es dejado al lector.

(c) Por induccion en el $k$ tal que $\alpha\in F_{k}^{\tau}$.
\end{proof}

\bigskip

\subsection{Un poco de semantica}

Dado que las estructuras de tipo $\tau$ constituyen los "mundos posibles"
donde las formulas de tipo $\tau$ se "interpretan" se suele llamar semantica
al estudio general de las estructuras y su vinculacion con el lenguaje. Aqui
daremos algunas nociones basicas de semantica.

\bigskip

\subsubsection{Homomorfismos}

La nocion de homomorfismo estaba restringida a unos pocos casos particulares
de estructuras estudiadas pero ahora con nuestra definicion general de
estructura podemos generalizarla en forma natural. Antes una notacion muy
util. Dado un modelo de tipo $\tau$, $\mathbf{A}=(A,i)$, para cada
$s\in\mathcal{C}\cup\mathcal{F}\cup\mathcal{R}$, usaremos $s^{\mathbf{A}}$
para denotar a $i(s)$.

Sean $\mathbf{A}$ y $\mathbf{B}$ modelos de tipo $\tau$. Una funcion
$F:A\rightarrow B$ sera un \textit{homomorfismo de }$\mathbf{A}$ \textit{en}
$\mathbf{B}$ si se cumplen las siguientes

\begin{enumerate}
\item[(1)] $F(c^{\mathbf{A}})=c^{\mathbf{B}}$, para todo $c\in\mathcal{C}$,

\item[(2)] $F(f^{\mathbf{A}}(a_{1},...,a_{n}))=f^{\mathbf{B}}(F(a_{1}%
),...,F(a_{n}))$, para cada $f\in\mathcal{F}_{n}$, $a_{1},...,a_{n}\in A$.

\item[(3)] $(a_{1},...,a_{n})\in r^{\mathbf{A}}$ implica $(F(a_{1}%
),...,F(a_{n}))\in r^{\mathbf{B}}$, para todo $r\in\mathcal{R}_{n}$,
$a_{1},...,a_{n}\in A$.
\end{enumerate}

\noindent Un \textit{isomorfismo de }$\mathbf{A}$ \textit{en} $\mathbf{B}$
sera un un homomorfismo de $\mathbf{A}$ en $\mathbf{B}$ el cual sea biyectivo
y cuya inversa sea un homomorfismo de $\mathbf{B}$ en $\mathbf{A}$. Diremos
que los modelos $\mathbf{A}$ y $\mathbf{B}$ son \textit{isomorfos} (en
simbolos: $\mathbf{A}\cong\mathbf{B}$), cuando haya un isomorfismo $F$ de
$\mathbf{A}$ en $\mathbf{B}$. Diremos que $F:\mathbf{A}\rightarrow\mathbf{B}$
\textit{es un homomorfismo} para expresar que $F$ es un homomorfismo de
$\mathbf{A}$ en $\mathbf{B}$. Analogamente diremos que $F:\mathbf{A}%
\rightarrow\mathbf{B}$ \textit{es un isomorfismo} para expresar que $F$ es un
isomorfismo de $\mathbf{A}$ en $\mathbf{B}$.

\bigskip

\textbf{Ejercicio:} Pruebe que la relacion $\cong$ es reflexiva, transitiva y simetrica.

\bigskip

\begin{lemma}
\label{F-respeta-term}Sea $F:\mathbf{A}\rightarrow\mathbf{B}$ un homomorfismo.
Entonces%
\[
F(t^{\mathbf{A}}[(a_{1},a_{2},...)]=t^{\mathbf{B}}[(F(a_{1}),F(a_{2}),...)]
\]
para cada $t\in T^{\tau}$, $(a_{1},a_{2},...)\in A^{\mathbf{N}}$.
\end{lemma}

\begin{proof}
Por induccion. Sea

\begin{enumerate}
\item[-] Teo$_{k}$: Si $F:\mathbf{A}\rightarrow\mathbf{B}$ es un homomorfismo,
entonces%
\[
F(t^{\mathbf{A}}[(a_{1},a_{2},...)]=t^{\mathbf{B}}[(F(a_{1}),F(a_{2}),...)]
\]
para cada $t\in T_{k}^{\tau}$, $(a_{1},a_{2},...)\in A^{\mathbf{N}}$.
\end{enumerate}

Teo$_{0}$ es trivial. Veamos que Teo$_{k}$ implica Teo$_{k+1}$. Supongamos que
vale Teo$_{k}$ y supongamos $F:\mathbf{A}\rightarrow\mathbf{B}$ es un
homomorfismo, $t\in T_{k+1}^{\tau}-T_{k}^{\tau}$ y $\vec{a}=(a_{1}%
,a_{2},...)\in A^{\mathbf{N}}$. Denotemos $(F(a_{1}),F(a_{2}),...)$ con
$F(\vec{a})$. Por Lema \ref{basic0}, $t=f(t_{1},...,t_{n})$, con $n\geq1
$,$\;f\in\mathcal{F}_{n}$ y $t_{1},...,t_{n}\in T_{k}^{\tau}$. Tenemos
entonces%
\[%
\begin{array}
[c]{ccl}%
F(t^{\mathbf{A}}[\vec{a}]) & = & F(f(t_{1},...,t_{n})^{\mathbf{A}}[\vec{a}])\\
& = & F(f^{\mathbf{A}}(t_{1}^{\mathbf{A}}[\vec{a}],...,t_{n}^{\mathbf{A}}%
[\vec{a}]))\\
& = & f^{\mathbf{B}}(F(t_{1}^{\mathbf{A}}[\vec{a}]),...,F(t_{n}^{\mathbf{A}%
}[\vec{a}]))\\
& = & f^{\mathbf{B}}(t_{1}^{\mathbf{B}}[F(\vec{a})],...,t_{n}^{\mathbf{B}%
}[F(\vec{a})]))\\
& = & f(t_{1},...,t_{n})^{\mathbf{B}}[F(\vec{a})]\\
& = & t^{\mathbf{B}}[F(\vec{a})]
\end{array}
\]

\end{proof}

\bigskip

\begin{lemma}
Supongamos que $F:\mathbf{A}\rightarrow\mathbf{B}$ es un isomorfismo. Sea
$\varphi\in F^{\tau}$. Entonces%
\[
\mathbf{A}\models\varphi\lbrack(a_{1},a_{2},...)]\text{ sii }\mathbf{B}%
\models\varphi\lbrack(F(a_{1}),F(a_{2}),...)]
\]
para cada $(a_{1},a_{2},...)\in A^{\mathbf{N}}$. En particular $\mathbf{A}$ y
$\mathbf{B}$ satisfacen las mismas sentencias de tipo $\tau$.
\end{lemma}

\begin{proof}
Por induccion. Sea

\begin{enumerate}
\item[-] Teo$_{k}$: Supongamos que $F:\mathbf{A}\rightarrow\mathbf{B}$ es un
isomorfismo. Sea $\varphi\in F_{k}^{\tau}$. Entonces%
\[
\mathbf{A}\models\varphi\lbrack(a_{1},a_{2},...)]\text{ sii }\mathbf{B}%
\models\varphi\lbrack(F(a_{1}),F(a_{2}),...)]
\]
para cada $(a_{1},a_{2},...)\in A^{\mathbf{N}}$
\end{enumerate}

Prueba de Teo$_{0}$. Supongamos que $F:\mathbf{A}\rightarrow\mathbf{B}$ es un
isomorfismo, $\varphi\in F_{0}^{\tau}$ y $(a_{1},a_{2},...)\in A^{\mathbf{N}}%
$. Probaremos que%
\[
\mathbf{A}\models\varphi\lbrack(a_{1},a_{2},...)]\text{ sii }\mathbf{B}%
\models\varphi\lbrack(F(a_{1}),F(a_{2}),...)]
\]
Hay dos casos. Caso $\varphi=r(t_{1},...,t_{n})$, con $n\geq1$,$\;r\in
\mathcal{R}_{n}$ y $t_{1},...,t_{n}\in T^{\tau}$. Denotemos con $\vec{a}$ a
$(a_{1},a_{2},...)$ y con $F(\vec{a})$ a $(F(a_{1}),F(a_{2}),...)$. Tenemos
entonces%
\[%
\begin{array}
[c]{ccl}%
\mathbf{A}\models\varphi\lbrack\vec{a}] & \text{sii} & (t_{1}^{\mathbf{A}%
}[\vec{a}],...,t_{m}^{\mathbf{A}}[\vec{a}])\in r^{\mathbf{A}}\text{ (def de
}\models\text{)}\\
& \text{sii} & (F(t_{1}^{\mathbf{A}}[\vec{a}]),...,F(t_{n}^{\mathbf{A}}%
[\vec{a}]))\in r^{\mathbf{B}}\text{ (}F\text{ es iso)}\\
& \text{sii} & (t_{1}^{\mathbf{B}}[F(\vec{a})]),...,t_{n}^{\mathbf{B}}%
[F(\vec{a})])\in r^{\mathbf{B}}\text{ (Lema \ref{F-respeta-term})}\\
& \text{sii} & \mathbf{B}\models\varphi\lbrack F(\vec{a})]
\end{array}
\]
Dejamos al lector completar la prueba de que Teo$_{k}$ implica Teo$_{k+1}$
\end{proof}

\bigskip

\subsubsection{Algebras}

Un tipo $\tau$ sera llamado \textit{algebraico} si no contiene nombres de
relacion (i.e. $\mathcal{R}=\emptyset$). Un modelo de un tipo algebraico
$\tau$ sera llamado una $\tau$-\textit{algebra}. Ejemplos clasicos de $\tau
$-algebras son los grupos ($\tau=(\{e\},\{.^{2}\},\emptyset,a)$), los
reticulados, los reticulados acotados, las algebras de Boole, etc. Muchos de
los resultados y definiciones dados en la Seccion
\ref{Seccion estructuras algebraicas ordenadas} para reticulados terna,
reticulados acotados y reticulados complementados pueden ahora ser
generalizados naturalmente para $\tau$-algebras. Desarrollaremos un poco esta
linea de "algebra general" la cual ha tenido un furte impacto en el area de
las especificaciones algebraicas de tipos de datos.

\begin{lemma}
\label{homobiyectivo}Supongamos $\tau$ es algebraico. Si $F:\mathbf{A}%
\rightarrow\mathbf{B}$ es un homomorfismo biyectivo, entonces $F$ es un isomorfismo.
\end{lemma}

\begin{proof}
Solo falta probar que $F^{-1}$ es un homomorfismo. Supongamos que
$c\in\mathcal{C}$. Ya que $F(c^{\mathbf{A}})=c^{\mathbf{B}}$, tenemos que
$F^{-1}(c^{\mathbf{B}})=c^{\mathbf{A}}$, por lo cual $F^{-1}$ cumple (1) de la
definicion de homomorfismo. Supongamos ahora que $f\in\mathcal{F}_{n}$ y sean
$b_{1},...,b_{n}\in B$. Sean $a_{1},...,a_{n}\in A$ tales que $F(a_{i})=b_{i}%
$, $i=1,...,n$. Tenemos que%
\[%
\begin{array}
[c]{ccl}%
F^{-1}(f^{\mathbf{B}}(b_{1},...,b_{n})) & = & F^{-1}(f^{\mathbf{B}}%
(F(a_{1}),...,F(a_{n})))\\
& = & F^{-1}(F(f^{\mathbf{A}}(a_{1},...,a_{n})))\\
& = & f^{\mathbf{A}}(a_{1},...,a_{n})\\
& = & f^{\mathbf{A}}(F^{-1}(b_{1}),...,F^{-1}(b_{n}))
\end{array}
\]
por lo cual $F^{-1}$ satisface (2) de la definicion de homomorfismo
\end{proof}

\bigskip

\paragraph{Subalgebras}

Dadas $\tau$-algebras $\mathbf{A}$ y $\mathbf{B}$, diremos que $\mathbf{A}$ es
una \textit{subalgebra de} $\mathbf{B}$ cuando se den las siguientes condiciones

\begin{enumerate}
\item[(1)] $A\subseteq B$

\item[(2)] $c^{\mathbf{A}}=c^{\mathbf{B}}$, para cada $c\in\mathcal{C}$

\item[(3)] $f^{\mathbf{A}}=f^{\mathbf{B}}|_{A^{n}}$, para cada $f\in
\mathcal{F}_{n}$, $n\geq1$
\end{enumerate}

\bigskip

Si $\mathbf{B}$ es una $\tau$-algebra, entonces un \textit{subuniverso de
}$\mathbf{B}$ es un conjunto $A$ el cual cumple las siguientes condiciones:

\begin{enumerate}
\item[(1)] $\emptyset\neq A\subseteq B$

\item[(2)] $c^{\mathbf{B}}\in A,$ para cada $c\in\mathcal{C}$

\item[(3)] $f^{\mathbf{B}}(a_{1},...,a_{n})\in A$, para cada $(a_{1}%
,...,a_{n})\in A^{n},$ $f\in\mathcal{F}_{n}$
\end{enumerate}

\bigskip

Es importante notar que si bien los conceptos de subalgebra y subuniverso
estan muy relacionados, se trata de objetos diferentes ya que las subalgebras
de un algebra dada son estructuras de tipo $\tau$ y por lo tanto son pares
ordenados y los subuniversos de un algebra dada son ciertos subconjuntos por
lo cual no son pares ordenados. A continuacion presisaremos la relacion que
hay entre estos dos conceptos. Notese que dado un subuniverso $A$ de una
$\tau$-algebra $\mathbf{B}$ podemos definir en forma natural una $\tau
$-algebra $\mathbf{A}$ de la siguiente manera:

\begin{enumerate}
\item[(1)] Universo de $\mathbf{A}=A$

\item[(2)] $c^{\mathbf{A}}=c^{\mathbf{B}},$ para cada $c\in\mathcal{C}$

\item[(3)] $f^{\mathbf{A}}=f^{\mathbf{B}}|_{A^{n}},$ para cada $f\in
\mathcal{F}_{n}$.
\end{enumerate}

\noindent Es facil chequear que el algebra $\mathbf{A}$ asi definida es una
subalgebra de $\mathbf{B}$. Lo anterior nos muestra que los subuniversos de un
algebra dada son precisamente los universos de las distintas subalgebras de
dicha algebra.

\bigskip

\begin{lemma}
Supongamos $\tau$ es algebraico. Si $F:\mathbf{A}\rightarrow\mathbf{B}$ es un
homomorfismo, entonces $I_{F}$ es un subuniverso de $\mathbf{B}$
\end{lemma}

\begin{proof}
Ya que $A\neq\emptyset,$ tenemos que $I_{F}\neq\emptyset.$ Es claro que
$c^{\mathbf{B}}=F(c^{\mathbf{A}})\in I_{F},$ para cada $c\in\mathcal{C}$. Sea
$f\in\mathcal{F}_{n}$ y sean $b_{1},...,b_{n}\in I_{F}$ Sean $a_{1},...,a_{n}$
tales que $F(a_{i})=b_{i},$ $i=1,...,n$. Tenemos que%
\[
f^{\mathbf{B}}(b_{1},...,b_{n})=f^{\mathbf{B}}(F(a_{1}),...,F(a_{n}%
))=F(f^{\mathbf{A}}(a_{1},...,a_{n}))\in I_{F}%
\]
por lo cual $I_{F}$ es cerrada bajo $f^{\mathbf{B}}$.
\end{proof}

\paragraph{Congruencias}

Sea $\mathbf{A}$ una $\tau$-algebra. Una \textit{congruencia} \textit{sobre}
$\mathbf{A}$ es una relacion de equivalencia $\theta$ sobre $A$ la cual cumple que%

\[
a_{1}\theta b_{1},...,a_{n}\theta b_{n}\text{ implica }f^{\mathbf{A}}%
(a_{1},...,a_{n})\theta f^{\mathbf{A}}(b_{1},...,b_{n})
\]
cualesquiera sean $a_{1},...,a_{n},b_{1},...,b_{n}\in A$ y$\;f\in
\mathcal{F}_{n}$.

Dada una congruencia $\theta$ sobre $\mathbf{A}$ se puede formar una nueva
algebra $\mathbf{A}/\theta$ de la siguiente manera:

\begin{enumerate}
\item[(1)] Universo de $\mathbf{A}/\theta=A/\theta=\{a/\theta:a\in
A\}=\{$clases de equivalencia de $\theta\}$

\item[(2)] $f^{\mathbf{A}/\theta}(a_{1}/\theta,...,a_{n}/\theta)=f^{\mathbf{A}%
}(a_{1},...,a_{n})/\theta,$ para cada $f\in\mathcal{F}_{n}.$

\item[(3)] $c^{\mathbf{A}/\theta}=c^{\mathbf{A}}/\theta,$ para cada
$c\in\mathcal{C}$
\end{enumerate}

\noindent$\mathbf{A}/\theta$ sera llamada el \textit{algebra cociente de
}$\mathbf{A}$ \textit{por} $\theta$.

\bigskip

\begin{lemma}
Supongamos $\tau$ es algebraico. Si $F:\mathbf{A}\rightarrow\mathbf{B}$ es un
homomorfismo, entonces $\ker F$ es una congruencia sobre $\mathbf{A}$
\end{lemma}

\begin{proof}
Sea $f\in\mathcal{F}_{n}$. Supongamos que $a_{1},...,a_{n},b_{1},...,b_{n}\in
A$ son tales que $a_{1}\ker Fb_{1},...,a_{n}\ker Fb_{n}$. Tenemos entonces que%
\[%
\begin{array}
[c]{ccl}%
F(f^{\mathbf{A}}(a_{1},...,a_{n})) & = & f^{\mathbf{B}}(F(a_{1}),...,F(a_{n}%
))\\
& = & f^{\mathbf{B}}(F(b_{1}),...,F(b_{n}))\\
& = & F(f^{\mathbf{A}}(b_{1},...,b_{n}))
\end{array}
\]
lo cual nos dice que $f^{\mathbf{A}}(a_{1},...,a_{n})\ker Ff^{\mathbf{A}%
}(b_{1},...,b_{n})$
\end{proof}

\bigskip

Al mapeo%
\[%
\begin{array}
[c]{lll}%
A & \rightarrow & A/\theta\\
a & \rightarrow & a/\theta
\end{array}
\]
lo llamaremos la \textit{proyeccion canonica} y lo denotaremos con
$\pi_{\theta}$.

\begin{lemma}
$\pi_{\theta}:\mathbf{A}\rightarrow\mathbf{A}/\theta$ es un homomorfismo cuyo
nucleo es $\theta$
\end{lemma}

\begin{proof}
Sea $c\in\mathcal{C}$. Tenemos que%
\[
\pi_{\theta}(c^{\mathbf{A}})=c^{\mathbf{A}}/\theta=c^{\mathbf{A}/\theta}%
\]
Sea $f\in\mathcal{F}_{n}$, con $n\geq1$ y sean $a_{1},...,a_{n}\in A$. Tenemos
que%
\[%
\begin{array}
[c]{ccl}%
\pi_{\theta}(f^{\mathbf{A}}(a_{1},...,a_{n})) & = & f^{\mathbf{A}}%
(a_{1},...,a_{n})/\theta\\
& = & f^{\mathbf{A}/\theta}(a_{1}/\theta,...,a_{n}/\theta)\\
& = & f^{\mathbf{A}/\theta}(\pi_{\theta}(a_{1}),...,\pi_{\theta}(a_{n}))
\end{array}
\]
con lo cual $\pi_{\theta}$ es un homomorfismo. Es trivial que $\ker\pi
_{\theta}=\theta$
\end{proof}

\begin{corollary}
\label{v(t,a1,...)/tita}Para cada $t\in T^{\tau}$, $\vec{a}\in A^{\mathbf{N}%
},$ se tiene que $t^{\mathbf{A}/\theta}[(a_{1}/\theta,a_{2}/\theta
,...)]=t^{\mathbf{A}}[(a_{1},a_{2},...)]/\theta.$
\end{corollary}

\begin{proof}
Ya que $\pi_{\theta}$ es un homomorfismo, se puede aplicar el Lema
\ref{F-respeta-term}.
\end{proof}

\bigskip

\begin{theorem}
Sea $F:\mathbf{A}\rightarrow\mathbf{B}$ un homomorfismo sobreyectivo. Entonces%
\[%
\begin{array}
[c]{lll}%
A/\ker F & \rightarrow & B\\
a/\ker F & \rightarrow & F(a)
\end{array}
\]
define sin ambiguedad una funcion $\bar{F}$ la cual es un isomorfismo de
$\mathbf{A}/\ker F$ en $\mathbf{B}$
\end{theorem}

\begin{proof}
Notese que la definicion de $\bar{F}$ es inambigua ya que si $a/\ker
F=a^{\prime}/\ker F$, entonces $F(a)=F(a^{\prime}).$ Ya que $F$ es sobre,
tenemos que $\bar{F}$ lo es. Supongamos que $\bar{F}(a/\ker F)=\bar
{F}(a^{\prime}/\ker F).$ Claramente entonces tenemos que $F(a)=F(a^{\prime})$,
lo cual nos dice que $a/\ker F=a^{\prime}/\ker F$. Esto prueba que $\bar{F}$
es inyectiva. Para ver que $\bar{F}$ es un isomorfismo, por el Lema
\ref{homobiyectivo}, basta con ver que $\bar{F}$ es un homomorfismo. Sea
$c\in\mathcal{C}$. Tenemos que%
\[
\bar{F}(c^{\mathbf{A}/\ker F})=\bar{F}(c^{\mathbf{A}}/\ker F)=F(c^{\mathbf{A}%
})=c^{\mathbf{B}}%
\]
Sea $f\in\mathcal{F}_{n}$. Sean $a_{1},...,a_{n}\in A$. Tenemos que%
\[%
\begin{array}
[c]{ccl}%
\bar{F}(f^{\mathbf{A}/\ker F}(a_{1}/\ker F,...,a_{n}/\ker F)) & = & \bar
{F}(f^{\mathbf{A}}(a_{1},...,a_{n})/\ker F)\\
& = & F(f^{\mathbf{A}}(a_{1},...,a_{n}))\\
& = & f^{\mathbf{B}}(F(a_{1}),...,F(a_{n}))\\
& = & f^{\mathbf{B}}(\bar{F}(a_{1}/\ker F),...,\bar{F}(a_{n}/\ker F))
\end{array}
\]
con lo cual $\bar{F}$ cunple (2) de la definicion de homomorfismo
\end{proof}

\bigskip

\paragraph{Producto directo de algebras}

Dadas $\tau$-algebras $\mathbf{A},\mathbf{B},$ definamos una nueva $\tau
$-algebra $\mathbf{A}\times\mathbf{B},$ de la siguiente manera

\begin{enumerate}
\item[(1)] Universo de $\mathbf{A}\times\mathbf{B}=A\times B$

\item[(2)] $c^{\mathbf{A}\times\mathbf{B}}=(c^{\mathbf{A}},c^{\mathbf{B}})$,
para cada $c\in\mathcal{C}$

\item[(3)] $f^{\mathbf{A}\times\mathbf{B}}((a_{1},b_{1}),...,(a_{n}%
,b_{n}))=(f^{\mathbf{A}}(a_{1},...,a_{n}),f^{\mathbf{B}}(b_{1},...,b_{n}))$,
para cada $f\in\mathcal{F}_{n}$
\end{enumerate}

\noindent Llamaremos a $\mathbf{A}\times\mathbf{B}$ el \textit{producto
directo} de $\mathbf{A}$ y $\mathbf{B}.$

Los mapeos%
\[%
\begin{array}
[c]{lll}%
\pi_{1}:A\times B & \rightarrow & A\\
\;\;\;\;\;(a,b) & \rightarrow & a
\end{array}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\begin{array}
[c]{lll}%
\pi_{2}:A\times B & \rightarrow & B\\
\;\;\;\;\;(a,b) & \rightarrow & b
\end{array}
\]
seran llamados las \textit{proyecciones canonicas} asociadas al producto
$A\times B$

\begin{lemma}
Los mapeos $\pi_{1}:\mathbf{A}\times\mathbf{B}\rightarrow\mathbf{A}$ y
$\pi_{2}:\mathbf{A}\times\mathbf{B}\rightarrow\mathbf{B}$ son homomorfismos
\end{lemma}

\begin{proof}
Veamos que $\pi_{1}$ es un homomorfismo. Primero notese que si $c\in
\mathcal{C}$, entonces%
\[
\pi_{1}(c^{\mathbf{A}\times\mathbf{B}})=\pi_{1}((c^{\mathbf{A}},c^{\mathbf{B}%
}))=c^{\mathbf{A}}%
\]
Sea $f\in\mathcal{F}_{n}$, con $n\geq1$ y sean $(a_{1},b_{1}),...,(a_{n}%
,b_{n})\in A\times B$. Tenemos que%
\[%
\begin{array}
[c]{ccl}%
\pi_{1}(f^{\mathbf{A}\times\mathbf{B}}((a_{1},b_{1}),...,(a_{n},b_{n})) & = &
\pi_{1}((f^{\mathbf{A}}(a_{1},...,a_{n}),f^{\mathbf{B}}(b_{1},...,b_{n}))\\
& = & f^{\mathbf{A}}(a_{1},...,a_{n})\\
& = & f^{\mathbf{A}}(\pi_{1}(a_{1},b_{1}),...,\pi_{1}(a_{n},b_{n}))
\end{array}
\]
con lo cual hemos probado que $\pi_{1}$ cumple (2) de la definicion de homomorfismo
\end{proof}

\begin{lemma}
Para cada $t\in T^{\tau},$ $((a_{1},b_{1}),(a_{2},b_{2}),...)\in(A\times
B)^{\mathbf{N}},$ se tiene que $t^{\mathbf{A}\times\mathbf{B}}[((a_{1}%
,b_{1}),(a_{2},b_{2}),...)]=(t^{\mathbf{A}}[(a_{1},a_{2},...)],t^{\mathbf{B}%
}[(b_{1},b_{2},...)])$
\end{lemma}

\bigskip

\subsection{Dos teoremas de reemplazo}

Probaremos dos teoremas muy importantes que en algun sentido nos dicen que el
reemplazo sintactico se lleva bien con la semantica. Antes nos dedicaremos a
desarrollar la notacion declaratoria la cual hace mas intuitivos los
resultados y ademas permite manejar las cosas con mas naturalidad y practicidad.

\subsubsection{Notacion declaratoria para terminos}

Si $t$ es un termino de tipo $\tau$, entonces escribiremos $t=_{d}%
t(v_{1},...,v_{n})$ para declarar que $v_{1},...,v_{n}$ son variables
distintas (con $n\geq1$) y tales que toda variable que ocurre en $t$ pertenece
a $\{v_{1},...,v_{n}\}$ (no necesariamente toda $v_{j}$ debe ocurrir en $t$).

El uso de declaraciones de la forma $t=_{d}t(v_{1},...,v_{n})$ sera muy util
cuando se lo convina con ciertas convenciones notacionales que describiremos a continuacion.

\begin{description}
\item[Convencion Notacional 1:] Cuando hayamos hecho la declaracion
$t=_{d}t(v_{1},...,v_{n})$, si $P_{1},...,P_{n}$ son palabras cualesquiera (no
necesariamente terminos), entonces $t(P_{1},...,P_{n})$ denotara la palabra
que resulta de reemplazar (simultaneamente) cada ocurrencia de $v_{1} $ en
$t$, por $P_{1}$, cada ocurrencia de $v_{2}$ en $t$, por $P_{2}$, etc.
\end{description}

\bigskip

Notese que cuando las palabras $P_{i}^{\prime}s$ son terminos, $t(P_{1}%
,...,P_{n})$ es un termino (Lema \ref{reemp-ter-en-ter}). Ademas notese que en
esta convencion notacional, el orden de las variables $v_{1},...,v_{n}$ es
clave. Por ejemplo si $\tau=(\emptyset,\{\mathrm{FU}\},\emptyset
,\{(\mathrm{FU},2)\})$ y $t=\mathrm{FU}(\mathrm{FU}(x_{2},x_{16}),x_{3})$ y
declaramos $t=_{d}t(x_{3},x_{2},x_{16})$, entonces

\begin{enumerate}
\item[-] Si declaramos $t=_{d}t(x_{3},x_{2},x_{16})$, entonces
$t(\#\#,\blacktriangle\#\blacktriangle,@@)$ denotara la palabra $\mathrm{FU}%
(\mathrm{FU}(\blacktriangle\#\blacktriangle,@@),\#\#)$

\item[-] Si declaramos $t=_{d}t(x_{16},x_{3},x_{2})$, entonces
$t(\#\#,\blacktriangle\#\blacktriangle,@@)$ denotara la palabra $\mathrm{FU}%
(\mathrm{FU}(@@,\#\#),\blacktriangle\#\blacktriangle)$
\end{enumerate}

Tambien podriamos haber declarado $t=_{d}t(x_{3},x_{200},x_{2},x_{16}%
,x_{100})$ y en tal caso $t(\#\#,!!!!,\blacktriangle\#\blacktriangle,@@,!!)$
denotara la palabra $\mathrm{FU}(\mathrm{FU}(\blacktriangle\#\blacktriangle
,@@),\#\#)$

\bigskip

\begin{description}
\item[Convencion Notacional 2:] Cuando hayamos declarado $t=_{d}%
t(v_{1},...,v_{n})$, si $\mathbf{A}$ es un modelo de tipo $\tau$ y
$a_{1},...,a_{n}\in A$, entonces con $t^{\mathbf{A}}[a_{1}...,a_{n}]$
denotaremos al elemento $t^{\mathbf{A}}[\vec{b}]$, donde $\vec{b}$ es una
asignacion tal que a cada $v_{i}$ le asigna el valor $a_{i}$. (Notese que esta
notacion es inhambigua gracias al Lema \ref{independencia del valor}.)
\end{description}

\bigskip

Nuevamente cabe destacar que en esta convencion notacional, el orden de las
variables $v_{1},...,v_{n}$ es clave. Por ejemplo si $\tau$ y $t$ son los
dados en el ejemplo anterior y $\mathbf{A}$ es dado por $A=\{1,2,3\}$ y
$\mathrm{FU}^{\mathbf{A}}(i,j)=j$, para cada $i,j\in A$, tenemos que

\begin{enumerate}
\item[-] $t^{\mathbf{A}}[2,1,3]=2$ si declaramos $t=_{d}t(x_{3},x_{2},x_{16})
$

\item[-] $t^{\mathbf{A}}[2,1,3]=1$ si declaramos $t=_{d}t(x_{16},x_{3},x_{2})
$
\end{enumerate}

Tambien podriamos haber declarado $t=_{d}t(x_{3},x_{200},x_{2},x_{16}%
,x_{100})$ y en tal caso $t^{\mathbf{A}}[2,10,1,3,1000]=2$.

Para establecer nuestra Convencion Notacional 3, debemos antes enunciar un
lema de "lectura de terminos una ves declarados" el cual el lector no tendra
inconvenientes en probar.

\begin{lemma}
[Lectura unica de terminos declarados]\label{si t = t(v1,...,vn) ...}Sea
$\tau$ un tipo cualquiera y supongamos $t\in T^{\tau}$. Si $t=_{d}%
t(v_{1},...,v_{n})$, entonces se da alguna de las siguientes

\begin{enumerate}
\item[(1)] $t=c,$ para algun $c\in\mathcal{C}$

\item[(2)] $t=v_{j},$ para algun $j$

\item[(3)] $t=f(t_{1},...,t_{m})$, con $f\in\mathcal{F}_{m}$ y $t_{1}%
,...,t_{m}\in T^{\tau}$ tales que las variables que ocurren en cada uno de
ellos estan en $\{v_{1},...,v_{n}\}$
\end{enumerate}
\end{lemma}

\begin{proof}
Rutina
\end{proof}

\bigskip

\begin{description}
\item[Convencion Notacional 3:] Cuando hayamos declarado $t=_{d}%
t(v_{1},...,v_{n})$ y se el caso (3) del Lema \ref{si t = t(v1,...,vn) ...}
supondremos tacitamente que tambien hemos hecho las declaraciones $t_{1}%
=_{d}t_{1}(v_{1},...,v_{n}),...,t_{m}=_{d}t_{m}(v_{1},...,v_{n})$.
\end{description}

\bigskip

Cabe destacar que esta ultima convencion notacional junto con la Convencion
Notacional 1, nos dice que cuando se de el caso (3) del Lema
\ref{si t = t(v1,...,vn) ...}, si $P_{1},...,P_{n}$ son palabras cualesquiera,
entonces%
\[
t(P_{1},...,P_{n})=f(t_{1}(P_{1},...,P_{n}),...,t_{m}(P_{1},...,P_{n}))
\]
El siguiente lema se basa en la Convencion Notacional 3 y nos permite darle
caracter recursivo a la notacion $t^{\mathbf{A}}[a_{1},....,a_{n}]$.

\begin{lemma}
[{Caracter recursivo de la notacion $t^{\mathbf{A}}[a_{1},....,a_{n}]$}]Sea
$\tau$ un tipo cualquiera y $t\in T^{\tau}$. Supongamos $t=_{d}t(v_{1}%
,...,v_{n})$. Sea $\mathbf{A}$ un modelo de tipo $\tau$. Sean $a_{1}%
,...,a_{n}\in A$. Se tiene que:

\begin{enumerate}
\item[(1)] Si $t=c,$ entonces $t^{\mathbf{A}}[a_{1},....,a_{n}]=c^{\mathbf{A}%
}$

\item[(2)] Si $t=v_{j},$ entonces $t^{\mathbf{A}}[a_{1},....,a_{n}]=a_{j}$

\item[(3)] Si $t=f(t_{1},...,t_{m})$, con $f\in\mathcal{F}_{m}$ y
$t_{1},...,t_{m}\in T^{\tau}$, entonces%
\[
t^{\mathbf{A}}[a_{1},....,a_{n}]=f^{\mathbf{A}}(t_{1}^{\mathbf{A}}%
[a_{1},....,a_{n}],...,t_{m}^{\mathbf{A}}[a_{1},....,a_{n}])
\]

\end{enumerate}
\end{lemma}

\begin{proof}
(1) y (2) son triviales.

(3) Sea $\vec{b}$ una asignacion tal que a cada $v_{i}$ le asigna el valor
$a_{i}$.\ Tenemos que%
\begin{align*}
t^{\mathbf{A}}[a_{1},....,a_{n}]  & =t^{\mathbf{A}}[\vec{b}]\text{ (por def.
de }t^{\mathbf{A}}[a_{1},....,a_{n}]\text{)}\\
& =f^{\mathbf{A}}(t_{1}^{\mathbf{A}}[\vec{b}],...,t_{m}^{\mathbf{A}}[\vec
{b}])\text{ (por def. de }t^{\mathbf{A}}[\vec{b}]\text{)}\\
& =f^{\mathbf{A}}(t_{1}^{\mathbf{A}}[a_{1},....,a_{n}],...,t_{m}^{\mathbf{A}%
}[a_{1},....,a_{n}])\text{ (por def. de cada }t_{i}^{\mathbf{A}}%
[a_{1},....,a_{n}]\text{)}%
\end{align*}

\end{proof}

\bigskip

\subsubsection{Teorema de reemplazo para terminos}

Ahora si podemos enunciar y probar el primero de nuestros teoremas de reemplazo

\begin{theorem}
[Teorema de reemplazo para terminos]\label{reemp-term}Supongamos
$t=_{d}t(w_{1},...,w_{k}),$ $s_{1}=_{d}s_{1}(v_{1},...,v_{n}),...,s_{k}%
=_{d}s_{k}(v_{1},...,v_{n})$. Todas las variables de $t(s_{1},...,s_{k})$
estan en $\{v_{1},...,v_{n}\}$ y si declaramos $t(s_{1},...,s_{k})=_{d}%
t(s_{1},...,s_{k})(v_{1},...,v_{n})$, entonces para cada estructura
$\mathbf{A}$ y $a_{1},....,a_{n}\in A,$ se tiene que%
\[
t(s_{1},...,s_{k})^{\mathbf{A}}[a_{1},....,a_{n}]=t^{\mathbf{A}}%
[s_{1}^{\mathbf{A}}[a_{1},....,a_{n}],...,s_{k}^{\mathbf{A}}[a_{1}%
,....,a_{n}]].
\]

\end{theorem}

\begin{proof}
Por induccion en el $l$ tal que $t\in T_{l}^{\tau}$. El caso $l=0$ es dejado
al lector. Supongamos entonces que el teorema vale siempre que $t\in
T_{l}^{\tau}$ y veamos que entonces vale cuando $t\in T_{l+1}^{\tau}%
-T_{l}^{\tau}$. Por el Lema \ref{si t = t(v1,...,vn) ...} hay $f\in
\mathcal{F}_{m}$ y $t_{1},...,t_{m}$ terminos tales $t=f(t_{1},...,t_{m})$ y
las variables que ocurren en cada $t_{i}$ estan en $\{w_{1},...,w_{k}\}$. Por
la unicidad de la lectura de terminos tenemos que $t_{1},...,t_{m}\in
T_{l}^{\tau}$ (por que?). Notese que por nuestra Convencion Notacional 3
asumimos ya hechas las declaraciones%
\[
t_{1}=_{d}t_{1}(w_{1},...,w_{k}),...,t_{m}=_{d}t_{m}(w_{1},...,w_{k})
\]
Por HI tenemos que las variables de cada $t_{i}(s_{1},...,s_{k})$ estan en
$\{v_{1},...,v_{n}\}$, lo cual nos permite hacer las siguientes declaraciones:%
\[
t_{i}(s_{1},...,s_{k})=_{d}t_{i}(s_{1},...,s_{k})(v_{1},...,v_{n}),\text{
}i=1,...,m
\]
Por HI tenemos entonces que%
\[
t_{i}(s_{1},...,s_{k})^{\mathbf{A}}[\vec{a}]=t_{i}^{\mathbf{A}}[s_{1}%
^{\mathbf{A}}[\vec{a}],...,s_{k}^{\mathbf{A}}[\vec{a}]],\text{ }i=1,...,m
\]
Ya que las variables de cada $t_{i}(s_{1},...,s_{k})$ estan en $\{v_{1}%
,...,v_{n}\}$, tenemos que las variables de $t(s_{1},...,s_{k})=f(t_{1}%
(s_{1},...,s_{k}),...,t_{m}(s_{1},...,s_{k}))$ estan en $\{v_{1},...,v_{n}\}$.
Declaremos entonces $t(s_{1},...,s_{k})=_{d}t(s_{1},...,s_{k})(v_{1}%
,...,v_{n})$. Solo nos falta probar que%
\[
t(s_{1},...,s_{k})^{\mathbf{A}}[a_{1},....,a_{n}]=t^{\mathbf{A}}%
[s_{1}^{\mathbf{A}}[a_{1},....,a_{n}],...,s_{k}^{\mathbf{A}}[a_{1}%
,....,a_{n}]].
\]
lo cual se detalla a continuacion%
\[%
\begin{array}
[c]{ccl}%
t(s_{1},...,s_{k})^{\mathbf{A}}[\vec{a}] & = & f(t_{1}(s_{1},...,s_{k}%
),...,t_{m}(s_{1},...,s_{k}))^{\mathbf{A}}[\vec{a}]\\
& = & f^{\mathbf{A}}(t_{1}(s_{1},...,s_{k})^{\mathbf{A}}[\vec{a}%
],...,t_{m}(s_{1},...,s_{k})^{\mathbf{A}}[\vec{a}])\\
& = & f^{\mathbf{A}}(t_{1}^{\mathbf{A}}[s_{1}^{\mathbf{A}}[\vec{a}%
],...,s_{k}^{\mathbf{A}}[\vec{a}]],...,t_{m}^{\mathbf{A}}[s_{1}^{\mathbf{A}%
}[\vec{a}],...,s_{k}^{\mathbf{A}}[\vec{a}]])\\
& = & t^{\mathbf{A}}[s_{1}^{\mathbf{A}}[\vec{a}],...,s_{k}^{\mathbf{A}}%
[\vec{a}]]
\end{array}
\]

\end{proof}

\bigskip

\subsubsection{Notacion declaratoria para formulas}

Si $\varphi$ es una formula de tipo $\tau$, entonces escribiremos
$\varphi=_{d}\varphi(v_{1},...,v_{n})$ para declarar que $v_{1},...,v_{n}$ son
variables distintas (con $n\geq1$) tales que $Li(\varphi)\subseteq
\{v_{1},...,v_{n}\}$. Tal como para el caso de terminos, el uso de
declaraciones de la forma $\varphi=_{d}\varphi(v_{1},...,v_{n})$ sera muy util
cuando se convina con ciertas convenciones notacionales que describiremos a continuacion.

\bigskip

\begin{description}
\item[Convencion Notacional 4:] Cuando hayamos hecho la declaracion
$\varphi=_{d}\varphi(v_{1},...,v_{n})$, si $P_{1},...,P_{n}$ son palabras
cualesquiera, entonces $\varphi(P_{1},...,P_{n})$ denotara la palabra que
resulta de reemplazar (simultaneamente) cada ocurrencia libre de $v_{1}$ en
$\varphi$, por $P_{1}$, cada ocurrencia libre de $v_{2}$ en $\varphi$, por
$P_{2}$, etc.
\end{description}

\bigskip

Notese que cuando las palabras $P_{i}^{\prime}s$ son terminos, $\varphi
(P_{1},...,P_{n})$ es una formula. Ademas notese que tal como para el caso de
terminos, en esta convencion notacional, el orden de las variables
$v_{1},...,v_{n}$ es clave. Es facil dar el ejemplo analogo al dado para terminos.

\bigskip

\begin{description}
\item[Convencion Notacional 5:] Cuando hayamos declarado $\varphi=_{d}%
\varphi(v_{1},...,v_{n})$, si $\mathbf{A}$ es un modelo de tipo $\tau$ y
$a_{1},...,a_{n}\in A$, entonces $\mathbf{A}\models\varphi\lbrack
a_{1}...,a_{n}]$ significara que $\mathbf{A}\models\varphi\lbrack\vec{b}], $
donde $\vec{b}$ es una asignacion tal que a cada $v_{i}$ le asigna el valor
$a_{i}$.\ (Notese que esta definicion es inambigua gracias al Lema
\ref{independencia1}). En gral $\mathbf{A}\not \models \varphi\lbrack
a_{1},....,a_{n}]$ significara que no sucede $\mathbf{A}\models\varphi\lbrack
a_{1},....,a_{n}]$
\end{description}

\bigskip

Nuevamente cabe destacar que en esta convencion notacional, el orden de las
variables $v_{1},...,v_{n}$ es clave y dejamos al lector encontrar un ejemplo
donde esto se vea claramente. Para establecer nuestra Convencion Notacional 6,
debemos antes enunciar un "lema de lectura unica de formulas declaradas" el
cual el lector no tendra inconvenientes en probar.

\begin{lemma}
[Lectura unica de formulas declaradas]\label{si fi = fi(v1,...,vn)}Sea $\tau$
un tipo cualquiera y $\varphi\in F^{\tau}$. Supongamos $\varphi=_{d}%
\varphi(v_{1},...,v_{n})$. Entonces se una y solo una de las siguientes:

\begin{enumerate}
\item[(1)] $\varphi=(t\equiv s)$, con $t,s\in T^{\tau}$\textit{, unicos y
tales que las variables que ocurren en }$t$ o en $s$ estan todas en
$\{v_{1},...,v_{n}\}$

\item[(2)] $\varphi=r(t_{1},...,t_{m})$, con $r\in\mathcal{R}_{m}$\textit{\ y
}$t_{1},...,t_{m}\in T^{\tau}$\textit{, unicos y tales que las variables que
ocurren en cada }$t_{i}$ estan todas en $\{v_{1},...,v_{n}\}$

\item[(3)] $\varphi=(\varphi_{1}\wedge\varphi_{2})$, con $\varphi_{1}%
,\varphi_{2}\in F^{\tau}$\textit{, unicas y} tales que $Li(\varphi_{1})\cup
Li(\varphi_{2})\subseteq\{v_{1},...,v_{n}\}$

\item[(4)] $\varphi=(\varphi_{1}\vee\varphi_{2})$, con $\varphi_{1}%
,\varphi_{2}\in F^{\tau}$\textit{, unicas y} tales que $Li(\varphi_{1})\cup
Li(\varphi_{2})\subseteq\{v_{1},...,v_{n}\}$

\item[(5)] $\varphi=(\varphi_{1}\rightarrow\varphi_{2})$, con $\varphi
_{1},\varphi_{2}\in F^{\tau}$\textit{, unicas y} tales que $Li(\varphi
_{1})\cup Li(\varphi_{2})\subseteq\{v_{1},...,v_{n}\}$

\item[(6)] $\varphi=(\varphi_{1}\leftrightarrow\varphi_{2})$, con $\varphi
_{1},\varphi_{2}\in F^{\tau}$\textit{, unicas y} tales que $Li(\varphi
_{1})\cup Li(\varphi_{2})\subseteq\{v_{1},...,v_{n}\}$

\item[(7)] $\varphi=\lnot\varphi_{1}$, con $\varphi_{1}\in F^{\tau}$, unica y
tal que $Li(\varphi_{1})\subseteq\{v_{1},...,v_{n}\}$

\item[(8)] $\varphi=\forall v_{j}\varphi_{1}$, con $v_{j}\in\{v_{1}%
,...,v_{n}\}$ y $\varphi_{1}\in F^{\tau}$, unicas y tales que $Li(\varphi
_{1})\subseteq\{v_{1},...,v_{n}\}$

\item[(9)] $\varphi=\forall v\varphi_{1}$, con $v\in Var-\{v_{1},...,v_{n}\}$
y $\varphi_{1}\in F^{\tau}$, unicas y tales que $Li(\varphi_{1})\subseteq
\{v_{1},...,v_{n},v\}$

\item[(10)] $\varphi=\exists v_{j}\varphi_{1}$, con $v_{j}\in\{v_{1}%
,...,v_{n}\}$ y $\varphi_{1}\in F^{\tau}$, unicas y tales que $Li(\varphi
_{1})\subseteq\{v_{1},...,v_{n}\}$

\item[(11)] $\varphi=\exists v\varphi_{1}$, con $v\in Var-\{v_{1},...,v_{n}\}$
y $\varphi_{1}\in F^{\tau}$, unicas y tales que $Li(\varphi_{1})\subseteq
\{v_{1},...,v_{n},v\}$
\end{enumerate}
\end{lemma}

\begin{proof}
Induccion en el $k$ tal que $\varphi\in F_{k}^{\tau}$
\end{proof}

\bigskip

\begin{description}
\item[Convencion Notacional 6:] Cuando hayamos declarado $\varphi=_{d}%
\varphi(v_{1},...,v_{n})$, entonces:

\item[-] si se da el caso (1) del Lema \ref{si fi = fi(v1,...,vn)},
supondremos tacitamente que tambien hemos hecho las declaraciones
$t=_{d}t(v_{1},...,v_{n})$ y $s=_{d}s(v_{1},...,v_{n})$.

\item[-] si se da el caso (2) del Lema \ref{si fi = fi(v1,...,vn)},
supondremos tacitamente que tambien hemos hecho las declaraciones $t_{1}%
=_{d}t_{1}(v_{1},...,v_{n}),...,t_{m}=_{d}t_{m}(v_{1},...,v_{n})$.

\item[-] si se da cualquiera de los casos (3), (4), (5) o (6) del Lema
\ref{si fi = fi(v1,...,vn)}, supondremos tacitamente que tambien hemos hecho
las declaraciones $\varphi_{1}=_{d}\varphi_{1}(v_{1},...,v_{n})$ y
$\varphi_{2}=_{d}\varphi_{2}(v_{1},...,v_{n})$.

\item[-] si se da cualquiera de los casos (7), (8) o (10) del Lema
\ref{si fi = fi(v1,...,vn)}, supondremos tacitamente que tambien hemos hecho
la declaracion $\varphi_{1}=_{d}\varphi_{1}(v_{1},...,v_{n})$.

\item[-] si se da el caso (9) o el caso (11) del Lema
\ref{si fi = fi(v1,...,vn)}, supondremos tacitamente que tambien hemos hecho
la declaracion $\varphi_{1}=_{d}\varphi_{1}(v_{1},...,v_{n},v)$.
\end{description}

\bigskip

El siguiente lema se basa en la Convencion Notacional 6 y nos permite darle
caracter recursivo a la notacion $\mathbf{A}\models\varphi\lbrack
a_{1},....,a_{n}]$.

\begin{lemma}
[Caracter recursivo de la notacion $\mathbf{A}\models\varphi\lbrack
a_{1},....,a_{n}]$]Supongamos $\varphi=_{d}\varphi(v_{1},...,v_{n})$. Sea
$\mathbf{A}=(A,i)$ un modelo de tipo $\tau$ y sean $a_{1},...,a_{n}\in A$. Entonces

\begin{enumerate}
\item[(1)] Si $\varphi=(t\equiv s)$, entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack a_{1},....,a_{n}]$ si y solo si
$t^{\mathbf{A}}[a_{1},...,a_{n}]=s^{\mathbf{A}}[a_{1},...,a_{n}]$
\end{enumerate}

\item[(2)] Si $\varphi=r(t_{1},...,t_{m})$, entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack a_{1},....,a_{n}]$ si y solo si
$(t_{1}^{\mathbf{A}}[a_{1},...,a_{n}],...,t_{m}^{\mathbf{A}}[a_{1}%
,...,a_{n}])\in r^{\mathbf{A}}$
\end{enumerate}

\item[(3)] Si $\varphi=(\varphi_{1}\wedge\varphi_{2}),$ entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack a_{1},....,a_{n}]$ si y solo si
$\mathbf{A}\models\varphi_{1}[a_{1},....,a_{n}]$ y $\mathbf{A}\models
\varphi_{2}[a_{1},....,a_{n}]$
\end{enumerate}

\item[(4)] Si $\varphi=(\varphi_{1}\vee\varphi_{2}),$ entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack a_{1},....,a_{n}]$ si y solo si
$\mathbf{A}\models\varphi_{1}[a_{1},....,a_{n}]$ o $\mathbf{A}\models
\varphi_{2}[a_{1},....,a_{n}]$
\end{enumerate}

\item[(5)] Si $\varphi=(\varphi_{1}\rightarrow\varphi_{2}),$ entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack a_{1},....,a_{n}]$ si y solo si
$\mathbf{A}\models\varphi_{2}[a_{1},....,a_{n}]$ o $\mathbf{A}\not \models
\varphi_{1}[a_{1},....,a_{n}]$
\end{enumerate}

\item[(6)] Si $\varphi=(\varphi_{1}\leftrightarrow\varphi_{2}),$ entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack a_{1},....,a_{n}]$ si y solo si ya
sea $\mathbf{A}\models\varphi_{1}[a_{1},....,a_{n}]$ y $\mathbf{A}%
\models\varphi_{2}[a_{1},....,a_{n}]$ o $\mathbf{A}\not \models \varphi
_{1}[a_{1},....,a_{n}]$ y $\mathbf{A}\not \models \varphi_{2}[a_{1}%
,....,a_{n}]$
\end{enumerate}

\item[(7)] Si $\varphi=\lnot\varphi_{1},$ entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack a_{1},....,a_{n}]$ si y solo si
$\mathbf{A}\not \models \varphi_{1}[a_{1},....,a_{n}]$
\end{enumerate}

\item[(8)] Si $\varphi=\forall v_{j}\varphi_{1},$ entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack a_{1},....,a_{n}]$ si y solo si
$\mathbf{A}\models\varphi_{1}[a_{1},....,a,...,a_{n}],$ para todo $a\in A.$
\end{enumerate}

\item[(9)] Si $\varphi=\forall v\varphi_{1},$ con $v\not \in \{v_{1}%
,...,v_{n}\}$, entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack a_{1},....,a_{n}]$ si y solo si
$\mathbf{A}\models\varphi_{1}[a_{1},....,a_{n},a]$, para todo $a\in A.$
\end{enumerate}

\item[(10)] Si $\varphi=\exists v_{j}\varphi_{1}$, entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack a_{1},....,a_{n}]$ si y solo si
$\mathbf{A}\models\varphi_{1}[a_{1},....,a,...,a_{n}]$, para algun $a\in A.$
\end{enumerate}

\item[(11)] Si $\varphi=\exists v\varphi_{1}$, con $v\not \in \{v_{1}%
,...,v_{n}\}$, entonces

\begin{enumerate}
\item[-] $\mathbf{A}\models\varphi\lbrack a_{1},....,a_{n}]$ si y solo si
$\mathbf{A}\models\varphi_{1}[a_{1},....,a_{n},a]$, para algun $a\in A.$
\end{enumerate}
\end{enumerate}
\end{lemma}

\begin{proof}
Rutina.
\end{proof}

\bigskip

\subsubsection{Alcance de la ocurrencia de un cuantificador en una formula}

\begin{lemma}
Si $Qv$ ocurre en $\varphi$ a partir de $i$, entonces hay una unica formula
$\psi$ tal que $Qv\psi$ ocurre en $\varphi$ a partir de $i$.
\end{lemma}

\begin{proof}
Por induccion en el $k$ tal que $\varphi\in F^{\tau}$.
\end{proof}

\bigskip

Dada una ocurrencia de $Qv$ en una formula $\varphi$, la formula $\psi$ del
lema anterior sera llamada el \textit{alcance }de dicha ocurrencia en
$\varphi$. Notese que dos ocurrencias distintas de $Qv$ en $\varphi$ pueden
tener alcances distintos.

\bigskip

\subsubsection{Sustitucion de variables libres}

Diremos que $v$ \textit{es sustituible por }$w$\textit{\ en} $\varphi$ cuando
ninguna ocurrencia libre de $v$ en $\varphi$ sucede dentro de una ocurrencia
de una subformula de la forma $Qw\psi$ en $\varphi$. En otras palabras $v$ no
sera sustituible por $w$ en $\varphi$ cuando alguna ocurrencia libre de $v$ en
$\varphi$ suceda dentro de una ocurrencia en $\varphi$ de una formula de la
forma $Qw\psi$. Notese que puede suceder que $v$ sea sustituible por $w$ en
$\varphi$ y que sin envargo haya una subformula de la forma $Qw\psi$ para la
cual $v\in Li(Qw\psi)$. Dejamos como ejercicio encontrar un ejemplo de esta situacion.

Usando lemas anteriores podemos ver que se dan las siguientes propiedades

\begin{enumerate}
\item[(1)] Si $\varphi$ es atomica, entonces $v$ es sustituible por $w$ en
$\varphi$

\item[(2)] Si $\varphi=(\varphi_{1}\eta\varphi_{2})$, entonces $v$ es
sustituible por $w$ en $\varphi$ sii $v$ es substituible por $w$ en
$\varphi_{1}$ y $v$ es

substituible por $w$ en $\varphi_{2}$

\item[(3)] Si $\varphi=\lnot\varphi_{1}$, entonces $v$ es sustituible por $w$
en $\varphi$ sii $v$ es substituible por $w$ en $\varphi_{1}$

\item[(4)] Si $\varphi=Qv\varphi_{1}$, entonces $v$ es sustituible por $w$ en
$\varphi$

\item[(5)] Si $\varphi=Qw\varphi_{1}$ y $v\in Li(\varphi_{1})$, entonces $v$
no es sustituible por $w$ en $\varphi$

\item[(6)] Si $\varphi=Qw\varphi_{1}$ y $v\not \in Li(\varphi_{1})$, entonces
$v$ es sustituible por $w$ en $\varphi$

\item[(7)] Si $\varphi=Qu\varphi_{1}$, con $u\neq v,w$, entonces $v$ es
sustituible por $w$ en $\varphi$ sii $v$ es sustituible por $w$ en
$\varphi_{1}$
\end{enumerate}

\bigskip

Notese que las propiedades (1),...,(7) pueden usarse para dar una definicion
recursiva de la relacion $"v$ $\mathit{es\ sustituible\ por\ }w\mathit{\ en}$
$\varphi"$.

\subsubsection{Teorema de reemplazo para formulas}

Ahora si podemos enunciar y probar el primero de nuestros teoremas de
reemplazo. Antes una definicion. Dado un termino $t$, diremos que una variable
$v$ \textit{es sustituible por }$t$ \textit{en} $\varphi$ cuando $v $ sea
sustituible en $\varphi$ por cada variable que ocurre en $t$.

\begin{theorem}
[Teorema de reemplazo para formulas]\label{reemp-term1}Supongamos
$\varphi=_{d}\varphi(w_{1},...,w_{k})$, $t_{1}=_{d}t_{1}(v_{1},...,v_{n}%
),...,t_{k}=_{d}t_{k}(v_{1},...,v_{n})$ y supongamos ademas que cada $w_{j}$
es sustituible por $t_{j}$ en $\varphi.$ Entonces

\begin{enumerate}
\item[(a)] $Li(\varphi(t_{1},...,t_{k}))\subseteq\{v_{1},...,v_{n}\}$

\item[(b)] Si declaramos $\varphi(t_{1},...,t_{k})=_{d}\varphi(t_{1}%
,...,t_{k})(v_{1},...,v_{n})$, entonces para cada estructura $\mathbf{A}$ y
$\vec{a}\in A^{n}$ se tiene%
\[
\mathbf{A}\models\varphi(t_{1},...,t_{k})[\vec{a}]\text{ si y solo si
}\mathbf{A}\models\varphi\lbrack t_{1}^{\mathbf{A}}[\vec{a}],...,t_{k}%
^{\mathbf{A}}[\vec{a}]]
\]

\end{enumerate}
\end{theorem}

\begin{proof}
Probaremos que se dan (a) y (b), por induccion en el $l$ tal que $\varphi\in
F_{l}^{\tau}.$ El caso $l=0$ es una consecuencia directa del Teorema
\ref{reemp-term}. Supongamos (a) y (b) valen para cada $\varphi\in F_{l}%
^{\tau}$ y sea $\varphi\in F_{l+1}^{\tau}-F_{l}^{\tau}.$ Notese que se puede
suponer que cada $v_{i}$ ocurre en algun $t_{i}$, y que cada $w_{i}\in
Li(\varphi)$, ya que para cada $\varphi$, el caso general se desprende del
caso con estas restricciones. Hay varios casos

CASO $\varphi=\forall w\varphi_{1}$, con $w\not \in \{w_{1},...,w_{k}\}$.

\noindent Notese que cada $w_{j}\in Li(\varphi_{1})$. Ademas notese que
$w\not \in \{v_{1},...,v_{n}\}$ ya que de lo contrario $w$ ocurriria en algun
$t_{j}$, y entonces $w_{j}$ no seria sustituible por $t_{j}$ en $\varphi$.
Sean%
\[%
\begin{array}
[c]{ccc}%
\tilde{t}_{1} & = & t_{1}\\
& \vdots & \\
\tilde{t}_{k} & = & t_{k}\\
\tilde{t}_{k+1} & = & w
\end{array}
\]
Declaremos%
\[
\tilde{t}_{j}=_{d}\tilde{t}_{j}(v_{1},...,v_{n},w)
\]
Notese que nuestra Convencion Notacional 6 nos dice que tenemos implicitamente
hecha la declaracion $\varphi_{1}=_{d}\varphi_{1}(w_{1},...,w_{k},w)$. Por (a)
de la hipotesis inductiva tenemos que%
\[
Li(\varphi_{1}(t_{1},...,t_{k},w))=Li(\varphi_{1}(\tilde{t}_{1},...,\tilde
{t}_{k},\tilde{t}_{k+1}))\subseteq\{v_{1},...,v_{n},w\}
\]
y por lo tanto%
\[
Li(\varphi(t_{1},...,t_{k}))\subseteq\{v_{1},...,v_{n}\}
\]
lo cual prueba (a). Finalmente para probar (b) declaremos $\varphi
(t_{1},...,t_{k})=_{d}\varphi(t_{1},...,t_{k})(v_{1},...,v_{n})$. Se tiene que%
\[%
\begin{array}
[c]{c}%
\mathbf{A}\models\varphi(t_{1},...,t_{k})\mathbf{[}\vec{a}]\\
\Updownarrow\\
\mathbf{A}\models\varphi_{1}(\tilde{t}_{1},...,\tilde{t}_{k},\tilde{t}%
_{k+1})[\vec{a},a]\text{, para todo }a\in A\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \Updownarrow\ \text{(por HI)}\\
\mathbf{A}\models\varphi_{1}[\tilde{t}_{1}^{\mathbf{A}}[\vec{a},a],...,\tilde
{t}_{k}^{\mathbf{A}}[\vec{a},a],\tilde{t}_{k+1}^{\mathbf{A}}[\vec
{a},a]]\text{, para todo }a\in A\\
\Updownarrow\\
\mathbf{A}\models\varphi_{1}[t_{1}^{\mathbf{A}}[\vec{a}],...,t_{k}%
^{\mathbf{A}}[\vec{a}],a]\text{, para todo }a\in A\\
\Updownarrow\\
\mathbf{A}\models\varphi\lbrack t_{1}^{\mathbf{A}}[\vec{a}],...,t_{k}%
^{\mathbf{A}}[\vec{a}]]
\end{array}
\]
lo cual pueba (b). Dejamos al lector los casos restantes.
\end{proof}

\bigskip

\begin{enumerate}
\item[Ejemplo:] Sea $\tau=(\emptyset,\{f\},\emptyset,\{(f,1)\})$. Sean
$\varphi=\exists v_{1}(f(v_{1})\equiv w_{1})$ y $t=v_{1}$, donde $v_{1}$ y
$w_{1}$ son variables distintas. Declaremos $\varphi=_{d}\varphi(w_{1})$ y
$t=_{d}t(v_{1})$. Notese que $w_{1}$ no es sustituible en $\varphi$ por $t$,
por lo cual el teorema anterior no se puede aplicar. De hecho la conclusion
del teorema no se da en este caso ya que puede verse facilmente que,
cualesquiera sea la estructura de tipo $\tau$, $\mathbf{A}$ y $a_{1}\in A$,
tenemos que:

\begin{enumerate}
\item $\mathbf{A}\models\varphi(t)[a_{1}]$ si y solo si $f^{\mathbf{A}}$ tiene
un pto fijo, es decir, $f^{\mathbf{A}}(a)=a$, para algun $a\in A$

\item $\mathbf{A}\models\varphi\lbrack t^{\mathbf{A}}[a_{1}]]$ si y solo si
$a_{1}$ esta en la imagen de $f^{\mathbf{A}}$
\end{enumerate}

\noindent las cuales son condiciones claramente no equivalentes.
\end{enumerate}

\bigskip

\subsection{Teorias de primer orden}

En esta seccion nos avocaremos a dar una solucion al punto (3) de nuestro
programa de logica dado en la Seccion \ref{programa}. O sea nos abocaremos al
siguiente problema:

\begin{enumerate}
\item[(3)] Dar un modelo matematico del concepto de prueba elemental en una
teoria elemental de tipo $\tau$.
\end{enumerate}

Este problema involucra el concepto de teoria elemental definido en la Seccion
\ref{Teorias elementales y pruebas elementales}, el cual es intuitivo, por lo
cual un primer paso en la resolucion de (3) sera dar un modelo matematico de
este concepto. Recordemos que una teoria elemental es un par $(\Sigma,\tau)$
tal que $\tau$ es un tipo cualquiera y $\Sigma$ es un conjunto de sentencias
elementales de tipo $\tau$, las cuales no tienen nombres de elementos fijos.
Dado que ya tenemos nuestro modelo matematico para las sentencias elementales
de tipo $\tau$, las cuales no tienen nombres de elementos fijos (i.e. las
sentencias de tipo $\tau$), podemos dar el siguiente modelo matematico del
concepto de teoria elemental:

Una \textit{teoria de primer orden} sera un par $(\Sigma,\tau)$, donde $\tau$
es un tipo y $\Sigma$ es un conjunto de sentencias de tipo $\tau$. Esto ya es
un buen comienzo en la resolucion del punto (3) pero aun nos queda por hacer
lo mas complicado.

Dada una teoria de primer orden $(\Sigma,\tau)$, los elementos de $\Sigma$
seran llamados \textit{axiomas propios} de $(\Sigma,\tau)$. Un \textit{modelo
de }$(\Sigma,\tau)$ sera una estructura de tipo $\tau$ la cual satisfaga todos
los axiomas propios de $(\Sigma,\tau)$.

Algunos ejemplos de teorias de primer orden:

\bigskip

\textbf{La teoria} $Po$. Sea%
\[
Po=(\{\mathrm{A}_{\leq R},\mathrm{A}_{\leq T},\mathrm{A}_{\leq A}\},\tau_{Po})
\]
donde $\tau_{Po}$ es el tipo de los posets, es decir $(\emptyset
,\emptyset,\{\leq\},\{(\leq,2)\})$ y%
\begin{align*}
\mathrm{A}_{\leq R}  & =\forall x_{1}\;\mathrm{\leq}(x_{1},x_{1})\\
\mathrm{A}_{\leq T}  & =\forall x_{1}\forall x_{2}\forall x_{3}%
\;((\mathrm{\leq}(x_{1},x_{2})\wedge\mathrm{\leq}(x_{2},x_{3}))\rightarrow
\mathrm{\leq}(x_{1},x_{3}))\\
\mathrm{A}_{\leq A}  & =\forall x_{1}\forall x_{2}\;((\mathrm{\leq}%
(x_{1},x_{2})\wedge\mathrm{\leq}(x_{2},x_{1}))\rightarrow(x_{1}\equiv x_{2}))
\end{align*}
Notese que una estructura $\mathbf{A}$ de tipo $\tau_{Po}$ es un modelo de
$Po$ si y solo si $\leq^{\mathbf{A}}$ es un orden parcial sobre $A$.
Estrictamente hablando un modelo de $Po$ no es un poset ya que es un par
$(A,i)$ donde $A$ es un conjunto no vacio e $i$ es una funcion con dominio
$\{\leq\}$ tal que $i(\leq)$ es un orden parcial sobre $A$. Es decir, un
modelo de $Po$ es un par $(A,\{(\leq,R)\})$ donde $A$ es un conjunto no vacio
y $R$ es un orden parcial sobre $A$. De todas maneras deberia quedar claro que
en esencia un poset y un modelo de $Po$ son la misma cosa por lo cual
llamaremos a $Po$ la \textit{teoria de los posets} y muchas veces nos
referiremos a los modelos de $Po$ como si fueran posets. Dejamos al lector el
ejercicio de encontrar una biyeccion natural entre la clase de los modelos de
$Po$ y la clase de los posets.

\bigskip

\textbf{La teoria }$RetCua$. Sea $\tau_{RetCua}=(\emptyset,\{\mathsf{s}%
^{2},\mathsf{i}^{2}\},\{\leq^{2}\},a)$ y sea $\Sigma_{RetCua}$ el siguiente
conjunto de sentencias:%
\begin{align*}
\mathrm{A}_{\leq R}  & =\forall x_{1}\;\mathrm{\leq}(x_{1},x_{1})\\
\mathrm{A}_{\leq T}  & =\forall x_{1}\forall x_{2}\forall x_{3}%
\;((\mathrm{\leq}(x_{1},x_{2})\wedge\mathrm{\leq}(x_{2},x_{3}))\rightarrow
\mathrm{\leq}(x_{1},x_{3}))\\
\mathrm{A}_{\leq A}  & =\forall x_{1}\forall x_{2}\;((\mathrm{\leq}%
(x_{1},x_{2})\wedge\mathrm{\leq}(x_{2},x_{1}))\rightarrow(x_{1}\equiv
x_{2}))\\
\mathrm{A}_{\mathsf{s}esC}  & =\forall x_{1}\forall x_{2}\;(\mathrm{\leq
}(x_{1},\mathsf{s}(x_{1},x_{2}))\wedge\mathrm{\leq}(x_{2},\mathsf{s}%
(x_{1},x_{2})))\\
\mathrm{A}_{\mathsf{s}\leq C}  & =\forall x_{1}\forall x_{2}\forall
x_{3}\;\left(  (\mathrm{\leq}(x_{1},x_{3})\wedge\mathrm{\leq}(x_{2}%
,x_{3}))\rightarrow\mathrm{\leq}(\mathsf{s}(x_{1},x_{2}),x_{3}\right)  )\\
\mathrm{A}_{\mathsf{i}esC}  & =\forall x_{1}\forall x_{2}\;(\mathrm{\leq
}(\mathsf{i}(x_{1},x_{2}),x_{1})\wedge\mathrm{\leq}(\mathsf{i}(x_{1}%
,x_{2}),x_{2}))\\
\mathrm{A}_{\mathsf{i}\geq C}  & =\forall x_{1}\forall x_{2}\forall
x_{3}\;\left(  (\mathrm{\leq}(x_{3},x_{1})\wedge\mathrm{\leq}(x_{3}%
,x_{2}))\rightarrow\mathrm{\leq}(x_{3},\mathsf{i}(x_{1},x_{2}))\right)
\end{align*}
Definamos $RetCua=(\Sigma_{RetCua},\tau_{RetCua})$. Como pueden notarse los
axiomas de $RetCua$ son justamente los axiomas con los que definimos el
concepto de reticulado cuaterna en la Seccion
\ref{Estructuras y su lenguaje elemental asociado}. Es decir una estructura
$\mathbf{A}$ de tipo $\tau_{RetCua}$ es modelo de $RetCua$ sii $(A,\mathsf{s}%
^{\mathbf{A}},\mathsf{i}^{\mathbf{A}},\leq^{\mathbf{A}})$ es un reticulado
cuaterna. Vayamos mas despacio:

\begin{enumerate}
\item[-] Una estructura $\mathbf{A}$ de tipo $\tau_{RetCua}$ satisface los 3
primeros axiomas de $RetCua$ si y solo si el par $(A,\leq^{\mathbf{A}})$ es un
poset. Aqui es muy importante notar que una estructura $\mathbf{A}$ de tipo
$\tau_{RetCua}$ puede satisfacer los 3 axiomas mencionados antes pero esto no
implica que las operaciones $\mathsf{s}^{\mathbf{A}}$ e $\mathsf{i}%
^{\mathbf{A}}$ deban ser las operaciones infimo y supremo respecto al orden
$\leq^{\mathbf{A}}$. De hecho el poset $(A,\leq^{\mathbf{A}})$ podria no tener
supremo para algun subconjunto $\{a,b\}$.

\item[-] Una estructura $\mathbf{A}$ que satisfaga los 3 primeros axiomas
satisfacera el axioma $\mathrm{A}_{\mathsf{s}esC}$ si y solo si cualesquiera
sean los elementos $a,b\in A$, se tiene que $a\;\mathsf{s}^{\mathbf{A}}\;b$ es
cota superior del conjunto $\{a,b\}$ en el poset $(A,\leq^{\mathbf{A}})$. Por
supuesto esto no garaniza que $a\;\mathsf{s}^{\mathbf{A}}\;b$ sea el supremo
de $\{a,b\}$ en $(A,\leq^{\mathbf{A}})$, solo nos dice que debe ser una cota superior

\item[-] Una estructura $\mathbf{A}$ que satisfaga los 3 primeros axiomas
cumplir\'{a} $\mathrm{A}_{\mathsf{s}\leq C}$ si y solo si cualesquiera sean
los elementos $a,b\in A$, se tiene que $a\;\mathsf{s}^{\mathbf{A}}\;b$ es
menor o igual que toda cota superior de $\{a,b\}$ en $(A,\leq^{\mathbf{A}})$.
Por supuesto esto tampoco garaniza que $a\;\mathsf{s}^{\mathbf{A}}\;b$ sea el
supremo de $\{a,b\}$ en $(A,\leq^{\mathbf{A}})$
\end{enumerate}

De las observaciones anteriores el lector ya se habra dado cuenta que dada una
estructura $\mathbf{A}$\ de tipo $\tau_{RetCua}$ son equivalentes

\begin{enumerate}
\item $\mathbf{A}$ es modelo de $RetCua$

\item $(A,\leq^{\mathbf{A}})$ es un poset y cualesquiera sean $a,b\in A$

\begin{enumerate}
\item $a\;\mathsf{s}^{\mathbf{A}}\;b=$ supremo de $\{a,b\}$ en $(A,\leq
^{\mathbf{A}})$

\item $a\;\mathsf{i}^{\mathbf{A}}\;b=$ infimo de $\{a,b\}$ en $(A,\leq
^{\mathbf{A}})$
\end{enumerate}
\end{enumerate}

\bigskip

\subsubsection{Definicion del concepto de prueba formal}

Recomendamos al lector repasar el concepto de prueba elemental en una teoria
elemental, dado en la Seccion
\ref{Estructuras y su lenguaje elemental asociado}. Aqui daremos un modelo
matematico del concepto de prueba elemental en una teoria $(\Sigma,\tau)$. Tal
como lo hemos visto en numerosos ejemplos, una prueba es una sucecion de
sentencias junto con una sucesion de "justificaciones" las cuales van
explicando o justificando por que es licito que cada una de dichas sentencias
aparezca en la sucesion. Por supuesto nuestra definicion sera precisa y
matematica por lo que deberemos trabajar bastante para poder escribirla
correctamente. Como objeto matematico una prueba resultara ser un par ordenado
de palabras cuya primera coordenada codificara en forma natural la sucesion de
sentencias y su segunda coordenada codificara la sucesion de justificaciones.

La formalizacion matematica del concepto de prueba elemental es uno de los
grandes logros de la ciencia moderna y este hecho se debe en gran medida a que
si elejimos bien la teoria, las pruebas elementales no son ni mas ni menos que
las pruebas de la matematica misma por lo cual se tiene una definicion
matematica de la deduccion matematica!

\bigskip

\paragraph{Reglas}

Definiremos una serie de conjuntos los cuales poseen informacion deductiva
basica, es decir representan las reglas usuales con las que los matematicos
dan pasos dentro de una demostracion (aunque muchas veces ellos lo hacen sin
avisar debido a la obviedad de dichas reglas).

Recordemos que si $\tau$ es un tipo cualquiera, un termino $t\in T^{\tau}$ es
llamado \textit{cerrado}\ si ninguna variable es subtermino de $t$. Con
$T_{c}^{\tau}$ denotamos el conjunto formado por todos los terminos cerrados.

Sean%
\begin{align*}
Partic^{\tau}  & =\{(\forall v\varphi(v),\varphi(t)):\varphi=_{d}\varphi(v)\in
F^{\tau}\ \mathrm{y\ }t\in T_{c}^{\tau}\}\\
Exist^{\tau}  & =\{(\varphi(t),\exists v\varphi(v)):\varphi=_{d}\varphi(v)\in
F^{\tau}\ \mathrm{y\ }t\in T_{c}^{\tau}\}\\
Evoc^{\tau}  & =\{(\varphi,\varphi):\varphi\in S^{\tau}\}\\
Absur^{\tau}  & =\{((\lnot\varphi\rightarrow(\psi\wedge\lnot\psi
)),\varphi):\varphi,\psi\in S^{\tau}\}\cup\{((\varphi\rightarrow(\psi
\wedge\lnot\psi)),\lnot\varphi):\varphi,\psi\in S^{\tau}\}\\
ConjElim^{\tau}  & =\{((\varphi\wedge\psi),\varphi):\varphi,\psi\in S^{\tau
}\}\cup\{((\varphi\wedge\psi),\psi):\varphi,\psi\in S^{\tau}\}\\
EquivElim^{\tau}  & =\{((\varphi\leftrightarrow\psi),(\varphi\rightarrow
\psi)):\varphi,\psi\in S^{\tau}\}\cup\{((\varphi\leftrightarrow\psi
),(\psi\rightarrow\varphi)):\varphi,\psi\in S^{\tau}\}\\
DisjInt^{\tau}  & =\{(\varphi,(\varphi\vee\psi)):\varphi,\psi\in S^{\tau
}\}\cup\{(\psi,(\varphi\vee\psi)):\varphi,\psi\in S^{\tau}\}\cup
\{((\lnot\varphi\rightarrow\psi),(\varphi\vee\psi)):\varphi,\psi\in S^{\tau}\}
\end{align*}
Diremos que $\varphi$ \textit{se deduce de }$\psi$ \textit{por la regla de
particularizacion} (resp. \textit{existencia, evocacion, absurdo,
conjuncion-eliminacion, equivalencia-eliminacion, disjuncion-introduccion}),
\textit{con respecto a }$\tau$ para expresar que $(\psi,\varphi)\in
Partic^{\tau}$ (resp. $(\psi,\varphi)\in Exist^{\tau}$, $(\psi,\varphi)\in
Evoc^{\tau}$, $(\psi,\varphi)\in Absur^{\tau}$, $(\psi,\varphi)\in
ConjElim^{\tau}$, $(\psi,\varphi)\in EquivElim^{\tau}$, $(\psi,\varphi)\in
DisjInt^{\tau}$).

Sea%
\[
Commut^{\tau}=Commut1^{\tau}\cup Commut2^{\tau}%
\]
donde%
\begin{align*}
Commut1^{\tau}  & =\{((t\equiv s),(s\equiv t)):s,t\in T_{c}^{\tau}\}\\
Commut2^{\tau}  & =\{((\varphi\leftrightarrow\psi),(\psi\leftrightarrow
\varphi)):\varphi,\psi\in S^{\tau}\}
\end{align*}
Diremos que $\varphi$ \textit{se deduce de }$\psi$ \textit{por la regla de
commutatividad}, \textit{con respecto a }$\tau$ para expresar que
$(\psi,\varphi)\in Commut^{\tau}$.

Sean%
\begin{align*}
ModPon^{\tau}  & =\{(\varphi,(\varphi\rightarrow\psi),\psi):\varphi,\psi\in
S^{\tau}\}\\
ConjInt^{\tau}  & =\{(\varphi,\psi,(\varphi\wedge\psi)):\varphi,\psi\in
S^{\tau}\}\\
EquivInt^{\tau}  & =\{((\varphi\rightarrow\psi),(\psi\rightarrow
\varphi),(\varphi\leftrightarrow\psi)):\varphi,\psi\in S^{\tau}\}\\
DisjElim^{\tau}  & =\{(\lnot\varphi,(\varphi\vee\psi),\psi):\varphi,\psi\in
S^{\tau}\}\cup\{(\lnot\psi,(\varphi\vee\psi),\varphi):\varphi,\psi\in S^{\tau
}\}
\end{align*}
Diremos que $\varphi$ \textit{se deduce de }$\psi_{1}$ y $\psi_{2}$
\textit{por la regla de Modus Ponens} (resp. \textit{conjuncion-introduccion,
equivalencia-introduccion, disjuncion-eliminacion}), \textit{con respecto a
}$\tau$ para expresar que $(\psi_{1},\psi_{2},\varphi)\in ModPon^{\tau}$
(resp. $(\psi_{1},\psi_{2},\varphi)\in ConjInt^{\tau}$, $(\psi_{1},\psi
_{2},\varphi)\in EquivInt^{\tau}$, $(\psi_{1},\psi_{2},\varphi)\in
DisjElim^{\tau}$). Sea%
\[
DivPorCas^{\tau}=\{((\varphi_{1}\vee\varphi_{2}),(\varphi_{1}\rightarrow
\psi),(\varphi_{2}\rightarrow\psi),\psi):\varphi_{1},\varphi_{2},\psi\in
S^{\tau}\}
\]
Diremos que $\varphi$ \textit{se deduce de }$\psi_{1}$, $\psi_{2}$ y $\psi
_{3}$ \textit{por la regla de division por casos,} \textit{con respecto a
}$\tau$ para expresar que $(\psi_{1},\psi_{2},\psi_{3},\varphi)\in
DivPorCas^{\tau}$. Sea%
\[
Reemp^{\tau}=Reemp1^{\tau}\cup Reemp2^{\tau}%
\]
donde

$Reemp1^{\tau}=\{((t\equiv s),\gamma,\tilde{\gamma}):s,t\in T_{c}^{\tau},$

$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \gamma\in
S^{\tau}\ \mathrm{y\ }\tilde{\gamma}=\mathrm{resultado\ de\ reemplazar\ en\ }%
\gamma\ \mathrm{una\ ocurrencia\ de\ }t\ \mathrm{por\ }s\}$

$Reemp2^{\tau}=\{(\forall v_{1}...\forall v_{n}(\varphi\leftrightarrow
\psi),\gamma,\tilde{\gamma}):\varphi,\psi\in F^{\tau}$, $Li(\varphi
)=Li(\psi)=\{v_{1},...,v_{n}\}$,

$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n\geq0,$
$\gamma\in S^{\tau}\ \mathrm{y\ }\tilde{\gamma}%
=\mathrm{resultado\ de\ reemplazar\ en\ }\gamma
\ \mathrm{una\ ocurrencia\ de\ }\varphi\ \mathrm{por\ }\psi\}$

\noindent Diremos que $\varphi$ \textit{se deduce de }$\psi_{1}$y $\psi_{2}$
\textit{por la regla de reemplazo,} \textit{con respecto a }$\tau$, para
expresar que $(\psi_{1},\psi_{2},\varphi)\in Reemp^{\tau}$. Sea%
\[
Trans^{\tau}=Trans1^{\tau}\cup Trans2^{\tau}\cup Trans3^{\tau}%
\]
donde%
\begin{align*}
Trans1^{\tau}  & =\{((t\equiv s),(s\equiv u),(t\equiv u)):t,s,u\in T_{c}%
^{\tau}\}\\
Trans2^{\tau}  & =\{((\varphi\rightarrow\psi),(\psi\rightarrow\Phi
),(\varphi\rightarrow\Phi)):\varphi,\psi,\Phi\in S^{\tau}\}\\
Trans3^{\tau}  & =\{((\varphi\leftrightarrow\psi),(\psi\leftrightarrow
\Phi),(\varphi\leftrightarrow\Phi)):\varphi,\psi,\Phi\in S^{\tau}\}
\end{align*}
Diremos que $\varphi$ \textit{se deduce de }$\psi_{1}$y $\psi_{2}$ \textit{por
la regla de transitividad,} \textit{con respecto a }$\tau$ para expresar que
$(\psi_{1},\psi_{2},\varphi)\in Trans^{\tau}$. Sea%
\[
Generaliz^{\tau}=\{(\varphi(c),\forall v\varphi(v)):\varphi=_{d}\varphi(v)\in
F^{\tau},\ Li(\varphi)=\{v\}\ \mathrm{y\ }c\in\mathcal{C}%
\ \mathrm{no\ ocurre\ en}\ \varphi\}
\]
Es importante el siguiente

\begin{lemma}
Si $(\varphi_{1},\varphi_{2})\in Generaliz^{\tau}$, entonces el nombre de
constante $c$ del cual habla la definicion de $Generaliz^{\tau}$ esta
univocamente determinado por el par $(\varphi_{1},\varphi_{2})$.
\end{lemma}

\begin{proof}
Notese que $c$ es el unico nombre de constante que ocurre en $\varphi_{1}$ y
no ocurre en $\varphi_{2}$
\end{proof}

\bigskip

Escribiremos $(\varphi_{1},\varphi_{2})\in Generaliz^{\tau}$ \textit{via} $c$
para expresar que $(\varphi_{1},\varphi_{2})\in Generaliz^{\tau}$ y que $c$ es
el unico nombre de constante que ocurre en $\varphi_{1}$ y no ocurre en
$\varphi_{2}$. Diremos que $\varphi_{2}$ \textit{se deduce de }$\varphi_{1}$
\textit{por la regla de generalizacion con nombre de constante }$c$,
\textit{con respecto a }$\tau$, para expresar que $(\varphi_{1},\varphi
_{2})\in Generaliz^{\tau}$ \textit{via} $c$

Sea%
\[
Elec^{\tau}=\{(\exists v\varphi(v),\varphi(e)):\varphi=_{d}\varphi(v)\in
F^{\tau},\ Li(\varphi)=\{v\}\ \mathrm{y\ }e\in\mathcal{C}%
\ \mathrm{no\ ocurre\ en}\ \varphi\}
\]
Es importante el siguiente

\begin{lemma}
Si $(\varphi_{1},\varphi_{2})\in Elec^{\tau}$, entonces el nombre de constante
$e$ del cual habla la definicion de $Elec^{\tau}$ esta univocamente
determinado por el par $(\varphi_{1},\varphi_{2})$.
\end{lemma}

\begin{proof}
Notese que $e$ es el unico nombre de constante que ocurre en $\varphi_{2}$ y
no ocurre en $\varphi_{1}$.
\end{proof}

$\bigskip$

Escribiremos $(\varphi_{1},\varphi_{2})\in Elec^{\tau}$ \textit{via} $e$ para
expresar que $(\varphi_{1},\varphi_{2})\in Elec^{\tau}$ y que $e$ es el unico
nombre de constante que ocurre en $\varphi_{2}$ y no ocurre en $\varphi_{1}$.
Diremos que $\varphi_{2}$ \textit{se deduce de }$\varphi_{1} $ \textit{por la
regla de eleccion con nombre de constante }$e$, \textit{con respecto a }$\tau$
para expresar que $(\varphi_{1},\varphi_{2})\in Elec^{\tau}$ \textit{via} $e$.

\bigskip

Como se puede notar hay muchas reglas y todas modelizan en forma muy natural
fragmentos deductivos usuales de las pruebas elementales. Una regla $R$ sera
llamada \textit{universal} cuando se de que si $\varphi$ se deduce de
$\psi_{1},...,\psi_{k}$ por $R$, entonces $\left(  (\psi_{1}\wedge
...\wedge\psi_{k})\rightarrow\varphi\right)  $ es una sentencia universalmente valida.

\bigskip

\begin{lemma}
\label{reglas universales}Sea $\tau$ un tipo. Todas las reglas exepto las
reglas de eleccion y generalizacion son universales.
\end{lemma}

\begin{proof}
Veamos que la regla de existencia es universal. Por definicion, un par de
$Exist^{\tau}$ es siempre de la forma $(\varphi(t),\exists v\varphi(v))$, con
$\varphi=_{d}\varphi(v)$ y $t\in T_{c}^{\tau}$. Sea $\mathbf{A}$ una
estructura de tipo $\tau$ tal que $\mathbf{A}\models\varphi(t)$. Sea
$t^{\mathbf{A}}$ el valor que toma $t$ en $\mathbf{A}$. Por el Lema
\ref{reemp-term1} tenemos que $\mathbf{A}\models\varphi\left[  t^{\mathbf{A}%
}\right]  $, por lo cual tenemos que $\mathbf{A}\models\exists v\varphi(v)$.

Veamos que la regla de reemplazo es universal. Debemos probar que si
$(\psi_{1},\psi_{2},\varphi)\in Reemp^{\tau}=Reemp1^{\tau}\cup Reemp2^{\tau}$,
entonces $\left(  (\psi_{1}\wedge\psi_{2})\rightarrow\varphi\right)  $ es una
sentencia universalmente valida. El caso en el que $(\psi_{1},\psi_{2}%
,\varphi)\in Reemp1^{\tau}$ es facil y lo dejaremos al lector. Para el caso en
el que $(\psi_{1},\psi_{2},\varphi)\in Reemp2^{\tau}$ nos hara falta un
resultado un poco mas general. Veamos por induccion en $k$ que si se dan las
siguientes condiciones

\begin{enumerate}
\item[-] $\alpha\in F_{k}^{\tau}$ y $\varphi,\psi\in F^{\tau}$

\item[-] $\mathbf{A}$ es una estructura de tipo $\tau$

\item[-] $\overline{\alpha}=$ resultado de reemplazar en $\alpha$ una
ocurrencia de $\varphi$ por $\psi$,

\item[-] $\mathbf{A}\models\varphi\left[  \vec{a}\right]  $ si y solo si
$\mathbf{A}\models\psi\left[  \vec{a}\right]  $, para cada $\vec{a}\in
A^{\mathbf{N}}$
\end{enumerate}

\noindent entonces se da que

\begin{enumerate}
\item[-] $\mathbf{A}\models\alpha\left[  \vec{a}\right]  $ si y solo si
$\mathbf{A}\models\overline{\alpha}\left[  \vec{a}\right]  $, para cada
$\vec{a}\in A^{\mathbf{N}}$.
\end{enumerate}

CASO $k=0.$

\noindent Entonces $\alpha$ es atomica y por lo tanto ya que $\alpha$ es la
unica subformula de $\alpha$, la situacion es facil de probar.

CASO $\alpha=\forall x_{i}\alpha_{1}.$

\noindent Si $\varphi=\alpha$, entonces la situacion es facil de probar. Si
$\varphi\neq\alpha$, entonces la ocurrencia de $\varphi$ a reemplazar sucede
en $\alpha_{1}$ y por lo tanto $\overline{\alpha}=\forall x_{i}\overline
{\alpha_{1}}.$ Se tiene entonces que para un $\vec{a}$ dado,%
\[%
\begin{array}
[c]{c}%
\mathbf{A}\models\alpha\left[  \vec{a}\right] \\
\Updownarrow\\
\mathbf{A}\models\alpha_{1}\left[  \downarrow_{i}^{a}\vec{a}\right]  ,\text{
para cada }a\in A\\
\Updownarrow\\
\mathbf{A}\models\overline{\alpha_{1}}\left[  \downarrow_{i}^{a}\vec
{a}\right]  ,\text{ para cada }a\in A\\
\Updownarrow\\
\mathbf{A}\models\overline{\alpha}\left[  \vec{a}\right]
\end{array}
\]
CASO $\alpha=(\alpha_{1}\vee\alpha_{2})$.

\noindent Si $\varphi=\alpha$, entonces la situacion es facil de probar.
Supongamos $\varphi\neq\alpha$ y supongamos que la ocurrencia de $\varphi$ a
reemplazar sucede en $\alpha_{1}$. Entonces $\overline{\alpha}=(\overline
{\alpha_{1}}\vee\alpha_{2})$ y tenemos que%
\[%
\begin{array}
[c]{c}%
\mathbf{A}\models\alpha\left[  \vec{a}\right] \\
\Updownarrow\\
\mathbf{A}\models\alpha_{1}\left[  \vec{a}\right]  \text{ o }\mathbf{A}%
\models\alpha_{2}\left[  \vec{a}\right] \\
\Updownarrow\\
\mathbf{A}\models\overline{\alpha_{1}}\left[  \vec{a}\right]  \text{ o
}\mathbf{A}\models\alpha_{2}\left[  \vec{a}\right] \\
\Updownarrow\\
\mathbf{A}\models\overline{\alpha}\left[  \vec{a}\right]
\end{array}
\]
Los demas casos son dejados al lector.

Dejamos al lector el chequeo de la universalidad del resto de las reglas.
\end{proof}

\bigskip

\paragraph{Axiomas logicos}

Recordemos que dada una teoria $(\Sigma,\tau)$, los elementos de $\Sigma$ son
llamados axiomas propios y en general no son sentencias universalmente validas.

En las pruebas formales sera necesario usar ciertas verdades universales y
obvias las cuales llamaremos \textit{axiomas logicos}. Mas concretamente,
llamaremos \textit{axiomas logicos de tipo }$\tau$ a todas las sentencias de
alguna de las siguientes formas.

\begin{enumerate}
\item $(\varphi\leftrightarrow\varphi)$

\item $(t\equiv t)$

\item $(\varphi\vee\lnot\varphi)$

\item $(\varphi\leftrightarrow\lnot\lnot\varphi)$

\item $(\lnot\forall v\psi\leftrightarrow\exists v\lnot\psi)$

\item $(\lnot\exists v\psi\leftrightarrow\forall v\lnot\psi)$
\end{enumerate}

\noindent donde $t\in T_{c}^{\tau}$, $\varphi\in S^{\tau}$, $\psi\in F^{\tau}%
$, $v\in Var$ y $Li(\psi)\subseteq\{v\}$. Con $AxLog^{\tau}$ denotaremos el
conjunto%
\[
\{\varphi\in S^{\tau}:\varphi\ \mathrm{es\ un\ axioma\ logico\ de\ tipo\ }%
\tau\}
\]
Notese que hay infinitos axiomas logicos de tipo $\tau$, es decir el conjunto
$AxLog^{\tau}$ es un conjunto infinito de palabras. Por ejemplo, el formato
dado en 1. produce una cantidad infinita de axiomas logicos, a saber todas las
sentencias de la forma $(\varphi\leftrightarrow\varphi)$, donde $\varphi$ es
cualquier sentencia de tipo $\tau$.

\bigskip

\begin{enumerate}
\item[Ejercicio:] Pruebe que cada sentencia de $AxLog^{\tau}$\ es
universalmente valida
\end{enumerate}

\bigskip

\paragraph{Justificaciones}

Remitimos a la Seccion \ref{SintaxisDeSsigma} por la definicion del alfabeto
$Num$ y la funcion $Dec:\omega\rightarrow Num^{\ast}$. Recordemos que para
$n\in\mathbf{N}$, la palabra $Dec(n)$ es la notacion usual decimal de $n$ y
que para hacer mas agil la notacion escribimos $\bar{n}$ en lugar de $Dec(n)$.

Sea $Nombres_{1}$ el conjunto formado por las siguientes palabras%

\begin{align*}
& \text{EXISTENCIA}\\
& \text{COMMUTATIVIDAD}\\
& \text{PARTICULARIZACION}\\
& \text{ABSURDO}\\
& \text{EVOCACION}\\
& \text{CONJUNCIONELIMINACION}\\
& \text{EQUIVALENCIAELIMINACION}\\
& \text{DISJUNCIONINTRODUCCION}\\
& \text{ELECCION}\\
& \text{GENERALIZACION}%
\end{align*}
Sea $Nombres_{2}$ el conjunto formado por las siguientes palabras%

\begin{align*}
& \text{MODUSPONENS}\\
& \text{TRANSITIVIDAD}\\
& \text{CONJUNCIONINTRODUCCION}\\
& \text{EQUIVALENCIAINTRODUCCION}\\
& \text{DISJUNCIONELIMINACION}\\
& \text{REEMPLAZO}%
\end{align*}
Una \textit{justificacion basica} es una palabra perteneciente a la union de
los siguientes conjuntos de palabras%
\[
\{\text{CONCLUSION},\text{AXIOMAPROPIO},\text{AXIOMALOGICO}\}
\]%
\[
\{\alpha(\bar{k}):k\in\mathbf{N}\text{ y }\alpha\in Nombres_{1}\}
\]%
\[
\{\alpha(\bar{j},\bar{k}):j,k\in\mathbf{N}\text{ y }\alpha\in Nombres_{2}\}
\]
%

\[
\{\text{DIVISIONPORCASOS}(\bar{j},\bar{k},\bar{l}):j,k,l\in\mathbf{N}\}
\]
Usaremos $JustBas$ para denotar el conjunto formado por todas las
justificaciones basicas. Una \textit{justificacion }es una palabra que ya sea
es una justificacion basica o pertenece a la union de los siguientes conjuntos
de palabras%
\[
\{\text{HIPOTESIS}\bar{k}:k\in\mathbf{N}\}
\]%
\[
\{\text{TESIS}\bar{j}\alpha:j\in\mathbf{N}\text{ y }\alpha\in JustBas\}
\]
Usaremos $Just$ para denotar el conjunto formado por todas las
justificaciones. Cabe destacar que los elementos de $Just$ son palabras del
alfabeto formado por los siguientes simbolos%
\[
(\ )\ ,\ 0\ 1\ 2\ 3\ 4\ 5\ 6\ 7\ 8\ 9\ \text{A\ B\ C\ D\ E\ G\ H\ I\ J\ L\ M\ N\ O\ P\ Q\ R\ S\ T\ U\ V\ X
Z}%
\]


\bigskip

\paragraph{Concatenaciones balanceadas de justificaciones}

Para construir el concepto de prueba elemental deberiamos trabajar con
sucesiones finitas de justificaciones pero el siguiente lema nos dice que
podemos reemplazarlas por ciertas palabras, i.e. sus concatenaciones, sin
perder informacion.

\begin{lemma}
\label{secuencia de justificaciones}Sea $\mathbf{J}\in Just^{+}$. Hay unicos
$n\geq1$ y $J_{1},...,J_{n}\in Just$ tales que $\mathbf{J}=J_{1}...J_{n}$.
\end{lemma}

\begin{proof}
Supongamos $J_{1},...,J_{n}$, $J_{1}^{\prime},...,J_{m}^{\prime}$, con
$n,m\geq1$, son justificaciones tales que $J_{1}...J_{n}=J_{1}^{\prime
}...J_{m}^{\prime}$. Es facil ver que entonces tenemos $J_{1}=J_{1}^{\prime}$,
por lo cual $J_{2}...J_{n}=J_{2}^{\prime}...J_{m}^{\prime}$. Un argumento
inductivo nos dice que entonces $n=m$ y $J_{i}=J_{i}^{\prime}$, $i=1,...,n$
\end{proof}

\bigskip

Es decir el lema anterior nos dice que la sucesion $J_{1},...,J_{n}$ se puede
codificar con la palabra $J_{1}...J_{n}$ sin perder informacion. Dada
$\mathbf{J}\in Just^{+}$, usaremos $n(\mathbf{J})$ y $\mathbf{J}%
_{1},...,\mathbf{J}_{n(\mathbf{J})}$ para denotar los unicos $n$ y
$J_{1},...,J_{n}$ cuya existencia garantiza el lema anterior.

Dados $i,j\in\omega$, usaremos $\left\langle i,j\right\rangle $ para denotar
el conjunto $\{l\in\omega:i\leq l\leq j\}$. A los conjuntos de la forma
$\left\langle i,j\right\rangle $ los llamaremos \textit{bloques}.

Dada $\mathbf{J}\in Just^{+}$ definamos%
\begin{gather*}
\mathcal{B}^{\mathbf{J}}=\{\left\langle i,j\right\rangle :1\leq i\leq j\leq
n(\mathbf{J})\text{ y \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\\
\exists k\ \mathbf{J}_{i}=\text{HIPOTESIS}\bar{k}\text{ y}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{J}_{j}=\text{TESIS}\bar{k}%
\alpha\text{ para algun }\alpha\in JustBas\}
\end{gather*}
Diremos que $\mathbf{J}\in Just^{+}$ es \textit{balanceada} si se dan las siguientes

\begin{enumerate}
\item[(1)] Por cada $k\in\mathbf{N}$ a lo sumo hay un $i$ tal que
$\mathbf{J}_{i}=$ $\mathrm{HIPOTESIS}\bar{k}$ y a lo sumo hay un $i$ tal que
$\mathbf{J}_{i}=$ $\mathrm{TESIS}\bar{k}\alpha$, con $\alpha\in JustBas$

\item[(2)] Si $\mathbf{J}_{i}=\mathrm{HIPOTESIS}\bar{k}$ entonces hay un $l>i
$ tal que $\mathbf{J}_{l}=\mathrm{TESIS}\bar{k}\alpha$, con $\alpha\in
JustBas$

\item[(3)] Si $\mathbf{J}_{i}=\mathrm{TESIS}\bar{k}\alpha$, con $\alpha\in
JustBas$, entonces hay un $l<i$ tal que $\mathbf{J}_{l}=\mathrm{HIPOTESIS}%
\bar{k}$

\item[(4)] Si $B_{1},B_{2}\in\mathcal{B}^{\mathbf{J}}$, entonces $B_{1}\cap
B_{2}=\emptyset$ o $B_{1}\subseteq B_{2}$ o $B_{2}\subseteq B_{1}$
\end{enumerate}

\bigskip

\begin{enumerate}
\item[Ejercicio:] Supongamos $\mathbf{J}\in Just^{+}$ es balanceada. Entonces

\begin{enumerate}
\item Si $\left\langle i,j\right\rangle \in\mathcal{B}^{\mathbf{J}}$, entonces
$i<j$

\item Si $\left\langle i,j\right\rangle ,\left\langle i^{\prime},j^{\prime
}\right\rangle \in\mathcal{B}^{\mathbf{J}}$ y $i=i^{\prime}$, entonces
$j=j^{\prime}$

\item Si $\left\langle i,j\right\rangle ,\left\langle i^{\prime},j^{\prime
}\right\rangle \in\mathcal{B}^{\mathbf{J}}$ y $j=j^{\prime}$, entonces
$i=i^{\prime}$
\end{enumerate}
\end{enumerate}

\bigskip

\paragraph{Pares adecuados}

Para construir el concepto de prueba elemental deberiamos trabajar con
sucesiones finitas de sentencias pero el siguiente lema nos dice que podemos
reemplazarlas por ciertas palabras, i.e. sus concatenaciones, sin perder informacion.

\begin{lemma}
\label{secuencia de sentencias}Sea $\mathbf{\varphi}\in S^{\tau+}$. Hay unicos
$n\geq1$ y $\varphi_{1},...,\varphi_{n}\in S^{\tau}$ tales que
$\mathbf{\varphi}=\varphi_{1}...\varphi_{n}$.
\end{lemma}

\begin{proof}
Solo hay que probar la unicidad la cual sigue de la Proposicion
\ref{mordisqueo}.
\end{proof}

\bigskip

Es decir el lema anterior nos dice que la sucesion $\varphi_{1},...,\varphi
_{n}$ se puede codificar con la palabra $\varphi_{1}...\varphi_{n}$ sin perder
informacion. Dada $\mathbf{\varphi}\in S^{\tau+}$, usaremos $n(\mathbf{\varphi
})$ y $\mathbf{\varphi}_{1},...,\mathbf{\varphi}_{n(\mathbf{\varphi})}$ para
denotar los unicos $n$ y $\varphi_{1},...,\varphi_{n}$ cuya existencia
garantiza el lema anterior.

Un \textit{par adecuado de tipo }$\tau$ es un par $(\mathbf{\varphi
},\mathbf{J})\in S^{\tau+}\times Just^{+}$ tal que $n(\mathbf{\varphi
})=n(\mathbf{J})$ y $\mathbf{J}$ es balanceada.

Sea $(\mathbf{\varphi},\mathbf{J})$ un par adecuado de tipo $\tau$. Si
$\left\langle i,j\right\rangle \in\mathcal{B}^{\mathbf{J}}$, entonces
$\mathbf{\varphi}_{i}$ sera la \textit{hipotesis }del bloque $\left\langle
i,j\right\rangle $ en $(\mathbf{\varphi},\mathbf{J})$ y $\mathbf{\varphi}_{j}$
sera la \textit{tesis} del bloque $\left\langle i,j\right\rangle $ en
$(\mathbf{\varphi},\mathbf{J})$. Diremos que $\mathbf{\varphi}_{i}$
\textit{esta bajo la hipotesis} $\mathbf{\varphi}_{l}$ \textit{en}
$(\mathbf{\varphi},\mathbf{J})$ o que $\mathbf{\varphi}_{l}$ \textit{es una
hipotesis de} $\mathbf{\varphi}_{i}$ \textit{en} $(\mathbf{\varphi}%
,\mathbf{J})$ cuando haya en $\mathcal{B}^{\mathbf{J}}$ un bloque de la forma
$\left\langle l,j\right\rangle $ el cual contenga a $i$. Sean $i,j\in
\left\langle 1,n(\mathbf{\varphi})\right\rangle .$ Diremos que $i$ es
\textit{anterior} a $j$ \textit{en} $(\mathbf{\varphi},\mathbf{J})$ si $i<j$ y
ademas para todo $B\in\mathcal{B}^{\mathbf{J}}$ se tiene que $i\in
B\Rightarrow j\in B$.

\bigskip

\subparagraph{Dependencia de constantes en pares adecuados}

Sea $(\mathbf{\varphi},\mathbf{J})$ un par adecuado de tipo $\tau$. Dadas
$e,d\in\mathcal{C}$, diremos que $e$ \textit{depende directamente de }$d$
\textit{en} $(\mathbf{\varphi},\mathbf{J})$ si hay numeros $1\leq l<j\leq
n(\mathbf{\varphi})$ tales que

\begin{enumerate}
\item[(1)] $l$ es anterior a $j$ en $(\mathbf{\varphi},\mathbf{J})$

\item[(2)] $\mathbf{J}_{j}=\alpha\mathrm{ELECCION}(\bar{l})$, con $\alpha
\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$ y
$(\mathbf{\varphi}_{l},\mathbf{\varphi}_{j})\in Elec^{\tau}$ via $e$

\item[(3)] $d$ ocurre en $\mathbf{\varphi}_{l}$.
\end{enumerate}

\bigskip

Dados $e,d\in\mathcal{C}$, diremos que $e$ \textit{depende de }$d$ \textit{en}
$(\mathbf{\varphi},\mathbf{J})$ si existen $e_{0},...,e_{k+1}\in\mathcal{C}$,
con $k\geq0$, tales que

\begin{enumerate}
\item[(1)] $e_{0}=e$ y $e_{k+1}=d$

\item[(2)] $e_{i}$ \textit{depende directamente de }$e_{i+1}$ \textit{en}
$(\mathbf{\varphi},\mathbf{J})$, para $i=0,...,k$.
\end{enumerate}

\bigskip

\paragraph{Definicion de prueba formal}

Ahora si estamos en condiciones de definir el concepto de prueba formal en una
teoria de primer orden. Sea $(\Sigma,\tau)$ una teoria de primer orden. Sea
$\varphi$ una sentencia de tipo $\tau$. Una \textit{prueba formal de }%
$\varphi$ \textit{en} $(\Sigma,\tau)$ sera un par adecuado $(\mathbf{\varphi
},\mathbf{J})$ de algun tipo $\tau_{1}=(\mathcal{C}\cup\mathcal{C}%
_{1},\mathcal{F},\mathcal{R},a)$, con $\mathcal{C}_{1}$ finito y disjunto con
$\mathcal{C}$, tal que

\begin{enumerate}
\item[(1)] Cada $\mathbf{\varphi}_{i}$ es una sentencia de tipo $\tau_{1}$

\item[(2)] $\mathbf{\varphi}_{n(\mathbf{\varphi})}=\varphi$

\item[(3)] Si $\left\langle i,j\right\rangle \in\mathcal{B}^{\mathbf{J}}$,
entonces $\mathbf{\varphi}_{j+1}=(\mathbf{\varphi}_{i}\rightarrow
\mathbf{\varphi}_{j})$ y $\mathbf{J}_{j+1}=\alpha\mathrm{CONCLUSION}$, con
$\alpha\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$

\item[(4)] Para cada $i=1,...,n(\mathbf{\varphi})$, se da una de las siguientes

\begin{enumerate}
\item[(a)] $\mathbf{J}_{i}=\mathrm{HIPOTESIS}\bar{k}$ para algun
$k\in\mathbf{N}$

\item[(b)] $\mathbf{J}_{i}=\alpha\mathrm{CONCLUSION}$, con $\alpha
\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$ y hay un $j$
tal que $\left\langle j,i-1\right\rangle \in\mathcal{B}^{\mathbf{J}}$ y
$\mathbf{\varphi}_{i}=(\mathbf{\varphi}_{j}\rightarrow\mathbf{\varphi}_{i-1})$

\item[(c)] $\mathbf{J}_{i}=\alpha\mathrm{AXIOMALOGICO}$, con $\alpha
\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$ y
$\mathbf{\varphi}_{i}$ es un axioma logico de tipo $\tau_{1}$

\item[(d)] $\mathbf{J}_{i}=\alpha\mathrm{AXIOMAPROPIO}$, con $\alpha
\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$ y
$\mathbf{\varphi}_{i}\in\Sigma$

\item[(e)] $\mathbf{J}_{i}=\alpha\mathrm{PARTICULARIZACION}(\bar{l})$, con
$\alpha\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$ $l $
anterior a $i$ y $(\mathbf{\varphi}_{l},\mathbf{\varphi}_{i})\in
Partic^{\tau_{1}}$

\item[(f)] $\mathbf{J}_{i}=\alpha\mathrm{COMMUTATIVIDAD}(\bar{l})$, con
$\alpha\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$, $l$
anterior a $i$ y $(\mathbf{\varphi}_{l},\mathbf{\varphi}_{i})\in
Commut^{\tau_{1}}$

\item[(g)] $\mathbf{J}_{i}=\alpha\mathrm{ABSURDO}(\bar{l})$, con $\alpha
\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$, $l$ anterior
a $i$ y $(\mathbf{\varphi}_{l},\mathbf{\varphi}_{i})\in Absur^{\tau_{1}}$

\item[(h)] $\mathbf{J}_{i}=\alpha\mathrm{EVOCACION}(\bar{l})$, con $\alpha
\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$, $l$ anterior
a $i$ y $(\mathbf{\varphi}_{l},\mathbf{\varphi}_{i})\in Evoc^{\tau_{1}}$

\item[(i)] $\mathbf{J}_{i}=\alpha\mathrm{EXISTENCIA}(\bar{l})$, con $\alpha
\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$, $l$ anterior
a $i$ y $(\mathbf{\varphi}_{l},\mathbf{\varphi}_{i})\in Exist^{\tau_{1}}$

\item[(j)] $\mathbf{J}_{i}=\alpha\mathrm{CONJUNCIONELIMINACION}(\bar{l})$, con
$\alpha\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$, $l$
anterior a $i$ y $(\mathbf{\varphi}_{l},\mathbf{\varphi}_{i})\in
ConjElim^{\tau_{1}}$

\item[(k)] $\mathbf{J}_{i}=\alpha\mathrm{DISJUNCIONINTRODUCCION}(\bar{l})$,
con $\alpha\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$,
$l$ anterior a $i$ y $(\mathbf{\varphi}_{l},\mathbf{\varphi}_{i})\in
DisjInt^{\tau_{1}}$

\item[(l)] $\mathbf{J}_{i}=\alpha\mathrm{EQUIVALENCIAELIMINACION}(\bar{l})$,
con $\alpha\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$,
$l$ anterior a $i$ y $(\mathbf{\varphi}_{l},\mathbf{\varphi}_{i})\in
EquivElim^{\tau_{1}}$

\item[(m)] $\mathbf{J}_{i}=\alpha\mathrm{MODUSPONENS}(\overline{l_{1}%
},\overline{l_{2}})$, con $\alpha\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar
{k}:k\in\mathbf{N}\}$, $l_{1}$ y $l_{2}$ anteriores a $i$ y $(\mathbf{\varphi
}_{l_{1}},\mathbf{\varphi}_{l_{2}},\mathbf{\varphi}_{i})\in ModPon^{\tau_{1}}$

\item[(n)] $\mathbf{J}_{i}=\alpha\mathrm{CONJUNCIONINTRODUCCION}%
(\overline{l_{1}},\overline{l_{2}})$, con $\alpha\in\{\varepsilon
\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$, $l_{1}$ y $l_{2}$ anteriores
a $i$ y $(\mathbf{\varphi}_{l_{1}},\mathbf{\varphi}_{l_{2}},\mathbf{\varphi
}_{i})\in ConjInt^{\tau_{1}}$

\item[(o)] $\mathbf{J}_{i}=\alpha\mathrm{EQUIVALENCIAINTRODUCCION}%
(\overline{l_{1}},\overline{l_{2}})$, con $\alpha\in\{\varepsilon
\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$, $l_{1}$ y $l_{2}$ anteriores
a $i$ y $(\mathbf{\varphi}_{l_{1}},\mathbf{\varphi}_{l_{2}},\mathbf{\varphi
}_{i})\in EquivInt^{\tau_{1}}$

\item[(p)] $\mathbf{J}_{i}=\alpha\mathrm{DISJUNCIONELIMINACION}(\overline
{l_{1}},\overline{l_{2}})$, con $\alpha\in\{\varepsilon\}\cup\{\mathrm{TESIS}%
\bar{k}:k\in\mathbf{N}\}$, $l_{1}$ y $l_{2}$ anteriores a $i$ y
$(\mathbf{\varphi}_{l_{1}},\mathbf{\varphi}_{l_{2}},\mathbf{\varphi}_{i})\in
DisjElim^{\tau_{1}}$

\item[(q)] $\mathbf{J}_{i}=\alpha\mathrm{REEMPLAZO}(\overline{l_{1}}%
,\overline{l_{2}})$, con $\alpha\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar
{k}:k\in\mathbf{N}\}$, $l_{1}$ y $l_{2}$ anteriores a $i$ y $(\mathbf{\varphi
}_{l_{1}},\mathbf{\varphi}_{l_{2}},\mathbf{\varphi}_{i})\in Reemp^{\tau_{1}}$

\item[(r)] $\mathbf{J}_{i}=\alpha\mathrm{TRANSITIVIDAD}(\overline{l_{1}%
},\overline{l_{2}})$, con $\alpha\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar
{k}:k\in\mathbf{N}\}$, $l_{1}$ y $l_{2}$ anteriores a $i$ y $(\mathbf{\varphi
}_{l_{1}},\mathbf{\varphi}_{l_{2}},\mathbf{\varphi}_{i})\in Trans^{\tau_{1}}$

\item[(s)] $\mathbf{J}_{i}=\alpha\mathrm{DIVISIONPORCASOS}(\overline{l_{1}%
},\overline{l_{2}},\overline{l_{3}})$, con $\alpha\in\{\varepsilon
\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$, $l_{1},l_{2}$ y $l_{3}$
anteriores a $i$ y y $(\mathbf{\varphi}_{l_{1}},\mathbf{\varphi}_{l_{2}%
},\mathbf{\varphi}_{l_{3}},\mathbf{\varphi}_{i})\in DivPorCas^{\tau_{1}}$

\item[(t)] $\mathbf{J}_{i}=\alpha\mathrm{ELECCION}(\bar{l})$, con $\alpha
\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$, $l$ anterior
a $i$ y $(\mathbf{\varphi}_{l},\mathbf{\varphi}_{i})\in Elec^{\tau_{1}}$ via
un nombre de cte $e$, el cual no pertenece a $\mathcal{C}$ y no ocurre en
$\mathbf{\varphi}_{1},...,\mathbf{\varphi}_{i-1}$.

\item[(u)] $\mathbf{J}_{i}=\alpha\mathrm{GENERALIZACION}(\bar{l})$, con
$\alpha\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$, $l$
anterior a $i$ y $(\mathbf{\varphi}_{l},\mathbf{\varphi}_{i})\in
Generaliz^{\tau_{1}}$ via un nombre de cte $c$ el cual cumple:

\begin{enumerate}
\item[(i)] $c\not \in \mathcal{C}$

\item[(ii)] $c$ no es un nombre de cte que ocurra en $\mathbf{\varphi}$ el
cual sea introducido por la aplicacion de la regla de eleccion; es decir para
cada $u\in\{1,...,n(\mathbf{\varphi})\}$, si $\mathbf{J}_{u}=\alpha
\mathrm{ELECCION}(\bar{v})$, con $\alpha\in\{\varepsilon\}\cup\{\mathrm{TESIS}%
\bar{k}:k\in\mathbf{N}\}$, entonces no se da que $(\mathbf{\varphi}%
_{v},\mathbf{\varphi}_{u})\in Elec^{\tau_{1}}$ via $c$.

\item[(iii)] $c$ no ocurre en ninguna hipotesis de $\mathbf{\varphi}_{l}$.

\item[(iv)] Ningun nombre de constante que ocurra en $\mathbf{\varphi}_{l}$ o
en sus hipotesis, depende de $c.$
\end{enumerate}
\end{enumerate}
\end{enumerate}

\bigskip

Las pruebas formales modelizan nuestras pruebas elementales y en la mayoria de
los casos la modelizacion es tan natural que el pasaje de la prueba elemental
a la formal es rutinario y obvio. Sin envargo hay ciertos casos en los que la
modelizacion es un poco mas burocratica y puede suceder que a uno no se le
ocurra como "formalizar" (i.e. ir haciendo la prueba formal) cierta parte de
una prueba elemental. Un ejemplo es cuando en una prueba elemental se va a
probar una sentencia de la forma $(\varphi\vee\psi)$. En estos casos es muy
comun que el matematico pruebe directamente que vale $(\lnot\varphi
\rightarrow\psi)$ (de lo cual es obvio que sigue $(\varphi\vee\psi)$). Este
razonamiento podriamos simularlo en nuestra prueba formal de la siguiente
manera: Primero suponemos el axioma logico $(\varphi\vee\lnot\varphi)$. Luego
probamos $(\varphi\rightarrow(\varphi\vee\psi))$ (es muy facil). Luego hacemos
la simulacion formal de la prueba de $(\lnot\varphi\rightarrow\psi)$ dada por
el matematico. Luego probamos $(\psi\rightarrow(\varphi\vee\psi))$ y por la
regla de transitividad obtenemos que $(\lnot\varphi\rightarrow(\varphi\vee
\psi))$. Finalmente aplicamos la regla de division por casos a $(\varphi
\vee\lnot\varphi)$, $(\varphi\rightarrow(\varphi\vee\psi))$ y $(\psi
\rightarrow(\varphi\vee\psi)) $ para obtener $(\varphi\vee\psi)$.

\bigskip

\bigskip

\bigskip

\bigskip

A continuacion daremos una prueba formal de $\mu=\forall x_{1}\forall
x_{2}((\forall x_{3}\ x_{3}\leq x_{1}\wedge\forall x_{3}\ x_{3}\leq
x_{2})\rightarrow x_{1}\equiv x_{2})$ en la teoria $Po$. La idea para hacerla
es ir copiando la estructura de la prueba elemental de $\mu$ dada al comienso
de la Seccion \ref{Estructuras y su lenguaje elemental asociado}. Para
facilitar la lectura la escribiremos secuencialmente%
\[%
\begin{array}
[c]{llll}%
1.\; & (\forall x_{3}\ x_{3}\leq a\wedge\forall x_{3}\ x_{3}\leq b) &  &
\text{\textrm{HIPOTESIS}}1\\
2.\; & \forall x_{3}\ x_{3}\leq a &  & \mathrm{CONJUNCIONELIMINACION}(1)\\
3.\; & b\leq a &  & \mathrm{PARTICULARIZACION}(2)\\
4.\; & \forall x_{3}\ x_{3}\leq b &  & \mathrm{CONJUNCIONELIMINACION}(1)\\
5. & a\leq b &  & \mathrm{PARTICULARIZACION}(4)\\
6. & a\leq b\wedge b\leq a &  & \text{CONJUNCIONINTRODUCCION}(5,3)\\
7. & \forall x_{1}\forall x_{2}\;((x_{1}\leq x_{2}\wedge x_{2}\leq
x_{1})\rightarrow x_{1}\equiv x_{2}) &  & \text{AXIOMAPROPIO}\\
8. & \forall x_{2}\;((a\leq x_{2}\wedge x_{2}\leq a)\rightarrow a\equiv
x_{2}) &  & \text{PARTICULARIZACION}(7)\\
9. & (a\leq b\wedge b\leq a)\rightarrow a\equiv b &  &
\text{PARTICULARIZACION}(8)\\
10. & a\equiv b &  & \text{TESIS}1\text{MODUSPONENS}(6,9)\\
11. & (\forall x_{3}\ x_{3}\leq a\wedge\forall x_{3}\ x_{3}\leq b)\rightarrow
a\equiv b &  & \text{CONCLUSION}\\
12. & \forall x_{2}((\forall x_{3}\ x_{3}\leq a\wedge\forall x_{3}\ x_{3}\leq
x_{2})\rightarrow a\equiv x_{2}) &  & \text{GENERALIZACION}(11)\\
13. & \forall x_{1}\forall x_{2}((\forall x_{3}\ x_{3}\leq x_{1}\wedge\forall
x_{3}\ x_{3}\leq x_{2})\rightarrow x_{1}\equiv x_{2}) &  &
\text{GENERALIZACION}(12)
\end{array}
\]
pero por supuesto, nuestra prueba formal es en realidad el par
$(\mathbf{\varphi},\mathbf{J})$ donde $\mathbf{\varphi}$ es la concatenacion
de la secuencia de sentencias de arriba y $\mathbf{J}$ es la concatenacion de
la secuencia de justificaciones de arriba

\bigskip

\subsubsection{El concepto de teorema}

Cuando haya una prueba formal de $\varphi$ en $(\Sigma,\tau)$, diremos que
$\varphi$ es un \textit{teorema }de la teoria $(\Sigma,\tau)$ y escribiremos
$(\Sigma,\tau)\vdash\varphi$. A continuacion se dan algunos ejemplos de
teoremas de la teoria $(\emptyset,\tau)$.

\begin{enumerate}
\item $(\varphi_{1}\vee\varphi_{2})\rightarrow(\varphi_{2}\vee\varphi_{1})$ es
un teorema de $(\emptyset,\tau)$. Una prueba formal:%
\[%
\begin{array}
[c]{llll}%
1. & (\varphi_{1}\vee\varphi_{2}) &  & \text{HIPOTESIS}1\\
2. & \varphi_{1} &  & \text{HIPOTESIS}2\\
3. & (\varphi_{2}\vee\varphi_{1}) &  & \text{TESIS}%
2\text{DISJUNCIONINTRODUCCION}(2)\\
4. & (\varphi_{1}\rightarrow(\varphi_{2}\vee\varphi_{1})) &  &
\text{CONCLUSION}\\
5. & \varphi_{2} &  & \text{HIPOTESIS}3\\
6. & (\varphi_{2}\vee\varphi_{1}) &  & \text{TESIS}%
3\text{DISJUNCIONINTRODUCCION}(5)\\
7. & \varphi_{2}\rightarrow(\varphi_{2}\vee\varphi_{1}) &  & \text{CONCLUSION}%
\\
8. & (\varphi_{2}\vee\varphi_{1}) &  & \text{TESIS}1\text{DIVISIONPORCASOS}%
(1,4,7)\\
9. & (\varphi_{1}\vee\varphi_{2})\rightarrow(\varphi_{2}\vee\varphi_{1}) &  &
\text{CONCLUSION}%
\end{array}
\]


\item $(\varphi_{1}\vee(\varphi_{2}\vee\varphi_{3}))\rightarrow((\varphi
_{1}\vee\varphi_{2})\vee\varphi_{3})$ es un teorema de $(\emptyset,\tau)$. Una
prueba formal:%
\[%
\begin{array}
[c]{llll}%
1. & (\varphi_{1}\vee(\varphi_{2}\vee\varphi_{3})) &  & \text{HIPOTESIS}1\\
2. & \varphi_{1} &  & \text{HIPOTESIS}2\\
3. & (\varphi_{1}\vee\varphi_{2}) &  & \text{DISJUNCIONINTRODUCCION}(2)\\
4. & ((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}) &  & \text{TESIS}%
2\text{DISJUNCIONINTRODUCCION}(3)\\
5. & \varphi_{1}\rightarrow((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}) &  &
\text{CONCLUSION}\\
6. & (\varphi_{2}\vee\varphi_{3}) &  & \text{HIPOTESIS}3\\
7. & \varphi_{2} &  & \text{HIPOTESIS}4\\
8. & (\varphi_{1}\vee\varphi_{2}) &  & \text{DISJUNCIONINTRODUCCION}(6)\\
9. & ((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}) &  & \text{TESIS}%
4\text{DISJUNCIONINTRODUCCION}(7)\\
10. & \varphi_{2}\rightarrow((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}) &  &
\text{CONCLUSION}\\
11. & \varphi_{3} &  & \text{HIPOTESIS}5\\
12. & ((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}) &  & \text{TESIS}%
5\text{DISJUNCIONINTRODUCCION}(11)\\
13. & \varphi_{3}\rightarrow((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}) &  &
\text{CONCLUSION}\\
14. & ((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}) &  & \text{TESIS}%
3\text{DIVISIONPORCASOS}(6,10,13)\\
15. & (\varphi_{2}\vee\varphi_{3})\rightarrow((\varphi_{1}\vee\varphi_{2}%
)\vee\varphi_{3}) &  & \text{CONCLUSION}\\
16. & ((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}) &  & \text{TESIS}%
1\text{DIVISIONPORCASOS}(1,5,15)\\
17. & (\varphi_{1}\vee(\varphi_{2}\vee\varphi_{3}))\rightarrow((\varphi
_{1}\vee\varphi_{2})\vee\varphi_{3}) &  & \text{CONCLUSION}%
\end{array}
\]
\bigskip

\item $((\varphi\wedge(\varphi\vee\psi))\leftrightarrow\varphi)$ es un teorema
de $(\emptyset,\tau)$. Una prueba formal:%
\[%
\begin{array}
[c]{llll}%
1.\; & (\varphi\wedge(\varphi\vee\psi)) &  & \text{HIPOTESIS}1\\
2.\; & \varphi &  & \text{TESIS}1\text{CONJUNCIONELIMINACION}(1)\\
3.\; & (\varphi\wedge(\varphi\vee\psi))\rightarrow\varphi &  &
\text{CONCLUSION}\\
4.\; & \varphi &  & \text{HIPOTESIS}2\\
5. & (\varphi\vee\psi) &  & \text{DISJUNCIONINTRODUCCION}(4)\\
6. & (\varphi\wedge(\varphi\vee\psi)) &  & \text{TESIS}%
2\text{CONJUNCIONINTRODUCCION}(4,5)\\
7. & \varphi\rightarrow(\varphi\wedge(\varphi\vee\psi)) &  & \text{CONCLUSION}%
\\
8. & ((\varphi\wedge(\varphi\vee\psi))\leftrightarrow\varphi) &  &
\text{EQUIVALENCIAINTRODUCCION}(3,7)
\end{array}
\]


\item $((\varphi\vee(\varphi\wedge\psi))\leftrightarrow\varphi)$ es un teorema
de $(\emptyset,\tau)$. Una prueba formal:%
\[%
\begin{array}
[c]{llll}%
1.\; & (\varphi\vee(\varphi\wedge\psi)) &  & \text{HIPOTESIS}1\\
2.\; & \varphi &  & \text{HIPOTESIS}2\\
3.\; & \varphi &  & \text{TESIS}2\text{EVOCACION}(2)\\
4.\; & \varphi\rightarrow\varphi &  & \text{CONCLUSION}\\
5. & (\varphi\wedge\psi) &  & \text{HIPOTESIS}3\\
6. & \varphi &  & \text{TESIS}3\text{CONJUNCIONELIMINACION}(5)\\
7. & (\varphi\wedge\psi)\rightarrow\varphi &  & \text{CONCLUSION}\\
8. & \varphi &  & \text{TESIS}1\text{DIVISIONPORCASOS}(1,4,7)\\
9. & (\varphi\vee(\varphi\wedge\psi))\rightarrow\varphi &  & \text{CONCLUSION}%
\\
10. & \varphi &  & \text{HIPOTESIS}4\\
11. & (\varphi\vee(\varphi\wedge\psi)) &  & \text{TESIS}%
4\text{DISJUNCIONINTRODUCCION}(10)\\
12. & \varphi\rightarrow(\varphi\vee(\varphi\wedge\psi)) &  &
\text{CONCLUSION}\\
13. & ((\varphi\vee(\varphi\wedge\psi))\leftrightarrow\varphi) &  &
\text{EQUIVALENCIAINTRODUCCION}(9,12)
\end{array}
\]


\item $(\varphi\wedge(\varphi_{1}\vee\varphi_{2}))\rightarrow((\varphi
\wedge\varphi_{1})\vee(\varphi\wedge\varphi_{2}))$ es un teorema de
$(\emptyset,\tau)$. Una prueba formal:%
\[%
\begin{array}
[c]{llll}%
1.\; & (\varphi\wedge(\varphi_{1}\vee\varphi_{2})) &  & \text{HIPOTESIS}1\\
2.\; & \varphi &  & \text{CONJUNCIONELIMINACION}(1)\\
3.\; & (\varphi_{1}\vee\varphi_{2}) &  & \text{CONJUNCIONELIMINACION}(1)\\
4.\; & \varphi_{1} &  & \text{HIPOTESIS}2\\
5. & (\varphi\wedge\varphi_{1}) &  & \text{CONJUNCIONINTRODUCCION}(2,4)\\
6. & ((\varphi\wedge\varphi_{1})\vee(\varphi\wedge\varphi_{2})) &  &
\text{TESIS}2\text{DISJUNCIONINTRODUCCION}(5)\\
7. & \varphi_{1}\rightarrow((\varphi\wedge\varphi_{1})\vee(\varphi
\wedge\varphi_{2})) &  & \text{CONCLUSION}\\
8. & \varphi_{2} &  & \text{HIPOTESIS}3\\
9. & (\varphi\wedge\varphi_{2}) &  & \text{CONJUNCIONINTRODUCCION}(2,8)\\
10. & ((\varphi\wedge\varphi_{1})\vee(\varphi\wedge\varphi_{2})) &  &
\text{TESIS}3\text{DISJUNCIONINTRODUCCION}(9)\\
11. & \varphi_{2}\rightarrow((\varphi\wedge\varphi_{1})\vee(\varphi
\wedge\varphi_{2})) &  & \text{CONCLUSION}\\
12. & ((\varphi\wedge\varphi_{1})\vee(\varphi\wedge\varphi_{2})) &  &
\text{TESIS}1\text{DIVISIONPORCASOS}(3,7,11)\\
13. & (\varphi\wedge(\varphi_{1}\vee\varphi_{2}))\rightarrow((\varphi
\wedge\varphi_{1})\vee(\varphi\wedge\varphi_{2})) &  & \text{CONCLUSION}%
\end{array}
\]


\item $((\varphi\wedge\varphi_{1})\vee(\varphi\wedge\varphi_{2}))\rightarrow
(\varphi\wedge(\varphi_{1}\vee\varphi_{2}))$ es un teorema de $(\emptyset
,\tau)$. Una prueba formal:%
\[%
\begin{array}
[c]{llll}%
1.\; & ((\varphi\wedge\varphi_{1})\vee(\varphi\wedge\varphi_{2})) &  &
\text{HIPOTESIS}1\\
2.\; & (\varphi\wedge\varphi_{1}) &  & \text{HIPOTESIS}2\\
3.\; & \varphi &  & \text{CONJUNCIONELIMINACION}(2)\\
4.\; & \varphi_{1} &  & \text{CONJUNCIONELIMINACION}(2)\\
5. & (\varphi_{1}\vee\varphi_{2}) &  & \text{DISJUNCIONINTRODUCCION}(4)\\
6. & \varphi\wedge(\varphi_{1}\vee\varphi_{2}) &  & \text{TESIS}%
2\text{CONJUNCIONINTRODUCCION}(3,5)\\
7. & (\varphi\wedge\varphi_{1})\rightarrow(\varphi\wedge(\varphi_{1}%
\vee\varphi_{2})) &  & \text{CONCLUSION}\\
8. & (\varphi\wedge\varphi_{2}) &  & \text{HIPOTESIS}3\\
9. & \varphi &  & \text{CONJUNCIONELIMINACION}(8)\\
10. & \varphi_{2} &  & \text{CONJUNCIONELIMINACION}(8)\\
11. & (\varphi_{1}\vee\varphi_{2}) &  & \text{DISJUNCIONINTRODUCCION}(10)\\
12. & \varphi\wedge(\varphi_{1}\vee\varphi_{2}) &  & \text{TESIS}%
3\text{CONJUNCIONINTRODUCCION}(9,11)\\
13. & (\varphi\wedge\varphi_{2})\rightarrow(\varphi\wedge(\varphi_{1}%
\vee\varphi_{2})) &  & \text{CONCLUSION}\\
14. & (\varphi\wedge(\varphi_{1}\vee\varphi_{2})) &  & \text{TESIS}%
1\text{DIVISIONPORCASOS}(1,7,13)\\
15. & ((\varphi\wedge\varphi_{1})\vee(\varphi\wedge\varphi_{2}))\rightarrow
(\varphi\wedge(\varphi_{1}\vee\varphi_{2})) &  & \text{CONCLUSION}%
\end{array}
\]


\item Sea $\tau$ un tipo cualquiera y sea $\varphi=_{d}\varphi(v)$ una formula
de tipo $\tau$. Ya que $(\lnot\forall v\varphi\leftrightarrow\exists
v\lnot\varphi)$ es un axioma logico, tenemos que%
\[
((\lnot\forall v\varphi\leftrightarrow\exists v\lnot\varphi
),\text{AXIOMALOGICO})
\]
es una prueba formal de $(\lnot\forall v\varphi\leftrightarrow\exists
v\lnot\varphi)$ en la teoria $(\emptyset,\tau)$. A continuacion se da una
prueba formal en la teoria $(\emptyset,\tau)$ de la sentencia $(\lnot\forall
v\varphi\leftrightarrow\exists v\lnot\varphi)$ la cual no usa el hecho de que
$(\lnot\forall v\varphi\leftrightarrow\exists v\lnot\varphi)$ sea un axioma
logico. Notar que en las primeras 10 lineas se prueba $(\lnot\exists
v\lnot\varphi\rightarrow\lnot\lnot\forall v\varphi)$, es decir el
contraresiproco de $(\lnot\forall v\varphi\rightarrow\exists v\lnot\varphi)$.
De la linea 11 hasta la 17 se prueba $(\lnot\forall v\varphi\rightarrow\exists
v\lnot\varphi)$. En las lineas restantes se prueba la implicacion reciproca de
$(\lnot\forall v\varphi\rightarrow\exists v\lnot\varphi)$, es decir $(\exists
v\lnot\varphi\rightarrow\lnot\forall v\varphi)$ y en el ultimo paso se obtiene
$(\lnot\forall v\varphi\leftrightarrow\exists v\lnot\varphi)$ por la regla de
equivalencia-introduccion. Cabe observar que esta prueba formal no es natural
u obvia, mas bien es dificil de encontrar.%
\[%
\begin{array}
[c]{clll}%
1. & \lnot\exists v\lnot\varphi &  & \text{HIPOTESIS}1\\
2. & \lnot\varphi(c) &  & \text{HIPOTESIS}2\\
3. & \exists v\lnot\varphi &  & \text{EXISTENCIAL}(2)\\
4. & (\exists v\lnot\varphi\wedge\lnot\exists v\lnot\varphi) &  &
\text{TESIS}2\text{CONJUNCIONINTRODUCCION}(3,1)\\
5. & \lnot\varphi(c)\rightarrow(\exists v\lnot\varphi\wedge\lnot\exists
v\lnot\varphi) &  & \text{CONCLUSION}\\
6. & \varphi(c) &  & \text{ABSURDO}(5)\\
7. & \forall v\varphi &  & \text{GENERALIZACION}(6)\\
8. & (\forall v\varphi\leftrightarrow\lnot\lnot\forall v\varphi) &  &
\text{AXIOMALOGICO}\\
9. & \lnot\lnot\forall v\varphi &  & \text{TESIS}1\text{REEMPLAZO}(7,8)\\
10. & (\lnot\exists v\lnot\varphi\rightarrow\lnot\lnot\forall v\varphi) &  &
\text{CONCLUSION}\\
11. & \lnot\forall v\varphi &  & \text{HIPOTESIS}3\\
12. & \lnot\exists v\lnot\varphi &  & \text{HIPOTESIS}4\\
13. & \lnot\lnot\forall v\varphi &  & \text{MODUSPONENS}(12,10)\\
14. & (\lnot\forall v\varphi\wedge\lnot\lnot\forall v\varphi) &  &
\text{TESIS}4\text{CONJUNCIONINTRODUCCION}(11,13)\\
15. & \lnot\exists v\lnot\varphi\rightarrow(\lnot\forall v\varphi\wedge
\lnot\lnot\forall v\varphi) &  & \text{CONCLUSION}\\
16. & \exists v\lnot\varphi &  & \text{TESIS}3\text{ABSURDO}(15)\\
17. & (\lnot\forall v\varphi\rightarrow\exists v\lnot\varphi) &  &
\text{CONCLUSION}\\
18. & \exists v\lnot\varphi &  & \text{HIPOTESIS}5\\
19. & \lnot\varphi(e) &  & \text{ELECCION}(18)\\
20. & \forall v\varphi &  & \text{HIPOTESIS}6\\
21. & \varphi(e) &  & \text{PARTICULARIZACION}(20)\\
22. & (\varphi(e)\wedge\lnot\varphi(e)) &  & \text{TESIS}%
6\text{CONJUNCIONINTRODUCCION}(21,19)\\
23. & \forall v\varphi\rightarrow(\varphi(e)\wedge\lnot\varphi(e)) &  &
\text{CONCLUSION}\\
24. & \lnot\forall v\varphi &  & \text{TESIS}5\text{ABSURDO}(23)\\
25. & (\exists v\lnot\varphi\rightarrow\lnot\forall v\varphi) &  &
\text{CONCLUSION}\\
26. & (\lnot\forall v\varphi\leftrightarrow\exists v\lnot\varphi) &  &
\text{EQUIVALENCIAINTRODUCCION}(17,25)
\end{array}
\]


En virtud de la prueba formal anterior se tiene que si redujeramos la lista de
axiomas logicos sacando las sentencias de la forma $(\lnot\forall
v\varphi\leftrightarrow\exists v\lnot\varphi)$, el concepto de prueba formal
resultante seria equivalente al dado. La razon por la cual se incluyen las
sentencias de la forma $(\lnot\forall v\varphi\leftrightarrow\exists
v\lnot\varphi)$ como axiomas logicos es que nuestra definicion de prueba
formal en una teoria intenta modelizar o describir en forma matematica a las
pruebas reales de los matematicos y la sentencia $(\lnot\forall v\varphi
\leftrightarrow\exists v\lnot\varphi)$ es obviamente cierta para un matematico
por lo cual seria un detalle artificioso de nuestra definicion si dicha
sentencia resultara dificil de probar con el concepto de prueba formalizado.
Dicho de otra forma, nuestro concepto de prueba formal no modelizaria en forma
natural a las pruebas matematicas reales. No sucede lo mismo con los axiomas
de la forma $(\lnot\exists v\varphi\leftrightarrow\forall v\lnot\varphi)$ los
cuales se pueden probar formalmente en forma directa (y sin usar axiomas de
las ultimas dos formas de axiomas logicos) tal como lo haria un matematico.
Sin envargo hemos elejido incluir a las sentencias de la forma $(\lnot\exists
v\varphi\leftrightarrow\forall v\lnot\varphi)$ como axiomas logicos por una
cuestion estetica y mnemotecnica ya que ambos axiomas $(\lnot\forall
v\varphi\leftrightarrow\exists v\lnot\varphi)$ y $(\lnot\exists v\varphi
\leftrightarrow\forall v\lnot\varphi)$ estan muy emparentados ya que nos dicen
como "eliminar la negacion de un cuantificador".
\end{enumerate}

$\bigskip$

\bigskip

\bigskip

\bigskip

\subsubsection{Propiedades basicas de pruebas y teoremas}

Por supuesto los numeros asociados a las hipotesis en una prueba son
completamente arbitrarios y pueden cambiarse, es decir:

\begin{lemma}
[Cambio de indice de hipotesis]\label{cambio-indice-hipotesis}Sea
$(\mathbf{\varphi},\mathbf{J})$ una prueba formal de $\varphi$ en
$(\Sigma,\tau)$. Sea $m\in\mathbf{N}$ tal que $\mathbf{J}_{i}\neq$
$\mathrm{HIPOTESIS}\bar{m}$, para cada $i=1,...,n(\mathbf{\varphi})$.
Supongamos que $\mathbf{J}_{i}=$ $\mathrm{HIPOTESIS}\bar{k}$ y que
$\mathbf{J}_{j}=$ $\mathrm{TESIS}\bar{k}\alpha$, con $[\alpha]_{1}\notin Num$.
Sea $\mathbf{\tilde{J}}$ el resultado de reemplazar en $\mathbf{J}$ la
justificacion $\mathbf{J}_{i}$ por $\mathrm{HIPOTESIS}\bar{m}$ y reemplazar la
justificacion $\mathbf{J}_{j}$ por $\mathrm{TESIS}\bar{m}\alpha$. Entonces
$(\mathbf{\varphi},\mathbf{\tilde{J}})$ es una prueba formal de $\varphi$ en
$(\Sigma,\tau)$.
\end{lemma}

\bigskip

Tambien podemos cambiar los nombres de cte auxiliares

\begin{lemma}
[Cambio de ctes auxiliares]\label{cambio-ctes}Sea $(\mathbf{\varphi
},\mathbf{J})$ una prueba formal de $\varphi$ en $(\Sigma,\tau)$. Sea
$\mathcal{C}_{1}$ el conjunto de nombres de constante que ocurren en
$\mathbf{\varphi}$ y que no pertenecen a $\mathcal{C}$. Sea $e\in
\mathcal{C}_{1}$. Sea $\tilde{e}\notin\mathcal{C}\cup\mathcal{C}_{1}$ tal que
$(\mathcal{C}\cup(\mathcal{C}_{1}-\{e\})\cup\{\tilde{e}\},\mathcal{F}%
,\mathcal{R},a)$ es un tipo. Sea $\mathbf{\tilde{\varphi}}_{i}=$ resultado de
reemplazar en $\mathbf{\varphi}_{i}$ cada ocurrencia de $e$ por $\tilde{e}.$
Entonces $(\mathbf{\tilde{\varphi}}_{1}...\mathbf{\tilde{\varphi}%
}_{n(\mathbf{\varphi})},\mathbf{J})$ es una prueba formal de $\varphi$ en
$(\Sigma,\tau)$.
\end{lemma}

\begin{proof}
Sean%
\begin{align*}
\tau_{1}  & =(\mathcal{C}\cup\mathcal{C}_{1},\mathcal{F},\mathcal{R},a)\\
\tau_{2}  & =(\mathcal{C}\cup(\mathcal{C}_{1}-\{e\})\cup\{\tilde
{e}\},\mathcal{F},\mathcal{R},a)
\end{align*}
Para cada $c\in\mathcal{C}\cup(\mathcal{C}_{1}-\{e\})$ definamos $\tilde{c}%
=c$. Notese que el mapeo $c\rightarrow\tilde{c}$ es una biyeccion entre el
conjunto de nombres de constante de $\tau_{1}$ y el conjunto de nombres de cte
de $\tau_{2}$. Para cada $t\in T^{\tau_{1}}$ sea $\tilde{t}=$ resultado de
reemplazar en $t$ cada ocurrencia de $c$ por $\tilde{c}$, para cada
$c\in\mathcal{C}\cup\mathcal{C}_{1}$. Analogamente para una formula $\psi\in
F^{\tau_{1}}$, sea $\tilde{\psi}=$ resultado de reemplazar en $\psi$ cada
ocurrencia de $c$ por $\tilde{c}$, para cada $c\in\mathcal{C}\cup
\mathcal{C}_{1}$. Notese que los mapeos $t\rightarrow\tilde{t}$ y
$\psi\rightarrow\tilde{\psi}$ son biyecciones naturales entre $T^{\tau_{1}} $
y $T^{\tau_{2}}$ y entre $F^{\tau_{1}}$ y $F^{\tau_{2}}$, respectivamente.
Notese que cualesquiera sean $\psi_{1},\psi_{2}\in F^{\tau_{1}}$, tenemos que
$\psi_{1}$ se deduce de $\psi_{2}$ por la regla de generalizacion con
constante $c$ sii $\tilde{\psi}_{1}$ se deduce de $\tilde{\psi}_{2}$ por la
regla de generalizacion con constante $\tilde{c} $. Para las otras reglas
sucede lo mismo. Notese tambien que $c$ ocurre en $\psi$ sii $\tilde{c}$
ocurre en $\tilde{\psi}.$ Mas aun notese que $c$ depende de $d$ en
$(\mathbf{\varphi},\mathbf{J})$ sii $\tilde{c}$ depende de $\tilde{d}$ en
$(\mathbf{\tilde{\varphi}},\mathbf{J})$, donde $\mathbf{\tilde{\varphi}%
}=\widetilde{\mathbf{\varphi}_{1}}...\widetilde{\mathbf{\varphi}%
_{n(\mathbf{\varphi})}}$. Ahora es facil chequear que $(\mathbf{\tilde
{\varphi}},\mathbf{J})$ es una prueba formal de $\varphi$ en $(\Sigma,\tau)$
basandose en que $(\mathbf{\varphi},\mathbf{J})$ es una prueba formal de
$\varphi$ en $(\Sigma,\tau)$.
\end{proof}

\bigskip

\begin{lemma}
[Propiedades basicas de $\vdash$]\label{prop-bas-prueb}Sea $(\Sigma,\tau)$ una teoria.

\begin{enumerate}
\item[(1)] (Uso de Teoremas) Si $(\Sigma,\tau)\vdash\varphi_{1},...,\varphi
_{n}$ y $(\Sigma\cup\{\varphi_{1},...,\varphi_{n}\},\tau)\vdash\varphi,$
entonces $(\Sigma,\tau)\vdash\varphi.$

\item[(2)] Supongamos $(\Sigma,\tau)\vdash\varphi_{1},...,\varphi_{n}$. Si $R$
es una regla distinta de GENERALIZACION y ELECCION y $\varphi$ se deduce de
$\varphi_{1},...,\varphi_{n}$ por la regla $R$, entonces $(\Sigma,\tau
)\vdash\varphi$.

\item[(3)] $(\Sigma,\tau)\vdash(\varphi\rightarrow\psi)$ si y solo si
$(\Sigma\cup\{\varphi\},\tau)\vdash\psi$.
\end{enumerate}
\end{lemma}

\begin{proof}
(1) Notese que basta con hacer el caso $n=1$. El caso con $n\geq2$ se obtiene
aplicando $n$ veces el caso $n=1$. Supongamos entonces que $(\Sigma
,\tau)\vdash\varphi_{1}$ y $(\Sigma\cup\{\varphi_{1}\},\tau)\vdash\varphi$.
Sea $(\alpha_{1}...\alpha_{h},I_{1}...I_{h})$ una prueba formal de
$\varphi_{1}$ en $(\Sigma,\tau)$. Sea $(\psi_{1}...\psi_{m},J_{1}...J_{m})$
una prueba formal de $\varphi$ en $(\Sigma\cup\{\varphi_{1}\},\tau)$. Notese
que por los Lemas \ref{cambio-indice-hipotesis} y \ref{cambio-ctes} podemos
suponer que estas dos pruebas no comparten ningun nombre de constante auxiliar
y que tampoco comparten numeros asociados a hipotesis o tesis. Para cada
$i=1,...,m$, definamos $\widetilde{J_{i}}$ de la siguiente manera.

\begin{enumerate}
\item[-] Si $J_{i}=\alpha\mathrm{AXIOMAPROPIO}$, con $\alpha\in\{\varepsilon
\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$ y $\psi_{i}=\varphi_{1}$,
entonces $\widetilde{J_{i}}=\alpha\mathrm{EVOCACION}(\overline{h})$

\item[-] Si $J_{i}=\alpha\mathrm{AXIOMAPROPIO}$, con $\alpha\in\{\varepsilon
\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$ y $\psi_{i}\notin\{\varphi
_{1}\}$, entonces $\widetilde{J_{i}}=\alpha\mathrm{AXIOMAPROPIO}$.

\item[-] Si $J_{i}=\alpha\mathrm{AXIOMALOGICO}$, con $\alpha\in\{\varepsilon
\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$, entonces $\widetilde{J_{i}%
}=\alpha\mathrm{AXIOMALOGICO}$

\item[-] Si $J_{i}=\alpha\mathrm{CONCLUSION}$, con $\alpha\in\{\varepsilon
\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$, entonces $\widetilde{J_{i}%
}=\alpha\mathrm{CONCLUSION}$.

\item[-] Si $J_{i}=\mathrm{HIPOTESIS}\bar{k}$, entonces $\widetilde{J_{i}%
}=\mathrm{HIPOTESIS}\bar{k}$

\item[-] Si $J_{i}=\alpha R(\overline{l_{1}},...,\overline{l_{k}})$, con
$\alpha\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$,
entonces $\widetilde{J_{i}}=\alpha R(\overline{l_{1}+h},...,\overline{l_{k}%
+h})$
\end{enumerate}

\noindent Es facil chequear que%
\[
(\alpha_{1}...\alpha_{h}\psi_{1}...\psi_{m},I_{1}...I_{h}\widetilde{J_{1}%
}...\widetilde{J_{m}})
\]
es una prueba formal de $\varphi$ en $(\Sigma,\tau)$

(2) Notese que%
\[%
\begin{array}
[c]{llll}%
1.\; & \varphi_{1} &  & \text{AXIOMAPROPIO}\\
2.\; & \varphi_{2} &  & \text{AXIOMAPROPIO}\\
\vdots & \vdots &  & \vdots\\
n. & \varphi_{n} &  & \text{AXIOMAPROPIO}\\
n+1. & \varphi &  & R(\bar{1},...,\bar{n})
\end{array}
\]
es una prueba formal de $\varphi$ en $(\Sigma\cup\{\varphi_{1},...,\varphi
_{n}\},\tau)$, lo cual por (1) nos dice que $(\Sigma,\tau)\vdash\varphi$.

(3) Supongamos $(\Sigma,\tau)\vdash(\varphi\rightarrow\psi)$. Entonces tenemos
que $(\Sigma\cup\{\varphi\},\tau)\vdash(\varphi\rightarrow\psi),\varphi$, lo
cual por (2) nos dice que $(\Sigma\cup\{\varphi\},\tau)\vdash\psi$. Supongamos
ahora que $(\Sigma\cup\{\varphi\},\tau)\vdash\psi$. Sea $(\varphi
_{1}...\varphi_{n},J_{1}...,J_{n})$ una prueba formal de $\psi$ en
$(\Sigma\cup\{\varphi\},\tau)$. Para cada $i=1,...,n$, definamos
$\widetilde{J_{i}}$ de la siguiente manera.

\begin{enumerate}
\item[-] Si $\varphi_{i}=\varphi$ y $J_{i}=\alpha\mathrm{AXIOMAPROPIO}$, con
$\alpha\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$,
entonces $\widetilde{J_{i}}=\alpha\mathrm{EVOCACION}(1)$

\item[-] Si $\varphi_{i}\neq\varphi$ y $J_{i}=\alpha\mathrm{AXIOMAPROPIO} $,
con $\alpha\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$,
entonces $\widetilde{J_{i}}=\alpha\mathrm{AXIOMAPROPIO}$

\item[-] Si $J_{i}=\alpha\mathrm{AXIOMALOGICO}$, con $\alpha\in\{\varepsilon
\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$, entonces $\widetilde{J_{i}%
}=\alpha\mathrm{AXIOMALOGICO}$

\item[-] Si $J_{i}=\alpha\mathrm{CONCLUSION}$, con $\alpha\in\{\varepsilon
\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$, entonces $\widetilde{J_{i}%
}=\alpha\mathrm{CONCLUSION}$

\item[-] Si $J_{i}=\mathrm{HIPOTESIS}\bar{k}$, entonces $\widetilde{J_{i}%
}=\mathrm{HIPOTESIS}\bar{k}$

\item[-] Si $J_{i}=\alpha R(\overline{l_{1}},...,\overline{l_{k}})$, con
$\alpha\in\{\varepsilon\}\cup\{\mathrm{TESIS}\bar{k}:k\in\mathbf{N}\}$,
entonces $\widetilde{J_{i}}=\alpha P(\overline{l_{1}+1},...,\overline{l_{k}%
+1})$
\end{enumerate}

\noindent Sea $m$ tal que ninguna $J_{i}$ es igual a $\mathrm{HIPOTESIS}%
\bar{m}$. Notese que $\widetilde{J_{n}}$ no es de la forma $\mathrm{TESIS}%
\bar{k}\beta$ ni de la forma $\mathrm{HIPOTESIS}\bar{k}$ (por que?) por lo
cual $\mathrm{TESIS}\bar{m}\widetilde{J_{n}}$ es una justificacion. Es facil
chequear que%
\[
(\varphi\varphi_{1}...\varphi_{n}(\varphi\rightarrow\psi),\text{HIPOTESIS}%
\bar{m}\widetilde{J_{1}}...\widetilde{J_{n-1}}\mathrm{TESIS}\bar{m}%
\widetilde{J_{n}}\text{CONCLUSION})
\]
es una prueba formal de $(\varphi\rightarrow\psi)$ en $(\Sigma,\tau)$
\end{proof}

\bigskip

\subsubsection{Consistencia}

Una teoria $(\Sigma,\tau)$ sera \textit{inconsistente} cuando haya una
sentencia $\varphi$ tal que $(\Sigma,\tau)\vdash(\varphi\wedge\lnot\varphi).$
Una teoria $(\Sigma,\tau)$ sera \textit{consistente} cuando no sea inconsistente.

\begin{lemma}
[Propiedades basicas de la consistencia]\label{prop-bas-consistencia}Sea
$(\Sigma,\tau)$ una teoria.

\begin{enumerate}
\item[(1)] Si $(\Sigma,\tau)$ es inconsistente, entonces $(\Sigma,\tau
)\vdash\varphi$, para toda sentencia $\varphi.$

\item[(2)] Si $(\Sigma,\tau)$ es consistente y $(\Sigma,\tau)\vdash\varphi$,
entonces $(\Sigma\cup\{\varphi\},\tau)$ es consistente.

\item[(3)] Si $(\Sigma,\tau)\not \vdash \lnot\varphi$, entonces $(\Sigma
\cup\{\varphi\},\tau)$ es consistente.
\end{enumerate}
\end{lemma}

\begin{proof}
(1) Si $(\Sigma,\tau)$ es inconsistente, entonces por definicion tenemos que
$(\Sigma,\tau)\vdash\psi\wedge\lnot\psi$ para alguna sentencia $\psi$. Dada
una sentencia cualquiera $\varphi$ tenemos que $\varphi$ se deduce por la
regla del absurdo a partir de $\psi\wedge\lnot\psi$ con lo cual (2) del Lema
\ref{prop-bas-prueb} nos dice que $(\Sigma,\tau)\vdash\varphi$

(2) Supongamos $(\Sigma,\tau)$ es consistente y $(\Sigma,\tau)\vdash\varphi$.
Si $(\Sigma\cup\{\varphi\},\tau)$ fuera inconsistente, entonces $(\Sigma
\cup\{\varphi\},\tau)\vdash\psi\wedge\lnot\psi$, para alguna sentencia $\psi$,
lo cual por (1) del Lema \ref{prop-bas-prueb} nos diria que $(\Sigma
,\tau)\vdash\psi\wedge\lnot\psi$, es decir nos diria que $(\Sigma,\tau)$ es inconsistente.

(3) es dejada al lector.
\end{proof}

\bigskip

\subsubsection{El teorema de correccion}

Como ya vimos en las secciones anteriores, el concepto matematico de prueba
formal en una teoria $(\Sigma,\tau)$ fue hecho como un intento de modelizar
ciertas pruebas que realizan los matematicos profecionales, a las que llamamos
pruebas elementales. Es claro que cuando un matematico hace una prueba
elemental de una setencia $\varphi$ en una teoria $(\Sigma,\tau)$, comienza
imaginando una estructura $\mathbf{A}$ de tipo $\tau$ de la cual lo unico que
sabe es que ella satisface todas las sentencias de $\Sigma$, y luego al
finalizar la prueba concluye que dicho modelo imaginario satisface la ultima
sentencia de la prueba. En algun sentido la mision de una prueba es justamente
eso: justificar con solidez que la sentencia a probar vale en todos los
modelos de la teoria.

O sea que si nuestro concepto de prueba formal permitiera probar sentencias
que no sean verdaderas en todos los modelos de la teoria, no seria correcto.
Este no es el caso y el teorema que asegura que las pruebas formales solo
prueban sentencias verdaderas en todos los modelos de la teoria se llama
Teorema de Correccion. Lo enunciaremos fomalmente a continuacion aunque no
daremos la prueba ya que es dificultosa. Antes una definicion. Dada
$(\Sigma,\tau)$ una teoria, escribiremos $(\Sigma,\tau)\models\varphi$ cuando
$\varphi$ sea verdadera en todo modelo de $(\Sigma,\tau)$.

\bigskip

\begin{theorem}
[Teorema de Correccion]$(\Sigma,\tau)\vdash\varphi$ implica $(\Sigma
,\tau)\models\varphi$.
\end{theorem}

\bigskip

Cabe destacar que el teorema de correccion hace parte de la tarea encomendada
en el punto (4) del programa de logica dado en la Seccion \ref{programa} ya
que asegura que nuestro concepto de prueba formal no es demaciado permisivo
como para permitir probar sentencias que son falsas en algun modelo de la
teoria. Pero dicho concepto podria ser incorrecto en el sentido que podria
haber pruebas elementales dadas por matematicos la cuales no puedan ser
simuladas por pruebas formales. Por ejemplo podria pasar que ma\~{n}ana un
matematico diera una prueba elemental de una sentencia $\varphi$ en una teoria
$(\Sigma,\tau)$ pero que no haya una prueba formal de $\varphi$ en
$(\Sigma,\tau)$. En tal caso nuestro modelo de prueba formal seria un modelo
erroneo del concepto de prueba elemental, por ser incompleto. Por supuesto en
ese caso podriamos mejorarlo viendo la prueba elemental dada por dicho
matematico y enrriquesiendo a la luz de dicha prueba nuestra definicion de
prueba formal. De todas maneras nos quedaria la duda de que aun esta nueva
definicion de prueba sea incompleta .... Como veremos el Teorema de
Completitud de Godel prueba que este no es el caso!

\bigskip

Un corolario muy importante del Teorema de Correccion es el siguiente.

\begin{corollary}
Si $(\Sigma,\tau)$ tiene un modelo, entonces $(\Sigma,\tau)$ es consistente.
\end{corollary}

\begin{proof}
Supongamos $\mathbf{A}$ es un modelo de $(\Sigma,\tau).$ Si $(\Sigma,\tau)$
fuera inconsistente, tendriamos que hay una $\varphi\in S^{t}$ tal que
$(\Sigma,\tau)\vdash(\varphi\wedge\lnot\varphi)$, lo cual por el Teorema de
Correccion nos diria que $\mathbf{A}\models(\varphi\wedge\lnot\varphi)$
\end{proof}

\bigskip

Concluimos la subseccion dando algunos ejemplos que muestran que si hacemos
mas permisiva la definicion de prueba formal, esta ya no resulta correcta.

\begin{enumerate}
\item[Ejemplo 1:] Este ejemplo muestra que en la sentencia a generalizar
(dentro de una prueba formal) no puede ocurrir un nombre de cte el cual
dependa del nombre de cte a generalizar. Sea $\tau=(\emptyset,\{f^{1}%
\},\emptyset,a)$ y sea $\Sigma=\{\forall y\exists x\;y\equiv f(x)\} $. Sea
$T=(\Sigma,\tau)$. Notese que una estructura $\mathbf{A}$ de tipo $\tau$ es
modelo de $T$ sii $f^{\mathbf{A}}$ es una funcion sobre. Consideremos%
\[%
\begin{array}
[c]{llll}%
\ 1. & \forall y\exists x\;y\equiv f(x) &  & \text{AXIOMAPROPIO}\\
\ 2. & \exists x\;y_{0}\equiv f(x) &  & \text{PARTICULARIZACION}(1)\\
\ 3. & y_{0}\equiv f(e) &  & \text{ELECCION}(2)\\
\ 4. & \forall y\;y\equiv f(e) &  & \text{GENERALIZACION}(3)\\
\ 5. & c\equiv f(e) &  & \text{PARTICULARIZACION}(4)\\
\ 6. & d\equiv f(e) &  & \text{PARTICULARIZACION}(4)\\
\ 7. & f(e)\equiv d &  & \text{COMMUTATIVIDAD}(6)\\
\ 8. & c\equiv d &  & \text{TRANSITIVIDAD}(5,7)\\
\ 9. & \forall y\;c\equiv y &  & \text{GENERALIZACION}(8)\\
10. & \forall x\forall y\;x\equiv y &  & \text{GENERALIZACION}(9)
\end{array}
\]
Obviamente, si permitieramos que lo anterior fuera una prueba formal, dejaria
de valer el teorema de correccion ya que hay muchos modelos de $T$, los cuales
no satisfacen $\forall x\forall y\;x\equiv y$.

\item[Ejemplo 2:] El siguiente ejemplo muestra que el nombre de cte a
generalizar no puede ocurrir en hipotesis de la sentencia a la cual se le
aplica la generalizacion. Sea $\tau=(\{1\},\emptyset,\emptyset,\emptyset) $ y
sea $T=(\emptyset,\tau)$. Consideremos%
\[%
\begin{array}
[c]{llll}%
1.\; & c\equiv1 &  & \text{HIPOTESIS}1\\
2.\; & \forall x\;x\equiv1 &  & \text{TESIS}1\text{GENERALIZACION}(1)\\
3.\; & (c\equiv1\rightarrow\forall x\;x\equiv1) &  & \text{CONCLUSION}\\
4.\; & \forall y\;\left(  y\equiv1\rightarrow\forall x\;x\equiv1\right)  &  &
\text{GENERALIZACION}(3)\\
5.\; & \left(  1\equiv1\rightarrow\forall x\;x\equiv1\right)  &  &
\text{PARTICULARIZACION}(4)\\
6. & 1\equiv1 &  & \text{AXIOMALOGICO}\\
7. & \forall x\;x\equiv1 &  & \text{MODUSPONENS}(5,6)
\end{array}
\]
Si permitieramos que lo anterior fuera una prueba formal, dejaria de valer el
teorema de correccion ya que hay muchos modelos de $T$ (toda estructura es un
modelo de $T$) los cuales no satisfacen $\forall x\;x\equiv1$.

\item[Ejemplo 3:] El siguiente ejemplo muestra que la sentencia a generalizar
no puede tener una hipotesis en las cual ocurra un nombre de cte que dependa
del nombre de cte que se generaliza. Sea $\tau=(\emptyset,\emptyset
,\emptyset,\emptyset)$ y sea $T=(\emptyset,\tau)$. Consideremos%
\[%
\begin{array}
[c]{llll}%
\ 1. & c\equiv c &  & \text{AXIOMALOGICO}\\
\ 2. & \exists z\;z\equiv c &  & \text{EXISTENCIA}(1)\\
\ 3. & e\equiv c &  & \text{ELECCION}(2)\\
\ 4. & d\equiv e &  & \text{HIPOTESIS}1\\
\ 5. & d\equiv c &  & \text{TRANSITIVIDAD}(4,3)\\
\ 6. & \forall y\;d\equiv y &  & \text{TESIS}1\text{GENERALIZACION}(5)\\
\ 7. & d\equiv e\rightarrow\forall y\;d\equiv y &  & \text{CONCLUSION}\\
\ 8. & \forall x(x\equiv e\rightarrow\forall y\;x\equiv y) &  &
\text{GENERALIZACION}(7)\\
\ 9. & e\equiv e\rightarrow\forall y\;e\equiv y &  & \text{PARTICULARIZACION}%
(8)\\
10. & e\equiv e &  & \text{AXIOMALOGICO}\\
11. & \forall y\;e\equiv y &  & \text{MODUSPONENS}(10,9)\\
12. & \forall y\;c\equiv y &  & \text{REEMPLAZO}(3,11)\\
13. & \forall x\forall y\;x\equiv y &  & \text{GENERALIZACION}(12)
\end{array}
\]

\end{enumerate}

\label{algebra.de.lindenbaum}

\subsubsection{El algebra de Lindenbaum}

Recordemos que dado un tipo $\tau$, con $S^{\tau}$ denotamos el conjunto de
las sentencias de tipo $\tau$, es decir%
\[
S^{\tau}=\{\varphi\in F^{\tau}:Li(\varphi)=\emptyset\}
\]
Sea $T=(\Sigma,\tau)$ una teoria. Podemos definir la siguiente relacion
binaria sobre $S^{\tau}$:%
\[
\varphi\dashv\vdash_{T}\psi\text{ si y solo si }T\vdash\left(  \varphi
\leftrightarrow\psi\right)
\]
Es decir%
\[
\dashv\vdash_{T}=\{(\varphi,\psi)\in S^{\tau}:T\vdash\left(  \varphi
\leftrightarrow\psi\right)  \}
\]


\begin{lemma}
$\dashv\vdash_{T}$ es una relacion de equivalencia.
\end{lemma}

\begin{proof}
La relacion es reflexiva ya que $(\varphi\leftrightarrow\varphi)$ es un axioma
logico y por lo tanto $((\varphi\leftrightarrow\varphi),$AXIOMALOGICO$)$ es
una prueba formal de $(\varphi\leftrightarrow\varphi)$ en $T$. Veamos que es
simetrica. Supongamos que $\varphi\dashv\vdash_{T}\psi$, es decir
$T\vdash\left(  \varphi\leftrightarrow\psi\right)  $. Ya que $\left(
\psi\leftrightarrow\varphi\right)  $ se deduce de $(\varphi\leftrightarrow
\psi)$ por la regla de commutatividad, (2) del Lema \ref{prop-bas-prueb} nos
dice que $T\vdash\left(  \psi\leftrightarrow\varphi\right)  $.

Analogamente, usando la regla de transitividad se puede probar que
$\dashv\vdash_{T}$ es transitiva.
\end{proof}

\bigskip

Una sentencia $\varphi$ se dice \textit{refutable en }$(\Sigma,\tau)$ si
$(\Sigma,\tau)\vdash\lnot\varphi$.

\begin{lemma}
\label{0 y 1 del algebra de lindembaum}Dada una teoria $T=(\Sigma,\tau)$, se
tiene que:

\begin{enumerate}
\item[(1)] $\{\varphi\in S^{\tau}:\varphi$ es un teorema de $T\}\in S^{\tau
}/\mathrm{\dashv\vdash}_{T}$

\item[(2)] $\{\varphi\in S^{\tau}:\varphi$ es refutable en $T\}\in S^{\tau
}/\mathrm{\dashv\vdash}_{T}$
\end{enumerate}
\end{lemma}

\begin{proof}
Haremos la prueba de (2) y dejaremos la prueba de (1) como ejercicio. Sean
$\varphi,\psi$ refutables en $T$, veremos que $\varphi\dashv\vdash_{T}\psi$.
Notese que%
\[%
\begin{array}
[c]{llll}%
1.\; & \varphi &  & \text{HIPOTESIS}1\\
2.\; & \lnot\psi &  & \text{HIPOTESIS}2\\
3.\; & \lnot\varphi &  & \text{AXIOMAPROPIO}\\
4.\; & (\varphi\wedge\lnot\varphi) &  & \text{TESIS}%
2\text{CONJUNCIONINTRODUCCION}(1,3)\\
5. & \lnot\psi\rightarrow(\varphi\wedge\lnot\varphi) &  & \text{CONCLUSION}\\
6. & \psi &  & \text{TESIS}1\text{ABSURDO}(5)\\
7. & (\varphi\rightarrow\psi) &  & \text{CONCLUSION}\\
8. & \psi &  & \text{HIPOTESIS}3\\
9. & \lnot\varphi &  & \text{HIPOTESIS}4\\
10. & \lnot\psi &  & \text{AXIOMAPROPIO}\\
11. & (\psi\wedge\lnot\psi) &  & \text{TESIS}4\text{CONJUNCIONINTRODUCCION}%
(8,10)\\
12. & \lnot\varphi\rightarrow(\psi\wedge\lnot\psi) &  & \text{CONCLUSION}\\
13. & \varphi &  & \text{TESIS}3\text{ABSURDO}(12)\\
14. & (\psi\rightarrow\varphi) &  & \text{CONCLUSION}\\
15. & (\varphi\leftrightarrow\psi) &  & \text{EQUIVALENCIAINTRODUCCION}(7,14)
\end{array}
\]
justifica que $(\Sigma\cup\{\lnot\varphi,\lnot\psi\},\tau)\vdash
(\varphi\leftrightarrow\psi)$ lo cual por (1) del Lema \ref{prop-bas-prueb}
nos dice que $(\Sigma,\tau)\vdash(\varphi\leftrightarrow\psi)$, obteniendo que
$\varphi\dashv\vdash_{T}\psi$. Para terminar de probar (2) faltaria ver que si
$\varphi$ es refutable en $T $ y $\varphi\dashv\vdash_{T}\psi$, entonces
$\psi$ es refutable en $T$. Dejamos al lector la prueba.
\end{proof}

\bigskip

Dada una teoria $T=(\Sigma,\tau)$ y $\varphi\in S^{\tau}$, $[\varphi]_{T}$
denotara la clase de $\varphi$ con respecto a la relacion de equivalencia
$\dashv\vdash_{T}$. Definiremos sobre $S^{\tau}/\mathrm{\dashv\vdash}_{T}$ las
siguiente operacion binaria $\mathsf{s}^{T}$:%
\[
\lbrack\varphi]_{T}\;\mathsf{s}^{T}\mathsf{\;}[\psi]_{T}=[(\varphi\vee
\psi)]_{T}%
\]
Una observacion importante es que para que la definicion anterior de la
operacion $\mathsf{s}^{T}$ sea inambigua, debemos probar la siguiente propiedad

\begin{enumerate}
\item[-] Si $[\varphi]_{T}=[\varphi^{\prime}]_{T}$ y $[\psi]_{T}=[\psi
^{\prime}]_{T}$ entonces $[(\varphi\vee\psi)]_{T}=[(\varphi^{\prime}\vee
\psi^{\prime})]_{T}$
\end{enumerate}

Es decir debemos probar que si $T\vdash\left(  \varphi\leftrightarrow
\varphi^{\prime}\right)  $ y $T\vdash\left(  \psi\leftrightarrow\psi^{\prime
}\right)  $, entonces $T\vdash((\varphi\vee\psi)\leftrightarrow(\varphi
^{\prime}\vee\psi^{\prime}))$. Pero esto sigue de (1) del Lema
\ref{prop-bas-prueb} ya que%
\[%
\begin{array}
[c]{llll}%
1.\; & \left(  \varphi\leftrightarrow\varphi^{\prime}\right)  &  &
\text{AXIOMAPROPIO}\\
2.\; & \left(  \psi\leftrightarrow\psi^{\prime}\right)  &  &
\text{AXIOMAPROPIO}\\
3.\; & ((\varphi\vee\psi)\leftrightarrow(\varphi\vee\psi)) &  &
\text{AXIOMALOGICO}\\
4.\; & ((\varphi\vee\psi)\leftrightarrow(\varphi^{\prime}\vee\psi)) &  &
\text{REEMPLAZO}(1,3)\\
5.\; & ((\varphi\vee\psi)\leftrightarrow(\varphi^{\prime}\vee\psi^{\prime})) &
& \text{REEMPLAZO}(2,4)
\end{array}
\]
atestigua que $(\Sigma\cup\{\left(  \varphi\leftrightarrow\varphi^{\prime
}\right)  ,\left(  \psi\leftrightarrow\psi^{\prime}\right)  \},\tau
)\vdash((\varphi\vee\psi)\leftrightarrow(\varphi^{\prime}\vee\psi^{\prime}))$.
En forma analoga se puede ver que las siguientes igualdades definen en forma
inambigua una operacion binaria $\mathsf{i}^{T}$\ sobre $S^{\tau
}/\mathrm{\dashv\vdash}_{T}$ y una operacion unaria $^{\mathsf{c}^{T}}$ sobre
$S^{\tau}/\mathrm{\dashv\vdash}_{T}$:%

\begin{align*}
\lbrack\varphi]_{T}\;\mathsf{i}^{T}\mathsf{\;}[\psi]_{T}  & =[(\varphi
\wedge\psi)]_{T}\\
([\varphi]_{T})^{\mathsf{c}^{T}}  & =[\lnot\varphi]_{T}%
\end{align*}
Dejamos al lector los detalles.

Dada una teoria $T=(\Sigma,\tau)$, denotemos con $1^{T}$ al conjunto
$\{\varphi\in S^{\tau}:\varphi$ es un teorema de $T\}$ y con $0^{T}$ al
conjunto $\{\varphi\in S^{\tau}:\varphi$ es refutable en $T\}$. Ya vimos en un
lema anterior que $0^{T}$ y $1^{T}$ pertenecen a $S^{\tau}/\mathrm{\dashv
\vdash}_{T}$. Podemos enunciar ahora el siguiente resultado, inspirado en la
idea clasica de Boole para el calculo proposicional.

\begin{theorem}
Sea $T=(\Sigma,\tau)$ una teoria. Entonces $(S^{\tau}/\mathrm{\dashv\vdash
}_{T},\mathsf{s}^{T},\mathsf{i}^{T},^{\mathsf{c}^{T}},0^{T},1^{T})$ es un
algebra de Boole.
\end{theorem}

\begin{proof}
Por definicion de algebra de Boole, debemos probar que cualesquiera sean
$\varphi_{1},\varphi_{2},\varphi_{3}\in S^{\tau}$, se cumplen las siguientes igualdades:

\begin{enumerate}
\item[(1)] $[\varphi_{1}]_{T}\;\mathsf{i}^{T}\mathsf{\;}[\varphi_{1}%
]_{T}=[\varphi_{1}]_{T}$

\item[(2)] $[\varphi_{1}]_{T}\;\mathsf{s}^{T}\mathsf{\;}[\varphi_{1}%
]_{T}=[\varphi_{1}]_{T}$

\item[(3)] $[\varphi_{1}]_{T}\;\mathsf{i}^{T}\mathsf{\;}[\varphi_{2}%
]_{T}=[\varphi_{2}]_{T}\;\mathsf{i}^{T}\mathsf{\;}[\varphi_{1}]_{T}$

\item[(4)] $[\varphi_{1}]_{T}\;\mathsf{s}^{T}\mathsf{\;}[\varphi_{2}%
]_{T}=[\varphi_{2}]_{T}\;\mathsf{s}^{T}\mathsf{\;}[\varphi_{1}]_{T}$

\item[(5)] $[\varphi_{1}]_{T}\;\mathsf{i}^{T}\mathsf{\;}([\varphi_{2}%
]_{T}\;\mathsf{i}^{T}\mathsf{\;}[\varphi_{3}]_{T})=([\varphi_{1}%
]_{T}\;\mathsf{i}^{T}\mathsf{\;}[\varphi_{2}]_{T})\;\mathsf{i}^{T}%
\mathsf{\;}[\varphi_{3}]_{T}$

\item[(6)] $[\varphi_{1}]_{T}\;\mathsf{s}^{T}\mathsf{\;}([\varphi_{2}%
]_{T}\;\mathsf{s}^{T}\mathsf{\;}[\varphi_{3}]_{T})=([\varphi_{1}%
]_{T}\;\mathsf{s}^{T}\mathsf{\;}[\varphi_{2}]_{T})\;\mathsf{s}^{T}%
\mathsf{\;}[\varphi_{3}]_{T}$

\item[(7)] $[\varphi_{1}]_{T}\;\mathsf{s}^{T}\mathsf{\;}([\varphi_{1}%
]_{T}\;\mathsf{i}^{T}\mathsf{\;}[\varphi_{2}]_{T})=[\varphi_{1}]_{T}$

\item[(8)] $[\varphi_{1}]_{T}\;\mathsf{i}^{T}\mathsf{\;}([\varphi_{1}%
]_{T}\;\mathsf{s}^{T}\mathsf{\;}[\varphi_{2}]_{T})=[\varphi_{1}]_{T}$

\item[(9)] $0^{T}\;\mathsf{s}^{T}\mathsf{\;}[\varphi_{1}]_{T}=[\varphi
_{1}]_{T}$

\item[(10)] $[\varphi_{1}]_{T}\;\mathsf{s}^{T}\mathsf{\;}1^{T}=1^{T}$

\item[(11)] $[\varphi_{1}]_{T}\;\mathsf{s}^{T}\mathsf{\;}([\varphi_{1}%
]_{T})^{\mathsf{c}^{T}}=1^{T}$

\item[(12)] $[\varphi_{1}]_{T}\;\mathsf{i}^{T}\mathsf{\;}([\varphi_{1}%
]_{T})^{\mathsf{c}^{T}}=0^{T}$

\item[(13)] $[\varphi_{1}]_{T}\;\mathsf{i}^{T}\mathsf{\;}([\varphi_{2}%
]_{T}\;\mathsf{s}^{T}\mathsf{\;}[\varphi_{3}]_{T})=([\varphi_{1}%
]_{T}\;\mathsf{i}^{T}\mathsf{\;}[\varphi_{2}]_{T})\;\mathsf{s}^{T}%
\mathsf{\;}([\varphi_{1}]_{T}\;\mathsf{i}^{T}\mathsf{\;}[\varphi_{3}]_{T})$
\end{enumerate}

Veamos por ejemplo que se da (10), es decir probaremos que $[\varphi_{1}%
]_{T}\;\mathsf{s}^{T}\mathsf{\;}1^{T}=1^{T}$, cualesquiera sea la sentencia
$\varphi_{1}$. Ya que $\forall x_{1}(x_{1}\equiv x_{1})$ es un teorema de $T$,
atestiguado por la prueba formal%
\[%
\begin{array}
[c]{llll}%
1.\; & c\equiv c &  & \text{AXIOMALOGICO}\\
2.\; & \forall x_{1}(x_{1}\equiv x_{1}) &  & \text{GENERALIZACION}(1)
\end{array}
\]
($c$ es un nombre de cte no perteneciente a $\mathcal{C}$ y tal que
$(\mathcal{C}\cup\{c\},\mathcal{F},\mathcal{R},a)$ es un tipo), tenemos que el
Lema \ref{0 y 1 del algebra de lindembaum} nos dice que $1^{T}=\{\varphi\in
S^{\tau}:\varphi$ es un teorema de $T\}=[\forall x_{1}(x_{1}\equiv x_{1}%
)]_{T}$. Es decir que para probar (10) debemos probar que para cualquier
$\varphi_{1}\in S^{\tau}$, se da que%
\[
\lbrack\varphi_{1}]_{T}\;\mathsf{s}^{T}\mathsf{\;}[\forall x_{1}(x_{1}\equiv
x_{1})]_{T}=\{\varphi\in S^{\tau}:\varphi\text{ }%
\mathrm{es\ un\ teorema\ de\ }T\}
\]
Ya que $[\varphi_{1}]_{T}\;\mathsf{s}^{T}\mathsf{\;}[\forall x_{1}(x_{1}\equiv
x_{1})]_{T}=[\varphi_{1}\vee\forall x_{1}(x_{1}\equiv x_{1})]_{T}$, debemos
probar que $\varphi_{1}\vee\forall x_{1}(x_{1}\equiv x_{1})$ es un teorema de
$T$, lo cual es atestiguado por la siguiente prueba formal%
\[%
\begin{array}
[c]{llll}%
1.\; & c\equiv c &  & \text{AXIOMALOGICO}\\
2.\; & \forall x_{1}(x_{1}\equiv x_{1}) &  & \text{GENERALIZACION}(1)\\
3. & (\varphi_{1}\vee\forall x_{1}(x_{1}\equiv x_{1})) &  &
\text{DISJUNCIONINTRODUCCION}(2)
\end{array}
\]
Veamos ahora que se da (6), es decir veamos que%
\[
\lbrack\varphi_{1}]_{T}\;\mathsf{s}^{T}\mathsf{\;}([\varphi_{2}]_{T}%
\;\mathsf{s}^{T}\mathsf{\;}[\varphi_{3}]_{T})=([\varphi_{1}]_{T}%
\;\mathsf{s}^{T}\mathsf{\;}[\varphi_{2}]_{T})\;\mathsf{s}^{T}\mathsf{\;}%
[\varphi_{3}]_{T}%
\]
cualesquiera sean $\varphi_{1},\varphi_{2},\varphi_{3}\in S^{\tau}$. Sean
$\varphi_{1},\varphi_{2},\varphi_{3}\in S^{\tau}$ fijas. Por la definicion de
la operacion $\mathsf{s}^{T}$ debemos probar que%
\[
\lbrack(\varphi_{1}\vee(\varphi_{2}\vee\varphi_{3}))]_{T}=[((\varphi_{1}%
\vee\varphi_{2})\vee\varphi_{3})]_{T}%
\]
es decir, debemos probar que%
\[
T\vdash((\varphi_{1}\vee(\varphi_{2}\vee\varphi_{3}))\leftrightarrow
((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}))
\]
Notese que por (2) del Lema \ref{prop-bas-prueb}, basta con probar que%
\begin{align*}
T  & \vdash((\varphi_{1}\vee(\varphi_{2}\vee\varphi_{3}))\rightarrow
((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}))\\
T  & \vdash(((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3})\rightarrow
(\varphi_{1}\vee(\varphi_{2}\vee\varphi_{3})))
\end{align*}
La siguiente es una prueba formal de $((\varphi_{1}\vee(\varphi_{2}\vee
\varphi_{3}))\rightarrow((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3})) $ en
$T$ y dejamos al lector la otra prueba formal.%
\[%
\begin{array}
[c]{llll}%
1. & (\varphi_{1}\vee(\varphi_{2}\vee\varphi_{3})) &  & \text{HIPOTESIS}1\\
2. & \varphi_{1} &  & \text{HIPOTESIS}2\\
3. & (\varphi_{1}\vee\varphi_{2}) &  & \text{DISJUNCIONINTRODUCCION}(2)\\
4. & ((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}) &  & \text{TESIS}%
2\text{DISJUNCIONINTRODUCCION}(3)\\
5. & \varphi_{1}\rightarrow((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}) &  &
\text{CONCLUSION}\\
6. & (\varphi_{2}\vee\varphi_{3}) &  & \text{HIPOTESIS}3\\
7. & \varphi_{2} &  & \text{HIPOTESIS}4\\
8. & (\varphi_{1}\vee\varphi_{2}) &  & \text{DISJUNCIONINTRODUCCION}(6)\\
9. & ((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}) &  & \text{TESIS}%
4\text{DISJUNCIONINTRODUCCION}(7)\\
10. & \varphi_{2}\rightarrow((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}) &  &
\text{CONCLUSION}\\
11. & \varphi_{3} &  & \text{HIPOTESIS}5\\
12. & ((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}) &  & \text{TESIS}%
5\text{DISJUNCIONINTRODUCCION}(11)\\
13. & \varphi_{3}\rightarrow((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}) &  &
\text{CONCLUSION}\\
14. & ((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}) &  & \text{TESIS}%
3\text{DIVISIONPORCASOS}(6,10,13)\\
15. & (\varphi_{2}\vee\varphi_{3})\rightarrow((\varphi_{1}\vee\varphi_{2}%
)\vee\varphi_{3}) &  & \text{CONCLUSION}\\
16. & ((\varphi_{1}\vee\varphi_{2})\vee\varphi_{3}) &  & \text{TESIS}%
1\text{DIVISIONPORCASOS}(1,5,15)\\
17. & (\varphi_{1}\vee(\varphi_{2}\vee\varphi_{3}))\rightarrow((\varphi
_{1}\vee\varphi_{2})\vee\varphi_{3}) &  & \text{CONCLUSION}%
\end{array}
\]
El resto de las propiedades pueden ser probadas en forma similar, algunas de
las pruebas formales necesarias han sido dadas en los ejemplos que siguen a la
definicion de prueba formal
\end{proof}

\bigskip

Dada una teoria $T=(\Sigma,\tau)$, denotaremos con $\mathcal{A}_{T}$ al
algebra de Boole $(S^{\tau}/\mathrm{\dashv\vdash}_{T},\mathsf{s}%
^{T},\mathsf{i}^{T},^{\mathsf{c}^{T}},0^{T},1^{T})$. El algebra $\mathcal{A}%
_{T}$ sera llamada el \textit{algebra de Lindenbaum de la teoria }$T$.
Denotaremos con $\leq^{T}$ al orden parcial asociado al algebra de Boole
$\mathcal{A}_{T}$ (es decir $[\varphi]_{T}\leq^{T}[\psi]_{T}$ si y solo si
$[\varphi]_{T}\;\mathsf{s}^{T}\mathsf{\;}[\psi]_{T}=[\psi]_{T}$). El siguiente
lema nos da una descripcion agradable de $\leq^{T}$.

\begin{lemma}
Sea $T$ una teoria. Se tiene que%
\[
\lbrack\varphi]_{T}\leq^{T}[\psi]_{T}\text{ si y solo si }T\vdash\left(
\varphi\rightarrow\psi\right)
\]

\end{lemma}

\begin{proof}
Supongamos que $[\varphi]_{T}\leq^{T}[\psi]_{T}$, es decir supongamos que
$[\varphi]_{T}\;\mathsf{s}^{T}\mathsf{\;}[\psi]_{T}=[\psi]_{T}$. Por la
definicion de $\mathsf{s}^{T}$ tenemos que $[(\varphi\vee\psi)]_{T}=[\psi
]_{T}$, es decir $T\vdash((\varphi\vee\psi)\leftrightarrow\psi)$. Es facil ver
entonces que $T\vdash\left(  \varphi\rightarrow\psi\right)  $. Reciprocamente
si $T\vdash\left(  \varphi\rightarrow\psi\right)  $, entonces facilmente
podemos probar que $T\vdash((\varphi\vee\psi)\leftrightarrow\psi)$, lo cual
nos dice que $[(\varphi\vee\psi)]_{T}=[\psi]_{T}$. Por la definicion
de$\ \mathsf{s}^{T}$ tenemos que $[\varphi]_{T}\;\mathsf{s}^{T}\mathsf{\;}%
[\psi]_{T}=[\psi]_{T}$, lo cual nos dice que $[\varphi]_{T}\leq^{T}[\psi]_{T}$
\end{proof}

\bigskip

\subsubsection{Teorema de completitud}

Hasta el momento tenemos una definicion matematica de prueba formal que
modeliza el concepto intuitivo de prueba elemental, el cual corresponde al
mundo real de los matematicos profecionales. Ahora bien, nada nos asegura que
no aparesca un matematico que realize una prueba elemental de una sentencia
$\varphi$ en una teoria $(\Sigma,\tau)$, y que no haya una prueba formal de
$\varphi$ en $(\Sigma,\tau)$. En tal caso nuestro concepto de prueba seria
incompleto (como modelo) aunque, como ya se vio, el mismo es correcto. Esto
podria pasar por ejemplo si nos hubiesemos olvidado de incluir en nuestra
definicion de prueba formal alguna regla o accion que el matematico usara para
probar dicha $\varphi$, es decir nos podria pasar que no podamos "traducir"
dicha prueba elemental a una prueba formal. Parese dificil poder asegurar o
probar que nuestro concepto de prueba formal sea completo en el sentido antes
descripto ya que el concepto de prueba elemental es empirico puesto que
depende de las acciones de la comunidad matematica profecional y ademas no
tiene una formulacion precisa. Por otra parte nada nos asegura que los
matematicos profecionales no vayan a descubrir en el futuro algun nuevo
"truco" elemental y que nuestro concepto que era completo pase a ser incompleto.

Fue un verdadero desafio cientifico (de los a\~{n}os cercanos a 1900) lidiar
con estos problemas, y el teorema de completitud de Godel resuelve todo de una
manera limpia y asombrosa. La razon es muy simple: Godel prueba que si una
sentencia $\varphi$ es verdadera en todos los modelos de $(\Sigma,\tau)$,
entonces hay una prueba formal de $\varphi$ en $(\Sigma,\tau)$. Ya que toda
prueba elemental que haga un matematico (ahora o en el futuro) siempre probara
una sentencia que es verdadera en cada modelo de $(\Sigma,\tau)$, el teorema
de Godel nos garantiza que para cada prueba elemental (de ahora y del futuro)
habra una prueba formal que pruebe la misma sentencia!

Por supuesto queda la posibilidad de que una prueba elemental dada por algun
matematico no sea traducible en forma natural a una prueba formal que pruebe
lo mismo (mas alla de que sepamos que hay una gracias a Godel). Sin envargo el
lector se ira convenciendo que esto es improbable que suceda, a medida que
vaya formalizando distintas pruebas elementales clasicas dadas por los
matematicos a lo largo de la historia.

Cabe destacar que entonces el Teorema de Correccion junto con el Teorema de
Completitud resuelven el punto (4) del programa de logica dado en la Seccion
\ref{programa}.

\bigskip

Para probar el teorema de completitud necesitaremos algunos resultados.

\begin{lemma}
\label{tipos-parecidos}Sean $\tau=(\mathcal{C},\mathcal{F},\mathcal{R},a)$ y
$\tau^{\prime}=(\mathcal{C}^{\prime},\mathcal{F}^{\prime},\mathcal{R}^{\prime
},a^{\prime})$ tipos.

\begin{enumerate}
\item[(1)] Si $\mathcal{C}\subseteq\mathcal{C}^{\prime}$, $\mathcal{F}%
\subseteq\mathcal{F}^{\prime}$, $\mathcal{R}\subseteq\mathcal{R}^{\prime} $ y
$a^{\prime}|_{\mathcal{F}\cup\mathcal{R}}=a$, entonces $(\Sigma,\tau
)\vdash\varphi$ implica $(\Sigma,\tau^{\prime})\vdash\varphi$

\item[(2)] Si $\mathcal{C}\subseteq\mathcal{C}^{\prime}$, $\mathcal{F}%
=\mathcal{F}^{\prime}$, $\mathcal{R}=\mathcal{R}^{\prime}$ y $a^{\prime}=a$,
entonces $(\Sigma,\tau^{\prime})\vdash\varphi$ implica $(\Sigma,\tau
)\vdash\varphi$, cada vez que $\Sigma\cup\{\varphi\}\subseteq S^{\tau}. $
\end{enumerate}
\end{lemma}

\begin{proof}
(1) Supongamos $(\Sigma,\tau)\vdash\varphi$. Entonces hay una prueba formal
$(\varphi_{1}...\varphi_{n},J_{1}...J_{n})$ de $\varphi$ en $(\Sigma,\tau)$.
Notese que aplicando varias veces el Lema \ref{cambio-ctes} podemos obtener
una prueba formal $(\tilde{\varphi}_{1}...\tilde{\varphi}_{n},J_{1}...J_{n})$
de $\varphi$ en $(\Sigma,\tau)$ la cual cumple que si $\mathcal{C}_{2}$ es el
conjunto de nombres de constante que ocurren en $\tilde{\varphi}_{1}%
...\tilde{\varphi}_{n}$ y que no pertenecen a $\mathcal{C} $, entonces:

\begin{enumerate}
\item[-] $\mathcal{C}_{2}\cap\mathcal{C}^{\prime}=\emptyset$

\item[-] $(\mathcal{C}^{\prime}\cup\mathcal{C}_{2},\mathcal{F}^{\prime
},\mathcal{R}^{\prime},a^{\prime})$ es un tipo
\end{enumerate}

\noindent Pero entonces $(\tilde{\varphi}_{1}...\tilde{\varphi}_{n}%
,J_{1}...J_{n})$ es una prueba formal de $\varphi$ en $(\Sigma,\tau^{\prime}%
)$, con lo cual $(\Sigma,\tau^{\prime})\vdash\varphi$

(2) Supongamos $(\Sigma,\tau^{\prime})\vdash\varphi$. Entonces hay una prueba
formal $(\mathbf{\varphi},\mathbf{J})$ de $\varphi$ en $(\Sigma,\tau^{\prime
})$. Veremos que $(\mathbf{\varphi},\mathbf{J})$ es una prueba formal de
$\varphi$ en $(\Sigma,\tau)$. Ya que $(\mathbf{\varphi},\mathbf{J})$ es una
prueba formal de $\varphi$ en $(\Sigma,\tau^{\prime}) $ hay un conjunto finito
$\mathcal{C}_{1}$, disjunto con $\mathcal{C}^{\prime}$, tal que $(\mathcal{C}%
^{\prime}\cup\mathcal{C}_{1},\mathcal{F},\mathcal{R},a)$ es un tipo y cada
$\mathbf{\varphi}_{i}$ es una sentencia de tipo $(\mathcal{C}^{\prime}%
\cup\mathcal{C}_{1},\mathcal{F},\mathcal{R},a)$. Sea $\widetilde
{\mathcal{C}_{1}}=\mathcal{C}_{1}\cup(\mathcal{C}^{\prime}-\mathcal{C})$.
Notese que $\mathcal{C}\cup\widetilde{\mathcal{C}_{1}}=\mathcal{C}^{\prime
}\cup\mathcal{C}_{1}$ por lo cual $(\mathcal{C}\cup\widetilde{\mathcal{C}_{1}%
},\mathcal{F},\mathcal{R},a)$ es un tipo y cada $\mathbf{\varphi}_{i}$ es una
sentencia de tipo $(\mathcal{C}\cup\widetilde{\mathcal{C}_{1}},\mathcal{F}%
,\mathcal{R},a)$. Esto nos dice que $(\mathbf{\varphi},\mathbf{J})$ cumple (1)
de la definicion de prueba formal en $(\Sigma,\tau)$. Todos los otros puntos
se cumplen en forma directa, exepto los puntos (4)(t) y (4)(u)(i) para los
cuales es necesario notar que $\mathcal{C}\subseteq\mathcal{C}^{\prime}$.
\end{proof}

\bigskip

\begin{lemma}
[Lema del infimo]\label{lema-del-infimo}Sea $T=(\Sigma,\tau)$ una teoria y
supongamos que $\tau$ tiene una cantidad infinita de nombres de cte que no
ocurren en las sentencias de $\Sigma$. Entonces para cada formula
$\varphi=_{d}\varphi(v) $, se tiene que en el algebra de Lindembaum
$\mathcal{A}_{T}$:%
\[
\lbrack\forall v\varphi(v)]_{T}=\inf(\{[\varphi(t)]_{T}:t\in T_{c}^{\tau}\}).
\]

\end{lemma}

\begin{proof}
Haremos primero el caso en que $v$ no ocurre acotadamente en $\varphi$.
Primero notese que $[\forall v\varphi(v)]_{T}\leq^{T}[\varphi(t)]_{T}$, para
todo termino cerrado $t$, ya que podemos dar la siguiente prueba formal:%
\[%
\begin{array}
[c]{cllll}%
1. & \forall v\varphi(v) &  &  & \text{HIPOTESIS}1\\
2. & \varphi(t) &  &  & \text{TESIS}1\text{PARTICULARIZACION}(1)\\
3. & (\forall v\varphi(v)\rightarrow\varphi(t)) &  &  & \text{CONCLUSION}%
\end{array}
\]
Supongamos ahora que $[\psi]_{T}\leq^{T}[\varphi(t)]_{T}$, para todo termino
cerrado $t.$ Por hipotesis hay un nombre de cte $c\in\mathcal{C}$ el cual no
ocurre en los elementos de $\Sigma\cup\{\psi,\varphi(v)\}$. Ya que $[\psi
]_{T}\leq^{T}[\varphi(c)]_{T}$, hay una prueba formal $(\varphi_{1}%
...\varphi_{n},J_{1}...J_{n})$ de $\left(  \psi\rightarrow\varphi(c)\right)  $
en $T$. Pero entonces es facil de chequear que la siguiente es una prueba
formal en $(\Sigma,(\mathcal{C}-\{c\},\mathcal{F},\mathcal{R},a))$ de $\left(
\psi\rightarrow\forall v\varphi(v)\right)  $:%
\[%
\begin{array}
[c]{rlcl}%
1. & \varphi_{1} &  & J_{1}\\
2. & \varphi_{2} &  & J_{2}\\
\vdots & \vdots &  & \vdots\\
n. & \varphi_{n}=\left(  \psi\rightarrow\varphi(c)\right)  &  & J_{n}\\
n+1. & \psi &  & \text{HIPOTESIS}\bar{m}\\
n+2. & \varphi(c) &  & \text{MODUSPONENS}(\overline{n+1},\bar{n})\\
n+3. & \forall v\varphi(v) &  & \text{TESIS}\bar{m}\text{GENERALIZACION}%
(\overline{n+2})\\
n+4. & \left(  \psi\rightarrow\forall v\varphi(v)\right)  &  &
\text{CONCLUSION}%
\end{array}
\]
(con $m$ elejido suficientemente grande y $\tau_{1}=\tau$ en la definicion de
prueba, es decir con $c$ como el unico nombre de cte auxiliar de la prueba).
Por el Lema \ref{tipos-parecidos} tenemos entonces que $T\vdash\left(
\psi\rightarrow\forall v\varphi(v)\right)  $. Cabe destacar que es necesaria
la asumsion de que $v$ no ocurra acotadamente en $\varphi$ para que
$(\varphi(c),\forall v\varphi(v))\in Generaliz^{\tau}$ (por que?).

Ahora supongamos el caso mas general donde $v$ puede ocurrir acotadamente en
$\varphi$. Sea $\gamma=\varphi(w)$, donde $w$ es una variable que no ocurre en
$\varphi$. Claramente $Li(\gamma)\subseteq\{w\}$. Declaremos $\gamma
=_{d}\gamma(w)$. Notese que $w$ no ocurre acotadamente en $\gamma$. Por lo ya
probado tenemos que%
\[
\lbrack\forall w\gamma(w)]_{T}=\inf(\{[\gamma(t)]_{T}:t\in T_{c}^{\tau}\})
\]
Pero $[\forall v\varphi(v)]_{T}=[\forall w\gamma(w)]_{T}$ ya que%
\[%
\begin{array}
[c]{rlcl}%
1. & \forall v\varphi(v) &  & \text{HIPOTESIS}1\\
2. & \varphi(c)\text{ (es igual a }\gamma(c)\text{)} &  &
\text{PARTICULARIZACION}(1)\\
3. & \forall w\gamma(w) &  & \text{TESIS}1\text{GENERALIZACION}(2)\\
4. & (\forall v\varphi(v)\rightarrow\forall w\gamma(w)) &  & \text{CONCLUSION}%
\end{array}
\]
y%
\[%
\begin{array}
[c]{rlcl}%
1. & \forall w\gamma(w) &  & \text{HIPOTESIS}1\\
2. & \varphi(c)\text{ (es igual a }\gamma(c)\text{)} &  &
\text{PARTICULARIZACION}(1)\\
3. &  &  & \text{TESIS}1\text{GENERALIZACION}(2)\\
4. & (\forall v\varphi(v)\rightarrow\forall w\gamma(w)) &  & \text{CONCLUSION}%
\end{array}
\]


es claro que%
\begin{align*}
\lbrack\forall v\varphi(v)]_{T}  & =[\forall w\gamma(w)]_{T}\\
\lbrack\varphi(t)]_{T}  & =[\gamma(t)]_{T}\text{ para cada }t\in T_{c}^{\tau
}\})
\end{align*}
por lo cual obtenemos que%
\[
\lbrack\forall v\varphi(v)]_{T}=\inf(\{[\varphi(t)]_{T}:t\in T_{c}^{\tau}\}).
\]

\end{proof}

\bigskip

\begin{lemma}
[Lema de Coincidencia]Sean $\tau$ y $\tau^{\prime}$ dos tipos cualesquiera y
sea $\tau_{\cap}=(\mathcal{C}_{\cap},\mathcal{F}_{\cap},\mathcal{R}_{\cap
},a_{\cap})$, donde%
\begin{align*}
\mathcal{C}_{\cap}  & =\mathcal{C}\cap\mathcal{C}^{\prime}\\
\mathcal{F}_{\cap}  & =\{f\in\mathcal{F}\cap\mathcal{F}^{\prime}%
:a(f)=a^{\prime}(f)\}\\
\mathcal{R}_{\cap}  & =\{r\in\mathcal{R}\cap\mathcal{R}^{\prime}%
:a(r)=a^{\prime}(r)\}\\
a_{\cap}  & =a|_{\mathcal{F}_{\cap}\cup\mathcal{R}_{\cap}}%
\end{align*}
Entonces $\tau_{\cap}$ es un tipo tal que $T^{\tau_{\cap}}=T^{\tau}\cap
T^{\tau^{\prime}}$ y $F^{\tau_{\cap}}=F^{\tau}\cap F^{\tau^{\prime}}$. Sean
$\mathbf{A}$ y $\mathbf{A}^{\prime}$ modelos de tipo $\tau$ y $\tau^{\prime}$
respectivamente. Supongamos que $A=A^{\prime}$ y que $c^{\mathbf{A}%
}=c^{\mathbf{A}^{\prime}}$, para cada $c\in\mathcal{C}_{\cap}$, $f^{\mathbf{A}%
}=f^{\mathbf{A}^{\prime}}$, para cada $f\in\mathcal{F}_{\cap}$ y
$r^{\mathbf{A}}=r^{\mathbf{A}^{\prime}}$, para cada $r\in\mathcal{R}_{\cap}$.

\begin{enumerate}
\item[(a)] Para cada $t=_{d}t(\vec{v})\in T^{\tau_{\cap}}$ se tiene que
$t^{\mathbf{A}}[\vec{a}]=t^{\mathbf{A}^{\prime}}[\vec{a}]$, para cada $\vec
{a}\in A^{n}$

\item[(b)] Para cada $\varphi=_{d}\varphi(\vec{v})\in F^{\tau_{\cap}}$ se
tiene que%
\[
\mathbf{A}\models\varphi\lbrack\vec{a}]\text{ si y solo si }\mathbf{A}%
^{\prime}\models\varphi\lbrack\vec{a}]\text{.}%
\]


\item[(c)] Si $\Sigma\cup\{\varphi\}\subseteq S^{\tau_{\cap}}$, entonces%
\[
(\Sigma,\tau)\models\varphi\text{ sii }(\Sigma,\tau^{\prime})\models
\varphi\text{.}%
\]

\end{enumerate}
\end{lemma}

\begin{proof}
Dejamos al lector probar que $\tau_{\cap}$ es un tipo, $T^{\tau_{\cap}%
}=T^{\tau}\cap T^{\tau^{\prime}}$ y $F^{\tau_{\cap}}=F^{\tau}\cap
F^{\tau^{\prime}}$.

(a) y (b) son directos por induccion.

(c) Supongamos que $(\Sigma,\tau)\models\varphi$. Sea $\mathbf{A}^{\prime}$ un
modelo de $\tau^{\prime}$ tal que $\mathbf{A}^{\prime}\models\Sigma$. Sea
$a\in A^{\prime}$ un elemento fijo. Sea $\mathbf{A}$ el modelo de tipo $\tau$
definido de la siguiente manera

\begin{enumerate}
\item[-] universo de $\mathbf{A}=$ $A^{\prime}$

\item[-] $c^{\mathbf{A}}=c^{\mathbf{A}^{\prime}}$, para cada $c\in
\mathcal{C}_{\cap}$,

\item[-] $f^{\mathbf{A}}=f^{\mathbf{A}^{\prime}}$, para cada $f\in
\mathcal{F}_{\cap}$

\item[-] $r^{\mathbf{A}}=r^{\mathbf{A}^{\prime}}$, para cada $r\in
\mathcal{R}_{\cap}$

\item[-] $c^{\mathbf{A}}=a$, para cada $c\in\mathcal{C}-\mathcal{C}_{\cap}$

\item[-] $f^{\mathbf{A}}(a_{1},...,a_{a(f)})=a$, para cada $f\in
\mathcal{F}-\mathcal{F}_{\cap}$, $a_{1},...,a_{a(f)}\in A^{\prime}$

\item[-] $r^{\mathbf{A}}=\emptyset$, para cada $r\in\mathcal{R-R}_{\cap}$
\end{enumerate}

\noindent Ya que $\mathbf{A}^{\prime}\models\Sigma$, (b) nos dice que
$\mathbf{A}\models\Sigma$, lo cual nos dice que $\mathbf{A}\models\varphi$.
Nuevamente por (b) tenemos que $\mathbf{A}^{\prime}\models\varphi$, con lo
cual hemos probado que $(\Sigma,\tau^{\prime})\models\varphi$
\end{proof}

\bigskip

\begin{lemma}
Sea $\tau$ un tipo. Hay una infinitupla $(\gamma_{1},\gamma_{2},...)\in
F^{\tau\mathbf{N}}$ tal que:

\begin{enumerate}
\item[(1)] $\left\vert Li(\gamma_{j})\right\vert \leq1$, para cada $j=1,2,...$

\item[(2)] Si $\left\vert Li(\gamma)\right\vert \leq1$, entonces
$\gamma=\gamma_{j}$, para algun $j\in\mathbf{N}$
\end{enumerate}
\end{lemma}

\begin{proof}
Notese que las formulas de tipo $\tau$ son palabras de algun alfabeto finito
$A$. Dado un orden total $\leq$ para $A$, podemos definir%
\begin{align*}
\gamma_{1}  & =\min\nolimits_{\alpha}^{\leq}\left(  \alpha\in F^{\tau}%
\wedge\left\vert Li(\alpha)\right\vert \leq1\right) \\
\gamma_{t+1}  & =\min\nolimits_{\alpha}^{\leq}\left(  \alpha\in F^{\tau}%
\wedge\left\vert Li(\alpha)\right\vert \leq1\wedge(\forall i\in\omega)_{i\leq
t}\alpha\neq\gamma_{i}\right)
\end{align*}
Notese que para $t\in\mathbf{N}$, tenemos que $\gamma_{t}=t$-esimo elemento de
$\{\alpha\in F^{\tau}:\left\vert Li(\alpha)\right\vert \leq1\}$, con respecto
al orden total de $A^{\ast}$ inducido por $\leq$. Claramente entonces la
infinitupla $(\gamma_{1},\gamma_{2},...)$ cumple (1) y (2).
\end{proof}

\bigskip

Ahora si, el famoso resultado de Godel.

\begin{theorem}
[Teorema de Completitud]Sea $T=(\Sigma,\tau)$ una teoria de primer orden. Si
$T\models\varphi$, entonces $T\vdash\varphi$.
\end{theorem}

\begin{proof}
Primero probaremos completitud para el caso en que $\tau$ tiene una cantidad
infinita de nombres de cte que no ocurren en las sentencias de $\Sigma$. Lo
probaremos por el absurdo, es decir supongamos que hay una sentencia
$\varphi_{0}$ tal que $T\models\varphi_{0}$ y $T\not \vdash \varphi_{0}$.
Notese que ya que $T\not \vdash \varphi_{0}$, tenemos que $[\lnot\varphi
_{0}]_{T}\not =0^{T}$. Por el lema anterior hay una infinitupla $(\gamma
_{1},\gamma_{2},...)\in F^{\tau\mathbf{N}}$ tal que:

\begin{enumerate}
\item[-] $\left\vert Li(\gamma_{j})\right\vert \leq1$, para cada $j=1,2,... $

\item[-] Si $\left\vert Li(\gamma)\right\vert \leq1$, entonces $\gamma
=\gamma_{j}$, para algun $j\in\mathbf{N}$
\end{enumerate}

\noindent Para cada $j\in\mathbf{N}$, sea $w_{j}\in Var$ tal que
$Li(\gamma_{j})\subseteq\{w_{j}\}$. Para cada $j$, declaremos $\gamma_{j}%
=_{d}\gamma_{j}(w_{j})$. Notese que por el Lema \ref{lema-del-infimo} tenemos
que $\inf\{[\gamma_{j}(t)]_{T}:t\in T_{c}^{\tau}\}=[\forall w_{j}\gamma
_{j}(w_{j})]_{T}$, para cada $j=1,2,...$. Por el Teorema de Rasiova y Sikorski
tenemos que hay un filtro primo $\mathcal{U}$ de $\mathcal{A}_{T}$, el cual cumple:

\begin{enumerate}
\item[(a)] $[\lnot\varphi_{0}]_{T}\in\mathcal{U}$

\item[(b)] Para cada $j\in\mathbf{N}$, $\{[\gamma_{j}(t)]_{T}:t\in T_{c}%
^{\tau}\}\subseteq\mathcal{U}$ implica que $[\forall w_{j}\gamma_{j}%
(w_{j})]_{T}\in\mathcal{U}$
\end{enumerate}

\noindent Ya que la infinitupla $(\gamma_{1},\gamma_{2},...)$ cubre todas las
formulas con a lo sumo una variable libre, podemos reescribir la propiedad (b)
de la siguiente manera

\begin{enumerate}
\item[(b)$^{\prime}$] Para cada $\varphi=_{d}\varphi(v)\in F^{\tau}$, si
$\{[\varphi(t)]_{T}:t\in T_{c}^{\tau}\}\subseteq\mathcal{U}$ entonces
$[\forall v\varphi(v)]_{T}\in\mathcal{U}$
\end{enumerate}

\noindent Definamos sobre $T_{c}^{\tau}$ la siguiente relacion:%
\[
t\bowtie s\text{ si y solo si }[(t\equiv s)]_{T}\in\mathcal{U}\text{.}%
\]
Veamos entonces que:

\begin{enumerate}
\item[(1)] $\bowtie$ es de equivalencia.

\item[(2)] Para cada $\varphi=_{d}\varphi(v_{1},...,v_{n})\in F^{\tau}$,
$t_{1},...,t_{n},s_{1},...,s_{n}\in T_{c}^{\tau}$, si $t_{1}\bowtie s_{1}$,
$t_{2}\bowtie s_{2}$, $...$, $t_{n}\bowtie s_{n}$, entonces $[\varphi
(t_{1},...,t_{n})]_{T}\in\mathcal{U}$ si y solo si $[\varphi(s_{1}%
,...,s_{n})]_{T}\in\mathcal{U}$.

\item[(3)] Para cada $f\in\mathcal{F}_{n}$, $t_{1},...,t_{n},s_{1}%
,...,s_{n}\in T_{c}^{\tau}$,%
\[
t_{1}\bowtie s_{1},t_{2}\bowtie s_{2},...,\;t_{n}\bowtie s_{n}\text{ implica
}f(t_{1},...,t_{n})\bowtie f(s_{1},...,s_{n}).
\]

\end{enumerate}

Probaremos (2). Notese que%
\[
T\vdash\left(  (t_{1}\equiv s_{1})\wedge(t_{2}\equiv s_{2})\wedge
...\wedge(t_{n}\equiv s_{n})\wedge\varphi(t_{1},...,t_{n})\right)
\rightarrow\varphi(s_{1},...,s_{n})
\]
lo cual nos dice que%
\[
\lbrack(t_{1}\equiv s_{1})]_{T}\;\mathsf{i}^{T}\mathsf{\;}[(t_{2}\equiv
s_{2})]_{T}\;\mathsf{i}^{T}\mathsf{\;}...\;\mathsf{i}^{T}\mathsf{\;}%
[(t_{n}\equiv s_{n})]_{T}\;\mathsf{i}^{T}\mathsf{\;}[\varphi(t_{1}%
,...,t_{n})]_{T}\leq^{T}[\varphi(s_{1},...,s_{n})]_{T}%
\]
de lo cual se desprende que%
\[
\lbrack\varphi(t_{1},...,t_{n})]_{T}\in\mathcal{U}\text{ implica }%
[\varphi(s_{1},...,s_{n})]_{T}\in\mathcal{U}%
\]
ya que $\mathcal{U}$ es un filtro. La otra implicacion es analoga.

Para probar (3) podemos tomar $\varphi=\left(  f(v_{1},...,v_{n})\equiv
f(s_{1},...,s_{n})\right)  $ y aplicar (2).

Definamos ahora un modelo $\mathbf{A}_{\mathcal{U}}$ de tipo $\tau$ de la
siguiente manera:

\begin{enumerate}
\item[-] Universo de $\mathbf{A}_{\mathcal{U}}=T_{c}^{\tau}/\mathrm{\bowtie}$

\item[-] $c^{\mathbf{A}_{\mathcal{U}}}=c/\mathrm{\bowtie}$, para cada
$c\in\mathcal{C}$.

\item[-] $f^{\mathbf{A}_{\mathcal{U}}}(t_{1}/\mathrm{\bowtie},...,t_{n}%
/\mathrm{\bowtie})=f(t_{1},...,t_{n})/\mathrm{\bowtie}$, para cada
$f\in\mathcal{F}_{n}$, $t_{1},...,t_{n}\in T_{c}^{\tau}\;$

\item[-] $r^{\mathbf{A}_{\mathcal{U}}}=\{(t_{1}/\mathrm{\bowtie}%
,...,t_{n}/\mathrm{\bowtie}):[r(t_{1},...,t_{n})]_{T}\in\mathcal{U}\}$, para
cada $r\in\mathcal{R}_{n}$.
\end{enumerate}

Notese que la definicion de $f^{\mathbf{A}_{\mathcal{U}}}$ es inambigua por
(3). Probaremos las siguientes propiedades basicas:

\begin{enumerate}
\item[(4)] Para cada $t=_{d}t(v_{1},...,v_{n})\in T^{\tau}$, $t_{1}%
,...,t_{n}\in T_{c}^{\tau}$, tenemos que%
\[
t^{\mathbf{A}_{\mathcal{U}}}[t_{1}/\mathrm{\bowtie},...,t_{n}/\mathrm{\bowtie
}]=t(t_{1},...,t_{n})/\mathrm{\bowtie}%
\]


\item[(5)] Para cada $\varphi=_{d}\varphi(v_{1},...,v_{n})\in F^{\tau}$,
$t_{1},...,t_{n}\in T_{c}^{\tau}$, tenemos que%
\[
\mathbf{A}_{\mathcal{U}}\models\varphi\lbrack t_{1}/\mathrm{\bowtie}%
,...,t_{n}/\mathrm{\bowtie}]\text{ si y solo si }[\varphi(t_{1},...,t_{n}%
)]_{T}\in\mathcal{U}.
\]

\end{enumerate}

La prueba de (4) es directa por induccion. Probaremos (5) por induccion en el
$k$ tal que $\varphi\in F_{k}^{\tau}$. El caso $k=0$ es dejado al lector.
Supongamos (5) vale para $\varphi\in F_{k}^{\tau}$. Sea $\varphi=_{d}%
\varphi(v_{1},...,v_{n})\in F_{k+1}^{\tau}-F_{k}^{\tau}$. Hay varios casos:

CASO $\varphi=\left(  \varphi_{1}\vee\varphi_{2}\right)  $.

Notese que por la Convencion Notacional 6, tenemos que $\varphi_{i}%
=_{d}\varphi_{i}(v_{1},...,v_{n})$. Tenemos entonces%
\[%
\begin{array}
[c]{c}%
\mathbf{A}_{\mathcal{U}}\models\varphi\lbrack t_{1}/\mathrm{\bowtie}%
,...,t_{n}/\mathrm{\bowtie}]\\
\Updownarrow\\
\mathbf{A}_{\mathcal{U}}\models\varphi_{1}[t_{1}/\mathrm{\bowtie}%
,...,t_{n}/\mathrm{\bowtie}]\text{ o }\mathbf{A}_{\mathcal{U}}\models
\varphi_{2}[t_{1}/\mathrm{\bowtie},...,t_{n}/\mathrm{\bowtie}]\\
\Updownarrow\\
\lbrack\varphi_{1}(t_{1},...,t_{n})]_{T}\in\mathcal{U}\text{ o }[\varphi
_{2}(t_{1},...,t_{n})]_{T}\in\mathcal{U}\\
\Updownarrow\\
\lbrack\varphi_{1}(t_{1},...,t_{n})]_{T}\ \mathsf{s}^{T}\mathsf{\ }%
[\varphi_{2}(t_{1},...,t_{n})]_{T}\in\mathcal{U}\\
\Updownarrow\\
\lbrack\left(  \varphi_{1}(t_{1},...,t_{n})\vee\varphi_{2}(t_{1}%
,...,t_{n})\right)  ]_{T}\in\mathcal{U}\\
\Updownarrow\\
\lbrack\varphi(t_{1},...,t_{n})]_{T}\in\mathcal{U}.
\end{array}
\]
CASO $\varphi=\forall v\varphi_{1}$, con $v\in Var-\{v_{1},...,v_{n}\}$.
Notese que por la Convencion Notacional 6, tenemos que $\varphi_{1}%
=_{d}\varphi_{1}(v_{1},...,v_{n},v)$. Tenemos entonces%
\[%
\begin{array}
[c]{c}%
\mathbf{A}_{\mathcal{U}}\models\varphi\lbrack t_{1}/\mathrm{\bowtie}%
,...,t_{n}/\mathrm{\bowtie}]\\
\Updownarrow\\
\mathbf{A}_{\mathcal{U}}\models\varphi_{1}[t_{1}/\mathrm{\bowtie}%
,...,t_{n}/\mathrm{\bowtie},t/\mathrm{\bowtie}]\text{, para todo }t\in
T_{c}^{\tau}\\
\Updownarrow\\
\lbrack\varphi_{1}(t_{1},...,t_{n},t)]_{T}\in\mathcal{U}\text{, para todo
}t\in T_{c}^{\tau}\\
\Updownarrow\\
\lbrack\forall v\varphi_{1}(t_{1},...,t_{n},v)]_{T}\in\mathcal{U}\\
\Updownarrow\\
\lbrack\varphi(t_{1},...,t_{n})]_{T}\in\mathcal{U}.
\end{array}
\]
CASO $\varphi=\exists v\varphi_{1}$, con $v\in Var-\{v_{1},...,v_{n}\}$.
Notese que por la Convencion Notacional 6, tenemos que $\varphi_{1}%
=_{d}\varphi_{1}(v_{1},...,v_{n},v)$. Tenemos entonces%
\[%
\begin{array}
[c]{c}%
\mathbf{A}_{\mathcal{U}}\models\varphi\lbrack t_{1}/\mathrm{\bowtie}%
,...,t_{n}/\mathrm{\bowtie}]\\
\Updownarrow\\
\mathbf{A}_{\mathcal{U}}\models\varphi_{1}[t_{1}/\mathrm{\bowtie}%
,...,t_{n}/\mathrm{\bowtie},t/\mathrm{\bowtie}]\text{, para algun }t\in
T_{c}^{\tau}\\
\Updownarrow\\
\lbrack\varphi_{1}(t_{1},...,t_{n},t)]_{T}\in\mathcal{U}\text{, para algun
}t\in T_{c}^{\tau}\\
\Updownarrow\\
([\varphi_{1}(t_{1},...,t_{n},t)]_{T})^{\mathsf{c}^{T}}\not \in
\mathcal{U}\text{, para algun }t\in T_{c}^{\tau}\\
\Updownarrow\\
\lbrack\lnot\varphi_{1}(t_{1},...,t_{n},t)]_{T}\not \in \mathcal{U}\text{,
para algun }t\in T_{c}^{\tau}\\
\Updownarrow\\
\lbrack\forall v\;\lnot\varphi_{1}(t_{1},...,t_{n},v)]_{T}\not \in
\mathcal{U}\\
\Updownarrow\\
([\forall v\;\lnot\varphi_{1}(t_{1},...,t_{n},v)]_{T})^{\mathsf{c}^{T}}%
\in\mathcal{U}\\
\Updownarrow\\
\lbrack\lnot\forall v\;\lnot\varphi_{1}(t_{1},...,t_{n},v)]_{T}\in
\mathcal{U}\\
\Updownarrow\\
\lbrack\varphi(t_{1},...,t_{n})]_{T}\in\mathcal{U}.
\end{array}
\]
Pero ahora notese que (5) en particular nos dice que para cada sentencia
$\psi\in S^{\tau}$, $\mathbf{A}_{\mathcal{U}}\models\psi$ si y solo si
$[\psi]_{T}\in\mathcal{U}.$ De esta forma llegamos a que $\mathbf{A}%
_{\mathcal{U}}\models\Sigma$ y $\mathbf{A}_{\mathcal{U}}\models\lnot
\varphi_{0}$, lo cual contradice la suposicion de que $T\models\varphi_{0}.$

Ahora supongamos que $\tau$ es cualquier tipo. Sean $s_{1}$ y $s_{2}$ un par
de simbolos no pertenecientes a la lista%
\[
\forall\ \ \exists\ \ \lnot\ \ \vee\ \ \wedge\ \ \rightarrow
\ \ \leftrightarrow\ \ (\ \ )\ \ ,\ \equiv\ \ \mathsf{X}\ \ \mathit{0}%
\ \ \mathit{1}\ \ ...\ \ \mathit{9}\ \ \mathbf{0}\ \ \mathbf{1}%
\ \ ...\ \ \mathbf{9}%
\]
y tales que ninguno ocurra en alguna palabra de $\mathcal{C}\cup
\mathcal{F}\cup\mathcal{R}.$ Si $T\models\varphi$, entonces usando el Lema de
Coincidencia se puede ver que $(\Sigma,(\mathcal{C}\cup\{s_{1}s_{2}s_{1}%
,s_{1}s_{2}s_{2}s_{1},...\},\mathcal{F},\mathcal{R},a))\models\varphi$, por lo
cual%
\[
(\Sigma,(\mathcal{C}\cup\{s_{1}s_{2}s_{1},s_{1}s_{2}s_{2}s_{1}%
,...\},\mathcal{F},\mathcal{R},a))\vdash\varphi.
\]
Pero por Lema \ref{tipos-parecidos}, tenemos que $T\vdash\varphi.$
\end{proof}

\begin{corollary}
Toda teoria consistente tiene un modelo.
\end{corollary}

\begin{proof}
Supongamos $(\Sigma,\tau)$ es consistente y no tiene modelos. Entonces
$(\Sigma,\tau)\models\left(  \varphi\wedge\lnot\varphi\right)  $, con lo cual
por completitud $(\Sigma,\tau)\vdash\left(  \varphi\wedge\lnot\varphi\right)
$, lo cual es absurdo.
\end{proof}

\label{wiki-teorema.de.compacidad}

\begin{corollary}
[Teorema de Compacidad]Sea $(\Sigma,\tau)$ una teoria.

\begin{enumerate}
\item[(a)] Si $(\Sigma,\tau)$ es tal que $(\Sigma_{0},\tau)$ tiene un modelo,
para cada subconjunto finito $\Sigma_{0}\subseteq\Sigma$, entonces
$(\Sigma,\tau)$ tiene un modelo

\item[(b)] Si $(\Sigma,\tau)\models\varphi$, entonces hay un subconjunto
finito $\Sigma_{0}\subseteq\Sigma$ tal que $(\Sigma_{0},\tau)\models\varphi$.
\end{enumerate}
\end{corollary}

\begin{proof}
(a) Veamos que $(\Sigma,\tau)$ es consistente. Supongamos lo contrario, es
decir supongamos $(\Sigma,\tau)\vdash\left(  \varphi\wedge\lnot\varphi\right)
$, para alguna sentencia $\varphi$. Notese que entonces hay un subconjunto
finito $\Sigma_{0}\subseteq\Sigma$ tal que la teoria $(\Sigma_{0},\tau
)\vdash\left(  \varphi\wedge\lnot\varphi\right)  $ ($\Sigma_{0} $ puede ser
formado con los axiomas de $\Sigma$ usados en una prueba formal que atestigue
que $(\Sigma,\tau)\vdash\left(  \varphi\wedge\lnot\varphi\right)  $). Pero
esto es absurdo ya que por hypotesis dicha teoria $(\Sigma_{0},\tau)$ tiene un
modelo. O sea que $(\Sigma,\tau)$ es consistente por lo cual tiene un modelo.

(b) Si $(\Sigma,\tau)\models\varphi$, entonces por completitud, $(\Sigma
,\tau)\vdash\varphi$. Pero entonces hay un subconjunto finito $\Sigma
_{0}\subseteq\Sigma$ tal que $(\Sigma_{0},\tau)\vdash\varphi$, es decir tal
que $(\Sigma_{0},\tau)\models\varphi$ (correccion).
\end{proof}

\bigskip

\paragraph{Interpretacion semantica del algebra de Lindembaum}

Usando lo teoremas de correccion y completitud podemos dar una representacion
semantica del algebra de Lindembaum. Sea $T=(\Sigma,\tau)$ una teoria. Dada
$\varphi\in S^{\tau}$ definamos%
\[
\mathrm{Mod}_{T}(\varphi)=\{\mathbf{A}:\mathbf{A}\text{ es modelo de }T\text{
y }\mathbf{A}\vDash\varphi\}
\]
Definamos tambien%
\[
\mathrm{Mod}_{T}=\{\mathbf{A}:\mathbf{A}\text{ es modelo de }T\}
\]
Dado $S\subseteq\mathrm{Mod}_{T}$ definamos%
\[
S^{c}=\mathrm{Mod}_{T}-S
\]


\begin{lemma}
\label{bolcitas}$\{\mathrm{Mod}_{T}(\varphi):\varphi\in S^{\tau}\}$ es un
subuniverso del algebra de Boole $(\mathcal{P}(\mathrm{Mod}_{T}),\cup
,\cap,^{c},\emptyset,\mathrm{Mod}_{T})$
\end{lemma}

\begin{proof}
Notese que%
\begin{align*}
\emptyset & =\mathrm{Mod}_{T}(\exists x_{1}\lnot(x_{1}\equiv x_{1}))\\
\mathrm{Mod}_{T}  & =\mathrm{Mod}_{T}(\forall x_{1}(x_{1}\equiv x_{1}))\\
\mathrm{Mod}_{T}(\varphi)\cap\mathrm{Mod}_{T}(\psi)  & =\mathrm{Mod}%
_{T}((\varphi\wedge\psi))\\
\mathrm{Mod}_{T}(\varphi)\cup\mathrm{Mod}_{T}(\psi)  & =\mathrm{Mod}%
_{T}((\varphi\vee\psi))\\
\mathrm{Mod}_{T}(\varphi)^{c}  & =\mathrm{Mod}_{T}(\lnot\varphi)
\end{align*}

\end{proof}

\bigskip

\begin{lemma}
Dadas $\varphi,\psi\in S^{\tau}$ se tiene:

\begin{enumerate}
\item[(1)] $[\varphi]_{T}\leq^{T}[\psi]_{T}$ sii $\mathrm{Mod}_{T}%
(\varphi)\subseteq\mathrm{Mod}_{T}(\psi)$

\item[(2)] $[\varphi]_{T}=[\psi]_{T}$ sii $\mathrm{Mod}_{T}(\varphi
)=\mathrm{Mod}_{T}(\psi)$

\item[(3)] $[\varphi]_{T}<^{T}[\psi]_{T}$ sii $\mathrm{Mod}_{T}(\varphi
)\subsetneqq\mathrm{Mod}_{T}(\psi)$
\end{enumerate}
\end{lemma}

\begin{proof}
(1) Dejamos al lector justificar las siguientes equivalencias:%
\[
\lbrack\varphi]_{T}\leq^{T}[\psi]_{T}\text{ sii }T\vdash(\varphi
\rightarrow\psi)\text{ sii }T\models(\varphi\rightarrow\psi)\text{ sii
}\mathrm{Mod}_{T}(\varphi)\subseteq\mathrm{Mod}_{T}(\psi)
\]
(2) y (3) siguen de (1)
\end{proof}

\bigskip

Ya que $\{\mathrm{Mod}_{T}(\varphi):\varphi\in S^{\tau}\}$ es un subuniverso
de $(\mathcal{P}(\mathrm{Mod}_{T}),\cup,\cap,^{c},\emptyset,\mathrm{Mod}_{T}%
)$, tenemos que $(\{\mathrm{Mod}_{T}(\varphi):\varphi\in S^{\tau}\},\cup
,\cap,^{c},\emptyset,\mathrm{Mod}_{T})$ es un algebra de Boole. Notese que (2)
del lema anterior nos asegura que%
\[%
\begin{array}
[c]{rcl}%
S^{\tau}/\mathrm{\dashv\vdash}_{T} & \rightarrow & \{\mathrm{Mod}_{T}%
(\varphi):\varphi\in S^{\tau}\}\\
\lbrack\varphi]_{T} & \rightarrow & \mathrm{Mod}_{T}(\varphi)
\end{array}
\]
define en forma inhambigua una funcion. Tenemos entonces el siguiente

\begin{theorem}
La funcion%
\[%
\begin{array}
[c]{rcl}%
S^{\tau}/\mathrm{\dashv\vdash}_{T} & \rightarrow & \{\mathrm{Mod}_{T}%
(\varphi):\varphi\in S^{\tau}\}\\
\lbrack\varphi]_{T} & \rightarrow & \mathrm{Mod}_{T}(\varphi)
\end{array}
\]
es un isomorfismo de $\mathcal{A}_{T}$ en $(\{\mathrm{Mod}_{T}(\varphi
):\varphi\in S^{\tau}\},\cup,\cap,^{c},\emptyset,\mathrm{Mod}_{T})$.
\end{theorem}

\begin{proof}
Llamemosle $f$ a la funcion del enunciado. Es claro que $f$ es sobre. Ademas
(2) del lema anterior nos dice que $f$ es inyectiva. Ademas usando las
igualdades de la prueba del Lema \ref{bolcitas}, facilmente podemos ver que
$f$ es un homomorfismo por lo cual es un isomorfismo.
\end{proof}

\bigskip

\subsubsection{Teorias completas}

Una teoria $(\Sigma,\tau)$ sera llamada \textit{completa} cuando para cada
$\varphi\in S^{\tau}$ se de que $(\Sigma,\tau)\vdash\varphi$ o $(\Sigma
,\tau)\vdash\lnot\varphi$. Es poco frecuente que una teoria consistente sea
completa y esto lo veremos claro despues del siguiente resultado.

\begin{proposition}
Sea $(\Sigma,\tau)$ una teoria consistente. Son equivalentes

\begin{enumerate}
\item[(1)] $(\Sigma,\tau)$ es completa

\item[(2)] Todos los modelos de $(\Sigma,\tau)$ son elementalmente equivalentes

\item[(3)] Hay una estructura $\mathbf{A}$ tal que los teoremas de
$(\Sigma,\tau)$ son exactamente las sentencias verdaderas en $\mathbf{A}$

\item[(4)] $\mathcal{A}_{(\Sigma,\tau)}$ tiene exactamente dos elementos
\end{enumerate}
\end{proposition}

\begin{proof}
Supongamos vale (1). Probaremos (2). Supongamos $\mathbf{A},\mathbf{B}$ son
modelos de $(\Sigma,\tau)$ y supongamos $\mathbf{A}\models\varphi$. Entonces
no puede darse $(\Sigma,\tau)\vdash\lnot\varphi$ (use correccion) por lo cual
tenemos que $(\Sigma,\tau)\vdash\varphi$. Ya que $\mathbf{B}$ es modelo de
$(\Sigma,\tau)$ tenemos entonces que $\mathbf{B}\models\varphi$. Esto prueba
que $\mathbf{A}$ y $\mathbf{B}$ son elementalmente equivalentes.

Ahora supongamos vale (2). Probaremos (3). Ya que $(\Sigma,\tau)$ es
consistente, tiene al menos un modelo. Sea $\mathbf{A}$ uno de ellos. Por
correccion todo teorema es verdadero en $\mathbf{A}$. Reciprocamente si
$\mathbf{A}\models\varphi$, entonces, por (2), todo modelo de $(\Sigma,\tau)$
satisface $\varphi$, lo cual nos dice que $\varphi$ es un teorema.

Obviamente (3) impica que toda sentencia es o un teorema o refutable y esto
nos dice que $0^{(\Sigma,\tau)}\cup1^{(\Sigma,\tau)}=S^{\tau}$. Ya que
$0^{(\Sigma,\tau)}\cap1^{(\Sigma,\tau)}=\emptyset$ puesto que $(\Sigma,\tau)$
es consistente, tenemos que vale (4).

Es trivial que (4) implica (1).
\end{proof}

\bigskip

Ya que lo mas comun es que una teoria tenga un par de modelos no
elementalmente equivalentes, la mayoria de las teorias no son completas.

\bigskip

\subsubsection{La aritmetica de Peano}

En esta seccion desarrollaremos las propiedades basicas de $Arit$, una teoria
de primer orden la cual modeliza a la aritmetica. Esta teoria ha sido
paradigmatica en el desarrollo de la logica.

Sea $\tau_{A}=(\{0,1\},\{+^{2},.^{2}\},\{\leq^{2}\},a)$. Denotemos con
$\mathbf{\omega}$ a la estructura de tipo $\tau_{A}$ que tiene a $\omega$ como
universo e interpreta los nombres de $\tau_{A}$ en la manera usual, es decir%
\[%
\begin{array}
[c]{l}%
0^{\mathbf{\omega}}=0\\
1^{\mathbf{\omega}}=1\\
\leq^{\mathbf{\omega}}=\{(n,m)\in\omega^{2}:n\leq m\}\\
+^{\mathbf{\omega}}(n,m)=n+m\text{, para cada }n,m\in\omega\\
.^{\mathbf{\omega}}(n,m)=n.m\text{, para cada }n,m\in\omega
\end{array}
\]
Sea $\Sigma$ el conjunto formado por las siguientes sentencias:

\begin{enumerate}
\item $\forall x_{1}\forall x_{2}\forall x_{3}\;x_{1}+(x_{2}+x_{3}%
)\equiv(x_{1}+x_{2})+x_{3}$

\item $\forall x_{1}\forall x_{2}\;x_{1}+x_{2}\equiv x_{2}+x_{1}$

\item $\forall x_{1}\forall x_{2}\forall x_{3}\;x_{1}.(x_{2}.x_{3}%
)\equiv(x_{1}.x_{2}).x_{3}$

\item $\forall x_{1}\forall x_{2}\;x_{1}.x_{2}\equiv x_{2}.x_{1}$

\item $\forall x_{1}\;x_{1}+0\equiv x_{1}$

\item $\forall x_{1}\;x_{1}.0\equiv0$

\item $\forall x_{1}\;x_{1}.1\equiv x_{1}$

\item $\forall x_{1}\forall x_{2}\forall x_{3}\;x_{1}.(x_{2}+x_{3}%
)\equiv(x_{1}.x_{2})+(x_{1}.x_{3})$

\item $\forall x_{1}\forall x_{2}\forall x_{3}\;(x_{1}+x_{3}\equiv x_{2}%
+x_{3}\rightarrow x_{1}\equiv x_{2})$

\item $\forall x_{1}\;x_{1}\leq x_{1}$

\item $\forall x_{1}\forall x_{2}\forall x_{3}\;((x_{1}\leq x_{2}\wedge
x_{2}\leq x_{3})\rightarrow x_{1}\leq x_{3})$

\item $\forall x_{1}\forall x_{2}\;((x_{1}\leq x_{2}\wedge x_{2}\leq
x_{1})\rightarrow x_{1}\equiv x_{2})$

\item $\forall x_{1}\forall x_{2}\;(x_{1}\leq x_{2}\vee x_{2}\leq x_{1})$

\item $\forall x_{1}\forall x_{2}\;(x_{1}\leq x_{2}\leftrightarrow\exists
x_{3}\;x_{2}\equiv x_{1}+x_{3})$

\item $0<1$
\end{enumerate}

Es facil ver que todas estas sentencias son satisfechas por $\mathbf{\omega} $
por lo cual $\mathbf{\omega}$ es un modelo de la teoria $(\Sigma,\tau_{A})$.
Definamos%
\[
Verd_{\mathbf{\omega}}=\{\varphi\in S^{\tau_{A}}:\mathbf{\omega}\models
\varphi\}
\]
Es claro que todo teorema de $(\Sigma,\tau_{A})$ pertenece a
$Verd_{\mathbf{\omega}}$ (por que?). Un pregunta interesante es si toda
sentencia $\varphi\in Verd_{\mathbf{\omega}}$ es un teorema de $(\Sigma
,\tau_{A})$, es decir puede ser probada en forma elemental partiendo de los
axiomas de $\Sigma$. La respuesta es no y lo explicaremos a continuacion. Sea
$\mathbf{Q}^{\geq0}$ la estructura de tipo $\tau_{A}$ que tiene a
$\{r\in\mathbf{Q}:r\geq0\}$ como universo e interpreta los nombres de
$\tau_{A}$ en la manera usual. Note que $\mathbf{Q}^{\geq0}$ tambien es un
modelo de $(\Sigma,\tau_{A})$. Pero entonces todo teorema de $(\Sigma,\tau
_{A})$ debe ser verdadero en $\mathbf{Q}^{\geq0}$. Pero la sentencia $\forall
x\ (x\leq1\rightarrow(x\equiv0\vee x\equiv1))$ es falsa en $\mathbf{Q}^{\geq
0}$ por lo cual no es un teorema de $(\Sigma,\tau_{A})$ y sin envargo
pertenece a $Verd_{\mathbf{\omega}}$. Es decir los axiomas de $\Sigma$ son
demaciado generales y deberiamos agregarle axiomas que sean mas
caracteristicos de la estructura particular de $\mathbf{\omega}$. En esa
direccion, a continuacion extenderemos el conjunto $\Sigma$ con axiomas que
nos permitiran hacer pruebas por induccion tal como se lo hace en la
aritmetica basica.

Dada una formula $\psi\in F^{\tau_{A}}$ y variables $v_{1},...,v_{n+1}$, con
$n\geq0$, tales que $Li(\psi)\subseteq\{v_{1},...,v_{n+1}\}$ y $v_{i}\neq
v_{j}$ siempre que $i\neq j$, denotaremos con $Ind_{\psi,v_{1},...,v_{n+1}}$ a
la siguiente sentencia de tipo $\tau_{A}$%
\[
\forall v_{1}...\forall v_{n}\ ((\psi(\vec{v},0)\wedge\forall v_{n+1}%
\ (\psi(\vec{v},v_{n+1})\rightarrow\psi(\vec{v},+(v_{n+1},1))))\rightarrow
\forall v_{n+1}\ \psi(\vec{v},v_{n+1}))
\]
donde suponemos que hemos declarado $\psi=_{d}\psi(v_{1},...,v_{n+1})$. Notese
que si por ejemplo $Li(\psi)\subseteq\{x_{1},x_{2},x_{3}\}$, entonces las seis sentencias%

\[
Ind_{\psi,x_{1},x_{2},x_{3}}\ \ Ind_{\psi,x_{1},x_{3},x_{2}}\ \ Ind_{\psi
,x_{2},x_{1},x_{3}}\ \ Ind_{\psi,x_{2},x_{3},x_{1}}\ \ Ind_{\psi,x_{3}%
,x_{1},x_{2}}\ \ Ind_{\psi,x_{3},x_{2},x_{1}}%
\]
son todas distintas.

Sea $\Sigma_{A}$ el conjunto que resulta de agregarle a $\Sigma$ todas las
sentencias de la forma $Ind_{\psi,v_{1},...,v_{n+1}}$. Notese que el conjunto
$\Sigma_{A}$ es infinito.

La teoria $(\Sigma_{A},\tau_{A})$ sera llamada \textit{Aritmetica de Peano} y
la denotaremos con $Arit$. Es intuitivamente claro que

\begin{lemma}
$\mathbf{\omega}$ es un modelo de $Arit$
\end{lemma}

\begin{proof}
Sean $\psi\in F^{\tau_{A}}$ y $v_{1},...,v_{n+1}$, con $n\geq0$, tales que
$Li(\psi)\subseteq\{v_{1},...,v_{n+1}\}$ y $v_{i}\neq v_{j}$ siempre que
$i\neq j$. Veremos que $\mathbf{\omega}\vDash Ind_{\psi,v_{1},...,v_{n+1}}$.
Declaremos $\psi=_{d}\psi(v_{1},...,v_{n},v_{n+1})$. Sea%
\[
\varphi=((\psi(\vec{v},0)\wedge\forall v_{n+1}\ (\psi(\vec{v},v_{n+1}%
)\rightarrow\psi(\vec{v},+(v_{n+1},1)))\rightarrow\forall v_{n+1}\ \psi
(\vec{v},v_{n+1}))
\]
Declaremos $\varphi=_{d}\varphi(v_{1},...,v_{n})$. Notese que $\mathbf{\omega
}\vDash Ind_{\psi,v_{1},...,v_{n+1}}$ si y solo si para cada $a_{1}%
,...,a_{n}\in\omega$ se tiene que $\mathbf{\omega}\vDash\varphi\lbrack\vec
{a}]$. Sean $a_{1},...,a_{n}\in\omega$ fijos. Probaremos que $\mathbf{\omega
}\vDash\varphi\lbrack\vec{a}]$. Notar que si%
\[
\mathbf{\omega}\nvDash(\psi(\vec{v},0)\wedge\forall v_{n+1}\ (\psi(\vec
{v},v_{n+1})\rightarrow\psi(\vec{v},+(v_{n+1},1)))[\vec{a}]
\]
entonces $\mathbf{\omega}\vDash\varphi\lbrack\vec{a}]$ por lo cual podemos
hacer solo el caso en que%
\[
\mathbf{\omega}\vDash(\psi(\vec{v},0)\wedge\forall v_{n+1}\ (\psi(\vec
{v},v_{n+1})\rightarrow\psi(\vec{v},+(v_{n+1},1)))[\vec{a}]
\]
Para probar que $\mathbf{\omega}\vDash\varphi\lbrack\vec{a}]$, deberemos
probar entonces que $\mathbf{\omega}\vDash\forall v_{n+1}\ \psi(\vec
{v},v_{n+1})[\vec{a}]$. Sea $S=\{a\in\omega:\mathbf{\omega}\vDash\psi(\vec
{v},v_{n+1})[\vec{a},a]\}$. Ya que $\mathbf{\omega}\vDash\psi(\vec{v}%
,0)[\vec{a}]$, es facil ver usando el lema de reemplazo que $\mathbf{\omega
}\vDash\psi(\vec{v},v_{n+1})[\vec{a},0]$, lo cual nos dice que $0\in S$. Ya
que $\mathbf{\omega}\vDash\forall v_{n+1}\ (\psi(\vec{v},v_{n+1}%
)\rightarrow\psi(\vec{v},+(v_{n+1},1))[\vec{a}]$, tenemos que

\begin{enumerate}
\item[(1)] Para cada $a\in\omega$, si $\mathbf{\omega}\vDash\psi(\vec
{v},v_{n+1})[\vec{a},a]$, entonces $\mathbf{\omega}\vDash\psi(\vec
{v},+(v_{n+1},1))[\vec{a},a]$.
\end{enumerate}

Pero por el lema de reemplazo, tenemos que $\mathbf{\omega}\vDash\psi(\vec
{v},+(v_{n+1},1))[\vec{a},a]$ sii $\mathbf{\omega}\vDash\psi(\vec{v}%
,v_{n+1})[\vec{a},a+1]$, lo cual nos dice que

\begin{enumerate}
\item[(2)] Para cada $a\in\omega$, si $\mathbf{\omega}\vDash\psi(\vec
{v},v_{n+1})[\vec{a},a]$, entonces $\mathbf{\omega}\vDash\psi(\vec{v}%
,v_{n+1})[\vec{a},a+1]$.
\end{enumerate}

Ya que $0\in S$ y (2) nos dice que $a\in S$ implica $a+1\in S$, tenemos que
$S=\omega$. Es decir que para cada $a\in\omega$, se da que $\mathbf{\omega
}\vDash\psi(\vec{v},v_{n+1})[\vec{a},a]$ lo cual nos dice que $\mathbf{\omega
}\vDash\forall v_{n+1}\ \psi(\vec{v},v_{n+1})[\vec{a}]$.

Es rutina probar que $\mathbf{\omega}$ satisface los otros 15 axiomas de
$Arit$.
\end{proof}

\bigskip

El modelo $\mathbf{\omega}$ es llamado el \textit{modelo standard} de $Arit$.

\bigskip

\begin{enumerate}
\item[Ejercicio:] Pruebe que $\mathbf{Q}^{\geq0}$ no es un modelo de $Arit$,
dando una "propiedad inductiva que no cumpla"
\end{enumerate}

\bigskip

Definamos el mapeo $\widehat{\ \ \ }:\omega\rightarrow
\{(\;)\;,\;+\;0\;1\}^{\ast}$ de la siguiente manera%
\begin{align*}
\widehat{0}  & =0\\
\widehat{1}  & =1\\
\widehat{n+1}  & =+(\widehat{n},1)\text{, para cada }n\geq1
\end{align*}


\bigskip

\begin{proposition}
Hay un modelo de $Arit$ el cual no es isomorfo a $\mathbf{\omega}$
\end{proposition}

\begin{proof}
Sea $\tau=(\{0,1,\blacktriangle\},\{+^{2},.^{2}\},\{\leq^{2}\},a)$ y sea
$\Sigma=\Sigma_{A}\cup\{\lnot(\widehat{n}\equiv\blacktriangle):n\in\omega\}$.
Por el Teorema de Compacidad la teoria $(\Sigma,\tau)$ tiene un modelo
$\mathbf{A}=(A,i)$. Ya que%
\[
\mathbf{A}\vDash\lnot(\widehat{n}\equiv\blacktriangle)\text{, para cada }%
n\in\omega
\]
tenemos que%

\[
i(\blacktriangle)\neq\widehat{n}^{\mathbf{A}}\text{, para cada }n\in\omega
\]
Por el Lema de Coincidencia la estructura $\mathbf{B}=(A,i|_{\{0,1,+,.,\leq
\}})$ es un modelo de $Arit$. Ademas dicho lema nos garantiza que $\widehat
{n}^{\mathbf{B}}=\widehat{n}^{\mathbf{A}}$, para cada $n\in\omega$, por lo
cual tenemos que%
\[
i(\blacktriangle)\neq\widehat{n}^{\mathbf{B}}\text{, para cada }n\in\omega
\]
Veamos que $\mathbf{B}$ no es isomorfo a $\mathbf{\omega}$. Supongamos
$F:\omega\rightarrow A$ es un isomorfismo de $\mathbf{\omega}$ en $\mathbf{B}%
$. Es facil de probar por induccion en $n$ que $F(n)=\widehat{n}^{\mathbf{B}}%
$, para cada $n\in\omega$. Pero esto produce un absurdo ya que nos dice que
$i(\blacktriangle)$ no esta en la imagen de $F$.
\end{proof}

\bigskip

\begin{enumerate}
\item[Ejercicio:] Dado un modelo $\mathbf{A}$ de $Arit$ y elementos $a,b\in A
$, diremos que $a$ \textit{divide a }$b$ \textit{en} $\mathbf{A}$ cuando haya
un $c\in A$ tal que $b=.^{\mathbf{A}}(c,a).$ Un elemento $a\in A$ sera llamado
\textit{primo en }$\mathbf{A}$\textit{\ }si $a\neq1^{\mathbf{A}}$ y sus unicos
divisores son $1^{\mathbf{A}}$ y $a$. Pruebe que hay un modelo de $Arit$,
$\mathbf{A}$, en el cual hay infinitos primos no pertenecientes a
$\{\widehat{n}^{\mathbf{A}}:n\in\omega\}$.
\end{enumerate}

\bigskip

\begin{lemma}
Las siguientes sentencias son teoremas de la aritmetica de Peano:

\begin{enumerate}
\item[(1)] $\forall x\;0\leq x$

\item[(2)] $\forall x\;(x\leq0\rightarrow x\equiv0)$

\item[(3)] $\forall x\forall y\;(x+y\equiv0\rightarrow(x\equiv0\wedge
y\equiv0))$

\item[(4)] $\forall x\;(\lnot(x\equiv0)\rightarrow\exists z\ (x\equiv z+1))$

\item[(5)] $\forall x\forall y\;(x<y\rightarrow x+1\leq y)$

\item[(6)] $\forall x\forall y\;(x<y+1\rightarrow x\leq y)$

\item[(7)] $\forall x\forall y\;(x\leq y+1\rightarrow(x\leq y\vee x\equiv
y+1))$

\item[(8)] $\forall x\forall y\;((x\leq y\wedge y\leq x+1)\rightarrow(x\equiv
y\vee x\equiv y+1))$ (use (7))

\item[(9)] $\forall x\forall y\;(\lnot y\equiv0\rightarrow\exists q\exists
r\;x\equiv q.y+r\wedge r<y)$
\end{enumerate}
\end{lemma}

\begin{proof}
(1) es dejada al lector.

(2)%
\[%
\begin{array}
[c]{lllll}%
\;1. & x_{0}\leq0 &  &  & \text{HIPOTESIS}1\\
\;2. & \forall x\;0\leq x &  &  & \text{TEOREMA}\\
\;3. & 0\leq x_{0} &  &  & \text{PARTICULARIZACION}(2)\\
\;4. & x_{0}\leq0\wedge0\leq x_{0} &  &  & \text{CONJUNCIONINTRODUCCION}%
(1,3)\\
\;5. & \forall x_{1}\forall x_{2}\;((x_{1}\leq x_{2}\wedge x_{2}\leq
x_{1})\rightarrow x_{1}\equiv x_{2}) &  &  & \text{AXIOMAPROPIO}\\
\;6. & \forall x_{2}\;((x_{0}\leq x_{2}\wedge x_{2}\leq x_{0})\rightarrow
x_{0}\equiv x_{2}) &  &  & \text{PARTICULARIZACION}(5)\\
\;7. & ((x_{0}\leq0\wedge0\leq x_{0})\rightarrow x_{0}\equiv0) &  &  &
\text{PARTICULARIZACION}(6)\\
\;8. & x_{0}\equiv0 &  &  & \text{TESIS}1\text{MODUSPONENS}(4,7)\\
\;9. & x_{0}\leq0\rightarrow x_{0}\equiv0 &  &  & \text{CONCLUSION}\\
10. & \forall x\ (x\leq0\rightarrow x\equiv0) &  &  & \text{GENERALIZACION}(9)
\end{array}
\]


(3)%
\[%
\begin{array}
[c]{lllll}%
\;1. & x_{0}+y_{0}\equiv0 &  &  & \text{HIPOTESIS}1\\
\;2. & 0\equiv x_{0}+y_{0} &  &  & \text{COMMUTATIVIDAD}(1)\\
\;3. & \exists x_{3}\ (0\equiv x_{0}+x_{3}) &  &  & \text{EXISTENCIAL}(2)\\
\;4. & \forall x_{1}\forall x_{2}\;(x_{1}\leq x_{2}\leftrightarrow\exists
x_{3}\;x_{2}\equiv x_{1}+x_{3}) &  &  & \text{AXIOMAPROPIO}\\
\;5. & x_{0}\leq0\leftrightarrow\exists x_{3}\;0\equiv x_{0}+x_{3}) &  &  &
\text{PARTICULARIZACION}^{2}(5)\\
\;6. & x_{0}\leq0 &  &  & \text{REEMPLAZO}(5,3)\\
\;7. & \forall x\ 0\leq x &  &  & \text{TEOREMA}\\
\;8. & 0\leq x_{0} &  &  & \text{PARTICULARIZACION}(7)\\
\;9. & x_{0}\leq0\wedge0\leq x_{0} &  &  & \text{CONJUNCIONINTRODUCCION}%
(6,8)\\
10. & \forall x_{1}\forall x_{2}\;((x_{1}\leq x_{2}\wedge x_{2}\leq
x_{1})\rightarrow x_{1}\equiv x_{2}) &  &  & \text{AXIOMAPROPIO}\\
11. & ((x_{0}\leq0\wedge0\leq x_{0})\rightarrow x_{0}\equiv0) &  &  &
\text{PARTICULARIZACION}^{2}(10)\\
12. & x_{0}\equiv0 &  &  & \text{MODUSPONENS}(9,11)\\
13. & 0+y_{0}\equiv0 &  &  & \text{REEMPLAZO}(12,1)\\
14. & \forall y\ y\equiv0+y &  &  & \text{TEOREMA}\\
15. & y_{0}\equiv0+y_{0} &  &  & \text{PARTICULARIZACION}(14)\\
16. & y_{0}\equiv0 &  &  & \text{TRANSITIVIDAD}(15,13)\\
17. & x_{0}\equiv0\wedge y_{0}\equiv0 &  &  & \text{TESIS}%
1\text{CONJUNCIONINTRODUCCION}(12,16)\\
18. & x_{0}+y_{0}\equiv0\rightarrow(x_{0}\equiv0\wedge y_{0}\equiv0) &  &  &
\text{CONCLUSION}\\
19. & \forall x\forall y\;(x+y\equiv0\rightarrow(x\equiv0\wedge y\equiv0)) &
&  & \text{GENERALIZACION}^{2}(18)
\end{array}
\]

\end{proof}

\bigskip

\begin{lemma}
Sean $n,m\in\omega$ y sea $t\in T_{c}^{\tau_{A}}$. Las siguientes sentencias
son teoremas de la aritmetica de Peano:

\begin{enumerate}
\item[(a)] $(+(\widehat{n},\widehat{m})\equiv\widehat{n+m})$

\item[(b)] $(.(\widehat{n},\widehat{m})\equiv\widehat{n.m})$

\item[(c)] $\forall x\;(x\leq\widehat{n}\rightarrow(x\equiv\widehat{0}\vee
x\equiv\widehat{1}\vee...\vee x\equiv\widehat{n}))$

\item[(d)] $(t\equiv\widehat{t^{\mathbf{\omega}}})$
\end{enumerate}
\end{lemma}

\bigskip

\bigskip

\begin{lemma}
Si $\varphi$ es una sentencia atomica o negacion de atomica y $\mathbf{\omega
}\models\varphi$, entonces $Arit\vdash\varphi$.
\end{lemma}

\begin{proof}
Hay cuatro casos.

Caso $\varphi=(t\equiv s)$, con $t,s$ terminos cerrados.

Ya que $\mathbf{\omega}\models\varphi$, tenemos que $t^{\mathbf{\omega}%
}=s^{\mathbf{\omega}}$ y por lo tanto $\widehat{t^{\mathbf{\omega}}}%
=\widehat{s^{\mathbf{\omega}}}$. Por el lema anterior tenemos que
$Arit\vdash(t\equiv\widehat{t^{\mathbf{\omega}}}),(s\equiv\widehat
{s^{\mathbf{\omega}}})$ lo cual, ya que $\widehat{t^{\mathbf{\omega}}}$ y
$\widehat{s^{\mathbf{\omega}}}$ son el mismo termino nos dice por la regla de
transitividad que $Arit\vdash(t\equiv s)$.

Caso $\varphi=(t\leq s)$, con $t,s$ terminos cerrados.

Ya que $\mathbf{\omega}\models\varphi$, tenemos que $t^{\mathbf{\omega}}\leq
s^{\mathbf{\omega}}$ y por lo tanto hay un $k\in\omega$ tal que
$t^{\mathbf{\omega}}+k=s^{\mathbf{\omega}}$. Se tiene entonces que
$\widehat{t^{\mathbf{\omega}}+k}=\widehat{s^{\mathbf{\omega}}}$. Por el lema
anterior tenemos que $Arit\vdash+(\widehat{t^{\mathbf{\omega}}},\widehat
{k})\equiv\widehat{t^{\mathbf{\omega}}+k}$ lo cual nos dice que%
\[
Arit\vdash+(\widehat{t^{\mathbf{\omega}}},\widehat{k})\equiv\widehat
{s^{\mathbf{\omega}}}%
\]
Pero el lema anterior nos dice que%
\[
Arit\vdash(t\equiv\widehat{t^{\mathbf{\omega}}}),(s\equiv\widehat
{s^{\mathbf{\omega}}})
\]
y por lo tanto la regla de reemplazo nos asegura que $Arit\vdash
+(t,\widehat{k})\equiv s$. Ya que%
\[
\forall x_{1}\forall x_{2}\;(x_{1}\leq x_{2}\leftrightarrow\exists
x_{3}\;x_{2}\equiv x_{1}+x_{3})
\]
es un axioma de $Arit$, tenemos que $Arit\vdash(t\leq s)$.

Caso $\varphi=\lnot(t\equiv s)$, con $t,s$ terminos cerrados.

Caso $\varphi=\lnot(t\leq s)$, con $t,s$ terminos cerrados.
\end{proof}

\bigskip

El siguiente lema muestra que en $Arit$ se pueden probar ciertas sentencias
las cuales emulan el principio de induccion completa.

\begin{lemma}
Sea $\varphi=_{d}\varphi(\vec{v},v)\in F^{\tau_{A}}$. Supongamos $v$ es
sustituible por $w$ en $\varphi$ y $w\notin\{v_{1},...,v_{n}\}$. Entonces:%
\[
Arit\vdash\forall\vec{v}((\varphi(\vec{v},0)\wedge\forall v(\forall
w(w<v\rightarrow\varphi(\vec{v},w))\rightarrow\varphi(\vec{v},v)))\rightarrow
\forall v\varphi(\vec{v},v))
\]

\end{lemma}

\begin{proof}
Sea $\tilde{\varphi}=\forall w(w\leq v\rightarrow\varphi(\vec{v},w))$. Notar
que $Li(\tilde{\varphi})\subseteq\{v_{1},...,v_{n},v\}$. Declaremos
$\tilde{\varphi}=_{d}\tilde{\varphi}(\vec{v},v)$. Para hacer la prueba formal
usaremos el axioma $Ind_{\tilde{\varphi},v_{1},...,v_{n},v}$. Salvo por el uso
de algunos teoremas simples y el uso simultaneo de las reglas de
particularizacion y generalizacion, la siguiente es la prueba formal buscada.%
\[%
\begin{array}
[c]{lllll}%
\;1. & (\varphi(\vec{c},0)\wedge\forall v(\forall w(w<v\rightarrow\varphi
(\vec{c},w))\rightarrow\varphi(\vec{c},v)) &  &  & \text{HIPOTESIS}1\\
\;2. & \;\;\;w_{0}\leq0 &  &  & \text{HIPOTESIS}2\\
\;3. & \;\;\;\forall x\;(x\leq0\rightarrow x\equiv0) &  &  & \text{TEOREMA}\\
\;4. & \;\;\;w_{0}\leq0\rightarrow w_{0}\equiv0 &  &  &
\text{PARTICULARIZACION}(3)\\
\;5. & \;\;\;w_{0}\equiv0 &  &  & \text{MODUSPONENS}(2,4)\\
\;6. & \;\;\;\varphi(\vec{c},0) &  &  & \text{CONJUNCIONELIMINACION}(1)\\
\;7. & \;\;\;\varphi(\vec{c},w_{0}) &  &  & \text{TESIS}2\text{REEMPLAZO}%
(5,6)\\
\;8. & w_{0}\leq0\rightarrow\varphi(\vec{c},w_{0}) &  &  & \text{CONCLUSION}\\
\;9. & \tilde{\varphi}(\vec{c},0) &  &  & \text{GENERALIZACION}(8)\\
10. & \;\;\;\tilde{\varphi}(\vec{c},v_{0}) &  &  & \text{HIPOTESIS}3\\
11. & \;\;\;\;\;\;w_{0}<v_{0}+1 &  &  & \text{HIPOTESIS}4\\
12. & \;\;\;\;\;\;\forall x,y\;x<y+1\rightarrow x\leq y &  &  & \text{TEOREMA}%
\\
13. & \;\;\;\;\;\;w_{0}<v_{0}+1\rightarrow w_{0}\leq v_{0} &  &  &
\text{PARTICULARIZACION}(12)\\
14. & \;\;\;\;\;\;w_{0}\leq v_{0} &  &  & \text{MODUSPONENS}(11,13)\\
15. & \;\;\;\;\;\;w_{0}\leq v_{0}\rightarrow\varphi(\vec{c},w_{0}) &  &  &
\text{PARTICULARIZACION}(10)\\
16. & \;\;\;\;\;\;\varphi(\vec{c},w_{0}) &  &  & \text{TESIS}%
4\text{MODUSPONENS}(14,15)\\
17. & \;\;\;w_{0}<v_{0}+1\rightarrow\varphi(\vec{c},w_{0}) &  &  &
\text{CONCLUSION}\\
18. & \;\;\;\forall w\;w<v_{0}+1\rightarrow\varphi(\vec{c},w) &  &  &
\text{GENERALIZACION}(17)\\
19. & \;\;\;\forall v(\forall w(w<v\rightarrow\varphi(\vec{c},w))\rightarrow
\varphi(\vec{c},v)) &  &  & \text{CONJUNCIONELIMINACION}(1)\\
20. & \;\;\;(\forall w(w<v_{0}+1\rightarrow\varphi(\vec{c},w))\rightarrow
\varphi(\vec{c},v_{0}+1)) &  &  & \text{PARTICULARIZACION}(19)\\
21. & \;\;\;\varphi(\vec{c},v_{0}+1) &  &  & \text{MODUSPONENS}(18,20)\\
22. & \;\;\;\;\;\;w_{0}\leq v_{0}+1 &  &  & \text{HIPOTESIS}5\\
23. & \;\;\;\;\;\;\forall x,y\;x\leq y+1\rightarrow(x\leq y\vee x\equiv y+1) &
&  & \text{TEOREMA}\\
24. & \;\;\;\;\;\;w_{0}\leq v_{0}+1\rightarrow(w_{0}\leq v_{0}\vee w_{0}\equiv
v_{0}+1) &  &  & \text{PARTICULARIZACION}(23)\\
25. & \;\;\;\;\;\;(w_{0}\leq v_{0}\vee w_{0}\equiv v_{0}+1) &  &  &
\text{MODUSPONENS}(22,24)\\
26. & \;\;\;\;\;\;w_{0}\leq v_{0}\rightarrow\varphi(\vec{c},w_{0}) &  &  &
\text{PARTICULARIZACION}(10)\\
27. & \;\;\;\;\;\;\;\;\;w_{0}\equiv v_{0}+1 &  &  & \text{HIPOTESIS}6\\
28. & \;\;\;\;\;\;\;\;\;\varphi(\vec{c},w_{0}) &  &  & \text{TESIS}%
6\text{REEMPLAZO}(21,27)\\
29. & \;\;\;w_{0}\equiv v_{0}+1\rightarrow\varphi(\vec{c},w_{0}) &  &  &
\text{CONCLUSION}\\
30. & \;\;\;\;\;\;\varphi(\vec{c},w_{0}) &  &  & \text{TESIS}%
5\text{DISJUNCIONELIM}(25,26,29)\\
31. & \;\;\;w_{0}\leq v_{0}+1\rightarrow\varphi(\vec{c},w_{0}) &  &  &
\text{CONCLUSION}\\
32. & \;\;\;\tilde{\varphi}(\vec{c},v_{0}+1) &  &  & \text{TESIS}%
3\text{GENERALIZACION}(31)\\
33. & \tilde{\varphi}(\vec{c},v_{0})\rightarrow\tilde{\varphi}(\vec{c}%
,v_{0}+1) &  &  & \text{CONCLUSION}\\
34. & \forall v\tilde{\varphi}(\vec{c},v)\rightarrow\tilde{\varphi}(\vec
{c},v+1) &  &  & \text{GENERALIZACION}(33)\\
35. & \tilde{\varphi}(\vec{c},0)\wedge\forall v\tilde{\varphi}(\vec
{c},v)\rightarrow\tilde{\varphi}(\vec{c},v+1) &  &  &
\text{CONJUNCIONINTRODUCCION}(9,34)\\
36. & Ind_{\tilde{\varphi},v_{1},...,v_{n},v} &  &  & \text{AXIOMAPROPIO}\\
37. & (\tilde{\varphi}(\vec{c},0)\wedge\forall v(\tilde{\varphi}(\vec
{c},v)\rightarrow\tilde{\varphi}(\vec{c},v+1))\rightarrow\forall
v\tilde{\varphi}(\vec{c},v) &  &  & \text{PARTICULARIZACION}(36)\\
38. & \forall v\tilde{\varphi}(\vec{c},v) &  &  & \text{MODUSPONENS}(35,37)\\
39. & \tilde{\varphi}(\vec{c},v_{0}) &  &  & \text{PARTICULARIZACION}(38)\\
40. & v_{0}\leq v_{0}\rightarrow\varphi(\vec{c},v_{0}) &  &  &
\text{PARTICULARIZACION}(39)\\
41. & \forall x\;x\leq x &  &  & \text{AXIOMAPROPIO}\\
42. & v_{0}\leq v_{0} &  &  & \text{PARTICULARIZACION}(41)\\
43. & \varphi(\vec{c},v_{0}) &  &  & \text{MODUSPONENS}(40,42)\\
44. & \forall v\varphi(\vec{c},v) &  &  & \text{TESIS}1\text{GENERALIZACION}%
(43)\\
45. & (\varphi(\vec{c},0)\wedge\forall v(\forall w(w<v\rightarrow\varphi
(\vec{c},w))\rightarrow\varphi(\vec{c},v)))\rightarrow\forall v\varphi(\vec
{c},v) &  &  & \text{CONCLUSION}\\
46. & \forall\vec{v}((\varphi(\vec{v},0)\wedge\forall v(\forall
w(w<v\rightarrow\varphi(\vec{v},w))\rightarrow\varphi(\vec{v},v)))\rightarrow
\forall v\varphi(\vec{v},v)) &  &  & \text{GENERALIZACION}(45)
\end{array}
\]

\end{proof}

\bigskip

\subsection{Logica ecuacional}

Dados $t,s\in T^{\tau}$, con $t\approx s$ denotaremos la siguiente sentencia
de tipo $\tau$:%
\[
\forall x_{1}...\forall x_{n}\;(t\equiv s)
\]
donde $n$ es el menor $j$ tal que $\{x_{1},...,x_{j}\}$ contiene a todas las
variables que ocurren en $t$ y $s$. Notese que este $n$ es $0$ cuando $t$ y
$s$ son terminos cerrados, es decir que $t\approx s$ denota a la sentencia
$(t\equiv s)$, cuando $t,s\in T_{c}^{\tau}$. Las sentencias $t\approx s$, con
$t,s\in T^{\tau}$, seran llamadas \textit{identidades de tipo }$\tau$. Notese
que $\mathbf{A}\models t\approx s$ sii $t^{\mathbf{A}}[\vec{a}]=s^{\mathbf{A}%
}[\vec{a}]$, para cada $\vec{a}\in A^{\mathbf{N}}$. Tambien, si $t=_{d}%
t(x_{1},...,x_{m})$ y $s=_{d}s(x_{1},...,x_{m})$, entonces dado una $\tau
$-algebra $\mathbf{A}$, tenemos que $\mathbf{A}\models t\approx s$ sii
$t^{\mathbf{A}}\left[  a_{1},...,a_{m}\right]  =s^{\mathbf{A}}\left[
a_{1},...,a_{m}\right]  $, para cada $(a_{1},...,a_{m})\in A^{m}$.
(Independientemente de que $m$ sea el menor $j$ tal que $\{x_{1},...,x_{j}\}$
contiene a las variables que ocurren en $t$ y $s$.)

Una teoria de primer orden $(\Sigma,\tau)$ sera llamada \textit{ecuacional} si
$\tau$ es un tipo algebraico y cada elemento de $\Sigma$ es una identidad de
tipo $\tau$. Por supuesto, el teorema de completitud de Godel nos garantiza
que si $T$ es una teoria ecuacional y $T\vDash t\approx s$, entonces hay una
prueba formal de $t\approx s$ en $T$. Sin envargo, en dicha prueba formal
puede haber sentencias las cuales no sean identidades. Una pregunta
interesante es la siguiente:

\begin{enumerate}
\item[Pregunta:] \textquestiondown Hay una nocion de "prueba ecuacional" la
cual sea:

\item[-] Correcta: si hay una prueba ecuacional de $t\approx s$ en $T$,
entonces $t\approx s$ es verdadera en cada modelo de $T$

\item[-] Completa: si $T\vDash t\approx s$, entonces hay una prueba ecuacional
de $t\approx s$ en $T$?
\end{enumerate}

En esta seccion veremos que, tal como lo probo Birkhoff, esto es posible y que
la nocion de prueba ecuacional que se puede dar es muy natural y simple, es
decir si sabemos que en una teoria todos los axiomas son identidades, entonces
a los fines de probar identidades las pruebas de primer orden clasicas pueden
ser reemplazadas por pruebas con un formato mucho mas amigable.

\subsubsection{Pruebas ecuacionales}

Primero introducimos una serie de conjuntos los cuales poseen informacion
deductiva ecuacional basica. Sea%
\[
TransEc^{\tau}=\{(t\approx s,s\approx p,t\approx p):t,s,p\in T^{\tau}\}
\]
Diremos que $\varphi$ \textit{se deduce de }$\psi_{1}$y $\psi_{2}$ \textit{por
la regla de transitividad ecuacional, respecto a }$\tau$ para expresar que
$(\psi_{1},\psi_{2},\varphi)\in TransEc^{\tau}$. Sea%
\[
SimEc^{\tau}=\{(t\approx s,s\approx t):t,s\in T^{\tau}\}
\]
Diremos que $\varphi$ \textit{se deduce de }$\psi_{1}$ \textit{por la regla de
simetria ecuacional, respecto a }$\tau$ para expresar que $(\psi_{1}%
,\varphi)\in SimEc^{\tau}$. Sea%
\[%
\begin{array}
[c]{c}%
SubsEc^{\tau}=\{(t\approx s,t(p_{1},...,p_{n})\approx s(p_{1},...,p_{n}%
)):t=_{d}t(x_{1},...,x_{n})\\
s=_{d}s(x_{1},...,x_{n})\ \mathrm{y}\ p_{1},...,p_{n}\in T^{\tau}\}
\end{array}
\]
Diremos que $\varphi$ \textit{se deduce de }$\psi_{1}$ \textit{por la regla de
substitucion ecuacional, respecto a }$\tau$ para expresar que $(\psi
_{1},\varphi)\in SubsEc^{\tau}$. Sea%
\[%
\begin{array}
[c]{c}%
ReempEc^{\tau}=\{(t\approx s,r\approx\tilde{r}):t,s,r\in T^{\tau
}\ \text{\textrm{y}}\ \tilde{r}=\mathrm{resultado}\\
\mathrm{de\ reemplazar\ algunas\ ocurrencias\ de\ }t\ \mathrm{en\ }%
r\ \mathrm{por\ }s\}
\end{array}
\]
Diremos que $\varphi$ \textit{se deduce de }$\psi_{1}$ \textit{por la regla de
reemplazo ecuacional, respecto a }$\tau$ para expresar que $(\psi_{1}%
,\varphi)\in ReempEc^{\tau}$.

La identidad $x_{1}\approx x_{1}$ sera llamada \textit{axioma logico
ecuacional de tipo }$\tau$. Notese que dicha identidad no es ni mas ni menos
que la sentencia $\forall x_{1}(x_{1}\equiv x_{1})$ la cual es universalmente valida.

\label{wiki-prueba.ecuacional}

\paragraph{Definicion de prueba ecuacional}

Dada una teoria ecuacional $(\Sigma,\tau)$ y una identidad $p\approx q$ de
tipo $\tau$, una \textit{prueba ecuacional} de $p\approx q$ en $(\Sigma,\tau)$
sera una palabra $\mathbf{\varphi}\in S^{\tau+}$ tal que

\begin{enumerate}
\item[(1)] Cada $\mathbf{\varphi}_{k}$, con $k=1,...,n(\mathbf{\varphi})$, es
una identidad de tipo $\tau$ y $\mathbf{\varphi}_{n(\mathbf{\varphi}%
)}=p\approx q$

\item[(2)] Para cada $k=1,...,n(\mathbf{\varphi})$, se da alguna de las siguientes

\begin{enumerate}
\item[(a)] $\mathbf{\varphi}_{k}=x_{1}\approx x_{1}$

\item[(b)] $\mathbf{\varphi}_{k}\in\Sigma$

\item[(c)] hay $i,j<k$ tales que $\mathbf{\varphi}_{k}$ se deduce por la regla
de transitividad ecuacional a partir de $\mathbf{\varphi}_{i}$ y
$\mathbf{\varphi}_{j}$

\item[(d)] hay $i<k$ tal que $\mathbf{\varphi}_{k}$ se deduce por la regla de
simetria ecuacional a partir de $\mathbf{\varphi}_{i}$

\item[(e)] hay $i<k$ tal que $\mathbf{\varphi}_{k}$ se deduce por la regla de
substitucion ecuacional a partir de $\mathbf{\varphi}_{i}$

\item[(f)] hay $i<k$ tal que $\mathbf{\varphi}_{k}$ se deduce por la regla de
reemplazo ecuacional a partir de $\mathbf{\varphi}_{i}$
\end{enumerate}
\end{enumerate}

\noindent Escribiremos $(\Sigma,\tau)\vdash_{ec}p\approx q$ cuando haya una
prueba ecuacional de $p\approx q$ en $(\Sigma,\tau)$.

\bigskip

\subsubsection{Correccion ecuacional}

Para probar que el concepto de prueba ecuacional es correcto nos hara falta el
siguiente lema.

\begin{lemma}
Todas las reglas introducidas en la seccion anterior son universales en el
sentido que si $\varphi$ se deduce de $\psi_{1},...,\psi_{k}$ por alguna de
estas reglas, entonces $\left(  (\psi_{1}\wedge...\wedge\psi_{k}%
)\rightarrow\varphi\right)  $ es una sentencia universalmente valida.
\end{lemma}

\begin{proof}
Veamos que la regla de reemplazo es universal. Basta con ver por induccion en
$k$ que

\begin{enumerate}
\item[-] Teo$_{k}$: Sean $t,s\in T^{\tau}$, $r\in T_{k}^{\tau}$ y sea
$\mathbf{A}$ una $\tau$-algebra tal que $t^{\mathbf{A}}[\vec{a}]=s^{\mathbf{A}%
}[\vec{a}]$, para cada $\vec{a}\in A^{\mathbf{N}}$. Entonces $r^{\mathbf{A}%
}[\vec{a}]=\tilde{r}^{\mathbf{A}}[\vec{a}]$, para cada $\vec{a}\in
A^{\mathbf{N}}$, donde $\tilde{r}$ es el resultado de reemplazar algunas
ocurrencias de $t$ en $r$ por $s.$
\end{enumerate}

La prueba de Teo$_{0}$ es dejada al lector. Asumamos que vale Teo$_{k}$ y
probemos que vale Teo$_{k+1}$. Sean $t,s\in T^{\tau}$, $r\in T_{k+1}^{\tau
}-T_{k}^{\tau}$ y sea $\mathbf{A}$ una $\tau$-algebra tal que $t^{\mathbf{A}%
}[\vec{a}]=s^{\mathbf{A}}[\vec{a}]$, para cada $\vec{a}\in A^{\mathbf{N}}$.
Sea $\tilde{r}$ el resultado de reemplazar algunas ocurrencias de $t$ en $r$
por $s$. El caso $t=r$ es trivial. Supongamos entonces que $t\neq r$.
Supongamos $r=f(r_{1},...,r_{n})$, con $r_{1},...,r_{n}\in T_{k}^{\tau}$ y
$f\in\mathcal{F}_{n}$. Notese que por Lema \ref{reemp-ter-en-ter} tenemos que
$\tilde{r}=f(\tilde{r}_{1},...,\tilde{r}_{n})$, donde cada $\tilde{r}_{i} $ es
el resultado de reemplazar algunas ocurrencias de $t$ en $r_{i}$ por $s$. Para
$\vec{a}\in A^{\mathbf{N}}$ se tiene que%
\[%
\begin{array}
[c]{cclll}%
r^{\mathbf{A}}[\vec{a}] & = & f(r_{1},...,r_{n})^{\mathbf{A}}[\vec{a}] &  & \\
& = & f^{\mathbf{A}}(r_{1}^{\mathbf{A}}[\vec{a}],...,r_{n}^{\mathbf{A}}%
[\vec{a}]) &  & \\
& = & f^{\mathbf{A}}(\tilde{r}_{1}^{\mathbf{A}}[\vec{a}],...,\tilde{r}%
_{n}^{\mathbf{A}}[\vec{a}]) &  & \text{por Teo}_{k}\\
& = & f(\tilde{r}_{1},...,\tilde{r}_{n})^{\mathbf{A}}[\vec{a}] &  & \\
& = & \tilde{r}^{\mathbf{A}}[\vec{a}] &  &
\end{array}
\]
lo cual prueba Teo$_{k+1}$

Veamos que la regla de substitucion es universal. Supongamos $\mathbf{A}%
\models t\approx s$, con $t=_{d}t(x_{1},...,x_{n})$ y $s=_{d}s(x_{1}%
,...,x_{n})$. Veremos que entonces $\mathbf{A}\models t(p_{1},...,p_{n}%
)\approx s(p_{1},...,p_{n}).$ Supongamos que $p_{i}=_{d}p_{i}(x_{1}%
,...,x_{m})$, para cada $i=1,...,n.$ Por (a) del Lema \ref{reemp-term},
tenemos que%
\begin{align*}
t(p_{1},...,p_{n})  & =_{d}t(p_{1},...,p_{n})(x_{1},...,x_{m})\\
s(p_{1},...,p_{n})  & =_{d}s(p_{1},...,p_{n})(x_{1},...,x_{m})
\end{align*}
Sea $\vec{a}\in A^{m}$. Tenemos que%
\[%
\begin{array}
[c]{rcl}%
t(p_{1},...,p_{n})^{\mathbf{A}}\left[  \vec{a}\right]  & = & t^{\mathbf{A}%
}\left[  p_{1}^{\mathbf{A}}\left[  \vec{a}\right]  ,...,p_{n}^{\mathbf{A}%
}\left[  \vec{a}\right]  \right] \\
& = & s^{\mathbf{A}}\left[  p_{1}^{\mathbf{A}}\left[  \vec{a}\right]
,...,p_{n}^{\mathbf{A}}\left[  \vec{a}\right]  \right] \\
& = & s(p_{1},...,p_{n})^{\mathbf{A}}\left[  \vec{a}\right]
\end{array}
\]
lo cual nos dice que $\mathbf{A}\models t(p_{1},...,p_{n})\approx
s(p_{1},...,p_{n})$
\end{proof}

\bigskip

\begin{theorem}
[Correccion]Si $(\Sigma,\tau)\vdash_{ec}p\approx q$, entonces $(\Sigma
,\tau)\models p\approx q$.
\end{theorem}

\begin{proof}
Sea $\mathbf{\varphi}$ una prueba ecuacional de $p\approx q$ en $(\Sigma
,\tau)$. Usando el lema anterior se puede probar facilmente por induccion en
$i$ que $(\Sigma,\tau)\models\mathbf{\varphi}_{i}$, por lo cual $(\Sigma
,\tau)\models p\approx q$
\end{proof}

\bigskip

\subsubsection{Completitud ecuacional}

Para probar que el concepto de prueba ecuacional es completo nos haran falta
algunos resultados basicos que tienen interes por si mismos.

\bigskip

\paragraph{El algebra de terminos}

Dado un tipo algebraico $\tau$, hay una forma natural de definir un algebra
$\mathbf{T}^{\tau}$ cuyo universo es $T^{\tau}$, de la siguiente manera

\begin{enumerate}
\item[(1)] $c^{\mathbf{T}^{\tau}}=c$, para cada $c\in\mathcal{C}$

\item[(2)] $f^{\mathbf{T}^{\tau}}(t_{1},...,t_{n})=f(t_{1},...,t_{n})$, para
todo $t_{1},...,t_{n}\in T^{\tau}$, $f\in\mathcal{F}_{n}$.
\end{enumerate}

\noindent Llamaremos a $\mathbf{T}^{\tau}$ el \textit{algebra de terminos de
tipo }$\tau$.

\begin{example}
Supongamos $\tau=(\emptyset,\{f\},\emptyset,\{(f,1)\}).$ Entonces el universo
de $\mathbf{T}^{\tau}$ es

$\{x_{1},f(x_{1}),f(f(x_{1})),...\}\cup$

$\{x_{2},f(x_{2}),f(f(x_{2})),...\}\cup$

$\{x_{3},f(x_{3}),f(f(x_{3})),...\}\cup$

$\;\;\;\;\;\;\;\;\;\;\vdots$

\noindent La funcion que interpreta a $f$ en $\mathbf{T}^{\tau}$ es la que a
cada elemento del conjunto anterior le asigna el primer elemento que esta a su
derecha. Notese entonces que $\mathbf{T}^{\tau}$ resulta isomorfa al algebra
$\mathbf{A}$ definida por%
\begin{align*}
A  & =\mathbf{N}\times\mathbf{N}\\
f^{\mathbf{A}}((n,m))  & =(n,m+1)
\end{align*}

\end{example}

\bigskip

\begin{lemma}
\label{v(t,t1,t2,...)}Dados $t_{1},...,t_{n}$,$\;t=_{d}t(x_{1},...,x_{n})\in
T^{\tau}$, se tiene que $t^{\mathbf{T}^{\tau}}[t_{1},...,t_{n}]=t(t_{1}%
,...,t_{n})$\textbf{.}
\end{lemma}

\begin{proof}
Para cada $k\geq0$, sea

\begin{enumerate}
\item[-] Teo$_{k}$: Dados $t_{1},...,t_{n}\in T^{\tau}$ y $t=_{d}%
t(x_{1},...,x_{n})\in T_{k}^{\tau}$, se tiene que $t^{\mathbf{T}^{\tau}}%
[t_{1},...,t_{n}]=t(t_{1},...,t_{n})$\textbf{.}
\end{enumerate}

Veamos que es cierto Teo$_{0}$. Hay dos casos

\noindent Caso $t=_{d}t(x_{1},...,x_{n})=c\in\mathcal{C}$.

Entonces tenemos%
\[%
\begin{array}
[c]{cll}%
t^{\mathbf{T}^{\tau}}[t_{1},...,t_{n}] & = & c^{\mathbf{T}^{\tau}}\\
& = & c\\
& = & t(t_{1},...,t_{n})
\end{array}
\]


Caso $t=_{d}t(x_{1},...,x_{n})=x_{i}$, para algun $i$.

Entonces tenemos%
\[%
\begin{array}
[c]{cll}%
t^{\mathbf{T}^{\tau}}[t_{1},...,t_{n}] & = & t_{i}\\
& = & t(t_{1},...,t_{n})
\end{array}
\]
Veamos que Teo$_{k}$ implica Teo$_{k+1}$. Supongamos que vale Teo$_{k}$. Sean
$t_{1},...,t_{n}\in T^{\tau}$ y $t=_{d}t(x_{1},...,x_{n})\in T_{k+1}^{\tau
}-T_{k}^{\tau}$. Hay $f\in\mathcal{F}_{m}$, con $m\geq1$, y terminos
$s_{1},...,s_{m}\in T_{k}^{\tau}$ tales que $t=f(s_{1},...,s_{m})$. Notese que
$s_{i}=_{d}s_{i}(x_{1},...,x_{n})$, $i=1,...,m$. Tenemos entonces que%
\[%
\begin{array}
[c]{lll}%
t^{\mathbf{T}^{\tau}}[t_{1},...,t_{n}] & = & f(s_{1},...,s_{m})^{\mathbf{T}%
^{\tau}}[t_{1},...,t_{n}]\\
& = & f^{\mathbf{T}^{\tau}}(s_{1}^{\mathbf{T}^{\tau}}[t_{1},...,t_{n}%
],...,s_{m}^{\mathbf{T}^{\tau}}[t_{1},...,t_{n}])\\
& = & f^{\mathbf{T}^{\tau}}(s_{1}(t_{1},...,t_{n}),...,s_{m}(t_{1}%
,...,t_{n}))\\
& = & f(s_{1}(t_{1},...,t_{n}),...,s_{m}(t_{1},...,t_{n}))\\
& = & t(t_{1},...,t_{n})
\end{array}
\]
con lo cual vale Teo$_{k+1}$
\end{proof}

\bigskip

El algebra de terminos tiene la siguiente propiedad fundamental:

\begin{lemma}
[Universal Maping Property]Si $\mathbf{A}$ es cualquier $\tau$-algebra y
$F:Var\rightarrow A$, es una funcion cualquiera, entonces $F$ puede ser
extendida a un homomorfismo $\bar{F}:\mathbf{T}^{\tau}\rightarrow\mathbf{A}$.
\end{lemma}

\begin{proof}
Definamos $\bar{F}$ de la siguiente manera:%
\[
\bar{F}(t)=t^{\mathbf{A}}[(F(x_{1}),F(x_{2}),...)]
\]
Es claro que $\bar{F}$ extiende a $F$. Veamos que es un homomorfismo. Dada
$c\in\mathcal{C}$, tenemos que%
\[%
\begin{array}
[c]{lll}%
\bar{F}(c^{\mathbf{T}^{\tau}}) & = & \bar{F}(c)\\
& = & c^{\mathbf{A}}[(F(x_{1}),F(x_{2}),...)]\\
& = & c^{\mathbf{A}}%
\end{array}
\]
Dados $f\in\mathcal{F}_{n}$, $t_{1},...,t_{n}\in T^{\tau}$ tenemos que%
\[%
\begin{array}
[c]{lll}%
\bar{F}(f^{\mathbf{T}^{\tau}}(t_{1},...,t_{n})) & = & \bar{F}(f(t_{1}%
,...,t_{n}))\\
& = & f(t_{1},...,t_{n})^{\mathbf{A}}[(F(x_{1}),F(x_{2}),...)]\\
& = & f^{\mathbf{A}}(t_{1}^{\mathbf{A}}[(F(x_{1}),F(x_{2}),...)],...,t_{n}%
^{\mathbf{A}}[(F(x_{1}),F(x_{2}),...)])\\
& = & f^{\mathbf{A}}(\bar{F}(t_{1}),...,\bar{F}(t_{n}))
\end{array}
\]
con lo cual hemos probado que $\bar{F}$ es un homomorfismo
\end{proof}

\bigskip

\begin{theorem}
[Completitud ecuacional]Sea $(\Sigma,\tau)$ una teoria ecuacional. Si
$(\Sigma,\tau)\models p\approx q$, entonces $(\Sigma,\tau)\vdash_{ec}p\approx
q.$
\end{theorem}

\begin{proof}
Supongamos $(\Sigma,\tau)\models p\approx q.$ Sea $\theta$ la siguiente
relacion binaria sobre $T^{\tau}$:%
\[
\theta=\{(t,s):(\Sigma,\tau)\vdash_{ec}t\approx s\}.
\]
Dejamos al lector probar que $\theta$ es una congruencia de $\mathbf{T}^{\tau
}$. Veamos que

\begin{enumerate}
\item[(*)] $t^{\mathbf{T}^{\tau}/\theta}[t_{1}/\theta,...,t_{n}/\theta
]=t(t_{1},...,t_{n})/\theta$, para todo $t_{1},...,t_{n}$, $t=_{d}%
t(x_{1},...,x_{n})$
\end{enumerate}

\noindent Por Corolario \ref{v(t,a1,...)/tita} tenemos que%
\[
t^{\mathbf{T}^{\tau}/\theta}[t_{1}/\theta,...,t_{n}/\theta]=t^{\mathbf{T}%
^{\tau}}[t_{1},...,t_{n}]/\theta
\]
Pero por Lema \ref{v(t,t1,t2,...)} tenemos que $t^{\mathbf{T}^{\tau}}%
[t_{1},...,t_{n}]=t(t_{1},...,t_{n})$ por lo cual (*) es verdadera.

Veamos que $\mathbf{T}^{\tau}/\theta\models\Sigma.$ Sea $t\approx s$ un
elemento de $\Sigma$, con $t=_{d}t(x_{1},...,x_{n})$ y $s=_{d}s(x_{1}%
,...,x_{n}).$ Veremos que $\mathbf{T}^{\tau}/\theta\models t\approx s$, es
decir veremos que%
\[
t^{\mathbf{T}^{\tau}/\theta}[t_{1}/\theta,...,t_{n}/\theta]=s^{\mathbf{T}%
^{\tau}/\theta}[t_{1}/\theta,...,t_{n}/\theta]
\]
para todo $t_{1}/\theta,...,t_{n}/\theta\in T^{\tau}/\theta$. Notese que%
\[
(\Sigma,\tau)\vdash_{ec}t(t_{1},...,t_{n})\approx s(t_{1},...,t_{n})
\]
por lo cual $t(t_{1},...,t_{n})/\theta=s(t_{1},...,t_{n})/\theta.$ Por (*)
tenemos entonces%
\[
t^{\mathbf{T}^{\tau}/\theta}[t_{1}/\theta,...,t_{n}/\theta]=t(t_{1}%
,...,t_{n})/\theta=s(t_{1},...,t_{n})/\theta=s^{\mathbf{T}^{\tau}/\theta
}[t_{1}/\theta,...,t_{n}/\theta],
\]
lo cual nos dice que $\mathbf{T}^{\tau}/\theta$ satisface la identidad
$t\approx s.$

Ya que $\mathbf{T}^{\tau}/\theta\models\Sigma$, por hipotesis tenemos que
$\mathbf{T}^{\tau}/\theta\models p\approx q.$ Es decir que si $p=_{d}%
p(x_{1},...,x_{n})$ y $q=_{d}q(x_{1},...,x_{n})$ tenemos que $p^{\mathbf{T}%
^{\tau}/\theta}[t_{1}/\theta,...,t_{n}/\theta]=q^{\mathbf{T}^{\tau}/\theta
}[t_{1}/\theta,...,t_{n}/\theta]$, para todo $t_{1},...,t_{n}\in T^{\tau}$. En
particular, tomando $t_{i}=x_{i}$, $i=1,...,n$ tenemos que%
\[
p^{\mathbf{T}^{\tau}/\theta}[x_{1}/\theta,...,x_{n}/\theta]=q^{\mathbf{T}%
^{\tau}/\theta}[x_{1}/\theta,...,x_{n}/\theta]
\]
lo cual por (*) nos dice que $p/\theta=q/\theta$, produciendo $(\Sigma
,\tau)\vdash_{ec}p\approx q$.
\end{proof}

\begin{corollary}
Sea $(\Sigma,\tau)$ una teoria ecuacional. Si $(\Sigma,\tau)\vdash p\approx
q$, entonces $(\Sigma,\tau)\vdash_{ec}p\approx q$.
\end{corollary}

@@finpagina@@

\bigskip

\label{wiki-aritmetica.de.peano}

\subsection{Teorema de incompletitud}

Sea%
\[
Verd_{\mathbf{\omega}}=\{\varphi\in S^{\tau_{A}}:\mathbf{\omega}\models
\varphi\}.
\]
Notese que por el teorema de correccion tenemos que todo teorema de $Arit$
pertenece a $Verd_{\mathbf{\omega}}$. Como puede notarse a medida que uno se
va familiarizando con la teoria $Arit$, todos los resultados clasicos de la
aritmetica los cuales pueden ser enunciados por medio de una sentencia de
$Verd_{\mathbf{\omega}}$ son en realidad teoremas de $Arit$. Sin envargo Godel
probo en su famoso teorema de incompletitud (1931) que hay una sentencia de
$Verd_{\mathbf{\omega}}$ la cual no es un teorema de $Arit$. Por a\~{n}os
nadie fue capaz de dar una sentencia de $Verd_{\mathbf{\omega}} $ la cual
tenga un genuino interes aritmetico y la cual no sea un teorema de $Arit$.
Recien en 1977 Paris y Harrington dieron el primer ejemplo de una tal
sentencia. Una ves sabido que los axiomas de $Arit$ no son suficientemente
poderosos como para probar toda sentencia verdadera en $\mathbf{\omega}$, una
pregunta interesante es

\begin{enumerate}
\item[-] Hay un conjunto "razonable" de axiomas $\Gamma\subseteq
Verd_{\mathbf{\omega}}$ tal que toda sentencia de $Verd_{\mathbf{\omega}}$ es
un teorema de $(\Gamma,\tau_{A})$
\end{enumerate}

Una respuesta negativa a este problema tambien es dada por el teorema de
incompletitud de Godel. En esta seccion daremos una prueba basada en las ideas
de la computabilidad.

\bigskip

\subsubsection{Analisis de recursividad del lenguaje de primer orden}

En esta seccion estudiaremos la recursividad de la sintaxis de $\tau_{A}$. Los
resultados obtenidos valen para un tipo cualquiera y hemos elejido a $\tau
_{A}$ solo para facilitar la exposicion.

Analizaremos la recursividad del concepto de prueba formal en una teoria de la
forma $(\Sigma,\tau_{A})$, donde $\Sigma$ es un conjunto recursivamente
enumerable. Para hacer mas concreto el tratamiento supondremos que los nombres
de constante auxiliares en las pruebas formales estaran siempre en el conjunto%
\[
Aux=\{\triangle\Box\triangle,\triangle\Box\Box\triangle,\triangle\Box\Box
\Box\triangle,...\}
\]
Esto no afectara nuestro analisis ya que es claro que toda prueba formal de
una teoria de la forma $(\Sigma,\tau_{A})$ puede ser reemplazada por una que
sus nombres de constante auxiliares esten en $Aux$. Es decir que las
sentencias involucradas en las pruebas formales que consideraremos seran
sentencias de tipo $\tau_{A}^{e}$ donde%
\[
\tau_{A}^{e}=(\{0,1\}\cup Aux,\{+^{2},.^{2}\},\{\leq^{2}\},a)
\]
Sea $\mathcal{A}$ el alfabeto formado por los siguientes simbolos%
\[
\forall\ \ \exists\ \ \lnot\ \ \vee\ \ \wedge\ \ \rightarrow
\ \ \leftrightarrow\ \ (\ \ )\ \ ,\ \ \equiv\ \ 0\ \ 1\ \ +\ \ .\ \ \leq
\ \ \triangle\ \ \Box\ \ \mathsf{X}\ \ \mathit{0}\ \ \mathit{1}%
\ \ ...\ \ \mathit{9}\ \ \mathbf{0}\ \ \mathbf{1}\ \ ...\ \ \mathbf{9}%
\]
Notese que los simbolos del alfabeto $\mathcal{A}$ son justamente los simbolos
que ocurren en las formulas de tipo $\tau_{A}^{e}$.

\begin{lemma}
Los conjuntos $T^{\tau_{A}^{e}}$, $F^{\tau_{A}^{e}}$, $T^{\tau_{A}}$ y
$F^{\tau_{A}}$ son $\mathcal{A}$-recursivos.
\end{lemma}

\begin{proof}
Notese que los conjuntos $T^{\tau_{A}^{e}}$, $F^{\tau_{A}^{e}}$, $T^{\tau_{A}%
}$ y $F^{\tau_{A}}$ son $\mathcal{A}$-efectivamente computables (justifique).
Entonces la Tesis de Church nos garantiza que dichos conjuntos son
$\mathcal{A}$-recursivos.

A continuacion daremos una prueba de que dichos conjuntos son en realidad
$\mathcal{A}$-primitivos recursivos. Veamos por ejemplo que $T^{\tau_{A}^{e}}
$ es $\mathcal{A}$-primitivo recursivo. Fijemos un orden total $\leq$ sobre
$\mathcal{A}$. Sea $P=\lambda x[\ast^{\leq}(x)\in T^{\tau_{A}^{e}}]$. Notese
que $P(0)=0$ y $P(x+1)=1$ si y solo si se da alguna de las siguientes

\begin{enumerate}
\item[-] $\ast^{\leq}(x+1)\in\{0,1\}\cup Aux$

\item[-] $(\exists u,v\in\omega)\ast^{\leq}(x+1)=+(\ast^{\leq}(u),\ast^{\leq
}(v))\wedge(P^{\downarrow}(x))_{u+1}\wedge(P^{\downarrow}(x))_{v+1}$

\item[-] $(\exists u,v\in\omega)\ast^{\leq}(x+1)=\mathrm{.}(\ast^{\leq
}(u),\ast^{\leq}(v))\wedge(P^{\downarrow}(x))_{u+1}\wedge(P^{\downarrow
}(x))_{v+1}$
\end{enumerate}

Por el Lema \ref{f-flecha} tenemos que $P$ es $\mathcal{A}$-p.r., por lo cual
$\chi_{T^{\tau_{A}^{e}}}^{\mathcal{A}^{\ast}}=P\circ\#^{\leq}$ lo es. Notese
que%
\[
t\in T^{\tau_{A}}\text{ sii }t\in T^{\tau_{A}^{e}}\wedge\triangle\text{ no
ocurre en }t\wedge\Box\text{ no ocurre en }t
\]
por lo cual $T^{\tau_{A}}$ es $\mathcal{A}$-p.r.
\end{proof}

\bigskip

Recordemos que en la Subseccion \ref{VariablesLibres} definimos cuando
$"v\mathit{\ ocurre\ libremente\ en\ }\varphi\mathit{\ a\ partir\ de\ }i"$,
para el caso en que $v\in Var$, $\varphi\in F^{\tau}$ y $i\in
\{1,...,\left\vert \varphi\right\vert \}$. Extendamos esta definicion diciendo
que cuando $v\in Var$, $\varphi\in F^{\tau}$ y $i\in\omega-\{1,...,\left\vert
\varphi\right\vert \}$, se da que $v\mathit{\ no\ ocurre\ libremente\ en\ }%
\varphi\mathit{\ a\ partir\ de\ }i$.

\bigskip

\begin{lemma}
Los siguientes predicados son $\mathcal{A}$-r.

\begin{enumerate}
\item[(1)] $"v$\textit{\ }ocurre libremente en\textit{\ }$\varphi$ a partir de
$i":\omega\times Var\times F^{\tau_{A}^{e}}\rightarrow\omega$

\item[(2)] $"v\in Li(\varphi)":Var\times F^{\tau_{A}^{e}}\rightarrow\omega$

\item[(3)] $"v$ es sustituible por $t$ \textit{en} $\varphi":Var\times
T^{\tau_{A}^{e}}\times F^{\tau_{A}^{e}}\rightarrow\omega$
\end{enumerate}
\end{lemma}

\begin{proof}
Notese que los predicados dados en (1), (2) y (3) son $\mathcal{A}%
$-efectivamente computables (justifique). Entonces la Tesis de Church nos
garantiza que dichos predicados son $\mathcal{A}$-recursivos.

En realidad dichos predicados son $\mathcal{A}$-p.r.. Veamos por ejemplo que
$P:\omega\times Var\times F^{\tau_{A}^{e}}\rightarrow\omega$, dado por%
\[
P(i,v,\varphi)=\left\{
\begin{array}
[c]{ccl}%
1 &  & \text{si }v\mathit{\ }\text{ocurre libremente en}\mathit{\ }%
\varphi\text{ a partir de }i\\
0 &  & \text{caso contrario}%
\end{array}
\right.
\]
es $\mathcal{A}$-p.r.. Sea $R:\mathbf{N}\times Var\rightarrow\omega$ el
predicado dado por $R(x,v)=1$ si y solo si $\ast^{\leq}((x)_{1})\in
F^{\tau_{A}^{e}}$ y $v\mathit{\ }$ocurre libremente en$\mathit{\ }\ast^{\leq
}((x)_{1})$ a partir de $(x)_{2}$. Sea $\bar{R}=R\cup C_{0}^{1,1}%
|_{\{0\}\times Var}$. $\mathrm{Nex}=\{\wedge,\vee,\rightarrow,\leftrightarrow
\}$. Notese que $F_{0}^{\tau_{A}^{e}}$ es $\mathcal{A}$-p.r. ya que%
\[
F_{0}^{\tau_{A}^{e}}=F^{\tau_{A}^{e}}\cap(\mathcal{A}-\{\forall,\exists
,\lnot,\vee,\wedge,\rightarrow,\leftrightarrow\})^{\ast}%
\]
Notese que $\bar{R}(0,v)=0$, para cada $v\in Var$ y que $\bar{R}(x+1,v)=1$ si
y solo si $(x+1)_{2}\geq1$ y se da alguna de las siguientes:

\begin{enumerate}
\item[-] $\ast^{\leq}((x+1)_{1})\in F_{0}^{\tau_{A}^{e}}\wedge v$ ocurre en
$\ast^{\leq}((x+1)_{1})$ a partir de $(x+1)_{2}$

\item[-] $(\exists\varphi_{1},\varphi_{2}\in F^{\tau_{A}^{e}})(\exists\eta
\in\mathrm{Nex})\ast^{\leq}((x+1)_{1})=(\varphi_{1}\eta\varphi_{2})\wedge$

$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \left(  (\bar{R}^{\downarrow}(x,v))_{\left\langle
\#^{\leq}(\varphi_{1}),(x+1)_{2}-1\right\rangle +1}=1\vee(\bar{R}^{\downarrow
}(x,v))_{\left\langle \#^{\leq}(\varphi_{2}),(x+1)_{2}-\left\vert (\varphi
_{1}\eta\right\vert \right\rangle +1}=1\right)  $

\item[-] $(\exists\varphi_{1}\in F^{\tau_{A}^{e}})\ast^{\leq}((x+1)_{1}%
)=\lnot\varphi_{1}\wedge(\bar{R}^{\downarrow}(x,v))_{\left\langle \#^{\leq
}(\varphi_{1}),(x+1)_{2}-1\right\rangle +1}=1 $

\item[-] $(\exists\varphi_{1}\in F^{\tau_{A}^{e}})(\exists w\in Var)(Q\in
\{\forall,\exists\})\;w\neq v\wedge$

$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ast^{\leq}%
((x+1)_{1})=Qw\varphi_{1}\wedge(\bar{R}^{\downarrow}(x,v))_{\left\langle
\#^{\leq}(\varphi_{1}),(x+1)_{2}-\left\vert (Qw\right\vert \right\rangle
+1}=1$
\end{enumerate}

\noindent Es decir que por el Lema \ref{f-flecha} tenemos que $\bar{R}$ es
$\mathcal{A}$-p.r.. Notese que para $(i,v,\varphi)\in\omega\times Var\times
F^{\tau_{A}^{e}}$, tenemos $P(i,v,\varphi)=\bar{R}(\left\langle \#^{\leq
}(\varphi),i\right\rangle ,v)$. Ahora es facil obtener la funcion $P$ haciendo
composiciones adecuadas con $\bar{R}$.
\end{proof}

\bigskip

Dados $v\in Var$ y $t,s\in T^{\tau_{A}^{e}}$, usaremos $\downarrow_{v}^{t}(s)$
para denotar el resultado de reemplazar simultaneamente cada ocurrencia de $v$
en $s$ por $t$. Similarmente, si $\varphi\in F^{\tau_{A}^{e}}$, usaremos
$\downarrow_{v}^{t}(\varphi)$ para denotar el resultado de reemplazar
simultaneamente cada ocurrencia libre de $v$ en $\varphi$ por $t$.

\begin{lemma}
Las funciones $\lambda svt[\downarrow_{v}^{t}(s)]$ y $\lambda\varphi
vt[\downarrow_{v}^{t}(\varphi)]$ son $\mathcal{A}$-r.
\end{lemma}

\begin{proof}
Notese que las funciones $\lambda svt[\downarrow_{v}^{t}(s)]$ y $\lambda
\varphi vt[\downarrow_{v}^{t}(\varphi)]$ son $\mathcal{A}$-efectivamente
computables (justifique). Entonces la Tesis de Church nos garantiza que dichas
funciones son $\mathcal{A}$-recursivas.

En realidad son $\mathcal{A}$-p.r.. Veamos por ejemplo que $\lambda
svt[\downarrow_{v}^{t}(s)]$ es $\mathcal{A}$-p.r.. Sea $\leq$ un orden total
sobre $\mathcal{A}$. Sea $h:\omega\times Var\times T^{\tau_{A}^{e}}%
\rightarrow\omega$ dada por%
\[
h(x,v,t)=\left\{
\begin{array}
[c]{ccc}%
\#^{\leq}(\downarrow_{v}^{t}(\ast^{\leq}(x))) &  & \text{si }\ast^{\leq}(x)\in
T^{\tau_{A}^{e}}\\
0 &  & \text{caso contrario}%
\end{array}
\right.
\]
Sea $P:\mathbf{N}\times\omega\times Var\times T^{\tau_{A}^{e}}\times
\mathcal{A}^{\ast}\rightarrow\omega$ tal que $P(A,x,v,t,\alpha)=1$ si y solo
si se da alguna de las siguientes

\begin{enumerate}
\item[-] $\ast^{\leq}(x+1)\notin T^{\tau_{A}^{e}}\wedge\alpha=\varepsilon$

\item[-] $\ast^{\leq}(x+1)=v\wedge\alpha=t$

\item[-] $\ast^{\leq}(x+1)\in(\{0,1\}\cup Aux)-\{v\}\wedge\alpha=\ast^{\leq
}(x+1)$

\item[-] $(\exists r,s\in T^{\tau_{A}^{e}})\ast^{\leq}(x+1)=+(r,s)\wedge
\alpha=+(\ast^{\leq}((A)_{\#^{\leq}(r)+1}),\ast^{\leq}((A)_{\#^{\leq}(s)+1}))$

\item[-] $(\exists r,s\in T^{\tau_{A}^{e}})\ast^{\leq}(x+1)=\mathrm{.}%
(r,s)\wedge\alpha=\mathrm{.}(\ast^{\leq}((A)_{\#^{\leq}(r)+1}),\ast^{\leq
}((A)_{\#^{\leq}(s)+1}))$
\end{enumerate}

\noindent Sea $\bar{P}=P\cup C_{0}^{2,2}|_{\{0\}\times\omega\times Var\times
T^{\tau_{A}^{e}}\times\mathcal{A}^{\ast}}$. Notese que $\bar{P}(h^{\downarrow
}(x,v,t),x,v,t,\alpha)=1$ si y solo si ya sea $\ast^{\leq}(x+1)\notin
T^{\tau_{A}^{e}}$ y $\alpha=\varepsilon$ o $\ast^{\leq}(x+1)\in T^{\tau
_{A}^{e}}$ y $\alpha=\mathrm{\downarrow}_{v}^{t}(\ast^{\leq}(x+1))$. Tenemos
entonces%
\begin{align*}
h(0,v,t)  & =0\\
h(x+1,v,t)  & =\#^{\leq}(\min_{\alpha}^{\leq}\bar{P}(h^{\downarrow
}(x,v,t),x,v,t,\alpha)),
\end{align*}
por lo cual el Lema \ref{f-flecha} nos dice que $h$ es $\mathcal{A}$-p.r.
Ahora es facil obtener la funcion $\lambda svt[\downarrow_{v}^{t}%
(s)]:T^{\tau_{A}^{e}}\times Var\times T^{\tau_{A}^{e}}\rightarrow T^{\tau
_{A}^{e}}$ haciendo composiciones adecuadas con $h$.
\end{proof}

\bigskip

\begin{lemma}
\label{reemp-formulas}El predicado $R:\mathcal{A}^{4}\rightarrow\omega$, dado
por%
\[
R(\alpha,\beta,\gamma,\zeta)=\left\{
\begin{array}
[c]{cccl}%
\begin{array}
[c]{c}%
1\\
\;\
\end{array}
&  &  &
\begin{array}
[c]{cl}%
\text{si }\beta= & \text{resultado de reemplazar una}\\
& \text{ocurrencia de }\gamma\text{ en }\alpha\text{ por }\zeta
\end{array}
\\
0 &  &  & \text{ caso contrario}%
\end{array}
\right.
\]
es $\mathcal{A}$-r..
\end{lemma}

\begin{proof}
Notese que el predicado $R$ es $\mathcal{A}$-efectivamente computable.
Entonces la Tesis de Church nos garantiza que $R$ es $\mathcal{A}$-recursivo.

En realidad $R$ es $\mathcal{A}$-p.r. y esto puede verse facilmente ya que
$R(\alpha,\beta,\gamma,\zeta)=1$ sii existen $\alpha_{1},\alpha_{2}$ tales que
$\alpha=\alpha_{1}\gamma\alpha_{2}$ y $\beta=\alpha_{1}\zeta\alpha_{2}$.
\end{proof}

\bigskip

\begin{lemma}
Los conjuntos $ModPon^{\tau_{A}^{e}}$, $Elec^{\tau_{A}^{e}}$, $Reem^{\tau
_{A}^{e}}$, $ConjInt^{\tau_{A}^{e}}$, $ConjElim^{\tau_{A}^{e}}$,
$EquivInt^{\tau_{A}^{e}}$, $DisjElim^{\tau_{A}^{e}}$, $DisjInt^{\tau_{A}^{e}}%
$, $EquivElim^{\tau_{A}^{e}}$, $Generaliz^{\tau_{A}^{e}}$, $Commut^{\tau
_{A}^{e}}$, $Trans^{\tau_{A}^{e}}$, $Exist^{\tau_{A}^{e}}$, $Evoc^{\tau
_{A}^{e}}$, $Absur^{\tau_{A}^{e}}$, $DivPorCas^{\tau_{A}^{e}}$, $Partic^{\tau
_{A}^{e}}$ son $\mathcal{A}$-r..
\end{lemma}

\begin{proof}
Dejamos al lector una prueba via la Tesis de Church. En realidad dichos
conjuntos son $\mathcal{A}$-p.r.. Veremos, por ejemplo que $Reem^{\tau_{A}%
^{e}}$ es $\mathcal{A}$-p.r.. Basta con ver que $Reem1^{\tau_{A}^{e}}$ y
$Reem2^{\tau_{A}^{e}}$ lo son. Veremos que $Reem2^{\tau_{A}^{e}}$ es
$\mathcal{A}$-p.r.. Sea $Q:F^{\tau_{A}^{e}}\times F^{\tau_{A}^{e}}\times
F^{\tau_{A}^{e}}\rightarrow\omega$ el predicado tal que $Q(\psi,\varphi
,\sigma)=1$ si y solo si

\begin{enumerate}
\item[ ] $(\exists\alpha\in(\forall Var)^{\ast})(\exists\psi_{1},\psi_{2}\in
F^{\tau_{A}^{e}})\ \psi=\alpha(\psi_{1}\leftrightarrow\psi_{2})\wedge$

\item[ ] $\ \ \ \ \ Li(\psi_{1})=Li(\psi_{2})\wedge\left(  (\forall v\in
Var)\ v\notin Li(\psi_{1})\vee v\text{ ocurre en }\alpha\right)  $

\item[ ] \ \ \ \ \ \ \ \ \ \ $\left(  (\forall v\in Var)\ v\text{ no ocurre en
}\alpha\vee v\in Li(\psi_{1})\right)  \wedge R(\varphi,\sigma,\psi_{1}%
,\psi_{2})$
\end{enumerate}

\noindent($R$ es el predicado dado por el Lema \ref{reemp-formulas}). Es facil
ver que $Q$ es $\mathcal{A}$-p.r. y que $\chi_{Reem2^{\tau_{A}^{e}}%
}^{\mathcal{A}^{4}}=Q|_{S^{\tau_{A}^{e}}\times S^{\tau_{A}^{e}}\times
S^{\tau_{A}^{e}}}$.
\end{proof}

\bigskip

\begin{lemma}
El predicado $"\psi$ se deduce de $\varphi$ por generalizacion con constante
$c$, con respecto a $\tau_{A}^{e}":S^{\tau_{A}^{e}}\times S^{\tau_{A}^{e}%
}\times Aux\rightarrow\omega$ es $\mathcal{A}$-r..
\end{lemma}

\begin{proof}
Es claro que el predicado en cuestion es $\mathcal{A}$-efectivamente
computable (justifique). Por la Tesis de Church tenemos entonces que dicho
predicado es $\mathcal{A}$-r.

Para probar que en realidad dicho predicado es $\mathcal{A}$-p.r., notese que:
$\psi$ se deduce de $\varphi$ por generalizacion con constante $c$ si y solo
si hay una formula $\gamma$ y una variable $v$ tales que

\begin{enumerate}
\item[-] $Li(\gamma)=\{v\}$

\item[-] $c$ no ocurre en $\gamma$

\item[-] $\varphi=\mathrm{\downarrow}_{v}^{c}(\gamma)\wedge\psi=\forall
v\gamma$
\end{enumerate}
\end{proof}

\bigskip

\begin{lemma}
El predicado $"\psi$ se deduce de $\varphi$ por eleccion con constante $e$,
con respecto a $\tau_{A}^{e}":S^{\tau_{A}^{e}}\times S^{\tau_{A}^{e}}\times
Aux\rightarrow\omega$ es $\mathcal{A}$-r..
\end{lemma}

\begin{proof}
Es claro que el predicado en cuestion es $\mathcal{A}$-efectivamente
computable (justifique). Por la Tesis de Church tenemos entonces que dicho
predicado es $\mathcal{A}$-r.

Dejamos al lector probar que en realidad dicho predicado es $\mathcal{A}$-p.r.
\end{proof}

\bigskip

Recordemos que%
\[
AxLog^{\tau_{A}^{e}}=\{\varphi\in S^{\tau_{A}^{e}}:\varphi\text{ es un axioma
logico de tipo }\tau_{A}^{e}\}
\]


\begin{lemma}
$AxLog^{\tau_{A}^{e}}$ es $\mathcal{A}$-r..
\end{lemma}

\begin{proof}
Es claro que el conjunto en cuestion es $\mathcal{A}$-efectivamente computable
(justifique). Por la Tesis de Church tenemos entonces que dicho conjunto es
$\mathcal{A}$-r.

Dejamos al lector probar que en realidad dicho conjunto es $\mathcal{A}$-p.r.
La prueba es completamente analoga a la prueba de que $\mathrm{Ins}^{\Sigma}$
es un conjunto $(\Sigma\cup\Sigma_{p})$-p.r. (Lema \ref{Ins-es-pr})
\end{proof}

\bigskip

\begin{lemma}
Sea $\Sigma$ un alfabeto finito. Sea $S\subseteq\Sigma^{\ast}$ un conjunto
$\Sigma$-r.. El conjunto $S^{+}$ es $\Sigma$-r.
\end{lemma}

\begin{proof}
Ya que $S$ es $\Sigma$-r., tenemos que $S$ es $\Sigma$-efectivamente
computable. Es facil ver que entonces $S^{+}$ es $\Sigma$-efectivamente
computable. Por la Tesis de Church tenemos entonces que $S$ es $\Sigma$-recursivo.

Ya que $\alpha\in S^{+}$ si y solo si%
\[
(\exists z\in\mathbf{N})(\forall i\in\mathbf{N})_{i\leq Lt(z)}\ast^{\leq
}((z)_{i})\in S\wedge\alpha=\mathrm{\subset}_{i=1}^{Lt(z)}\ast^{\leq}((z)_{i})
\]
se puede probar tambien este lema sin usar la Tesis de Church. Dejamos al
lector los detalles.
\end{proof}

\bigskip

Recordemos que dada $\mathbf{\varphi}\in S^{\tau_{A}^{e}+}$, usamos
$n(\mathbf{\varphi})$ y $\mathbf{\varphi}_{1},...,\mathbf{\varphi
}_{n(\mathbf{\varphi})}$ para denotar los unicos $n$ y $\varphi_{1}%
,...,\varphi_{n}$ tales que $\mathbf{\varphi}=\varphi_{1}...\varphi_{n}$ (la
unicidad es garantizada en Lema \ref{secuencia de sentencias}). Extendamos
esta notacion definiendo $\mathbf{\varphi}_{i}=\varepsilon$ para $i=0$ o
$i>n(\mathbf{\varphi})$.

\begin{lemma}
Las funciones%
\[%
\begin{array}
[c]{ccc}%
S^{\tau_{A}^{e}+} & \rightarrow & \omega\\
\mathbf{\varphi} & \rightarrow & n(\mathbf{\varphi})
\end{array}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\begin{array}
[c]{ccc}%
\omega\times S^{\tau_{A}^{e}+} & \rightarrow & S^{\tau_{A}^{e}}\cup
\{\varepsilon\}\\
(i,\mathbf{\varphi}) & \rightarrow & \mathbf{\varphi}_{i}%
\end{array}
\]
son $\mathcal{A}$-r.
\end{lemma}

\begin{proof}
Es claro que la funciones en cuestion son $\mathcal{A}$-efectivamente
computables (justifique). Por la Tesis de Church tenemos entonces que son
$\mathcal{A}$-r.

Dejamos al lector probar que en realidad dicho conjunto es $\mathcal{A}$-p.r.
La prueba es completamente analoga a la prueba de que $\lambda\mathcal{P}%
\left[  n(\mathcal{P})\right]  $ y $\lambda i\mathcal{P}\left[  I_{i}%
^{\mathcal{P}}\right]  $ son funciones $(\Sigma\cup\Sigma_{p})$-p.r. (Lema
\ref{Pro-es-pr})
\end{proof}

\bigskip

Recordemos que dada $\mathbf{J}\in Just^{+}$, usamos $n(\mathbf{J})$ y
$\mathbf{J}_{1},...,\mathbf{J}_{n(\mathbf{J})}$ para denotar los unicos $n$ y
$J_{1},...,J_{n}$ tales que $\mathbf{J}=J_{1}...J_{n}$ (la unicidad es
garantizada en Lema \ref{secuencia de justificaciones}). Extendamos esta
notacion definiendo $\mathbf{J}_{i}=\varepsilon$ para $i=0$ o $i>n(\mathbf{J}%
)$.

Sea $\mathcal{B}$ el alfabeto que consiste en todos los simbolos que ocurren
en alguna palabra de $Just$. Es decir $\mathcal{B}$ consiste de los simbolos%
\[
(\ )\ ,\ 0\ 1\ 2\ 3\ 4\ 5\ 6\ 7\ 8\ 9\ \text{A\ B\ C\ D\ E\ G\ H\ I\ J\ L\ M\ N\ O\ P\ Q\ R\ S\ T\ U\ V\ X
Z}%
\]


\begin{lemma}
$Just$ es $\mathcal{B}$-r. Las funciones%
\[%
\begin{array}
[c]{ccc}%
Just^{+} & \rightarrow & \omega\\
\mathbf{J} & \rightarrow & n(\mathbf{J})
\end{array}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\begin{array}
[c]{ccc}%
\omega\times Just^{+} & \rightarrow & Just\cup\{\varepsilon\}\\
(i,\mathbf{J}) & \rightarrow & \mathbf{J}_{i}%
\end{array}
\]
son $\mathcal{B}$-r.
\end{lemma}

\begin{proof}
Es claro que la funciones en cuestion son $\mathcal{B}$-efectivamente
computables (justifique). Por la Tesis de Church tenemos entonces que son
$\mathcal{B}$-r.
\end{proof}

\bigskip

\begin{lemma}
El predicado $"\left\langle i,j\right\rangle \in\mathcal{B}^{\mathbf{J}%
}":\omega\times\omega\times Just^{+}\rightarrow\omega$ es $\mathcal{B}$-r
\end{lemma}

\begin{proof}
Es claro que dicho predicado es $\mathcal{B}$-efectivamente computable
(justifique). Por la Tesis de Church tenemos entonces que es $\mathcal{B}$-r.
\end{proof}

\bigskip

\begin{lemma}
El conjunto $\{\mathbf{J}\in Just^{+}:\mathbf{J}$ es balanceada$\}$ es
$\mathcal{B}$-r.
\end{lemma}

\begin{proof}
Es claro que dicho conjunto es $\mathcal{B}$-efectivamente computable
(justifique). Por la Tesis de Church tenemos entonces que es $\mathcal{B}$-r.
\end{proof}

\bigskip

\begin{lemma}
Los predicados%
\[%
\begin{array}
[c]{rcl}%
\omega\times S^{\tau_{A}^{e}}\times S^{\tau_{A}^{e}+}\times Just^{+} &
\rightarrow & \omega\\
(i,\varphi,\mathbf{\varphi},\mathbf{J}) & \rightarrow & \left\{
\begin{array}
[c]{ccl}%
1 &  & \text{si }(\mathbf{\varphi},\mathbf{J})\text{ es adecuado y }%
\varphi\text{ es hipotesis de }\mathbf{\varphi}_{i}\text{ en }(\mathbf{\varphi
},\mathbf{J})\\
0 &  & \text{caso contrario}%
\end{array}
\right.
\end{array}
\]%
\[%
\begin{array}
[c]{rcl}%
\omega\times\omega\times S^{\tau_{A}^{e}+}\times Just^{+} & \rightarrow &
\omega\\
(i,\varphi,\mathbf{\varphi},\mathbf{J}) & \rightarrow & \left\{
\begin{array}
[c]{ccl}%
1 &  & \text{si }(\mathbf{\varphi},\mathbf{J})\text{ es adecuado y }i\text{ es
anterior a }j\text{ en }(\mathbf{\varphi},\mathbf{J})\\
0 &  & \text{caso contrario}%
\end{array}
\right.
\end{array}
\]
son $(\mathcal{A}\cup\mathcal{B})$-r..
\end{lemma}

\begin{proof}
Es claro que los predicados en cuestion son $(\mathcal{A}\cup\mathcal{B}%
)$-efectivamente computables (justifique). Por la Tesis de Church tenemos
entonces que dichos predicados son $(\mathcal{A}\cup\mathcal{B})$-r.
\end{proof}

\bigskip

\begin{lemma}
El predicado%
\[%
\begin{array}
[c]{rcl}%
Aux\times Aux\times S^{\tau_{A}^{e}+}\times Just^{+} & \rightarrow & \omega\\
(e,d,\mathbf{\varphi},\mathbf{J}) & \rightarrow & \left\{
\begin{array}
[c]{ccl}%
1 &  & \text{si }(\mathbf{\varphi},\mathbf{J})\text{ es adecuado y }e\text{
depende de }d\text{ en }(\mathbf{\varphi},\mathbf{J})\\
0 &  & \text{caso contrario}%
\end{array}
\right.
\end{array}
\]
es $(\mathcal{A}\cup\mathcal{B})$-r..
\end{lemma}

\begin{proof}
Es claro que el predicado en cuestion es $(\mathcal{A}\cup\mathcal{B}%
)$-efectivamente computable (justifique). Por la Tesis de Church tenemos
entonces que dicho predicado es $(\mathcal{A}\cup\mathcal{B})$-r.
\end{proof}

\bigskip

Dada una teoria de la forma $(\Sigma,\tau_{A})$, diremos que una prueba formal
$(\mathbf{\varphi},\mathbf{J})$ de $\varphi$ en $(\Sigma,\tau_{A}) $ es
\textit{normal} si solo usa nombres de ctes auxiliares de $Aux$, es decir si
$\mathbf{\varphi}\in S^{\tau_{A}^{e}+}$. Definamos%
\[
Pruebas_{(\Sigma,\tau_{A})}=\{(\mathbf{\varphi},\mathbf{J}):\exists
\varphi\ (\mathbf{\varphi},\mathbf{J})\text{ es una prueba normal de }%
\varphi\text{ en }(\Sigma,\tau_{A})\}
\]


\begin{lemma}
Sea $(\Sigma,\tau_{A})$ una teoria tal que $\Sigma$ es $\mathcal{A}$-r.e.
(resp. $\mathcal{A}$-recursivo). Entonces $Pruebas_{(\Sigma,\tau_{A})}$ es
$(\mathcal{A}\cup\mathcal{B})$-r.e. (resp. $(\mathcal{A}\cup\mathcal{B})$-recursivo).
\end{lemma}

\begin{proof}
Supongamos que $\Sigma$ es $\mathcal{A}$-r.e.. Claramente entonces $\Sigma$ es
$\mathcal{A}$-efectivamente computable por lo cual hay una funcion
$g:\omega\rightarrow\Sigma$ la cual es $\mathcal{A}$-efectivamente computable
y suryectiva. Sea $\leq$ un orden total sobre $\mathcal{A}\cup\mathcal{B}$. A
continuacion describimos como hacer un procedimiento efectivo enumere a
$Pruebas_{(\Sigma,\tau_{A})}$. Dejamos al lector completar los detalles. Dada
una prueba formal $(\mathbf{\varphi},\mathbf{J})$ cualquiera, definamos%
\[
Ax((\mathbf{\varphi},\mathbf{J}))=\{\mathbf{\varphi}_{i}:\text{existe }%
\alpha\text{ tal que }\mathbf{J}_{i}=\alpha\mathrm{AXIOMAPROPIO}\}
\]


Etapa 1

\noindent Si $x=0$, detenerse y dar como salida $((0\equiv
0),\mathrm{AXIOMALOGICO})$. En caso contrario ir a Etapa 2

Etapa 2.

\noindent Si $(\ast^{\leq}((x)_{1}),\ast^{\leq}((x)_{2}))$ es una prueba
formal y $Ax((\ast^{\leq}((x)_{1}),\ast^{\leq}((x)_{2})))\subseteq
\{g(0),...,g((x)_{3})\}$, entonces detenerse y dar como salida $(\ast^{\leq
}((x)_{1}),\ast^{\leq}((x)_{2}))$. Caso contrario detenerse y dar como salida
$((0\equiv0),\mathrm{AXIOMALOGICO})$

Por la Tesis de Church tenemos entonces que $Pruebas_{(\Sigma,\tau_{A})}$ es
$(\mathcal{A}\cup\mathcal{B})$-r.e.
\end{proof}

\bigskip

Dada una teoria $(\Sigma,\tau_{A})$, definamos%
\[
Teo_{(\Sigma,\tau_{A})}=\{\varphi\in S^{\tau_{A}}:(\Sigma,\tau_{A}%
)\vdash\varphi\}
\]


\begin{proposition}
\label{teoremas son r.e.}Si $(\Sigma,\tau_{A})$ es una teoria tal que $\Sigma$
es $\mathcal{A}$-r.e., entonces $Teo_{(\Sigma,\tau_{A})}$ es $\mathcal{A}$-r.e.
\end{proposition}

\begin{proof}
Como se vio en el lema anterior, tenemos que $Pruebas_{(\Sigma,\tau_{A})}$ es
$(\mathcal{A}\cup\mathcal{B})$-efectivamente enumerable. Es facil ahora,
usando un procedimiento efectivo que enumere a $Pruebas_{(\Sigma,\tau_{A})} $,
dise\~{n}ar un procedimiento efectivo que enumere a $Teo_{(\Sigma,\tau_{A})}$.
Es decir que $Teo_{(\Sigma,\tau_{A})}$ es $\mathcal{A}$-efectivamente
enumerable, lo cual por la Tesis de Church nos dice que es $\mathcal{A}$-r.e.

A continuacion daremos una prueba que no usa la Tesis de Church. Ya que
$Pruebas_{(\Sigma,\tau_{A})}$ es $(\mathcal{A}\cup\mathcal{B})$-r.e. (lema
anterior) tenemos que hay una funcion $F:\omega\rightarrow S^{\tau_{A}^{e}%
+}\times Just^{+}$ la cual cumple que $p_{1}^{0,2}\circ F$ y $p_{2}^{0,2}\circ
F$ son $(\mathcal{A}\cup\mathcal{B})$-r. y ademas $I_{F}=Pruebas_{(\Sigma
,\tau_{A})}$. Sea%
\[%
\begin{array}
[c]{rcl}%
g:S^{\tau_{A}^{e}+} & \rightarrow & S^{\tau_{A}^{e}}\\
\mathbf{\varphi} & \rightarrow & \mathbf{\varphi}_{n(\mathbf{\varphi})}%
\end{array}
\]
Por lemas anteriores $g$ es $\mathcal{A}$-r.. Notese que $I_{(g\circ
p_{1}^{0,2}\circ F)}=Teo_{(\Sigma,\tau_{A})}$, lo cual dice que $Teo_{(\Sigma
,\tau_{A})}$ es $(\mathcal{A}\cup\mathcal{B})$-r.e. (Teorema
\ref{equivalencias-r.e.}). Por el teorema de independencia del alfabeto
tenemos que $Teo_{(\Sigma,\tau_{A})}$ es $\mathcal{A}$-r.e..
\end{proof}

\bigskip

\subsubsection{Funciones representables}

Sea $n\geq1$. Una funcion $f:D_{f}\subseteq\omega^{n}\rightarrow\omega$ sera
llamada \textit{representable} si hay una formula $\varphi=_{d}\varphi
(v_{1},...,v_{n},v)\in F^{\tau_{A}}$, la cual cumpla%
\[
\mathbf{\omega}\models\varphi\left[  k_{1},...,k_{n},k\right]
\mathrm{\ si\ y\ solo\ si\ }f(k_{1},...,k_{n})=k
\]
cualesquiera sean $k_{1},...,k_{n},k\in\omega$. En tal caso diremos que la
formula $\varphi$ \textit{representa} a la funcion $f$, con respecto a la
declaracion $\varphi=_{d}\varphi(v_{1},...,v_{n},v)$. Notese que cuando
$(k_{1},...,k_{n})\notin D_{f}$ entonces debera suceder que $\mathbf{\omega
}\nvDash\varphi\left[  k_{1},...,k_{n},k\right]  $, cualquiera sea $k\in
\omega$. Cabe destacar que una formula $\varphi$ puede representar a $f$, con
respecto a una declaracion y con respecto a otra declaracion puede no
representarla. Por ejemplo la formula $(x_{3}\equiv x_{1}+x_{2})$ representa a
la operacion suma con respecto a las declaraciones $\varphi=_{d}\varphi
(x_{1},x_{2},x_{3})$ y $\varphi=_{d}\varphi(x_{2},x_{1},x_{3})$ pero con
respecto a la declaracion $\varphi=_{d}\varphi(x_{3},x_{2},x_{1})$ no
representa a dicha operacion. Para dar otro ejemplo, tomemos $\varphi
=(x_{5}\equiv1)$. Entonces

\begin{enumerate}
\item[-] Con respecto a la declaracion $\varphi=_{d}\varphi(x_{2},x_{5})$ la
formula $\varphi$ representa a la funcion con dominio $\omega$ y valor
constantemente 1

\item[-] Con respecto a la declaracion $\varphi=_{d}\varphi(x_{10},x_{5})$ la
formula $\varphi$ representa a la funcion con dominio $\omega$ y valor
constantemente 1

\item[-] Con respecto a la declaracion $\varphi=_{d}\varphi(x_{2},x_{6}%
,x_{5})$ la formula $\varphi$ representa a la funcion con dominio $\omega^{2}$
y valor constantemente 1
\end{enumerate}

El concepto de funcion representable sera clave en nuestra prueba del teorema
de incompletitud. El resultado clave desde el cual sale facilmente el teorema
de incompletitud es la Proposicion \ref{verd no es r.e.} en la que se prueba
que el conjunto $Verd_{\mathbf{\omega}}$ no es $\mathcal{A}$-r.e..Para probar
dicha proposicion primero probaremos que toda funcion $\emptyset$-recursiva es
representable. Aqui es clave una funcion introducida por Godel. Sea%
\[
\beta=\lambda xyi[R(x,y(i+1)+1)]
\]
donde%
\[%
\begin{array}
[t]{rll}%
R:\omega\times\mathbf{N} & \rightarrow & \omega\\
(x,y) & \rightarrow & \text{resto de la division de }x\text{ por }y
\end{array}
\]
Notese que $D_{\beta}=\omega^{3}$. Esta funcion, conocida como la
\textit{funcion }$\beta$\textit{\ de Godel}, es representable ya que por
ejemplo la formula%
\[
\varphi=\exists x_{5}\;(x_{1}\equiv x_{5}.(x_{2}.(x_{3}+1)+1)+x_{4}\wedge
x_{4}<x_{2}.(x_{3}+1)+1)
\]
la representa, con respecto a la declaracion $\varphi=_{d}\varphi(x_{1}%
,x_{2},x_{3},x_{4})$. Ahora veremos un lema que muestra que la funcion $\beta$
tiene una propiedad sorprendente en el sentido de que cualquier sucesion
finita de elementos de $\omega$ es producida por $\beta$ si fijamos
adecuadamente sus dos primeras entradas. Dados $x,y\in\omega$, diremos que $x$
e $y$ son \textit{coprimos} cuando $1$ sea el unico elemento de $\omega$ que
divide a ambos. Notese que $x$ e $y$ no son son coprimos sii existe un numero
primo $p\in\omega$ que los divide a ambos

\begin{lemma}
Cualesquiera sean $z_{0},...,z_{n}\in\omega$, $n\geq0$, hay $x,y\in\omega$,
tales que $\beta(x,y,i)=z_{i}$, $i=0,...,n$
\end{lemma}

\begin{proof}
Dados $x,y,m\in\omega$ con $m\geq1$, usaremos $x\equiv y(m)$ para expresar que
$x$ es congruente a $y$ modulo $m$, es decir para expresar que $x-y$ es
divisible por $m$. Usaremos en esta prueba el Teorema Chino del Resto:

\begin{enumerate}
\item[-] Dados $m_{0},...,m_{n},z_{0},...,z_{n}\in\omega$ tales que
$m_{0},...,m_{n}$ son coprimos de a pares, hay un $x\in\omega$ tal que
$x\equiv z_{i}(m_{i})$, para $i=0,...,n.$
\end{enumerate}

Sea $y=\max(z_{0},...,z_{n},n)!$. Sean $m_{i}=y(i+1)+1$, $i=0,...,n$. Veamos
que $m_{0},...,m_{n}$ son coprimos de a pares. Supongamos $p$ divide a $m_{i}$
y a $m_{j}$ con $i<j$. Entonces $p$ divide a $m_{j}-m_{i}=y(j-i)$ y ya que $p$
no puede dividir a $y$, tenemos que $p$ divide a $j-i$. Pero ya que $j-i<n$
tenemos que $p<n$ lo cual es absurdo ya que implicaria que $p$ divide $y$.

Por el Teorema Chino del Resto hay un $x$ tal que $x\equiv z_{i}(m_{i})$, para
$i=0,...,n$. Ya que $z_{i}<m_{i}$, tenemos que%
\[
\beta(x,y,i)=R(x,y(i+1)+1)=R(x,m_{i})=z_{i}\text{, }i=0,...,n\text{.}%
\]

\end{proof}

\bigskip

El lema anterior nos permite probar:

\begin{proposition}
Si $h$ es $\emptyset$-recursiva, entonces $h$ es representable
\end{proposition}

\begin{proof}
Supongamos $f:S_{1}\times...\times S_{n}\rightarrow\omega$ y $g:\omega
\times\omega\times S_{1}\times...\times S_{n}\rightarrow\omega$ son
representables, con $S_{1},...,S_{n}\subseteq\omega$ y $n\geq0$. Probaremos
que entonces $R(f,g):\omega\times S_{1}\times...\times S_{n}\rightarrow\omega$
lo es. Para esto primero notese que para $t,x_{1},...,x_{n},z\in\omega$, las
siguientes son equivalentes

\begin{enumerate}
\item[(1)] $R(f,g)(t,\vec{x})=z$

\item[(2)] Hay $z_{0},...,z_{t}\in\omega$ tales que%
\begin{align*}
z_{0}  & =f(\vec{x})\\
z_{i+1}  & =g(z_{i},i,\vec{x})\text{, }i=0,...,t-1\\
z_{t}  & =z
\end{align*}


\item[(3)] Hay $x,y\in\omega$ tales que%
\begin{align*}
\beta(x,y,0)  & =f(\vec{x})\\
\beta(x,y,i+1)  & =g(\beta(x,y,i),i,\vec{x})\text{, }i=0,...,t-1\\
\beta(x,y,t)  & =z
\end{align*}

\end{enumerate}

\noindent Sean $\varphi_{\beta}$, $\varphi_{f}$ y $\varphi_{g}$ formulas que
representen a las funciones $\beta$, $f$ y $g$, con respecto a las
declaraciones%
\begin{align*}
\varphi_{\beta}  & =_{d}\varphi_{\beta}(x_{1},x_{2},x_{3},x_{4})\\
\varphi_{f}  & =_{d}\varphi_{f}(x_{1},...,x_{n},x_{n+1})\\
\varphi_{g}  & =_{d}\varphi_{g}(x_{1},...,x_{n+2},x_{n+3})
\end{align*}
respectivamente. Sean $v_{1},...,v_{n+1},v$, $y_{1},y_{2},y_{3},y_{4}%
,z_{1},z_{2}$ variables todas distintas y tales que cada una de las variebles
libres de $\varphi_{\beta}$, $\varphi_{f}$ y $\varphi_{g}$ es sustituible por
cada una de las variables $v_{1},...,v_{n+1},v$, $y_{1},y_{2},y_{3}%
,y_{4},z_{1},z_{2}$. Sea $\varphi_{R(f,g)}$ la siguiente formula

\begin{enumerate}
\item[ ] $\exists z_{1},z_{2}\;(\exists y_{1}\varphi_{\beta}(z_{1}%
,z_{2},0,y_{1})\wedge\varphi_{f}(v_{2},...,v_{n+1},y_{1}))\wedge$

\item[ ] $\ \ \ \ \ \ \ \ \ \varphi_{\beta}(z_{1},z_{2},v_{1},v)\wedge\forall
y_{2}(y_{2}<v_{1}\rightarrow\exists y_{3},y_{4}\;\varphi_{\beta}(z_{1}%
,z_{2},y_{2}+1,y_{3})\wedge$

\item[ ]
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \varphi
_{\beta}(z_{1},z_{2},y_{2},y_{4})\wedge\varphi_{g}(y_{4},y_{2},v_{2}%
,...,v_{n+1},y_{3}))$
\end{enumerate}

\noindent Es facil usando (3) ver que la formula $\varphi_{R(f,g)}$ representa
a $R(f,g)$, con respecto a la declaracion $\varphi_{R(f,g)}=_{d}%
\varphi_{R(f,g)}(v_{1},...,v_{n+1},v)$.

En forma analoga se puede probar que las reglas de composicion y minimizacion
preservan representabilidad por lo cual ya que los elementos de $\mathrm{R}%
_{0}^{\emptyset}$ son representables, por induccion tenemos que lo es toda
funcion $\emptyset$-r.
\end{proof}

\bigskip

\subsubsection{Prueba del teorema de incompletitud}

Nuestra estrategia sera probar que $Verd_{\mathbf{\omega}}$ no es
$\mathcal{A}$-r.e., para lo cual necesitamos los siguientes dos lemas. El
primero consiste en dar una funcion total numerica la cual codifique al
predicado $"$el programa $\mathcal{P}$ se detiene luego de $t$ pasos,
partiendo del estado $((0,0,...),(\mathcal{P},\varepsilon,...))"$.

\begin{lemma}
\label{halt-codificado}Hay un predicado $P:\omega\times\omega\rightarrow
\omega$ el cual es $\emptyset$-p.r. y tal que el predicado $Q=\lambda x\left[
(\exists t\in\omega)P(t,x)\right]  :\omega\rightarrow\omega$ no es $\emptyset$-r.
\end{lemma}

\begin{proof}
Sea $\Sigma=\Sigma_{p}$. Recordemos que el predicado%
\[
P_{1}=\lambda t\mathcal{P}\left[  i^{0,1}(t,\mathcal{P},\mathcal{P}%
)=n(\mathcal{P})+1\right]
\]
es $\Sigma_{p}$-p.r. ya que la funcion $i^{0,1}$ lo es. Notese que el dominio
de $P_{1}$ es $\omega\times\mathrm{Pro}^{\Sigma_{p}}$. Por Lema \ref{autohalt}
tenemos que%
\[
AutoHalt^{\Sigma_{p}}=\lambda\mathcal{P}\left[  (\exists t\in\omega
)\;P_{1}(t,\mathcal{P})\right]
\]
no es $\Sigma_{p}$-recursivo. Sea $\leq$ un orden total sobre $\Sigma_{p}$.
Definamos $P:\omega\times\omega\rightarrow\omega$ de la siguiente manera%
\[
P(t,x)=\left\{
\begin{array}
[c]{ccc}%
P_{1}(t,\ast^{\leq}(x)) & \text{si} & \ast^{\leq}(x)\in\mathrm{Pro}%
^{\Sigma_{p}}\\
0 & \text{si} & \ast^{\leq}(x)\notin\mathrm{Pro}^{\Sigma_{p}}%
\end{array}
\right.
\]
Claramente $P$ es $\Sigma_{p}$-p.r., por lo cual el teorema de independencia
del alfabeto nos dice que $P$ es $\emptyset$-p.r.. Sea $Q=\lambda x\left[
(\exists t\in\omega)P(t,x)\right]  $. Notese que%
\[
AutoHalt^{\Sigma_{p}}=Q\circ\#^{\leq}\mathrm{|}_{\mathrm{Pro}^{\Sigma_{p}}}%
\]
lo cual dice que $Q$ no es $\Sigma_{p}$-r. ya que de serlo, el predicado
$AutoHalt^{\Sigma_{p}}$ lo seria. Por el teorema de independencia del alfabeto
tenemos entonces que $Q$ no es $\emptyset$-recursivo.
\end{proof}

\begin{corollary}
No toda funcion representable es $\emptyset$-recursiva
\end{corollary}

\begin{proof}
Dejamos como ejercicio para el lector probar que el predicado $Q$ del lema
anterior es representable, lo cual completa la prueba de este corolario ya que
$Q$ no es $\emptyset$-recursivo.
\end{proof}

\bigskip

Recordemos que para $\alpha\in\Sigma^{\ast}$, definimos%
\[
^{\curvearrowright}\alpha=\left\{
\begin{array}
[c]{lll}%
\left[  \alpha\right]  _{2}...\left[  \alpha\right]  _{\left\vert
\alpha\right\vert } & \text{si} & \left\vert \alpha\right\vert \geq2\\
\varepsilon & \text{si} & \left\vert \alpha\right\vert \leq1
\end{array}
\right.
\]


\begin{lemma}
Si $Verd_{\mathbf{\omega}}$ es $\mathcal{A}$-r.e., entonces es $\mathcal{A}$-r.
\end{lemma}

\begin{proof}
Supongamos $Verd_{\mathbf{\omega}}$ es $\mathcal{A}$-r. e. Sea $f:\omega
\rightarrow Verd_{\mathbf{\omega}}$ una funcion sobre y $\mathcal{A}$-r. Sea
$g:S^{\tau_{A}}\rightarrow S^{\tau_{A}}$, dada por%
\[
g(\varphi)=\left\{
\begin{array}
[c]{ccc}%
^{\curvearrowright}\varphi & \;\; & \text{si }\left[  \varphi\right]
_{1}=\lnot\\
\lnot\varphi & \;\; & \text{caso contrario}%
\end{array}
\right.
\]
Notar que $g$ es $\mathcal{A}$-p.r. por lo cual $g\circ f$ es $\mathcal{A}$-r.
Ya que $I_{g\circ f}=S^{\tau_{A}}-Verd_{\mathbf{\omega}}$ (justifique),
tenemos que $S^{\tau_{A}}-Verd_{\mathbf{\omega}}$ es $\mathcal{A}$-r. e., por
lo cual%
\[
\mathcal{A}^{\ast}-Verd_{\mathbf{\omega}}=(\mathcal{A}^{\ast}-S^{\tau_{A}%
})\cup(S^{\tau_{A}}-Verd_{\mathbf{\omega}})
\]
lo es. Es decir que $Verd_{\mathbf{\omega}}$ y su complemento son
$\mathcal{A}$-r.e. por lo cual $Verd_{\mathbf{\omega}}$ es $\mathcal{A}$-r.
\end{proof}

\bigskip

Ahora podemos probar el importante resultado anunciado.

\begin{proposition}
\label{verd no es r.e.}$Verd_{\mathbf{\omega}}$ no es $\mathcal{A}$-r.e.
\end{proposition}

\begin{proof}
Por el Lema \ref{halt-codificado} hay un predicado $\emptyset$-p.r.,
$P:\omega\times\omega\rightarrow\omega$ tal que el predicado $Q=\lambda
x\left[  (\exists t\in\omega)P(t,x)\right]  :\omega\rightarrow\omega$ no es
$\emptyset$-recursivo. Notese que $Q$ tampoco es $\mathcal{A}$-recursivo. Ya
que $P$ es representable, hay una formula $\varphi=_{d}\varphi(v_{1}%
,v_{2},v)\in F^{\tau_{A}}$ la cual cumple%
\[
\mathbf{\omega}\models\varphi\left[  t,x,k\right]  \text{ si y solo si
}P(t,x)=k
\]
cualesquiera sean $t,x,k\in\omega.$ Sea $\psi=\varphi(v_{1},v_{2},1)$.
Declaremos $\psi=_{d}\psi(v_{1},v_{2})$. Tenemos entonces%
\[
\mathbf{\omega}\models\psi\left[  t,x\right]  \text{ si y solo si }P(t,x)=1
\]
cualesquiera sean $t,x\in\omega.$ Sea $\psi_{0}=\exists v_{1}\ \psi
(v_{1},v_{2})$. Declaremos $\psi_{0}=_{d}\psi_{0}(v_{2})$. Tenemos entonces%
\[
\mathbf{\omega}\models\psi_{0}\left[  x\right]  \text{ si y solo si }Q(x)=1
\]
cualesquiera sea $x\in\omega$. Por el lema de reemplazo tenemos que para
$x\in\omega$,%
\[
\mathbf{\omega}\models\psi_{0}\left[  x\right]  \text{ si y solo si
}\mathbf{\omega}\models\psi_{0}(\widehat{x})
\]
(justifique), por lo cual%
\[
\mathbf{\omega}\models\psi_{0}(\widehat{x})\text{ si y solo si }Q(x)=1
\]
cualesquiera sea $x\in\omega$. Ya que $\psi_{0}(\widehat{x})$ es una
sentencia,%
\[
\psi_{0}(\widehat{x})\in Verd_{\mathbf{\omega}}\text{ si y solo si }Q(x)=1
\]
Sea $h:\omega\rightarrow\mathcal{A}^{\ast}$, dada por $h(x)=\psi_{0}%
(\widehat{x})$. Es facil ver que $h$ es $\mathcal{A}$-recursiva. Ya que
$Q=\chi_{Verd_{\mathbf{\omega}}}^{\mathcal{A}^{\ast}}\circ h$ y $Q$ no es
$\mathcal{A}$-recursivo, tenemos que $\chi_{Verd_{\mathbf{\omega}}%
}^{\mathcal{A}^{\ast}}$ no es $\mathcal{A}$-recursiva, es decir que
$Verd_{\mathbf{\omega}}$ es un conjunto no $\mathcal{A}$-recursivo. El lema
anterior nos dice entonces que es $Verd_{\mathbf{\omega}}$ no es $\mathcal{A}$-r.e..
\end{proof}

\bigskip

Ahora si, estamos en condiciones de probar facilmente el famoso resultado de Godel.

\begin{theorem}
[Teorema de incompletitud]Si $\Sigma\subseteq Verd_{\mathbf{\omega}}$ es
$\mathcal{A}$-r.e., entonces $Teo_{(\Sigma,\tau_{A})}\subsetneq
Verd_{\mathbf{\omega}}$
\end{theorem}

\begin{proof}
Ya que $\mathbf{\omega}$\ es un modelo de $(\Sigma,\tau_{A})$, por el Teorema
de Correccion, tenemos que $Teo_{(\Sigma,\tau_{A})}\subseteq
Verd_{\mathbf{\omega}}$. Ya que $Teo_{(\Sigma,\tau_{A})}$ es $\mathcal{A}$-r.e
(Proposicion \ref{teoremas son r.e.}) y $Verd_{\mathbf{\omega}}$ no lo es,
tenemos que $Teo_{(\Sigma,\tau_{A})}\neq Verd_{\mathbf{\omega}}$.
\end{proof}

\begin{corollary}
Existe $\varphi\in S^{\tau_{A}}$ tal que $Arit\nvdash\varphi$ y $Arit\nvdash
\lnot\varphi$.
\end{corollary}

\begin{proof}
Dejamos al lector la prueba de que el conjunto $\Sigma_{A}$ es $\mathcal{A}%
$-r.e.. Una ves probado esto, podemos aplicar el teorema anterior a la teoria
$Arit=(\Sigma_{A},\tau_{A})$, lo cual nos dice que $Teo_{Arit}\subsetneq
Verd_{\mathbf{\omega}}$. Sea $\varphi\in Verd_{\mathbf{\omega}}-Teo_{Arit} $.
O sea que $Arit\nvdash\varphi$ y $\varphi\in Verd_{\mathbf{\omega}}$. Ya que
$\lnot\varphi\notin Verd_{\mathbf{\omega}}$, tenemos que $\lnot\varphi\notin
Teo_{Arit}$, es decir $Arit\nvdash\lnot\varphi$.
\end{proof}


\end{document}